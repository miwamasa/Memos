# ã²ãŸã™ã‚‰LLMé–¢é€£æƒ…å ±ã‚’è¿½ã†ã€
ã“ã‚Œã¯ã€å€‹äººã®twitter bookmarkã‚’æ¯é€±ãŠã•ã‚‰ã„ã—ã¦ã„ã‚‹ã€‚

## 2/19

-  LlamaIndex v0.10
	- https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8
	- https://x.com/llama_index/status/1757121818115322076?s=20
	- our biggest open-source release to date, and a massive step towards production-readiness.
	- Create a core package, split off every integration/template into separate PyPi packages.
	- Refactor LlamaHub to become a central hub of all integrations (WIP)
	- Deprecate ServiceContext: Your dev UX is now way better without this object.
- Chat with RTX from NVIDIA
	- https://x.com/NVIDIAAIDev/status/1757447655674819053?s=20
- Deepreneur-blue-lizard
	- https://huggingface.co/Deepreneur/blue-lizard
	- æ±äº¬å¤§å­¦æ¾å°¾ç ”ç©¶å®¤ç™ºã®AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€æ ªå¼ä¼šç¤¾Deepreneur(ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ¬ãƒŠãƒ¼
	- Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚  
	- 70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨éå¸¸ã«è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšã€JGLUEï¼ˆæ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã‚’ç”¨ã„ãŸè©•ä¾¡ã§ã¯ã€ChatGPT-3.5ã‚’è¶…ãˆã‚‹ã‚¹ã‚³ã‚¢ãŒç®—å‡ºã•ã‚Œã¦ãŠã‚Šã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯æœ€é«˜æ€§èƒ½ã«ãªã‚Šã¾ã™ã€‚
	-  æ ªå¼ä¼šç¤¾Deepreneurã€ChatGPT-3.5ã‚’ä¸Šå›ã‚‹æ—¥æœ¬èªLLMã€Œblue-lizardã€ã‚’é–‹ç™ºã€‚å„ç¤¾ç‹¬è‡ªã®é«˜ç²¾åº¦ã‚ªãƒ³ãƒ—ãƒ¬å‹ã®LLMã®æ§‹ç¯‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹
- 

## 2/13

ä»Šé€±ã¯ã€ã»ã¼äºˆå®šé€šã‚Šï¼ˆï¼‘æ—¥ãŠãã‚Œï¼Ÿï¼‰BardãŒGeminiï¼ˆã‚¸ã‚§ãƒãƒŠã‚¤ã¨èª­ã‚€ï¼‰ã«æ”¹åã•ã‚ŒãŸã€‚ä¸€æ–¹ã€æ–°ãŸã«Gemini Advancedã¨ã„ã†åå‰ã§Gemini UltraãŒæœ‰å„Ÿã§ã‚¹ã‚¿ãƒ¼ãƒˆã€‚ä½•æ°—ãªã„ãƒ•ã‚¡ãƒŸãƒã®å†™çœŸã‹ã‚‰ç”»åƒã‹ã‚‰èªè­˜ã—ãŸæƒ…å ±ç‰‡ã‚’ã¤ãªã’ã¦åº—èˆ—ã‚’ç‰¹å®šã—ãŸã‚Šã¨ã€ã‚³ãƒŠãƒ³å›ãªã¿ã®æ¨ç†ã‚’ã—ã¦ã„ã‚‹ã®ãŒä½•æ°—ã«ã™ã”ã„ã€‚OSSã®LLMã§ã¯ã€ã‚¢ãƒªãƒãƒã®Qwen1.5ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã®ãŒæœ€å¤§ã®è©±é¡Œã€75B-chatã®ãƒ‡ãƒ¢ãªã©ã§ã‚‚GPT-4ã«è¿«ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã¨è©•åˆ¤ã€Huggingfaceã®ãƒ‡ãƒ¢è©¦ã™ã¨ãŸã—ã‹ã«ãƒ¬ã¹ãƒã‹ã‚‚ã€‚åŸºæœ¬æ€§èƒ½ãŒé«˜ã„ã®ã‹ã€0.5Bã‚’Transfomer.jsã§ä½¿ã£ãŸä¾‹ã§ã‚‚ãã‚Œãªã‚Šã®æ€§èƒ½ãŒã§ã‚‹ã¨ã„ã†è©±ã€‚æ—©é€Ÿã€é‡å­åŒ–ã¨ã‹ã€Ollamaã®å¯¾å¿œãŒç™ºè¡¨ã•ã‚ŒãŸã‚Šã•ã‚Œã¦ã‚‹ã€‚ãŸã¶ã‚“ã€æ—¥æœ¬èªLLMã‚‚rinnaå½“ãŸã‚Šã‹ã‚‰Qwen-1.5ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªLLMã®ç™ºè¡¨ãŒç¶šãã¨æ€ã†ãã€‚Style-Bert-VITS2ã€ãªã‚“ã¦è‡ªç„¶ãªæ—¥æœ¬èªã‚’è©±ã™ã‚“ã ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè©±ã—ã£ã·ã‚Šã«ã³ã£ãã‚Šã€ã©ã“ã‹ã®è·æ¥­ãŒä¸¸ã”ã¨ãªããªã‚‹æ€§èƒ½ã ã€‚ Open AIã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã®é–“ã‚’ã¤ãªã„ã§ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é–‹ç™ºã‚’å®£è¨€ã€ã“ã‚Œã£ã¦AppleScriptã¨ã‹PowerShellã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã¿ãŸã„ãªè©±ã ã‹ã‚‰ã€Microsoftã¨ã‚‚é€£æºã—ã¦ã‚‹ã‚“ã ã‚ã†ã‘ã©ã€RPAï¼ˆã™ã§ã«æ­»èªï¼Ÿï¼‰ã«ã¨ã©ã‚ã‚’åˆºã™ã ã‚ã†ãªã€‚ã€Œå°ã•ãªLLMã€ã€è‹±èªã§ã‚‚"Smaller LLM"ã¨å‘¼ã°ã‚Œã‚‹ã‚‰ã—ã„ã€å°ã•ãªLLMã§ã„ã„ã‚“ã ãªã€LLMã®Largeã¯ãƒ¢ãƒ‡ãƒ«ã®å¤§å°ã§ã¯ãªã„ã¨ã†ã“ã¨ã€è©•ä¾¡ã«ã‚ˆã‚‹ã¨Flan-T5ãŒã¶ã£ã¡ãã‚Šï¼Ÿ MoEé–¢ä¿‚ã§ã¯ã€Mixtral-8x7Bã®æ—¥æœ¬èªå‘ã‘ã®LoRaã¨ã‹ã€MoEã‚’å˜ç´”åŒ–ã—ã¦Expertã®åˆ‡ã‚Šæ›¿ãˆã‚’è©¦ã—ã¦ã¿ã‚‹ä¾‹ã¨ã‹é¢ç™½ã„ã€‚åŸºç›¤é¢ã§ã¯ã€æ¢ç´¢ãªã—ã§Transfomerã ã‘ã§ãƒã‚§ã‚¹ãƒã‚¹ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹ã®ï¼¡ï¼©ãŒä½œã‚Œã‚‹ã‚‰ã—ã„ã€‚ä¸€æ–¹ã€Transformerã®æ¬¡ä¸–ä»£åŸºç›¤ã®ä¸€ã¤ã¨ã•ã‚Œã‚‹Mambaã€æ—¥æœ¬èªã§ã®è©³ç´°ãªè§£èª¬ã‚„ã€MoEã§ã‚‚ã‚ã‚‹BlackMambaã¨ã‹ã€ã„ã‚ã„ã‚å‡ºã¦ããŸãªã€‚ç†è«–é¢ã§ã¯ã€å²¡é‡ã•ã‚“ã®è§£èª¬ã€The Consensus Gameã€RAGã®æ”¹è‰¯ã‚’ç”ŸæˆAIã¨è­˜åˆ¥AIã®é–“ã®ã‚²ãƒ¼ãƒ ã¨ã—ã¦ã¨ã‚‰ãˆã‚‹ã¨ã¯ã€‚NVIDIAã‚‚è‡ªã‚‰canary-1bã¨ã‹ã€Audio Flamingoã¨ã‹éŸ³å£°ã‚„å¯¾è©±é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã€è‡ªå‹•é‹è»¢ã§ã¯é‹è»¢æ‰‹ã¨ã®å¯¾è©±ãŒå¿…è¦ãªã®ã¯ãã®ã¨ãŠã‚Šãªã‚“ã ã‚ã†ã€‚RAGé–¢ä¿‚ã‚‚ã€Self RAGã¨ã‹ã€Guardrailsã¨ã‹ã€GPT-4ã¨çµ„ã¿ã‚ã›ãŸåŒ»ç™‚åˆ†é‡ã§ã®è©•ä¾¡ã¨ã‹ã„ã‚ã„ã‚é€²ã‚“ã§ã„ã‚‹ãŒã€è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ragas 0.1ãŒã§ãŸã®ã¯å¤§ãã„ã€‚æ—¥æœ¬èªLLMã‚‚ã€æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ•´ç†ã‚„ã€ŒLLM-jp 13B v1.1ã€ã®ãƒªãƒªãƒ¼ã‚¹ã¨ã‹ç€å®Ÿã«é€²ã‚“ã§ã„ã‚‹ã®ãŒå¿ƒå¼·ã„ã€ã¯ã‚ˆNEDOã®æˆæœã‚’ï¼ã€‚çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¨ã®LLMã®èåˆã€Wikidata ã¨ã‹ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã¨ã‹ã€Research Insightã¨ã‹è©±é¡Œã¯ç¶šã„ã¦ã„ã‚‹ã€‚

-  Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
	- https://arxiv.org/abs/2402.00841
	- Next to RoBERTa, FLAN-T5 is also a great go-to model for training text classifiers
	- Flan-T5é ‘å¼µã‚‹ãªã‚
-  å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç§‘å­¦çš„ç™ºè¦‹ã«ä¸ãˆã‚‹å½±éŸ¿ï¼šGPT-4ã‚’ç”¨ã„ãŸäºˆå‚™çš„ç ”ç©¶
	- https://ai-scholar.tech/articles/large-language-models/impact_of_LLM
	- GPT-4ã¯ç§‘å­¦çš„ç™ºè¦‹æ´»å‹•ã«ã‚‚å¤§ããå¯„ä¸ã—ã¤ã¤ã‚ã‚Šã¾ã™ã€‚  
	- å‰µè–¬ã€ç”Ÿç‰©å­¦ã€è¨ˆç®—åŒ–å­¦ã€ææ–™è¨­è¨ˆã€åå¾®åˆ†æ–¹ç¨‹å¼ã¨å¹…åºƒãã€GPT-4ã®å¿œç”¨ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ãã‚Œãã‚Œã®å¿œç”¨ã§ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚  
	- ç¾æ™‚ç‚¹ã§ã®GPT-4ã‚’ç”¨ã„ã‚‹ã†ãˆã§ã®ä¸è¶³ç‚¹ã‚’æ•´ç†ã—ã€å°†æ¥ã¸ã®å±•æœ›ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚
	- çŸ¥è¦‹
		- å…¨ä½“çš„ã«è¨€ãˆã°ã€GPT-4ã¯å‰µè–¬ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã¨å€‹ã€…ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
		- GPT-4ã¯é€†åˆæˆã®äºˆæ¸¬ç²¾åº¦ãŒ20.
		- GPT-4ãŒå‰µè–¬ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãŸã‚ã®æ­£ã—ã„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã®ã«å½¹ç«‹ã¤
		- å®šé‡è¨ˆç®—ï¼š GPT-4ã¯ç”Ÿç‰©å­¦çš„ãªè¨€èªç†è§£ã¨å‡¦ç†ã«å„ªã‚Œã¦ã„ã¾ã™ãŒã€å®šé‡çš„ãªè¨ˆç®—ã«ã¯é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚ä¿¡é ¼ã§ãã‚‹çµè«–ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€æ‰‹å‹•ã§æ¤œè¨¼ã™ã‚‹ã‹ã€åˆ¥ã®è¨ˆç®—ãƒ„ãƒ¼ãƒ«ã§æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™
- Qwen2-14Bã®MTBenchãŒ7.99ã§Claude-1è¶…ãˆã¦ã‚‹ã®ã¯ãƒã‚¸ã‚„ã°ã„ by ã†ã¿ã‚†ã
	- https://x.com/umiyuki_ai/status/1754435534511050870?s=20
	- Qwen2ã¨ã—ã¦ã‚¦ãƒ¯ã‚µã«ãªã£ã¦ãŸãƒ¢ãƒ‡ãƒ«ãŒQwen1.5ã¨ã—ã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸï¼Mistral-Mediumã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ï¼ä»Šå›ã¯æœ€åˆã‹ã‚‰Transformerã§ä½¿ãˆã‚‹ä¸Šã«ã€AWQãƒ¢ãƒ‡ãƒ«ã€GPTQãƒ¢ãƒ‡ãƒ«ã€GGUFã‚‚å…¨éƒ¨å…¬å¼ã§æœ€åˆã‹ã‚‰ãƒªãƒªãƒ¼ã‚¹ï¼vLLMã‚„Ollamaã‚‚OKï¼
-  Large Language Models on Graphs: A Comprehensive Survey
	- https://arxiv.org/abs/2312.02783
	- We have finalized our ğ‹ğ‹ğŒğ¬ ğ¨ğ§ ğ†ğ«ğšğ©ğ¡ğ¬ survey by adding more insightful discussions. If you are interested in LLMs on structure data, don't miss this paper (with a resource repo)!
- Home Credit - Credit Risk Model Stability
	- https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/
	- Kaggleæ–°ã‚³ãƒ³ãƒš ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰åˆ©ç”¨è€…ã®å¤–éƒ¨åŠã³å†…éƒ¨ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é•·æœŸã®è²¸å€’ã‚Œäºˆæ¸¬ã‚¿ã‚¹ã‚¯ã€‚ä¹…æ–¹ã¶ã‚Šã®æ­£çµ±æ´¾ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒš
- Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities
	- Nvidia presents Audio Flamingo
	- https://huggingface.co/papers/2402.01831
-  A Survey of Constraint Formulations in Safe Reinforcement Learning
	- https://arxiv.org/abs/2402.02025
	- å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹å®‰å…¨æ€§åˆ¶ç´„ã®è¨˜è¿°æ–¹æ³•ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã‚’ arXiv ã«ã¦å…¬é–‹ã—ã¾ã—ãŸ ã€‚ä¸»è¦ãªå®šå¼åŒ–ã®ç†è«–çš„ãªé–¢ä¿‚æ€§ã‚’è­°è«–ã—ã¦ã„ã‚‹ã®ãŒé¢ç™½ã„ã¨æ€ã„ã¾ã™
-  BlackMamba: Mixture of Experts for State-Space Models
	- https://huggingface.co/papers/2402.01771
- è‹±å›½AI Safety Instituteã‚ˆã‚Š3rd Progress Repoertã€‚
	- https://www.gov.uk/government/publications/uk-ai-safety-institute-third-progress-report/ai-safety-institute-third-progress-report
	- Google DeepMindã®Geoffrey Irvingæ°ã€Oxfordå¤§ã®ç¥çµŒç§‘å­¦è€…Chris Summerfieldæ°å‚ç”»ã€‚ã‚³ã‚¢KPIã®ã€Œãƒ¡ãƒ³ãƒãƒ¼ã®ã€å…ˆç«¯AIãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ç´¯ç©çµŒé¨“å¹´æ•°ã€ãŒ11æœˆã®150å¹´ã‹ã‚‰168å¹´ã«å¢—åŠ ã€‚
-  Stable Diffusion WebUI Forge
	- https://github.com/lllyasviel/stable-diffusion-webui-forge
	- Stable-Diffusion-WebUI-Forge is a new platform to 
		- (1) completely solve the speed and VRAM problem and 
		- (2) adding UNet Patcher System to webui so that many new features can be implemented in about 100 lines of codes
-  Unifying Large Language Models and Knowledge Graphs: A Roadmap
	- https://arxiv.org/abs/2306.08302v3
	- ã“ã® Knowledge Graph ã¨LLMã®é–¢ä¿‚ã«ã¤ã„ã¦çºã‚ãŸè«–æ–‡ã™ã”ã„ã€‚ 
	- Knowledge Graphã¨LLMãŒç›¸äº’æˆé•·ã™ã‚‹ä»•çµ„ã¿ãŒéå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ããƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åŒ–ã—ã¦çºã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ è«–æ–‡ã¨ã„ã†ã‚ˆã‚Šç¾çŠ¶ã®æ•´ç†ã«è¿‘ã„
-  Wikidata from LangChain
	- https://python.langchain.com/docs/integrations/tools/wikidata
	- WikiData allows you to easily connect to a free and open knowledge base
-  Qwen1.5
	- https://qwenlm.github.io/blog/qwen1.5/
	- https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524
	- Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.
- Ollamaã§ã‚‚Qwin1.5ã‚’ã‚µãƒãƒ¼ãƒˆ
	- https://ollama.com/library/qwen
-  Repeat After Me: Transformers are Better than State Space Models at Copying
	- https://arxiv.org/abs/2402.01032
	- Our recent work on the comparison between Transformers and State Space Models for sequence modeling now on arxiv! TLDR - we find a key disadvantage of SSMs compared to Transformers: they cannot copy from their input
-  Self RAG
	- https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb
	- Weâ€™re excited to feature Self-RAG, a special RAG technique where an LLM can do self-reflection for dynamic retrieval, critique, and generation
- The Majority of AI Compute Spend is Not on Training but on Inference
	- https://x.com/rohanpaul_ai/status/1754843805507887477?s=20
	- As per report - "2023: The State of Generative AI in the Enterprise"
- Qwen1.5-0.5B-chat with Transformer.js
	- Qwen1.5 is out: a collection of powerful LLMs with sizes ranging from 0.5B to 72B parameters.
	- https://x.com/xenovacom/status/1754873501536645292?s=20
	- Even at 8-bit quantization, the smallest one (0.5B) is surprisingly good for its size! Here's a demo I made with Transformers.js (v2.15), running 100% locally in the browser w/ WASM!
	- https://github.com/xenova/transformers.js	
- Gradio demo of Qwen1.5-72B-Chat
	- https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat
- ollamaã§Mixtralã‚’å‹•ã‹ã—ã¦LangChainã®agentã§ neo4jã™ã‚‹
	- Managed to get Mixtral on @ollama working as an function calling @LangChainAI agent that interacts with @neo4j  through a semantic layer. Needs some tidying up and I'll be able to share it.
	- https://x.com/tb_tomaz/status/1754861855929958488?s=20
- Style-Bert-VITS2ãŒå³åº§ã«æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ« JP-Extraã‚’å–ã‚Šè¾¼ã‚“ã§ãã‚Œã¦ã€æ—¥æœ¬èªç™ºéŸ³ãŒã‚¨ã‚°ã„ã§ã™
	- https://github.com/litagin02/Style-Bert-VITS2/releases/tag/2.0
	- ã€ŒStyle-Bert-VITS2ã€ã¯ã€è‡ªå‹•ã§æ–‡è„ˆãŒæŠŠæ¡ã•ã‚Œã€æ„Ÿæƒ…è¡¨ç¾ãŒèª¿æ•´ã•ã‚Œã‚‹
	- https://huggingface.co/spaces/litagin/Style-Bert-VITS2-JVNV
	- ã„ã‚„ã“ã‚Œã¯ã™ã”ã„
- NVIDIAãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼å‘ã‘GPUå¸‚å ´ã§98ï¼…ã®ã‚·ã‚§ã‚¢ã‚’ç‹¬å ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã€AIæ€§èƒ½ãŒæ˜æš—ã‚’åˆ†ã‘ã‚‹çµæœã« - GIGAZINE
	- https://gigazine.net/news/20240205-nvidia-gpu-market/
- ã ã‚ã€‚çµ¶å¯¾ã€‚ by ã‚­ãƒ ãƒ¯ã‚¤ãƒ—
	- https://x.com/kimwipes_crecia/status/1754757418595336404?s=20
-  OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
	- https://huggingface.co/papers/2402.01739
	- To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE,
- Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report
	- https://arxiv.org/abs/2402.01733
	- GPT-4ã«RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€è‡¨åºŠåŒ»å­¦ã®å•é¡Œã«ãŠã„ã¦ã€äººé–“ã®åŒ»å¸«ã‚ˆã‚Šã‚‚é«˜ã„ç²¾åº¦ãŒé”æˆã§ããŸã¨å ±å‘Š
	- é©åˆ‡ãªRAGã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã«ã‚ˆã‚Šã€GPT-4å˜ä½“ã‚ˆã‚Šã‚‚10%ä»¥ä¸Šç²¾åº¦ãŒå‘ä¸Šã—ã€äººé–“åŒ»å¸«ã‚ˆã‚Šã‚‚5%ä»¥ä¸Šé«˜ã„ã‚¹ã‚³ã‚¢ã‚’å‡º
	- ç ”ç©¶è€…ã‚‰ã¯ã“ã®çµæœã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã¨ã—ã¤ã¤ã€ã‚ˆã‚Šåºƒç¯„ãªåˆ†é‡ã§å®Ÿé¨“ã‚’é‡ã­ã¦ã„ãã¹ãã¨ã—ã¦ã„ã¾ã™ã€‚ 
	- ã¾ãŸã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒä½ã„ã¨ã¯ã„ãˆã€åŒ»å­¦ã«ãŠã‘ã‚‹è‡ªå‹•åŒ–ã¯æ…é‡ã§ã‚ã‚‹ã¹ãã¨ã‚‚è¿°ã¹ã¦ã„ã¾ã™ã€‚
- Open AI shifts its battleground to Software
	- https://x.com/bioshok3/status/1755376649816953209?s=20
	- Open AIã¯ç¾åœ¨2ç¨®é¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆAIã‚’æ§‹ç¯‰ä¸­
	- 1ã¤ã¯ã‚ã‚Šã¨è‡ªç”±ã«ãƒ‡ãƒã‚¤ã‚¹ã‚’æ“ä½œå¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ 
		- é¡§å®¢ã¯ ChatGPT ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã€åˆ†æã®ãŸã‚ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã—ãŸã‚Šã€çµŒè²»å ±å‘Šæ›¸ã‚’è‡ªå‹•çš„ã«è¨˜å…¥ã—ã¦ä¼šè¨ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«å…¥åŠ›ã—ãŸã‚Šã™ã‚‹ã‚ˆã†ä¾é ¼ã§ãã¾ã™
	- ã‚‚ã†ä¸€ã¤ã¯WEBä¸Šã§æ§˜ã€…ãªæ“ä½œå¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ï¼ˆ1ã¤ç›®ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚„ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æ‡¸å¿µã™ã‚‹äººã‚‚ã„ã‚‹ã®ã§ã‚‚ã†ä¸€ã¤ã®ã‚¿ã‚¤ãƒ—ã‚’é–‹ç™ºã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ï¼‰
- Fully local RAG using @Teknium1 OpenHermes, @ollama and @streamlit
	- GPT4 level performance at 0% of the cost
	- https://github.com/phidatahq/phidata/tree/main/cookbook/local_rag
- æµ·å¤–é«˜æ€§èƒ½è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªåŒ–ç ”ç©¶ã®ä¸€ç’°ã¨ã—ã¦Mixtral-8x7Bã®æ—¥æœ¬èªå‡ºåŠ›ã‚’å®‰å®šã•ã›ã‚‹Loraä½œæˆã€å…¬é–‹   
	- https://huggingface.co/aixsatoshi/Mixtral-8x7B-ja-Lora-sft-ChatbotArenaJAcalm2
	- Mixtral-8x7Bã¯é«˜æ€§èƒ½ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€æ—¥æœ¬èªå‡ºåŠ›ã«å¤šè¨€èªãŒæ··å…¥ã™ã‚‹code-switchingãŒã‚ˆãè¦‹ã‚‰ã‚Œã¾ã™ã€‚ å…ƒã®æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€æ—¥æœ¬èªç”Ÿæˆã‚’å®‰å®šã•ã›ã‚‹æ–¹æ³•ã¨ã—ã¦ã€Loraã®åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã—ãŸ
	- æ—¥æœ¬èªãŒæµæš¢ãªcalm2ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã¾ã™ Baseãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ã€ä¸€å®šã®æ€§èƒ½ç¢ºä¿ã—ã¦æ—¥æœ¬èªåŒ–ã§ãã¾ã—ãŸ
-  Apple Vision Proã¯HoloLensã®å®Œæˆå½¢ã€‚ç¾æ™‚ç‚¹ã§ã®é™ç•Œå€¤ by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/nd36c04f9133a?sub_rt=share_h
	- ã€Œã¤ã„ã«ã“ã“ã¾ã§æ¥ãŸã‹ã€
-  Step-wise Queries by llamaindes
	- https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html#step-wise-queries
	- In our brand-new cookbook, learn how to build a custom agent that can execute complex queries over your data, and can also be interrupted in the middle of execution with user inputs!
- Ollama OpenAI compatibility is here!
	- https://ollama.com/blog/openai-compatibility
	- ã¤ã¾ã‚Šã€ollamaã§openaiã®APIã¤ã‹ã£ã¦URLã‚’ollamaã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«å¤‰ãˆã‚‹ã ã‘ã§å‹•ãã¨ã„ã†ã“ã¨
-  RAG Research Insights
	- https://www.promptingguide.ai/research/rag#rag-research-insights
	- So we have created a new section in the RAG overview to summarize and help you keep track of insights into the latest RAG techniques.
- Nvidia releases canary-1b
	- https://huggingface.co/spaces/nvidia/canary-1b
	- With 1 billion parameters, Canary-1B supports automatic speech-to-text recognition (ASR) in 4 languages (English, German, French, Spanish) and translation from English to German/French/Spanish and fromâ€¦
- Bard ã¯ Geminiï¼ˆã‚¸ã‚§ãƒŸãƒ‹ï¼‰ã«ãªã‚Šã¾ã™ï¼
	- https://x.com/googlejapan/status/1755607418103587148?s=20
	- Gemini ã¯ Bard ã«æ­è¼‰ã•ã‚Œã¦ã„ã‚‹ AI ãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€ã“ã®é«˜åº¦ãªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ã‚ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹ãŸã‚ã«ã€åå‰ã‚’å¤‰ãˆã¾ã—ãŸ
	- https://gemini.google.com/app
- The Consensus Game: Language Model Generation via Equilibrium Search by å²¡é‡ã•ã‚“
	- https://openreview.net/forum?id=n9xeGcI4Yg
	- LLMã§è³ªå•å¿œç­”ç­‰ã®ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã™å ´åˆã€ç”Ÿæˆçš„ã«è§£ãå ´åˆï¼ˆp(y|x,v=çœŸ)) ã¨è­˜åˆ¥çš„ã«è§£ãå ´åˆï¼ˆp(v=çœŸ|x, y)ï¼‰ã§å¾—æ„/ä¸å¾—æ„ãŒç•°ãªã‚ŠçµæœãŒç•°ãªã‚‹ã€‚ã‚²ãƒ¼ãƒ ç†è«–ã«åŸºã¥ã„ã¦äºŒã¤ãŒåˆæ„ã™ã‚‹è§£ã‚’æ±‚ã‚ã‚‰ã‚Œã‚‹å‡è¡¡é †ä½ä»˜ã‘ã‚’ææ¡ˆã€‚å¤šãã®ã‚¿ã‚¹ã‚¯ã§å†å­¦ç¿’ãªãã€æ€§èƒ½ã‚’å¤§ããæ”¹å–„ã§ãã‚‹
- OpenAnimateAnyone
	- https://github.com/fenghan0430/Open-AnimateAnyone
	- ã‚¢ãƒªãƒãƒã¯AIã®ç ”ç©¶çµæœã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã§å‡ºã—ã¦ãã‚Œã¦ãŸã‘ã©ã€ã„ã–AnimateAnyoneã¿ãŸã„ãªæœ‰æœ›ãªæˆæœç‰©ãŒã§ããŸã‚‰ã‚¹ãƒƒã¨ã‚¯ãƒ­ãƒ¼ã‚ºã«ã—ã¦ã‚·ãƒ¥ãƒƒã¨è‡ªç¤¾ã‚¢ãƒ—ãƒªã«çµ„ã¿è¾¼ã‚€ã€‚ã¤ã¾ã‚Šä»Šã¾ã§ã¯è‡ªç¤¾ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯ä½¿ãˆã‚“ã‚¯ã‚ªãƒªãƒ†ã‚£ã ã‹ã‚‰ä¸ç”¨å“ãƒªã‚µã‚¤ã‚¯ãƒ«ã¨ã—ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã«ã—ã¦ãŸã ã‘ï¼Ÿ
-  Grandmaster-Level Chess Without Search
	- https://arxiv.org/abs/2402.04494
	- ãƒã‚§ã‚¹ã§ã©ã®æ‰‹ãŒè‰¯ã„ã‹ã‚’Transformerã§æ•™å¸«ã‚ã‚Šå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯æ¢ç´¢ã‚’ä½¿ã‚ãªãã¦ã‚‚äººã‚ˆã‚Šå¼·ããªã‚‹ï¼ˆæ¢ç´¢ã‚ã‚ŠAIã‚ˆã‚Šã¯å¼±ã„ï¼‰ã€‚æ•™å¸«ã‚ã‚Šãƒ‡ãƒ¼ã‚¿ã¯Stockfish 16ã§ä½œæˆã—ã¦ãŠã‚Šã€ç§‘å­¦åˆ†é‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã¨ã¿ãªã›ã‚‹ã€‚
- ragas 0.1 release
	- https://github.com/explodinggradients/ragas
	- We are releasing version 0.1 of Ragas today, the open-source standard for evaluating RAG applications.
-  Perplexityã‚’ã‚‚ã¨ã«ï½¤è¤‡æ•°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¦æ¨è«–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®ç°¡å˜ãªã‚³ãƒ¼ãƒ‰å®Ÿè£…
	- https://note.com/kan_hatakeyama/n/nb5625d6411a8?sub_rt=share_pb
	- ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®ç°¡å˜ãªå®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã™ã€‚  æœ€è¿‘ã¯ï½¤æ™®é€šã«mergekitã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒï½¤å‹‰å¼·ã‚‚å…¼ã­ãŸå®Ÿè£…ã§ã™
	- ä¸ãˆã‚‰ã‚ŒãŸå…¥åŠ›æ–‡ç« ã«å¯¾ã™ã‚‹Perplexityï¼ˆå›°æƒ‘ã•ï¼‰ã‚’æŒ‡æ¨™ã«ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚Šã¾ã™
	- ä»Šå›ã¯è©¦ã—ã«ã€è‹±èªãŒå¾—æ„ãªLLama2-7bã¨ã€æ—¥æœ¬èªã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸElyza-7bã‚’çµ±åˆï¼ˆmergeï¼‰ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã£ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚
- æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
	- https://github.com/lighttransport/japanese-llama-experiment
-  Real-World Robot Applications of Foundation Models: A Review
	- https://arxiv.org/abs/2402.05741
	- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿãƒ­ãƒœãƒƒãƒˆå¿œç”¨ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼Meta AI Researchã®Chrisã•ã‚“@chris_j_paxton , Google Deepmindã®Andyã•ã‚“ @andyzeng_ ã¨ã„ã†ï¼Œã“ã®åˆ†é‡ã§æœ€å…ˆç«¯ã‚’é€²ã‚€ãŠäºŒäººã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘ãªãŒã‚‰åŸ·ç­†
-  OpenAIã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã€åŠå°ä½“ã®è³‡é‡‘èª¿é”ã§äº¤æ¸‰ã€€ç±³å ±é“
	- https://www.nikkei.com/article/DGXZQOGN095R00Z00C24A2000000/
	- å¿…è¦è³‡é‡‘750å…†å††
-  Multilingual E5 Text Embeddings: A Technical Report
	- https://huggingface.co/papers/2402.05672
	- Microsoftã®E5ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®å®Ÿè£…ãƒšãƒ¼ãƒ‘ãƒ¼ã€ä»Šé ƒã§ã‚‹ã‚‚ã®ãªã®ã‹ãƒ»
- Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents
	- https://github.com/yifanlu0227/ChatSim
	- ç”ŸæˆAIã®å‡ºç¾ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®ä¸–ç•Œã‚‚å¤§ããå¤‰åŒ–ã€‚ æ˜¨æ—¥å‡ºãŸChatSimã§ã¯è‡ªç„¶è¨€èªã‚’å…¥åŠ›ã—ã¦ãƒ‰ãƒ©ã‚¤ãƒ“ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’è‡ªç”±ã«ç·¨é›†ã™ã‚‹ã“ã¨ãŒã§ãã‚‹
-  LangChain 101: Part 3a. Talking to Documents: Load, Split, and simple RAG with LCEL
	- https://pub.towardsai.net/langchain-101-part-3a-talking-to-documents-load-split-and-simple-rag-with-lcel-26b005ccb30a
	- Loading documents and splitting them are a key part of RAG
- mambaã®ç†è«–ã‚’ç†è§£ã™ã‚‹â‘ ï¼šHiPPOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨LSSL
	- https://zenn.dev/izmyon/articles/8374a11d272602
	- mambaã®ç†è«–ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®è§£èª¬è¨˜äº‹ã‚’æ›¸ãå§‹ã‚ã¾ã—ãŸã€‚ã‹ãªã‚Šæ•°å¼ã®å°å‡ºãªã©ä¸å¯§ã«æ›¸ã„ã¦ã‚‹ã®ã§ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚ä½•ã‹è¨‚æ­£ã‚„è£œè¶³ãŒã‚ã‚Œã°å„ªã—ãæ•™ãˆã¦ãã ã•
-  Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
	- https://arxiv.org/abs/2402.00841
	- Can "small" finetuned LLMs with less than 2B parameters outperform larger openly available LLMs (Mixtral, Llama 2 Chat) and proprietary LLMs (ChatGPT)? Here's a closer look at the Tiny Titans paper
	- Flan-T5ãŒæœ€å¼·ã‚‰ã—ã„ã€
-  GPTã¯ä»–è€…ã®å¿ƒã®çŠ¶æ…‹ã‚’æ¨æ¸¬ã§ãã‚‹ï¼ŸAIÃ—å¿ƒç†å­¦ã®ã™ã‚ã‚
	- https://ai-scholar.tech/articles/computation-and-language/Theory-of-Mind
	- GPTã¯ä»–è€…ã®å¿ƒã‚’èª­ã‚ã‚‹ã®ã‹ï¼Ÿ å®Ÿé¨“ã«ãŠã„ã¦ã€GPT-3.5ã¨GPT-4ã¯é«˜ã„æ­£ç­”ç‡ã‚’ãƒãƒ¼ã‚¯ã—ã¾ã—ãŸã€‚ 
	- è‘—è€…ã¯ã€GPTãŒå¿ƒã®çŠ¶æ…‹ã‚’æ¨æ¸¬ã§ãã‚‹ç†ç”±ã¨ã—ã¦ã€Œè¨€èªèƒ½åŠ›ã®å‘ä¸Šã«ã‚ˆã£ã¦è‡ªç™ºçš„ã«å‡ºç¾ã—ãŸã®ã§ã¯ã€ã¨æŒ‡æ‘˜ã€‚ AIç ”ç©¶ã«ãŠã‘ã‚‹å¿ƒç†å­¦çš„ãªè¦–ç‚¹ã®é‡è¦æ€§ã‚’è§£
- In-Context Principle Learning from Mistakes
	- https://arxiv.org/abs/2402.05403
	- LLMã«æ•¢ãˆã¦é–“é•ã‚ã›ã¦ãƒ«ãƒ¼ãƒ«ã‚’è¦šãˆã•ã›åŒã˜ãƒŸã‚¹ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ 
		- 1. ãƒ¢ãƒ‡ãƒ«ãŒé–“é•ã„ã‚’çŠ¯ã™ã‚ˆã†ã«ä¿ƒã™ 
		- 2. ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã«ã€é–“é•ã„ã«å¯¾ã™ã‚‹èª¬æ˜ã‚’ç”Ÿæˆã•ã›ã€ã¾ãšã¯ä½ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’å½¢æˆã€‚ 
		- 3. ä½ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’ã¾ã¨ã‚ã€ç´„5ã¤ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«åœ§ç¸®ã—ã¦é«˜ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’ç”Ÿæˆ 
		- 4. é«˜ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’æœªè¦‹ã®ä¾‹ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹éš›ã«åˆ©ç”¨ 
	- â– å®Ÿé¨“ã¨çµæœ å®Ÿé¨“ã¨çµæœã®è¦ç´„: 
		- GPT-3.5-Turboã¨GPT-4ã®è³ªå•å¿œç­”æ€§èƒ½ãŒä¸€è²«ã—ã¦æ”¹å–„ã•ã‚Œã€GPT-4ãŒ7.5%ã®æ”¹å–„ã‚’è¦‹ã›ãŸ
		- æ•°å­¦æ¨è«–ã‚¿ã‚¹ã‚¯ã§ã‚‚GPT-3.5-turboã¨GPT-4ã§åŸºæº–ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸ
		- Big-Bench Hardã‚¿ã‚¹ã‚¯ã§ã‚‚ã‚¹ã‚³ã‚¢ãŒä¸€å®šç¨‹åº¦ä¸Šæ˜‡ã—ãŸ
- Step-by-step guide to build AI agents for structured and unstructured data.
	- https://x.com/Saboo_Shubham_/status/1756123156400546251?s=20
	- Step 1: Define the Chunking Strategy
	- Step 2: Apply an Embedding Strategy
	- Step 3: Implement a Document Retriever for Text
	- Step 4: Use a Large Language Model (LLM)
	- Step 5: Extract Metadata
	- Step 6: Implement a Document Retriever for Metadata
	- Step 7: Integrate SQL Querying with a Data Warehouse
	- Step 8: Develop a Prompt Refinement Engine
	- Step 9: Create a Response Post-processor
	- Step 10: Deliver the Response
- Buffer Overflow in Mixture of Experts
	- https://arxiv.org/abs/2402.05526
	- "Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious
- WolframEngine+JupyterNotebookã§ç–‘ä¼¼Mathematica
	- https://x.com/blkcatman/status/1756219896026067052?s=20
- ã€Mambaã€‘Transformerã‚’å‡Œé§•ã—ã†ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¾¹åº•è§£èª¬ï¼ˆã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚ã‚Šï¼‰
	- https://qiita.com/peony_snow/items/649ecb307cd3b5c10aa7
	- ï¼‘ï¼Mambaã¯Attentionã‚„MLPBlockã‚’æŒãŸãªã„ç°¡ç´ åŒ–ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æœ‰ã—ã¾ã™ã€‚é¸æŠçš„çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆSelective SSMï¼šSelective State Space Modelï¼‰ã¨ã„ã†æ–°ã—ã„æ§‹é€ ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€å¿…è¦ãªæƒ…å ±ã®ã¿ã«æ³¨ç›®ã—ã€è¨ˆç®—åŠ¹ç‡ã®å¤§å¹…ãªå‘ä¸Šã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚
	- ï¼’ï¼é«˜é€Ÿãªæ¨è«–ï¼ˆTransformerã®ç´„5å€ï¼‰ã‚’å¯èƒ½ã«ã™ã‚‹ã¨ã¨ã‚‚ã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ãªã©ã®ã“ã¨ï¼‰ã®å¢—å¤§ã«å¯¾ã—ã¦ã€æ¨è«–ã‚³ã‚¹ãƒˆãŒç·šå½¢ã«å¢—å¤§ã™ã‚‹ã¨ã„ã†ç‰¹å¾´ã‚’æœ‰ã—ã¾ã™ï¼ˆã“ã‚Œã¾ã§ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯éç·šå½¢çš„ãªå¢—å¤§ãŒã‚ã‚Šã¾ã—ãŸï¼‰ã€‚ã“ã®æ€§èƒ½å‘ä¸Šã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹æ¤œè¨¼ã§ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒ1000kï¼ˆï¼‘ï¼ï¼ä¸‡ï¼‰ã«ãŠã„ã¦ã¾ã§ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚
	- ï¼“ï¼GPUãƒ¡ãƒ¢ãƒªéšå±¤é–“ã®ç§»å‹•ã‚’æœ€å°é™åŒ–ã™ã‚‹ã¨ã¨ã‚‚ã«ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šé«˜é€Ÿãªè¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã€è¦æ±‚ã•ã‚Œã‚‹ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚‚è»½æ¸›ã•ã‚Œã¾ã™
	- ï¼”ï¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°2.8Bä»¥ä¸Šã®å ´åˆã«ãŠã„ã¦Mambaã¯æ©Ÿèƒ½ã™ã‚‹ã®ã‹ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã¯Transformerãªã©ã¨åŒã˜ãªã®ã‹ã€å­¦ç¿’ã®ä¸å®‰å®šæ€§ã¯ã©ã†ãªã®ã‹ã¨ã„ã£ãŸç‚¹ã«é–¢ã—ã¦ã¯ã¾ã ä¸æ˜ã§ã‚ã‚Šã€ä»Šå¾Œã®ç ”ç©¶ãŒå¾…ãŸã‚Œã¾ã™ã€‚
	- ï¼•ï¼ã¾ã ä¸æ˜ãªç‚¹ã‚‚å¤šã„ã§ã™ãŒã€æ§˜ã€…ãªè§’åº¦ã‹ã‚‰ã®ç ”ç©¶ã«ã‚ˆã£ã¦ã€Transformerã‚’ä»£æ›¿ã—ã†ã‚‹æœ‰æœ›ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ã¨ã„ã†ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚‚å–å¾—ã•ã‚Œã¤ã¤ã‚ã‚Šã€ä»Šå¾ŒMambaã‚’çŸ¥ã‚‰ãªã‘ã‚Œã°æœ€å…ˆç«¯ã®ç ”ç©¶ã‹ã‚‰å–ã‚Šæ®‹ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
-  æ —ç”°å·¥æ¥­ã€æ©Ÿæ¢°å­¦ç¿’ä½¿ã£ãŸææ–™æ¢ç´¢ã§ä½ç’°å¢ƒè² è·ã®é˜²é£Ÿå‰¤é–‹ç™ºã¸
	- https://xtech.nikkei.com/atcl/nxt/news/24/00208/?n_cid=nbpnxt_twbn
	- æ —ç”°å·¥æ¥­ã•ã‚“ã‚‰ã¯å†·å´æ°´ã®é˜²é£Ÿå‰¤ã®é–‹ç™ºã®ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šæ•°ç™¾ä¸‡ã®åˆ†å­ã‹ã‚‰æœ‰æœ›ææ–™ã‚’æŠ½å‡º
- NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit
	- https://towardsdatascience.com/nemo-guardrails-the-ultimate-open-source-llm-security-toolkit-0a34648713ef
	- Advanced RAG with Guardrails
	- If you want to build user-facing RAG, you not only need to setup advanced retrieval, but also need to apply requisite layers of input/output filters for the following:
- pandas-ai
	- https://github.com/gventuri/pandas-ai
	- æ©Ÿèƒ½ã¨ã—ã¦ã¯pandasã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾ã—ã¦ç›´æ¥è‡ªç„¶è¨€èªã§å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã€è»½ãè¦‹ãŸæ„Ÿã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„ã«æ–°ã—ã„ã‚‚ã®ã¯ãªã•ãã†ãªã‚‚ã®ã®http://df.chat(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)ã¨ã„ã†å½¢å¼ã§ã®æ“ä½œã¯æ–¬æ–°
- LLM-jp 13B v1.1ãƒªãƒªãƒ¼ã‚¹
	- https://llm-jp.nii.ac.jp/blog/2024/02/09/v1.1-tuning.html
	- å„ç¨®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã™ã”ã„æµæš¢ã«ãªã£ã¦ã‚‹ã€‚å­¦ç¿’è©³ç´°ã‚‚å…¬é–‹ã•ã‚Œã¦ã¦å‚è€ƒã«ãªã‚‹ã€‚
- The biggest Collection of Colab Based LLMs Fine tuning Notebooks
	- https://github.com/ashishpatel26/LLM-Finetuning
-  Google Colab ã§ LLM-jp 13B v1.1 ã‚’è©¦ã™ by nakaã•ã‚“
	- https://note.com/npaka/n/n2c272727d95a?sub_rt=share_h
	- ã€ŒLLM-jp 13B v1.1ã€ã¯ã€ã€ŒLLM-jp 13Bã€ã®æœ€æ–°ç‰ˆã§ã™ã€‚æ—¥è‹±ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹SFTã€ichikaraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ +DPOã§å¯¾è©±å¿œç­”æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚
- OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
	- https://arxiv.org/abs/2402.06044
	- A wild Theory of Mind Benchmark has appeared:
- 



## 2/5

ä»Šé€±ã‚‚ç››ã‚Šã ãã•ã‚“ã€‚ã¾ãšã¯ã€Metaã®CodeLlamaã®70Bç‰ˆãƒªãƒªãƒ¼ã‚¹ã€‚æ—©é€ŸSQLã®å¤‰æ›SQLCoder-70BãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã‚Šã€4bitåŒ–ã•ã‚Œã¦MLXçµŒç”±ã§Macã§å‹•ã‹ã—ãŸã‚Šã¨ä¸€æ°—ã«ã«ãã‚„ã‹ã«ã€‚Metaã¯ã€35ä¸‡å€‹ã® H100ã‚’æ•´å‚™ã—ã€OSSã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã«å–ã‚Šçµ„ã‚€ã¨ã„ã†ã“ã¨ã§ã€æ ªä¾¡ã¯20%ã‚¢ãƒƒãƒ—ã€‚ä¸€æ–¹Googleã¯ã€Bardã®backendã®Gemini Proã®å›½éš›å¯¾å¿œã‚’ãƒªãƒªãƒ¼ã‚¹ã€‚æ—¥æœ¬èªãªã‚“ã‹ã¾ã å¤‰ï¼ˆä¾‹ã€ãƒã‚¤ã‚¯ã‚’è‡ªè»¢è»Šã¨èªçŸ¥ï¼‰ã§ã™ãŒã€ç”»åƒèªè­˜æ©Ÿèƒ½ãªã©Gemini Proã‚’æ‰‹å…ƒã§è©¦ã›ã‚‹ã€‚OSSç‰ˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMä»£è¡¨çš„ãªLLaVA-1.6ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã€Gemini Proè¶Šãˆã¨ã®è©•ä¾¡ã‚‚ã€‚LLMã®è»½é‡åŒ–ã®æ–°æ˜ŸSliceGPTã€è»½ãã¦ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯å¤§æ­“è¿ã€‚miqu-70Bã¨ã„ã†Mixtral 8x7Bã®é‡å­åŒ–ç‰ˆã‚‰ã—ãã‚‚ã®ãŒã€EQ-benchã§çªç„¶ä¸Šä½ã«ç™»å ´ã€æ¬¡ã®å¤§ããªãƒªãƒªãƒ¼ã‚¹ã®æ–¥å€™ã‹ã€ãªãŠmiqueã£ã¦ãƒŸã‚¯ã ã£ãŸã®ã‹ï¼Ÿã€‚Phixtralã®è«–æ–‡ã§ç´¹ä»‹ã•ã‚ŒãŸMoEã®å®Ÿè£…ã€æœ¬å®¶ã¨ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé•ã†ã¿ãŸã„ã ãŒMoEã®å®Ÿè£…ã«ã‚‚ã„ã‚ã„ã‚ã‚ã‚‹ã‚‚ã®ã ã€‚ICRA2024ã§ã®æ¡æŠè«–æ–‡ãƒ»æŠ€è¡“ã®è©±é¡Œã‚‚ã¡ã‚‰ã»ã‚‰ã€‚å›½ç”£LLMã§ã¯ã€700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼LLMã€ŒKARAKURI LMã€ãŒç™»å ´ã€Llama 2ã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‰ã—ã„ãŒã‚„ãŸã‚‰æ€§èƒ½ãŒé«˜ã„ã¨è©±é¡Œã«ã€‚ggufç‰ˆã‚„ã€MLXã‚’ã¤ã‹ã£ã¦M2 Macã§ã®å‹•ä½œç¢ºèªç­‰ãŒè¡Œã‚ã‚Œã€ã“ã‚Œã¯åŸºç¤èƒ½åŠ›ãŒé«˜ãã†ã€‚å°ã•ãè¨€èªãƒ¢ãƒ‡ãƒ«ã‚‚ã€Allen.AIã®OLMoã‚„ã€Kaggleé–¢é€£ã®H2O-Danube-1.8Bãªã©ãŒç™»å ´ã€‚RAGé–¢ä¿‚ã ã¨ã€ã¾ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã®æ¯”è¼ƒè«–æ–‡ã€ã©ã†ã‚‚ã¾ã ï¼²ï¼¡ï¼§ã®ã»ã†ãŒåˆ©ãŒã‚ã‚‹ã€‚ã‚¯ã‚¨ãƒªå¤‰æ›ã£ã¦ã®ã‚‚é‡è¦ãªæŠ€è¡“ã€‚ã—ã‹ã—ã€èµ¤ã¡ã‚ƒã‚“ã®é ­ã«ãƒ“ãƒ‡ã‚ªã‚’è£…ç€ã—ã¦å¾—ã‚‰ã‚ŒãŸ61 æ™‚é–“åˆ†ã®ç”»åƒã‹ã‚‰ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚‹ã¨ã„ã†é€”æ–¹ã‚‚ãªã„ç ”ç©¶ã«ã¯ã³ã£ãã‚Šã—ãŸã€‚Hugging FaceãŒGPT Storeã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ç‰ˆï¼ˆAssistantï¼‰ã‚’é–‹å§‹ã€Googleã¨ã®ææºã§ãƒªã‚½ãƒ¼ã‚¹ãŒå¼·åŒ–ã•ã‚ŒãŸï¼Ÿã€‚æ±äº¬è—å¤§ã®å’æ¥­å±•ç¤ºã«â€œAIã‚¢ãƒ‹ãƒ¡â€ãŒå‡ºãŸã“ã¨ãŒè©±é¡Œã«ãªã£ãŸãŒã€Makingæƒ…å ±ã‚’è¦‹ã‚‹ã¨ã€ã˜ã¤ã¯ç›¸å½“ï¼¬ï¼¬ï¼­ã‚’ä½¿ã„ã“ãªã—ã¦ã„ã¦ã€ã‚·ãƒŠãƒªã‚ªã®ChatGPTã§ã®ä½œæˆãƒ­ã‚°ç­‰ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå‚è€ƒã«ãªã‚‹ã¨ã„ã†è©±ã«ã€‚Googleã®ã‚ã‚‰ã‚†ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’Decoder-onlyã®ãƒ¢ãƒ‡ãƒ«ã«ã¶ã£è¾¼ã‚“ã§æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ä½œã‚‹è©±ã€é•·æœŸæ™‚ç³»åˆ—äºˆæ¸¬ã§ã©ã†ã—ã¦ãã‚“ãªã«æ€§èƒ½ãŒé«˜ã„ã®ã‹ã€æ°—è±¡äºˆæ¸¬ã«ã‚‚é©ç”¨ã™ã‚‹ã®ã‹ï¼Ÿã€‚NEDOã®å›½å†…ç”ŸæˆAIã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºæ”¯æ´ã€ã•ã™ãŒã¨æ€ã‚ã‚Œã‚‹ä¼šç¤¾ã‚„ç ”ç©¶æ©Ÿé–¢ãŒä¸¦ã¶ã€‚å‚åŠ æ©Ÿé–¢ã®ï¼‘ã¤NIIã§ã¯ã€å›½ä¼šå›³æ›¸é¤¨ã®ã‚‚ã¤å›½å†…ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–äº‹æ¥­ã®æˆæœãŒæ´»ç”¨ã•ã‚Œã‚‹ã¨ã„ã†ã“ã¨ã ã€‚ä¸€æ–¹æ°‘é–“ã§ã¯Ricor-13Bã®ã‚ˆã†ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸLLMæä¾›ãƒ“ã‚¸ãƒã‚¹ã‚‚ã¯ã˜ã¾ã£ãŸã€‚ææ–™ç³»ã®ç ”ç©¶ã¸ã®LLMã®å¿œç”¨ã‚‚ç€å®Ÿã«é€²ã‚€ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œç’°å¢ƒã‚‚ã‚‚ollamaãŒvisionå¯¾å¿œã¨ã‹ã€function callå¯¾å¿œã¨ã‹ç€å®Ÿã«é€²ã‚“ã§ã‚‹ã€‚ã•ã¦æ¥é€±ã‚‚ã€Gemini Ultra ãŒ2/7ã«ãƒªãƒªãƒ¼ã‚¹ã¨ã®ã†ã‚ã•ã‚‚ã‚ã‚Šã€äººå‹ãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—Figureã«å‡ºè³‡ã—ãŸMicrosoft/OpenAIã®æ¬¡ã®æ‰‹ã‚„ã€Vison Proã‚’å‡ºã—ãŸAppleã®ï¼¡ï¼©æˆ¦ç•¥ã‚‚æ°—ã«ãªã‚‹ã¨ã“ã‚ã€‚

- google/siglip-base-patch16-256-multilingual ã‚’ä½¿ã£ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”»åƒã‚’æ—¥æœ¬èªã§æ¤œç´¢ã—ã¦ã¿ã‚‹
	- https://note.com/eurekachan/n/n9d4f62b80ad6?sub_rt=share_pb
	- 1æœˆã«ã€Googleã‹ã‚‰ã€SigLIPã¨ã„ã†ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã®multilingualç‰ˆï¼ˆå¤šè¨€èªå¯¾å¿œç‰ˆï¼‰ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚transformers 4.37ä»¥é™ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
	- CUDAã‚’ä½¿ã„ã¾ã—ãŸãŒã€GPUã¸ã®è² è·ã‚‚ä½ã‹ã£ãŸã®ã§ã€æ¡ˆå¤–CPUã§ã‚‚å‹•ã‹ã›ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
-  Are Transformers Effective for Time Series Forecasting?
	- decisively highlighting the shortcomings and deficiencies in research surrounding the use of transformers for
	- This paper effectively exposes the deceptive practices employed by various authors in their papers, such as inadequate benchmarking and other tactics, which have previously led to inflated claims regarding the performance of transformers in this domain.
- Googleãªã©ç±³ITã€1æœˆ1ä¸‡äººå‰Šæ¸›ã€€çµ„ç¹”ã‚¹ãƒªãƒ åŒ–ã§AIé›†ä¸­
	- https://www.nikkei.com/article/DGXZQOGN1757C0X10C24A1000000/
- DSPy lets you prototype LLM Programs like AlphaCodium
	- https://x.com/CShorten30/status/1751656468879708496?s=20
- LangGraph Financial Agent w/ Polygon
	- https://gist.github.com/virattt/4d764c427892ce9fdf4534209edfb1f4
	- LangGraphã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã£ã¦æ ªä¾¡ã‚’ã¨ã£ã¦ãã‚‹ç°¡å˜ãªä¾‹
- Ollamaã§ã€ Mistral-7B finetuned for function callingã€€ã‚’ã‚µãƒãƒ¼ãƒˆ
	- https://ollama.ai/calebfahlgren/natural-functions
-  çŸ¥è­˜0ã§ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ã¿ã‚‹ï¼å‚ã‚Œæµã—é…ä¿¡ã€ã‚´ãƒªãƒ©ã‚¸ã€‘
	- https://www.youtube.com/watch?v=C1yFEMDLddc
- MetaãŒã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°LLMã®CodeLlamaã®70Bç‰ˆã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ãƒªãƒªãƒ¼ã‚¹ã€‚
	- https://ai.meta.com/blog/code-llama-large-language-model-coding/?utm_source=twitter&utm_medium=organic_social&utm_campaign=codellama&utm_content=reply
	- HumanEvalã§GPT-4è¶…ãˆã—ãŸã‚‰ã—ã„ã€‚å…¥åŠ›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚‚100kã¾ã§è¡Œã‘ã‚‹ã‚‰ã—ã„
	- Metaã¯ä»¥å‰ã‹ã‚‰ã€ŒGPT-4ä¸¦ã®LLMã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã«ã™ã‚‹ã€ã¨äºˆå‘Šã—ã¦ã„ã¾ã—ãŸãŒï¼Œå¹´æ˜ã‘æ—©ã€…ï¼Œã¾ãšã¯ã‚³ãƒ¼ãƒ‰ç”Ÿæˆé ˜åŸŸã§ã‚„ã£ã¦ãã¾ã—ãŸ
	- https://labs.perplexity.ai/ ã§ãŸã‚ã›ã‚‹ã‚‰ã—ã„
-  Inverse Molecular Design with Multi-Conditional Diffusion Guidance
	- https://arxiv.org/abs/2401.13858
	- è¤‡æ•°ã®åˆ¶ç´„ä¸‹ã§ã®åˆ†å­ç”Ÿæˆã®è«–æ–‡
	- å¾“æ¥ã¯åˆæˆå¯èƒ½æ€§ã¨ã‚¬ã‚¹é€éæ€§ãªã©ï¼’ã¤ä»¥ä¸Šã®åˆ¶ç´„ã‚’æº€ãŸã™ã‚ˆã†ãªåˆ†å­ã‚’ï¼‘ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸãŒ 
	- åˆ¶ç´„ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸTransformerãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šä½åˆ†å­ãƒ»é«˜åˆ†å­å…±ã«ã†ã¾ãç”Ÿæˆã§ããŸãã†ã§ã™ã€‚
- ã‚¢ãƒªãƒãƒãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMä½¿ã£ã¦ä½œã£ãŸã‚¹ãƒãƒ›ã‚’æ“ä½œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Mobile-Agentã‚’ç™ºè¡¨
	- https://x.com/umiyuki_ai/status/1752183108873687439?s=20
- SliceGPT: Compress Large Language Models by Deleting Rows and Columns
	- https://arxiv.org/abs/2401.15024
	- Microsoftã¨ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ’å·¥ç§‘å¤§ã®ç ”ç©¶è€…ã«ã‚ˆã‚Šã€LLMã‚’ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆè¡Œã‚„åˆ—ã‚’å‰Šé™¤ï¼‰ã—ã¦è»½ãã™ã‚‹åŠ¹æœçš„ãªæ‰‹æ³•
	- å®Ÿé¨“ã§ã¯æœ€å¤§30%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šæ¸›ã—ã¤ã¤æ€§èƒ½ã®90%ä»¥ä¸Šã‚’ä¿ã¤ã“ã¨ãŒã§ããŸã¨
	- â– ææ¡ˆæ‰‹æ³• 
		- 1. ä¸»æˆåˆ†åˆ†æã‚’ç”¨ã„ã¦é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º 
		- 2. é‡è¦ã§ãªã„æƒ…å ±ã‚’å–ã‚Šé™¤ããŸã‚ã«è¡Œã‚„åˆ—ã‚’å‰Šæ¸› â†’ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã§å‹•ä½œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. OPT, LLAMA-2, Phi-2ã‚’å®Ÿé¨“å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã«è¨­å®š 
		- 2. HuggingFace Transformersã¨PyTorchã§å®Ÿè£… 
		- 3. ã„ãã¤ã‹ã®ã‚¹ãƒ©ã‚¤ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’åˆ†ã‘ã¦å®Ÿé¨“ 
		- 4. æœ€å¤§30%ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ãŒå®Ÿç¾ã—ãŸ 
		- 5. Llama 2ã¨Phi-2ãƒ¢ãƒ‡ãƒ«ã¯90%ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒ
- Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
	- https://arxiv.org/abs/2312.05934
	- Microsoftã‚ˆã‚Šã€ŒFine Tuningã¨RAGã®ã©ã¡ã‚‰ãŒé«˜ç²¾åº¦ã‹ï¼Ÿã€ã«ç­”ãˆãŸè«–æ–‡
	- æ—¢å­˜/æ–°è¦çŸ¥è­˜ã®ä¸¡æ–¹ã«ãŠã„ã¦RAGãŒè‰¯å¥½ãªçµæœã«ã€‚Fine Tuningã¯ç¶™ç¶šäº‹å‰å­¦ç¿’ã€è©•ä¾¡ã¯MMLUã‚’LM-Evaluation-Harnessã§å®Ÿæ–½ã€‚
- The Power of Noise: Redefining Retrieval for RAG System
	- https://arxiv.org/abs/2401.14887
	- LLMã«ãŠã‘ã‚‹RAGï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¾ã›ã‚‹ï¼‰ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã€Œç„¡é–¢ä¿‚ãªã€æ–‡æ›¸ã‚’æ··ãœãŸã»ã†ãŒæ¤œç´¢ç²¾åº¦ãŒä¸ŠãŒã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– ãªãœãã‚“ãªã“ã¨ãŒèµ·ã“ã‚‹ã®ã‹ 
		- 1. é–¢é€£æ€§ãŒé«˜ã„æ–‡æ›¸ã°ã‹ã‚Šã ã¨éå‰°é©åˆãŒèµ·ã“ã‚‹ 
		- 2. ç„¡é–¢ä¿‚æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹èƒ½åŠ›ãŒä¸ŠãŒ
- ä¸€æ˜¨æ—¥ãã‚‰ã„ã‹ã‚‰mistralã®æœ‰æ–™ç‰ˆã§ã‚ã‚‹mistral-medium(70Bã€MoEã§ã¯ãªã„)ã®é‡ã¿ãŒãƒªãƒ¼ã‚¯ã—ãŸã¨ã„ã†å™‚ãŒã‚ã‚‹
	- https://x.com/webbigdata/status/1752304557336801408?s=20
-  Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery
	- https://arxiv.org/abs/2309.14425
	- æ¾å°¾ç ”ã®RAã§å­¦éƒ¨2å¹´ç”Ÿã®ç™½å‚ç¿ èŒã•ã‚“ãŒä¸»è‘—ã—ãŸï¼ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ãªãŒã‚‰å¤±æ•—ã«ã‚‚æŸ”è»Ÿã«å¯¾å¿œã™ã‚‹å®¶åº­å†…ã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹ç ”ç©¶ãŒICRA2024ã«æ¡æŠã•ã‚Œã¾ã—ãŸ
- çªå¦‚ã¨ã—ã¦ç¾ã‚ŒãŸmiqu-70BãŒEQ-Bench ã§ã¯ 83.5 ã‚’ç²å¾—ã— (ãƒ­ãƒ¼ã‚«ãƒ«ã§è©•ä¾¡)ã€GPT-4ç³»ã«æ¬¡ãæ€§èƒ½ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜
	- https://x.com/N8Programs/status/1752441060133892503?s=20
	- ã©ã†ã‚‚ã€Mixtral 8x7Bã®é‡å­åŒ–ç‰ˆã®ãƒªãƒ¼ã‚¯ã ã£ãŸã‚‰ã—ã„
-  LangGraphã§å§‹ã‚ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
	- https://speakerdeck.com/peisuke/langgraphdeshi-merumarutiezientosisutemu
	- Function Callingã ã‘ã§å‰²ã¨ã‚ˆãå‹•ã„ã¦ã‚‹ã¨ã“ã‚ã‚‹ã‚“ã ã‘ã©ã€ã‚‚ã†å°‘ã—çµ±åˆã—ãŸãã¦SupervisorãŒå¿…è¦ãã†ãªãƒ•ãƒ­ãƒ¼ã‹ã‚‰è©¦ã—ã¦ã¿ã‚ˆã†ã‹ãª
- Mixtral8x7Bã®æ—¥æœ¬èªå¯¾å¿œLoraã®å­¦ç¿’å®Œäº†ã—ã¾ã—ãŸ
	- https://x.com/AiXsatoshi/status/1752509354849546417?s=20
	- æ¨™æº–ã®Mixtral8x7Bã§ã¯ã€å¿œç­”ã«å¤šè¨€èªé–“ã‚’è¡Œãæ¥ã™ã‚‹switchingãŒç™ºç”Ÿã—ã¾ã™ãŒã€æ”¹å–„ã—ã¦ã„ã¾ã™
	- æ±ç”¨æ€§èƒ½ãŒè½ã¡ã¦ã„ã‚‹å¯èƒ½æ€§ã‚ã‚‹ã®ã§ã€ã‚‚ã†å°‘ã—æ¤œè¨¼ã—ã¾ã™
- å­¦ç¿’æ¸ˆã¿ã® LLM ã‚’æŸã­ã¦ Mixture of Experts ã‚’ä½œã‚‹ãƒ†ã‚¯
	- https://zenn.dev/zaburo_ch/articles/88e35e5c80f974
	- Phixtralã®è©±ã®ç´¹ä»‹
	- ã€ŒPhi-2 ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã„ãã¤ã‹ä½¿ã£ã¦ Mixture of Experts (MoE) ã‚’ä½œã£ãŸã‚‰å˜ä½“ã‚ˆã‚Šã‚‚è‰¯ã„æ€§èƒ½ãŒé”æˆã§ãã¾ã—ãŸã€
	- **Few-shot ã§ Gating ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºã‚ã‚‹æ‰‹æ³•**ãŒä½¿ã‚ã‚Œã¦ã„ã¦é¢ç™½ã‹ã£ãŸ
	- Gating ã®è©±ã‚’å¿˜ã‚Œã‚Œã°ã€Œãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºã‚ã¦ MLP ä»¥å¤–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å…¨éƒ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚‚ã®ã‚’ã€MLP ã¯ MoE Layer ã«ç½®ãæ›ãˆã¦å„ãƒ¢ãƒ‡ãƒ«ã® MLP ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã†ã€ã¨ã„ã†æ–¹æ³•ã§ MoE ãƒ¢ãƒ‡ãƒ«ãŒä½œã‚Œãã†ã§ã™
	- å„ Expert ã«ã¤ã„ã¦ã€ãã® Expert ã‚’ä½¿ã†ã¨æœ‰åˆ©ã«ãªã‚Šãã†ãª Prompt (ä¾‹ãˆã° Code ã§ Fine-Tuning ã•ã‚ŒãŸ Expert ãªã‚‰ Code ã® Prompt) ã‚’ã„ãã¤ã‹ç”¨æ„ã—ã¦ã€ãã® Prompt ã‚’ forward ã—ãŸã¨ãã® hidden_state ã‚’ä½¿ã£ã¦ weâ€‹ ã‚’ä½œã‚ã†
	- Domain ã”ã¨ã« Expert ã‚’ä½¿ã„åˆ†ã‘ã¦ãã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹æ„Ÿã˜ã§ã™ã­
- CodeLlama-70Bã‚’PostgreSQLã®ç”Ÿæˆã«ç‰¹åŒ–ã•ã›ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€SQLCoder-70B
	- https://huggingface.co/defog/sqlcoder-70b-alpha
	- æ€§èƒ½è©•ä¾¡ã‚‚GPT-4ã«10ãƒã‚¤ãƒ³ãƒˆä»¥ä¸Šå·®ã‚’ã¤ã‘ã‚‹åœ§å€’çš„ãªå‹åˆ©ã§ã€ç‰¹åŒ–å‹ã®ã‚³ãƒ¼ãƒ‰ç”ŸæˆLLMã®å°é ­ã‚’äºˆæ„Ÿã•ã›ã‚‹ã‚ˆã†ãªãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’ç§˜ã‚ã¦ã„ã‚‹
- LLaVA-1.6ã®ãƒªãƒªãƒ¼ã‚¹ã€Gemini Proè¶Šãˆï¼Ÿ
	- https://x.com/imhaotian/status/1752621754273472927?s=20
	- https://llava-vl.github.io/blog/2024-01-30-llava-1-6/
	- improved reasoning, OCR, and world knowledge. It supports higher-res inputs, more tasks, and exceeds Gemini Pro on several benchmarks!
	- LLaVA-1.6ã€æ™®é€šã«ç”»åƒä¸­ã®å¹ãå‡ºã—ã‚’æ—¥æœ¬èªã§å–‹ã£ã¦ã„ã‚‹ã¨ã‹èªè­˜ã§ãã¦ã€Gemini Proè¶…ãˆã¯ä¼Šé”ã§ã¯ãªã„ãªã¨ãªã‚‹
- 700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼LLMã€ŒKARAKURI LMã€ã‚’ä¸€èˆ¬å…¬é–‹
	- https://karakuri.ai/seminar/news/karakuri-lm/
	- GPT-4ã‚’è©•ä¾¡è€…ã¨ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(MT-Bench-jp)ã§ã€å›½ç”£LLMã¨ã—ã¦ã¯1ä½ã®æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸ
	- https://lm.karakuri.cc/ ã§ãŠè©¦ã—
- è«–æ–‡ã€ŒRAG VS Fine-tuningã€ã‚’èª­ã‚€
	- https://zenn.dev/neoai/articles/e75b6f033a4fd9
- æ™®é€šã®äººãŒè³‡ç”£é‹ç”¨ã§99ç‚¹ã‚’å–ã‚‹æ–¹æ³•
	- https://hayatoito.github.io/2020/investing/
		- 1.  ç¢ºå®šæ‹ å‡ºå¹´é‡‘ (iDeCo ã¾ãŸã¯ ä¼æ¥­å‹ DCï¼‰ã‚’å§‹ã‚ã¾ã™ã€‚
		- 2.  æ–° NISA ã§ã¤ã¿ãŸã¦ã®è¨­å®šã‚’ã—ã¾ã™ã€‚
		- 3.  ã•ã‚‰ã«ä½™è£•ãŒã‚ã‚‹æ–¹ã¯ã€ç‰¹å®šå£åº§ã§ã¤ã¿ãŸã¦ã®è¨­å®šã‚’ã—ã¾ã™ã€‚
		- 4.  è³‡ç”£é‹ç”¨ã‚’å§‹ã‚ãŸç›´å¾Œã‚„ã€ã¾ã¨ã¾ã£ãŸè³‡é‡‘ã‚’ä¸€æ™‚çš„ã«å…¥æ‰‹ã—ãŸã¨ããªã©ã€ååˆ†ãªä½™å‰°è³‡é‡‘ï¼ˆç¾é‡‘ï¼‰ã‚’ã‚‚ã£ã¦ã„ã‚‹ã®ã§ã‚ã‚Œã°ã€è‡ªåˆ†ã®ãƒªã‚¹ã‚¯è¨±å®¹åº¦ã®ç¯„å›²å†…ã§ã€é©åˆ‡ãªå‰²åˆã®è³‡ç”£ã‚’  _ä¸€æ‹¬_  ã§æŠ•è³‡ã—ã¾ã™ã€‚è©³ã—ãã¯å¾Œè¿°ã®ã€Œã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
		- 5.  å®šæœŸçš„ã«ï¼ˆå¹´ã« 1 å›ã€ã‚ã‚‹ã„ã¯æ•°å¹´ã« 1 å›ï¼‰ã€ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦è¦‹ç›´ã—ã¾ã—ã‚‡ã†ã€‚
-  Self-supervised Learning: Generative or Contrastive
	- https://arxiv.org/abs/2006.08218
- Proactive Detection of Voice Cloning with Localized Watermarking
	- https://huggingface.co/papers/2401.17264
	- Meta presents Proactive Detection of Voice Cloning with Localized Watermarking
- ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã‚µã‚¤ãƒˆãªã©ã‹ã‚‰ä¸­å¤ã®RTX 3090ã‚’8å°ã‹ãé›†ã‚ã¦ãƒã‚·ãƒ³ã‚’æ§‹ç¯‰ã—ãŸäººã®ãŠè©±
	- https://www.kyleboddy.com/2024/01/28/building-deep-learning-machines-unorthodox-gpus/
- Google's AI Makes Stunning Progress with Logical Reasoning
	- https://www.youtube.com/watch?v=NrNjvIrCqII
- Microsoft and OpenAI are in talks to invest $100 million into Figure
	- https://x.com/AndrewCurran_/status/1752463084550262805?s=20
	- Figureã¯ã€äººå‹ãƒ­ãƒœãƒƒãƒˆã‚’é–‹ç™ºã™ã‚‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—
-  ReGAL: Refactoring Programs to Discover Generalizable Abstractions
	- https://huggingface.co/papers/2401.16467
- miqudev/miqu-1-70b
	- https://huggingface.co/miqudev/miqu-1-70b
	- ãˆã£ï¼ã€miquã£ã¦ãƒŸã‚¯ã®ã“ã¨ã ã£ãŸã®ã‹ã€‚
- H2O-Danube-1.8B Technical Report
	- https://arxiv.org/abs/2401.16818
	- Open-sources a high-competitive 1.8B LM trained on 1T tokens following the core principles of LLama 2 and Mistral
	- long context small LLM trained by a team of some of the best Kagglers in the world
	- ã©ã†ã‚‚å°è¦æ¨¡LLMã§Kagglerã«ã‚ˆã‚Štrainigã•ã‚ŒãŸã‚‚ï½
-  Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
	- https://arxiv.org/abs/2401.17263
	- Significantly improves robustness to held-out jailbreaks, reducing the attack success rate from 84% to 8.66% across 20 jailbreaks
- quantized CodeLlama 70b base model to 4-bit with MLX
	- https://huggingface.co/mlx-community/CodeLlama-70b-hf-4bit-MLX
	- you can now run this model on your Apple Silicon.
- StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis
	- https://arxiv.org/abs/2401.17093
- Memphis-CoT 3B
	- https://huggingface.co/euclaise/Memphis-CoT-3B
	- A small reasoning-focused model using a novel iterative contrastive finetuning procedure, trained on only human data, outperforming much larger human data models and similarly sized SFT models.
-  RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank
	- ICLR24 Spotlight: To train general-purpose SSL models, it's important to measure the quality of representations during training. But how can we do this w/o downstream labels? 
	- We propose a new label-free metric to eval SSL models, called Linear Discrimination Analysis Rank(LiDAR)
-  [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717v1)
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã€Œæ¨¡å€£ã€ã¯æœ‰ç”¨ã‹ï¼Ÿ
	- https://ai-scholar.tech/articles/chatgpt/Imitating-Proprietary-LLMs
	- æœ€æ–°ç ”ç©¶ã«ã‚ˆã‚Œã°ã€æ–°ã—ãé–‹ç™ºã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨¡å€£ã¯éå¸¸ã«é›£ã—ã„ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚å¾®èª¿æ•´ã«ã‚ˆã‚‹æ”¹å–„ãŒæœ‰åŠ¹ã§ãªãã€ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªçŸ¥è­˜ã¯ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„ã“ã¨ãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚  
	- ä¸­å°ä¼æ¥­ã‚„å¤§ä¼æ¥­ãŒåŒã˜åˆ©ç‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒé›£ã—ããªã‚Šã€ç‰¹ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ´»ã‹ã—ã¦èƒ½åŠ›å·®ã‚’ç”Ÿã‹ã™ä¼æ¥­ãŒç«¶äº‰ä¸Šã®å„ªä½æ€§ã‚’ç¯‰ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
	- æ–°ã—ã„æ‰‹æ³•ã‚„ãƒ‡ãƒ¼ã‚¿ã®å°å…¥ãŒé‡è¦ã§ã‚ã‚Šã€æŠ€è¡“çš„ãªåˆ¶ç´„ã«ã‚‚ç•™æ„ã™ã‚‹ã“ã¨ãŒæŒç¶šçš„ãªç™ºå±•ã«å¯„ä¸ã™ã‚‹ã§ã—ã‚‡ã†ã€‚
- Accelerating the Science of Language Models
	- https://allenai.org/olmo/olmo-paper.pdf
	- AllenAIã«ã‚ˆã‚‹Open Language Model (OLMo), a 7B parameter model.
	- There is also a smaller version of it, OLMo 1B.
- ãƒ–ãƒ©ã‚¦ã‚¶ã§Rubyã‚’å‹•ã‹ã™å¤¢
	- https://mametter.hatenablog.com/entry/2024/02/01/105413
	- å…ƒåŒåƒšã®é è—¤ã•ã‚“ã€é ‘å¼µã£ã¦ã‚‹ãªã€ã¿ã‚“ãªä½¿ã£ã¦ã‚ã’ã¦ï¼
- SEMSCORE: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity
	- https://arxiv.org/pdf/2401.17072.pdf
	- ã“ã‚Œã§Japanese MT-benchã‚„Elyza-tasksãŒæ¯å›GPT-4ã‚’ä½¿ã‚ãšã«è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Œã°å‰²ã¨å®‰ä¾¡ã§æ—¥æœ¬èªLLM leaderboardãŒä½œã‚Œãã†
- llamaindexã‚’ä½¿ã£ãŸã€ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªå¤‰æ›ã®è§£èª¬è¨˜äº‹
	- https://akash-mathur.medium.com/advanced-rag-query-augmentation-for-next-level-search-using-llamaindex-d362fed7ecc3
	-  Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex
	- ã‚¯ã‚¨ãƒªå¤‰æ›ã¯ã€ŒLLM ã¸ã®å…¥åŠ›ï¼ˆã‚¯ã‚¨ãƒªï¼‰ã‚’ã‚ˆã‚Šè‰¯ã„æƒ…å ±æŠ½å‡ºã‚’å¯èƒ½ã¨ã™ã‚‹è¡¨ç¾ã¸å¤‰æ›ã™ã‚‹ã€ã“ã¨ã§ï¼ŒRAG ã®è³ªã‚’é«˜ã‚ã‚‹æ‰‹æ³•
	- è¨˜äº‹å†…ã§ã¯ï¼Œä»£è¡¨çš„ãª 5 ã¤ã®æ‰‹æ³•ã‚’ code ã¤ãã§è§£èª¬
- Build Long-context RAG from scratch: Nomic Embeddings + Mistral
	- https://x.com/LangChainAI/status/1753149741599428926?s=20
	- nomic_ai has launched a new open source, long context embedding model:
		- 8k token context window (using RoPE)
		- Strong performance on several benchmarks 
		- API (and local support coming soon)
	- ãã—ã¦long contexã®RAGã‚’ã¤ãã‚‹ã«ã¯ã€
		- nomic_ai:new 8k context window embeddings
		- trychroma:vectorstoreã€€MistralAI-instruct 32k context window via  ollama
- Apple presents Can Large Language Models Understand Context
	- https://huggingface.co/papers/2402.0085
	- We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.
-  Grounded language acquisition through the eyes and ears of a single child
	- https://www.science.org/doi/10.1126/science.adi1374
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã»ã©å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ãªãã¦ã‚‚å­ä¾›ã¯è¨€èªã‚’ç²å¾—ã™ã‚‹ã€‚ãã‚Œã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§å†ç¾ã§ãã‚‹ã‹ç¢ºèªã™ã‚‹ãŸã‚ã€èµ¤ã¡ã‚ƒã‚“ï¼‘äººã®é ­ã«ç”Ÿå¾Œ 6ã€œ25 ãƒ¶æœˆã®é–“ã«éŒ²ç”»ç”¨ãƒ“ãƒ‡ã‚ªã‚’å»¶ã¹ 61 æ™‚é–“è£…ç€ã—ã¦éŸ³å£°ï½¥æ˜ åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã‚’æˆåŠŸã•ã›ãŸã¨ã™ã‚‹å ±å‘Š
- karakuri-lm-70b-chat-v0.1-gguf ã® q5_K_S ã‚’ ãƒ­ãƒ¼ã‚«ãƒ«ã§è©¦ã™ã€‚ã¨ã¦ã‚‚å„ªç§€ã€‚
	- https://x.com/npaka123/status/1753336604759118014?s=20
	-  Llama.cppã§5.82 token/s (M3 Max)
	- https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf
- è—å¤§ã®å™‚ã®ç”ŸæˆAIã®ã‚„ã¤ã€èª¬æ˜ã¨ã‹è¦‹ãŸã‚‰æ€ã£ãŸæ•°ç™¾å€æ‰‹ã®è¾¼ã‚“ã ã“ã¨ã‚„ã£ã¦ã¦ã™ã’ã‡ã£ã¦ãªã£ãŸã€‚ç”ŸæˆAIã®è‰¯ã„ä½¿ã„æ–¹ã£ã™ã­ã€‚
	- https://x.com/413s9/status/1753300577516433830?s=20
	- è—å¤§ã®AIã‚¢ãƒ‹ãƒ¡ã€KALINã•ã‚“ã®æ³¨é‡ˆã‚’èª­ã‚€é™ã‚Š 
		- ç‰©èªéƒ¨â†’chatgpt 
		- ç”»åƒç”Ÿæˆâ†’midjourney,nijijourney 
		- AIå‹•ç”»åŒ–â†’runway,pika 
		- ç”»åƒä¿®æ­£â†’photoshop ã¨æ›¸ã‹ã‚Œã¦ã„ãŸã¯ãšãªã®ã§ï¼ˆã‚¢ãƒ‹ãƒ¡éƒ¨ã¯ã‚¯ãƒªã‚¹ã‚¿ã‹ã‚‚ï¼Ÿï¼‰ 
	- åŸºæœ¬çš„ã«KALINã•ã‚“ã¯ãƒ­ãƒ¼ã‚«ãƒ«SDã‚’å€‹äººã§å‹•ã‹ã™ã¨ã„ã†ä½œæ¥­ã¯è¡Œã£ã¦ã„ãªã‹ã£ãŸã®ã ã‚ã†ã¨æ¨æ¸¬ã€‚
	- ä¾‹ã®AIã‚¢ãƒ‹ãƒ¡ã®ChatGPTã®ãƒ­ã‚°ã‚’ã–ã£ã¨çœºã‚ãŸãŒã€å®Œå…¨ã«GPT-3.5ã®ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ã‚’è¶…ãˆã¦ã„ã‚‹ãƒ¬ãƒ™ãƒ«ã§ä½¿ã„è¾¼ã‚“ã§ã„ã¦å‰²ã¨çµ¶å¥ã—ãŸ
		- https://chat.openai.com/share/a6f6052e-a22c-49aa-8847-9c7f12b011e0
-  A Prompt-Engineered Large Language Model, Deep Learning Workflow for Materials Classification
	- https://arxiv.org/abs/2401.17788
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ææ–™åˆ†é¡ã®è«–æ–‡
	- Geminã«ã‚ˆã‚Šææ–™æƒ…å ±ã‚’æŒ‡å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›ã—ã€æ•´ãˆãŸãƒ‡ãƒ¼ã‚¿ã§BERTã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€é‡‘å±ã‚¬ãƒ©ã‚¹ã«ãªã‚‹ã‹å¦ã‹ã‚’é«˜ç²¾åº¦ã«åˆ¤å®šã§ããŸãã†ã§ã™ã€‚
	- è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ãƒ«æ´»ç”¨ã€‚ç–ãªãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã†ã¾ãäºˆæ¸¬ã§ãã‚‹ç‚¹ãŒãƒ¡ãƒªãƒƒãƒˆã®ã‚ˆã†ã§ã™
- Build a RAG backend over any website in a single CLI command
	- https://github.com/run-llama/LlamaIndexTS/tree/main/packages/create-llama
- ãƒªã‚³ãƒ¼ãŒLlama-2-13Bã‚’ãƒ™ãƒ¼ã‚¹ã«é«˜æ€§èƒ½ãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«Ricor-13Bã‚’é–‹ç™º
	- https://x.com/umiyuki_ai/status/1753312415503245762?s=20
	- ãŸã ã—ã‚ªãƒ¼ãƒ—ãƒ³ã«ã¯ã—ãªã„ã€‚é¡§å®¢ä¼æ¥­ã®æ¥­ç¨®ã«åˆã‚ã›ã¦ã‚«ã‚¹ã‚¿ãƒ ï¼ˆå¾®èª¿æ•´ãªã®ã‹ï¼ŸRAGãªã®ã‹ï¼Ÿï¼‰ã—ã¦ã‚¯ãƒ©ã‚¦ãƒ‰ã§æä¾›ã™ã‚‹B2Bãƒ“ã‚¸ãƒã‚¹ã‚’é–‹å§‹
- 2024å¹´1æœˆ30æ—¥ å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¸ã®å”åŠ›ã«ã¤ã„ã¦
	- https://www.ndl.go.jp/jp/news/fy2023/240130_01.html
	- å›½ä¼šå›³æ›¸é¤¨ã¯å›½å†…ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–äº‹æ¥­ã‚’ã‚„ã£ã¦ãŸã‘ã©ã€ã“ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã®æ•°åå„„ä»¶ã®URLã‚’å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ã«æä¾›ã™ã‚‹ã‚“ã ã£ã¦ã€‚å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€ã¯ã“ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ä½œã£ã¦LLMæ§‹ç¯‰ã«ä½¿ã†ã‚“
	- https://x.com/umiyuki_ai/status/1753651801688273040?s=20
-  KARAKURI LMã®è§£èª¬
	- https://medium.com/karakuri/karakuri-lm%E3%81%AE%E8%A7%A3%E8%AA%AC-4b6cf9c3d40f
	- KARAKURI LMã¯ã€Llama 2ã‚’åŸºã«é–‹ç™ºã—ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚  
	- æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã€æ—¥æœ¬èªã¨å¤šè¨€èªã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ··ãœã¦è¿½åŠ ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†ã“ã¨ã§ã€Llama 2ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’å¼·åŒ–ã—ã¦ã„ã¾ã™ã€‚
	- KARAKURI LM Chatã¯ã€KARAKURI LMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™
	- å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ç‹¬è‡ªã§é–‹ç™ºã—ãŸéå…¬é–‹ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··ãœã¦å­¦ç¿’ã•ã›ã¦ã„ã¾ã™ã€‚
- ã€Œãƒã‚¹ãƒˆï¼•ï¼§æƒ…å ±é€šä¿¡ã‚·ã‚¹ãƒ†ãƒ åŸºç›¤å¼·åŒ–ç ”ç©¶é–‹ç™ºäº‹æ¥­ï¼ãƒã‚¹ãƒˆï¼•ï¼§æƒ…å ±é€šä¿¡ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™ºã€
	- NEDOãŒå›½å†…ã®ç”ŸæˆAIã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®ãŸã‚ã«å®Ÿæ–½ã—
	- ABEJAã€Sakana AIã€NIIã€ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ã€Turingã€æ±äº¬å¤§å­¦ã€Preferred Elements
	- Preferred Elementsï¼ˆPFEï¼‰ãŒã€çµŒç”£çœã¨NEDOãŒé–‹å§‹ã™ã‚‹ã€ŒGENIACï¼ˆGenerative AI Accelerator Challengeï¼‰ã€ã«ãŠã„ã¦ã€1000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨ã€1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã®æ¤œè¨¼ã‚’é–‹å§‹ã—ã¾ã™ã€‚
	- æ±å¤§æ¾å°¾ç ”ã€ NEDOã®æ¡æŠã‚’å—ã‘ã€å…¬é–‹å‹ã§ã®500å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚’é–‹å§‹ã—ã¾ã™ã€‚
- A decoder-only foundation model for time-series forecasting
	- https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html
	- TimesFM is a forecasting model, pre-trained on a large time-series corpus of 100 billion real world time-points, that displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities.
	- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§æ™‚ç³»åˆ—äºˆæ¸¬ï¼Ÿï¼ŸgoogleãŒã‚‚ã¤å¤§é‡ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹ã«ã‹ãå­¦ç¿’ï¼Ÿï¼Ÿ
	- ã‚ã‚‰ã‚†ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’Decoder-onlyã®ãƒ¢ãƒ‡ãƒ«ã«ã¶ã£è¾¼ã‚“ã§æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ä½œã‚‹è©±
- Ollama vision is here
	- https://x.com/ollama/status/1753530905069748506?s=20
- Googleã¨OpenAIã¯ã€Œå¾Œå‡ºã—ã‚¸ãƒ£ãƒ³ã‚±ãƒ³ã—ãŸã‚‚ã‚“å‹ã¡ã€ã‚’ç‹™ã£ã¦è† ç€çŠ¶æ…‹ï¼Ÿ
	- https://x.com/ImAI_Eruel/status/1753389879965429892?s=20
	- æœ€è¿‘AIç•ŒéšˆãŒå¦™ã«é™ã‹ã ã¨è¨€ã‚ã‚Œã¦ã‚‹ã‚„ã¤ï¼ŒGoogleã¨OpenAIãŒäº’ã„ã«ï¼Œã€ŒGemini Ultraã€ã¨ã€ŒGPT-4.5 or GPT-5ã€ã¨è¨€ã†åˆ‡ã‚Šæœ­ãŒæ—¢ã«ã»ã¼å…¬é–‹å¯èƒ½ãªçŠ¶æ…‹ãªã“ã¨ã‚’å®£è¨€ã—ã¦ã„ã¦ï¼Œä»Šã¾ã§ã®çµŒéã‚’è¦‹ã‚‹ã¨å¾Œã‹ã‚‰å…¬é–‹ã—ãŸæ–¹ãŒå¤©ä¸‹ã‚’å–ã£ã¦ã‚‹
-  karakuri-lm-70b-chatã‚’OpenAIäº’æ›ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒã¨ã—ã¦å‹•ã‹ã—ã¦ã¿ãŸ
	- https://qiita.com/takaaki_inada/items/3a22b982a3541e6f214c?utm_campaign=post_article&utm_medium=twitter&utm_source=twitter_share
	- karakuri-lm-70b-chatã®4bité‡å­ç‰ˆggufã‚’ãƒ­ãƒ¼ã‚«ãƒ«PCã§å‹•ã‹ã—ã¦ã¿ãŸ
	- json formatå‡ºåŠ›ãŒå‡ºæ¥ãŸã‚Šã€å°‘ã—è¤‡é›‘ãªsystem promptã‚‚åŠ¹ã„ã¦ãã‚Œã¦è‰¯ã„
	- text-generation-webui ã§OpenAIäº’æ›ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒã¨ã—ã¦èµ·å‹•
	- GPUãƒ¡ãƒ¢ãƒªã«48/81ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†ãƒ¢ãƒ‡ãƒ«ã‚’ã®ã›ã¦ã‚µãƒ¼ãƒã‚’èµ·å‹•
-  WSL2ã¨llama.cppã§KARAKURI MLã‚’è©¦ã—ã¦ã¿ã‚‹
	- https://note.com/ngc_shj/n/n46ced665b378?sub_rt=share_h
- Corrective Retrieval Augmented Generation
	- https://arxiv.org/abs/2401.15884
	- Googleãªã©ã®ç ”ç©¶è€…ã‚‰ã¯ã€LLMã®æ¤œç´¢ã«ãŠã‘ã‚‹æ­£ç¢ºæ€§ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆã€CRAGã€ï¼‰ã‚’ææ¡ˆ
	- æ¤œç´¢çµæœã‚’æ¤œè¨¼ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’å°å…¥ã™ã‚‹æ‰‹æ³•ã§
- Hugging FaceãŒGPT Storeã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ç‰ˆï¼ˆAssistantã¨ã„ã†å‘¼ç§°ï¼‰ã‚’ãƒªãƒªãƒ¼ã‚¹
	- https://x.com/ytiskw/status/1753600673063784789?s=20
- Metaã€H100ã‚’35ä¸‡æ©Ÿãã‚ãˆã€OSSè²¢çŒ®ã«å¤§ããèˆµã‚’åˆ‡ã‚‹ã¨å…¬è¡¨ã€æ ªä¾¡ã¯ï¼’ï¼ï¼…ã‚¢ãƒƒãƒ—ï¼Ÿ
	- AI at Meta: 350k H100s by the end of the year, open source AI software infrastructure, new data centers with custom chips for AI inference serving hundreds of millions of users of AI tools.
	- https://x.com/ylecun/status/1753431180861419947?s=20
	- https://x.com/AIatMeta/status/1753195225311563848?s=20
-  Llama.cpp ã§ Karakuri LM ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n582c88a157e2?sub_rt=share_h
- Gemini Ultra 2/7ã«ãƒªãƒªãƒ¼ã‚¹ã®å¯èƒ½æ€§ã€‚
	- https://www.reddit.com/r/Bard/comments/1ahmsnf/advanced/
	- BardãŒGeminiã¨ã„ã†åå‰ã«å¤‰æ›´
	- Gemini Ultra 1.0ã§ã‚ã‚‹Gemini Advanced ãŒé–‹æ”¾ 
	- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ç­‰ã¯ä»Šå¾Œæ‹¡å¼µäºˆå®š 
	- GeminiãŒã‚¹ãƒãƒ›ã§ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚


## 1/29

ä¸­å›½ã‚ªãƒªã‚ªãƒ³ã‚¹ã‚¿ãƒ¼ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ï¼ˆOrionStarï¼‰ã¨ã„ã†ä¼šç¤¾ã‹ã‚‰æ–°æ˜ŸLLMã§ã‚ã‚‹Orionç™»å ´ã€æ—¥æœ¬èªã‚„éŸ“å›½èªãŒå¾—æ„ãªã®ã¨é•·æ–‡ãƒ¢ãƒ‡ãƒ«ã‚’æŒã£ã¦ã„ã‚‹ã€ä¸­è¯LLMã¯æ—¥æœ¬èªã‚‚å¾—æ„ã£ã¦ã®ã¯ã‚ˆãè¨€ã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã€æ¨è«–é«˜é€Ÿã§å›ç­”ã‚‚è‡ªç„¶ã§è‰¯ã„æ„Ÿã˜ã ãã†ã ã€‚LLMã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚‚ã€RLHFã«ä»£ã‚ã£ã¦ã€å—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã¤ã‹ã£ãŸã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®è‡ªå‹•åŒ–DPOãŒã¯ã‚„ã£ã¦ããŸã€Metaã®æœ¬å®¶ã¨ã¯é•ã†DPOã®å®Ÿè£…ã‚‚å‡ºã¦ããŸã—ã€CALM2ã‚’DPOã—ãŸãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¬é–‹ãªã©ã‚‚ã‚ã£ãŸã€‚DPOã«å¿…è¦ãªå—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªä½“ã®æ§‹ç¯‰æ”¯æ´KTOãªã©ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆé–¢ä¿‚ã®é€²æ—ãŒç›®ç«‹ã¤ã€‚MoEã®æ§‹ç¯‰ã‚‚Colabã®ç„¡æ–™æ ã§å®Ÿç¾ã™ã‚‹äº‹ä¾‹ãŒå‡ºã¦ããŸã€Sparseæ€§ãŒãƒã‚¤ãƒ³ãƒˆãªã®ã‹ã€‚æ—¢å­˜ã®LLMã‚’èåˆã•ã›ã¦å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã€ŒçŸ¥è­˜èåˆã€ã£ã¦ã®ãŒå‡ºã¦ããŸã€åˆä½“ã¨ã„ã†ã‚ˆã‚Šã€ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨è’¸ç•™ã«è¿‘ã„æ„Ÿã˜ã‚‰ã—ã„ã€‚LLMã®ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ã€1)Synthetic training dataã€2)LLM safetyã€3)Knowledge injectionã®ï¼“ã¤ã ãã†ã ã€‚Phi-2ã£ã¦1)Synthetic training dataãŒç‰¹å¾´ã‹ã¨ãŠã‚‚ã£ã¦ãŸã®ã«ã€3)Knowledge injectionãŒã†ã¾ãå‹•ã„ãŸä¾‹ã§ã‚‚ã‚ã‚‹ã®ã­ã€‚DPOã¯ã‚‚ã¡ã‚ã‚“ã€2)LLM safetyã¨é–¢ä¿‚ã‚ã‚‹ã€‚AIãŒè‡ªåˆ†è‡ªèº«ã«å ±é…¬ã‚’ä¸ãˆã¦é€²åŒ–ã™ã‚‹ã€Œè‡ªå·±å ±é…¬å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ãŒã€ç¹°ã‚Šè¿”ã—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦æ”¹å–„ã•ã‚Œã‚‹ã¨ã®ã“ã¨ã€è‡ªçµ¦è‡ªè¶³ãƒ¢ãƒ‡ãƒ«ã‹ã€‚åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã€é ˜åŸŸã‚’çµã£ãŸãƒ¢ãƒ‡ãƒ«ãŒé«˜æ€§èƒ½ã§ã‚ã£ãŸã‚Šã€Q&Aã‚¿ã‚¹ã‚¯ã«çµã£ã¦llama2ã‚’ï¼’æ®µéšã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€GPT-4ã«è¿«ã‚‹çµæœãªã©ã€ãã‚Œã¯ãã†ã ãŒãã‚Œã‚’ç¢ºã‹ã‚ãŸã®ãŒå°Šã„ã€‚LeCunå…ˆç”Ÿã«ã‚ˆã‚‹ã¨ã€DGNNã®è«–æ–‡ã§ã©ã“ã«ã‚‚æŠ•ç¨¿ã—ã¦ãªã‹ã£ãŸã®ã‹ã€‚ã€‚OpenAIã® æ–°ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ  ã¨ APIã®æ›´æ–°ã‚‚ã‚ã‚Šã¾ã—ãŸã€å®‰ããªã£ã¦æ€§èƒ½ãŒä¸ŠãŒã‚‹ã€OpenAIã¡ã‚ƒã‚“ã¨ä»•äº‹ã—ã¦ã¾ã™ã­ã€‚LangGraphã£ã¦LCELã®æ‹¡å¼µã ã£ãŸã®ã‹ã€ã‚¢ãƒ­ãƒ¼è¨€èªã¨ã‹ãã†ã„ã†ã®ã«è¿‘ã„ã®ã‹ã‚‚ã€‚HuggingFaceã¨Googleã®ãƒ‘ãƒ¼ãƒˆãƒŠã‚·ãƒƒãƒ—ã€Colabç’°å¢ƒã¨ã‚ˆã‚Šå¯†ã«ãªã‚ŠAIã®æ°‘ä¸»åŒ–çš„ã«ã¯æœ—å ±ãªã‚ã‘ã§ã™ãŒã€Googleä½•ã‚’ç‹™ã£ã¦ã„ã‚‹ï¼ŸT4ã®æ™®åŠï¼Ÿï¼ŸMacã§LLMã®åˆ©ç”¨ã‚‚ç€å®Ÿã«é€²æ­©ã€MLXã§Xwin-70Bã®ggufãŒå‹•ãã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚‚ColBERTã¨ã„ã†æ–°æ‰‹ãŒã‚ã‚‹ã®ã‹ï¼Ÿtrasformerã‚‚v4.37ã§Qwen2, Phi-2, SigLIPãªã©ãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚

- Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine
	- https://arxiv.org/abs/2311.16452
	- Microsoftã‚ˆã‚Šã€ŒGPT-4ç­‰ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã€é ˜åŸŸã‚’çµã£ãŸãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒãã®é ˜åŸŸã§é«˜æ€§èƒ½ãªã®ã§ã¯ãªã„ã‹ï¼Ÿã€ã‚’èª¿ã¹ãŸè«–æ–‡ã€‚çµæœã€åŒ»ç™‚ã®å•é¡Œã§GPT-4ãŒMed-PaLM2ã‚’ä¸Šå›ã‚‹çµæœã«
- Orion-14B
	- https://github.com/OrionStarAI/Orion
	- ä¸­å›½ç™ºLLMã®æ–°æ˜Ÿ
	- 2.5Tå­¦ç¿’ã€æ—¥æœ¬èª100Bäº‹å‰å­¦ç¿’æ¸ˆ èªå½™ã‚µã‚¤ã‚ºã¯84,608 
	- é•·æ–‡ãƒ¢ãƒ‡ãƒ«ã¯ã€200k-320kå¯¾å¿œ RAGã€
	- function callingå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨æ„
- transformer v4.37
	- https://github.com/huggingface/transformers/releases/tag/v4.37.0
	- Release v4.37 Qwen2, Phi-2, SigLIP, ViP-LLaVA, Fast2SpeechConformer, 4-bit serialization, Whisper longform generation Â· huggingface/transformers Â· GitHub
-  DPO ã«ã‚ˆã‚‹LLMã®Preferenceãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° by npakaã•ã‚“
	- https://note.com/npaka/n/n8be32e899c8a?sub_rt=share_b
	- ã€ŒDPOã€(Direct Preference Optimization)ã€ã€ŒIPOã€(Identity Preference Optimization)ã€ã€ŒKTOã€(Kahneman-Taversky Optimization) ã¨ã„ã†3ã¤ã®æœ‰æœ›ãªLLMã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©•ä¾¡
	- DPO
		- ã€Œ**DPO**ã€ã¯LLMã‚’äººé–“ã¾ãŸã¯AIã®å¥½ã¿ã«åˆã‚ã›ã‚‹ãŸã‚ã®æœ‰æœ›ãªä»£æ›¿æ‰‹æ®µã¨ã—ã¦æµ®ä¸Šã—ã¦ã„ã¾ã™ã€‚ã€Œå¼·åŒ–å­¦ç¿’ã€ã«åŸºã¥ãå¾“æ¥ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã¯ç•°ãªã‚Šã€ã€ŒDPOã€ã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®å®šå¼åŒ–ã‚’ã€å—œå¥½ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã§ç›´æ¥æœ€é©åŒ–ã§ãã‚‹å˜ç´”ãªæå¤±é–¢æ•°ã¨ã—ã¦å†æ§‹æˆã—ã¾ã™ã€‚
		- ã“ã‚Œã«ã‚ˆã‚Šã€ã€ŒDPOã€ã¯ä½¿ã„ã‚„ã™ããªã‚Šã€ã€ŒZephyrã€ã‚„ã€ŒNeuralChatã€ãªã©ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§æˆåŠŸã—ã¦ã„ã¾ã™ã€‚
	- IPO
		- ã€ŒDPOã€ã®æ¬ ç‚¹ã®1ã¤ã¯ã€å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã™ãã«éå‰°é©åˆã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã‚’å›é¿ã™ã‚‹ãŸã‚ã«ã€ã€ŒGoogle DeepMindã€ã¯ã€ŒIPOã€ã‚’å°å…¥ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã€ŒDPOã€æå¤±ã«æ­£å‰‡åŒ–é …ãŒè¿½åŠ ã•ã‚Œã€æ—©æœŸåœæ­¢ãªã©ã®ãƒˆãƒªãƒƒã‚¯ã‚’å¿…è¦ã¨ã›ãšã«ãƒ¢ãƒ‡ãƒ«ã‚’åæŸã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
	- KTO
		- ContextualAIã¯æœ€è¿‘ã€ã€ŒKTOã€ã¨å‘¼ã°ã‚Œã‚‹èˆˆå‘³æ·±ã„ä»£æ›¿æ¡ˆã‚’ææ¡ˆã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ã€Œgoodã€ã¾ãŸã¯ã€Œbadã€ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸå€‹ã€…ã®ä¾‹ã«é–¢ã—ã¦æå¤±é–¢æ•°ã‚’å®Œå…¨ã«å®šç¾©ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ©ãƒ™ãƒ«ã¯å–å¾—ã™ã‚‹ã®ãŒã¯ã‚‹ã‹ã«ç°¡å˜ã§ã‚ã‚Šã€ã€ŒKTOã€ã¯æœ¬ç•ªç’°å¢ƒã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ç¶™ç¶šçš„ã«æ›´æ–°ã™ã‚‹æœ‰æœ›ãªæ–¹æ³•ã«ãªã‚Šã¾ã™ã€‚
- LLMã®RLHFâ†’DPOâ†’KTOã£ã¦ãƒˆãƒ¬ãƒ³ãƒ‰ã®æµã‚Œã‚’æŠ‘ãˆã‚ˆã† by ã†ã¿ã‚†ã
	- https://x.com/umiyuki_ai/status/1749670491227672797?s=20
	- ã‚ªãƒ¼ãƒ—ãƒ³LLMã¯ãã‚“ãªé‡‘ã‹ã‘ã¦RLHFã‚„ã‚‹ãªã‚“ã¦ç„¡ç†ã ã£ãŸã€‚ãã“ã§ç™ºæ˜ã•ã‚ŒãŸã®ãŒDPOã ã€‚
	- DPOã¯äººåŠ›ã§è©•ä¾¡ã™ã‚‹å¿…è¦ãŒç„¡ã„ã‹ã‚‰ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‰ãªã„ã€‚ä»£ã‚ã‚Šã«â€å—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆâ€ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚å—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã£ã¦ã®ã¯ã€ã‚ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸæ™‚ã®äºŒã¤ã®å›ç­”ãŒã‚ã£ã¦ã€ã“ã£ã¡ã®å›ç­”ã®æ–¹ãŒã‚¤ã‚±ã¦ã¦ã€ã“ã£ã¡ã®æ–¹ãŒè‰¯ããªã„ã€‚ã¿ãŸã„ãªãƒ‡ãƒ¼ã‚¿ãŒå¤§é‡ã«ç”¨æ„ã•ã‚Œã¦ã‚‹ãƒ¢ãƒã€‚RLHFã¨DPOã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹äº‹ãŒã‚­ãƒƒãƒãƒªè¨¼æ˜ã•ã‚Œã¦ã‚‹ã€‚
	- å—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‹è¨€ã‚ã‚Œã¦ã‚‚ã€ãã‚“ãªã‚‚ã‚“ç”¨æ„ã™ã‚‹ã®ã ã£ã¦ã¾ã ã¾ã æ‰‹é–“ãŒã‹ã‹ã£ã¦å¤§å¤‰ã ã€‚ãã†ã„ã†ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œã‚’ã©ã†ã«ã‹ã™ã‚‹æ–°ã—ã„ãƒ†ã‚¯ãŒKTOã€‚KTOã§ã¯å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”ãŒã‚ã£ã¦ã€ãã®å›ç­”ã«ã€Œã„ã„ã­ã€ã‹ã€Œã‚ˆããªã„ã­ã€ã®è©•ä¾¡ã ã‘ä»˜ã„ã¦ã‚Œã°ã„ã„ã€‚
	- KTOã«ã‚ˆã£ã¦LLMã®ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆä½œæ¥­ã¯ç›¸å½“ç°¡å˜ã«ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ããŸã‚ã‘ã ã€‚ãŸã ã€ãã†ã‚„ã£ã¦ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒæ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€ã‚„ã£ã±KTOã‚ˆã‚ŠDPOã®æ–¹ãŒã‚„ã‚„é«˜æ€§èƒ½ã¿ãŸã„ã 
- GoogleDeepmindãŒSpatialVLMã‚’ç™ºè¡¨
	- ã¾ã§ã®è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã¯ç©ºé–“æ„Ÿè¦šã«æ¬ ã‘ã¦ã„ãŸã€‚ä¾‹ãˆã°ã€Œå†™çœŸã«å†™ã£ã¦ã‚‹ãƒãƒƒã‚¿ãƒ¼ã¨å¯©åˆ¤ã®è·é›¢ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ï¼Ÿã€ã¨ã‹è¨Šã„ã¦ã‚‚ç­”ãˆã‚‰ã‚Œã‚“ã‹ã£ãŸã€‚ãã‚Œã‚’æ”¹å–„ã—ãŸã®ãŒSpatialVLMã€‚
- makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch
	- https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch
	- Colabã‚‚å…¬é–‹ã—ã¦ãã‚Œã¦ã„ã‚‹ã®ã§ç„¡æ–™ç‰ˆColabã®T4ã§ã‚‚å‹•ã‹ã›ã¾ã™ã€‚max_itersã‚’500ãã‚‰ã„ã«ä¿®æ­£ã™ã‚Œã°æ‰€è¦æ™‚é–“ã‚‚10åˆ†ç¨‹åº¦
	- ãŸã ã—ã€æœ€å¾Œã‹ã‚‰3ç•ªç›®ã®ã‚»ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¦ä¿®æ­£ 
		- metrics = {"train_loss": losses['train'], "val_loss": losses['val']} ã€€
		- â†“ metrics = {"train_loss": float(losses['train']), "val_loss": float(losses['val'])}
- æ·±å±¤å­¦ç¿’ã®åŸç†ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ç†è«–ã®è©¦ã¿ã€€by ä»Šæ³‰ã•ã‚“
	- https://drive.google.com/file/d/1bNN6VjsgdpJAqxvZ4EKAPpMGq9wfjHqf/view
	- ã€Œãªãœæ·±å±¤å­¦ç¿’ã§ã†ã¾ãã„ãã®ã‹ã€ã¨ã„ã†ç´ æœ´ãªç–‘å•ã«å¯¾ã—ã€ç†è«–çš„ã«ã‚ã‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’å¹³æ˜“ã«è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã€‚éå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚
- Orion-14B-Chat-Int4 ã‚’è©¦ã™ã€‚
	- https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4
	- https://note.com/npaka/n/nd5025f5f7ac1?sub_rt=share_h
	- æ¨è«–é«˜é€Ÿã§å›ç­”ã‚‚è‡ªç„¶ã§è‰¯ã„æ„Ÿã˜ã€‚ ãƒ­ãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆç”¨ã€RAGç”¨ã€Function Callingç”¨ãªã©ã‚‚ã‚ã‚‹
-  RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture
	- https://arxiv.org/abs/2401.08406
	- Microsoftã‚ˆã‚Šè¾²æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’ä¾‹ã«ã€LLMã§RAGã¨Fine-Tuningã‚’æ¯”è¼ƒåˆ†æã—ãŸè«–æ–‡ã€‚
	- æ¯”è¼ƒçµæœã®è¦ç´„ã¯è¡¨22-23ã®é€šã‚Š(å›³å¼•ç”¨)ã€‚åŒæ–¹ã®ä½¿ã„åˆ†ã‘ãƒã‚¤ãƒ³ãƒˆã¯è¡¨23ã®æœ€ä¸‹è¡Œã«ã‚ã‚Šã€‚
- AIãŒè‡ªåˆ†è‡ªèº«ã«å ±é…¬ã‚’ä¸ãˆã¦é€²åŒ–ã™ã‚‹ã€Œè‡ªå·±å ±é…¬å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã€€ç±³Metaãªã©ãŒé–‹ç™ºã€å®Ÿé¨“ã§GPT-4ã‚’ä¸Šå›ã‚‹ã€ç ”ç©¶ç´¹ä»‹ã€‘
	- https://levtech.jp/media/article/column/detail_374/
	- ã“ã®è¨“ç·´æ–¹æ³•ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®æŒ‡ç¤ºã«å¾“ã†èƒ½åŠ›ã¨å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°èƒ½åŠ›ãŒåå¾©ã”ã¨ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚
	- ãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªåˆ†ã®ç­”ãˆã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã¨åŒæ™‚ã«ã€è‡ªåˆ†è‡ªèº«ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚æ©Ÿèƒ½ã€‚é€šå¸¸ã¯å›ºå®šã•ã‚Œã¦ã„ã‚‹å ±é…¬ãƒ¢ãƒ‡ãƒ«ãŒã€ç¹°ã‚Šè¿”ã—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦æ”¹å–„ã•ã‚Œã‚‹ã€‚
	- **ã“ã‚Œã¯ã€äººé–“ãªã©ã®å¤–éƒ¨ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¸è¦ã«ã—ã€å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒè‡ªåˆ†è‡ªèº«ã‚’ã‚ˆã‚Šã‚ˆãæ”¹å–„ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€è‡ªå·±æ”¹å–„ã®å¥½å¾ªç’°ã‚’ç”Ÿã¿å‡ºã™ã€‚**
- Summarize gigantic JSON datasets in seconds with JSONalyze, our latest query engine: 
	- https://docs.llamaindex.ai/en/latest/examples/query_engine/JSONalyze_query_engine.html
	- JSONãŒå·¨å¤§ã«ãªã‚‹ã¨ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè†¨å¤§ã«ãªã‚‹ã®ã§ã€åœ§ç¸®ï¼Ÿã™ã‚‹ã‚‰ã—ã„
-  Self-Rewarding Language Model (wip)
	- https://github.com/lucidrains/self-rewarding-lm-pytorch
	- Metaã®DPOã®ã€ç‹¬ç«‹å®Ÿè£…ãŒç™»å ´ã‚‰ã—ã„
- ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆåŠã³å¯¾æˆ¦ç›¸æ‰‹ã®èƒ½åŠ›ã‚’è€ƒæ…®ã—ãŸãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ããƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®èƒ½åŠ›è©•ä¾¡æŒ‡æ¨™
	- https://www.jstage.jst.go.jp/article/jscswabun/36/2/36_99/_article/-char/ja/
	- æ»‹è³€å¤§æ™‚ä»£ã®å­¦ç”Ÿã‚„åŒåƒšãŸã¡ã¨æ›¸ã„ãŸãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«é¸æ‰‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã«é–¢ã™ã‚‹è«–æ–‡ãŒæ²è¼‰ã•ã‚ŒãŸã¨ã‚‰ã—ã„
-  WARM: On the Benefits of Weight Averaged Reward Models
	- https://huggingface.co/papers/2401.12187
	- Google Deepmind presents WARM
- GoogleColobã§å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(0.15B)ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã¿ã‚‹
	- https://ayousanz.hatenadiary.jp/entry/2024/01/23/225623
	- äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«(0.15B)ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸ ã¡ã‚ƒã‚“ã¨ä½¿ãˆã‚‹ãƒ¬ãƒ™ãƒ«ã«ã™ã‚‹ãŸã‚ã«ã¯ã€ç´„200å€ãã‚‰ã„ã‹ã‘ãªã„ã¨ã„ã‘ãªã„ã¿ãŸã„ã§ã™
- ChatGPTã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒ32kã«ãªã£ã¦ã‚‹ã‹ã‚‰é’ç©ºæ–‡åº«ã®å°èª¬ã¨ã‹ã‚’2ä¸‡æ–‡å­—ãã‚‰ã„ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦è‡ªåˆ†ã®ä»£ã‚ã‚Šã«èª­ã‚“ã§ã‚‚ã‚‰ã£ã¦å†…å®¹æ•™ãˆã¦ã‚‚ã‚‰ã†äº‹ã‚‚çµæ§‹ã§ãã‚‹ã€‚
	- ã¾ã‚Claudeãªã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·100kã ã‹ã‚‰ã‚‚ã£ã¨å¤§é‡ã®æ–‡ç« ã‚’ã¾ã¨ã‚ã¦èª­ã‚“ã§ã‚‚ã‚‰ãˆã‚‹
	- https://x.com/umiyuki_ai/status/1749775772850749556?s=20
- Knowledge Fusion of Large Language Models", ICLR 2024ã‚ˆã‚Š
	- https://arxiv.org/abs/2401.10491
	- æ—¢å­˜ã®LLMã‚’èåˆã•ã›ã¦å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æ‰‹æ³•ã€ŒçŸ¥è­˜èåˆã€ãŒé–‹ç™º
	- æ··åˆãƒ¢ãƒ‡ãƒ«ã‚’æå”±ã™ã‚‹"Blending Is All You Need"ã¨ã¯ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãƒ»è©•ä¾¡æ–¹æ³•ã¨ã‚‚ã«ç•°ãªã‚‹ç ”ç©¶ã§ã™
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. ã€ŒLlama-2ã€ã€ŒOpenLLaMAã€ã€ŒMPTã€ã‚’èåˆã—ã¦ã€ŒFUSELLMã€ã‚’ä½œæˆã—ãŸ 
		- 2. ä¸‹è¨˜ã‚¿ã‚¹ã‚¯ã‚’ä¸­å¿ƒã«é¡•è‘—ã«æ€§èƒ½ãŒå‘ä¸Šã—ãŸ - è«–ç† - å¸¸è­˜ - ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
- LLMã®ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ä»¥ä¸‹ã®ï¼“ã¤
	- https://x.com/cwolferesearch/status/1749867258107543615?s=20
	- (1) Synthetic training data:
		- [1]ã§ã¯ã€æœ€å…ˆç«¯ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€åˆæˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
		- [2]ã§ã¯ã€æ•°å­¦ã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã«å¯¾ã—ã¦åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ã«ç”Ÿæˆã—ã€æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒã§ãã€LLMã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
	- (2) LLM safety:
		- [3]ã®ç ”ç©¶ã§ã¯ã€LLMã«è¨“ç·´ã•ã‚ŒãŸãƒãƒƒã‚¯ãƒ‰ã‚¢æ”»æ’ƒã¯ã€åºƒç¯„ãªå®‰å…¨è¨“ç·´å¾Œã‚‚æŒç¶šã—ã€äººé–“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¬ºãã‚¹ãƒªãƒ¼ãƒ‘ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™
		- [4]ã§ã€é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã•ãˆã‚ã‚Œã°ã€å¤šãã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’çµŒãŸLLMã§ã‚ã£ã¦ã‚‚ã€ã»ã¼å…¨ã¦ã®LLMã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã‚‹ã“ã¨ã‚’å­¦ã³ã¾ã—ãŸã€‚
	- (3) Knowledge injection
		- [6]ã®è‘—è€…ã¯æ¤œç´¢æ‹¡å¼µä¸–ä»£ï¼ˆRAGï¼‰ã‚’ææ¡ˆã—ã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒçŸ¥è­˜é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
		- LIMA [7]ã¯ã€LLMã®ã»ã¼å…¨ã¦ã®çŸ¥è­˜ãŒäº‹å‰å­¦ç¿’ä¸­ã«å­¦ç¿’ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
		- Phi-1[8]ã¯ã€çŸ¥è­˜è±Šå¯ŒãªLLMãŒã€ã‚ˆã‚Šå°ã•ãªã€ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã¤ã¾ã‚Šæ•™ç§‘æ›¸ï¼‰ã«å¯¾ã—ã¦å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
-  Reading Analog Gauges
	- https://huggingface.co/spaces/Synanthropic/reading-analog-gauge
	- Simply Reading Analog Gauges â€“ GPT4, CogVLM Can't
	- This model reads analog dial gauge by detecting, applying perspective correction, and gauge reading. The model was build only with synthetic data (e.g. examples
- OpenAI GPT-4Vï¼ChatGPTï¼GPTs äººå·¥çŸ¥èƒ½ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å®Ÿè·µå…¥é–€
	- å¸ƒç•™å·ã•ã‚“ã®ã€æ–°åˆŠã€
	- https://wgn-obs.shop-pro.jp/?pid=179128392
	- æ˜¨å¹´11æœˆã®å¤§è¦æ¨¡ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆå¯¾å¿œã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚„GPTã‚¹ãƒˆã‚¢ãªã©ã®æ–°æ©Ÿèƒ½ã‚‚è§£èª¬ã—ã¦ã¾ã™ã€‚æŠ€è¡“ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒæ—©ã™ãã‚‹ã“ã¨ã‚‚ã‚ã‚Šã€PDFã®ã¿ã«ãªã‚Šã¾ã™ã€‚
- ChatQA: Building GPT-4 Level Conversational QA Models
	- https://arxiv.org/abs/2401.10225
	- GPT-4ãƒ¬ãƒ™ãƒ«ã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯æ€§èƒ½ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®Llama 2ã§å®Ÿç¾ã™ã‚‹æ–¹æ³•ãŒã€NVIDIAã‚ˆã‚Šç™ºè¡¨ã•ã‚Œã¾ã—ãŸã€‚
	- é•·æ–‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•ã„ã«ç­”ãˆã‚‹èƒ½åŠ›ã§GPT-3.5ã‚ˆã‚Šé¥ã‹ã«å‹ã‚‹çµæœãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– æ–¹æ³•è«– ä»¥ä¸‹ã®ã‚ˆã†ãªï¼’æ®µéšã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã† 
		- 1. æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ï¼ˆsupervised fine-tuningï¼‰ 
		- 2. æ–‡è„ˆå¼·åŒ–ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ï¼ˆcontext-enhanced instruction tuningï¼‰
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. Llama-2ã‚’èª¿æ•´ã—ã¦ã€ŒChatQAã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ãŸ 
		- 2. é•·æ–‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãQAã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—ãŸ 
		- 3. GPT-3.5ã®æ€§èƒ½ã‚’é¥ã‹ã«ä¸Šå›ã£ãŸ 4. GPT-4ã¨ã¯åŒç­‰ã¨è¨€ãˆã‚‹ãƒ¬ãƒ™ãƒ«ã ã£ãŸ
-  Prompt Engineering with Llama 2
	- https://github.com/facebookresearch/llama-recipes/blob/main/examples/Prompt_Engineering_with_Llama_2.ipynb?utm_source=twitter&utm_medium=organic_social&utm_campaign=llama&utm_content=video
	- Introducing 'Prompt Engineering with Llama 2' â€” an interactive guide covering prompt engineering & best practices for developers, researchers & enthusiasts working with large language models.
- Ollama Python and JavaScript libraries
	- https://ollama.ai/blog/python-javascript-libraries
	- Both libraries make it possible to integrate new and existing apps with Ollama in a few lines of code, and share the features and feel of the Ollama REST API.
- CALM2ã‚’Direct Preference Optimization (DPO)ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ« calm2-7b-chat-dpo ã‚’CC-BY 4.0ã§å…¬é–‹ã—ã¾ã—ãŸã€‚
	- https://huggingface.co/cyberagent/calm2-7b-chat-dpo-experimental
	- calm2-7b-chat-dpoã‚’ELYZA-tasks-100ã¨Japanese MT-Benchã§è©•ä¾¡ã‚’è¡Œã£ãŸã¨ã“ã‚ã€CALM2ã‚ˆã‚Šã‚‚æ›´ã«é«˜ã„ã‚¹ã‚³ã‚¢ãŒå¾—ã‚‰ã‚Œã‚‹ã¨ã„ã†çµæœã«ãªã‚Šã¾ã—ãŸ
	- ã¾ãŸã€ã‚ã‚ã›ã¦DPOã«ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’CC-BY 4.0ã§å…¬é–‹ã—ã¾ã—ãŸ
	- https://huggingface.co/datasets/cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental
- ä»Šæ›´ãªãŒã‚‰ï½¤GPT3.5ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ãŸï½¡ 
	- https://x.com/kanhatakeyama/status/1750331895853039745?s=20
	- guiã§æ“ä½œã§ãã‚‹ã—ï½¤gpuãƒã‚·ãƒ³ã‚’ç”¨æ„ã—ãªãã¦è‰¯ã„ã—ï½¤éå¸¸ã«ãŠæ‰‹è»½ãªå°è±¡ã§ã—ãŸï½¡ 3ä¸¦åˆ—ã¾ã§å­¦ç¿’å›ã›ã¾ã—ãŸï½¡
	- 2,3æ™‚é–“ã®ä½¿ç”¨ã§ï½¤$30ã»ã©ã‹ã‹ã‚Šã¾ã—ãŸï½¡
-  LLM ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¾ã¨ã‚ by npakaã•ã‚“
	- https://note.com/npaka/n/n686d987adfb1?sub_rt=share_b
- Hugging Face and Google partner for open AI collaboration
	- https://huggingface.co/blog/gcp-partnership
	- We will collaborate with Google to foster open AI innovation across open science, open-source, cloud, and hardware
	- A collaboration for Google Cloud customers
	- A collaboration for Hugging Face Hub users
-  OpenAIã® æ–°ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ  ã¨ APIã®æ›´æ–° by npakaã•ã‚“
	- https://note.com/npaka/n/nd8c5e9c65335?sub_rt=share_h
	- ãƒ»æ–°ã—ã„Embeddingãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ 
	- ãƒ»GPT-4 Turbo Previewã®æ›´æ–°
	- ãƒ»GPT-3.5 Turboã®æ›´æ–°
	- ãƒ»ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
	- ãƒ»APIã‚­ãƒ¼ã®ç®¡ç†æ–¹æ³•ã®æ”¹å–„
- å®Ÿã¯Swallowã¯baseãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®æ€§èƒ½ã¯ã„ã„ã§ã™ãŒã€instruct ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯public instruction datasetã‚’ä½¿ç”¨ã—ãŸã“ã¨ã‚‚ã‚ã‚Šã€baseãƒ¢ãƒ‡ãƒ«ã®é«˜ã„æ€§èƒ½ã®å‰²ã«ã¯ã‚ã¾ã‚Šé«˜ãã‚ã‚Šã¾ã›ã‚“
	- https://x.com/okoge_kaz/status/1750805452676608177?s=20
- CoTã®æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒLLMã®æ¨è«–èƒ½åŠ›ã«åŠã¼ã™å½±éŸ¿ã‚’è©³ç´°ã«æ¤œè¨¼ã—ãŸçµæœ
	- https://ai-data-base.com/archives/62364
	- GPT-4ãªã©ã®LLMã«æ€è€ƒã®é€£é–ï¼ˆCoTï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã©ã§ã€Œè€ƒãˆã‚‹æ™‚é–“ã€ã‚’ä¸ãˆã‚‹ã¨åŸºæœ¬çš„ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚ 
	- ãã“ã§ä»Šå›ã€é©åˆ‡ãªæ¨è«–ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒæ¤œè¨¼ã•ã‚Œã¾ã—ãŸã€‚ è¨˜äº‹ã§ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã¨ã¨ã‚‚ã«çµæœã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚
-  MambaByte: Token-free Selective State Space Model
	- https://arxiv.org/abs/2401.13660
	- MambaByteã¯ã€MambaãŒé•·ã„ç³»åˆ—ã‚‚æ‰±ãˆã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã›ãšãƒã‚¤ãƒˆå˜ä½ã§è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã€‚åŒç­‰ã®è¨ˆç®—é‡ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¸è¦ã®MegaByteã‚„é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–Transformerã¨æ¯”ã¹æ€§èƒ½ã§ä¸Šå›ã‚Šã€1/3ã®æŠ•å…¥è¨ˆç®—é‡ã§Transformerã®æå¤±ã«åˆ°é”ã€‚å°è¦æ¨¡å®Ÿé¨“ã®çµæœã ãŒæœ‰æœ›
-  Dense X Retrieval: What Retrieval Granularity Should We Use?
	- https://arxiv.org/abs/2312.06648
	- The "Dense X Retriever" paper shows that it significantly outperforms the traditional chunk-based retriever
-  Deep Convolutional Networks on Graph-Structured Data
	- https://arxiv.org/abs/1506.05163
	- My most-cited, never-accepted, ArXiv-only paper has over 1880 citations. "Deep Convolutional Networks on Graph-Structured Data" Mikael Henaff, Joan Bruna, Yann LeCun
-  FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design
	- https://huggingface.co/papers/2401.14112
	- Microsoft presents FP6-LLM 
	- Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design
	- Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality
- çŸ¥è­˜èåˆã€å›³ã‚’è¦‹ã‚‹ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚„Mixture of Expertsã¨ã¯é•ã£ã¦æœ¬å½“ã«çŸ¥è­˜ãã®ã‚‚ã®ã‚’æŠ½å‡ºã—ã¦ã„ã‚‹æ„Ÿã˜ã‹ã€‚ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨è’¸ç•™ã«è¿‘ã„æ„Ÿã˜ã‚‚ã‚ã‚Šç”»æœŸçš„ãªæ‰‹æ³•ã®ã‚ˆã†ã«æ€ãˆã‚‹ã€‚
	- https://x.com/koheiichi/status/1751060499310301550?s=20
- Python library that adds Generative AI capabilities to Pandas
	- https://github.com/gventuri/pandas-ai
	- Introducing PandasAI, now you can analyze complex data frames and plot visualizations just by using natural language
- XWin 70B ã§ LLM å‡ºåŠ›æ—¥æœ¬èªæ–‡ç« ã®è‡ªå‹•è©•ä¾¡ã‚’è¡Œã†è©¦ã¿
	- https://zenn.dev/syoyo/articles/4f4f8645af1cee
	- æ—¥æœ¬èª LLM ã®è‡ªå‹•è©•ä¾¡(ELYZAã¡ã‚ƒã‚“ task 100 ã¨ã‹)ã‚’ãƒ­ãƒ¼ã‚«ãƒ« LLM ã§è¡Œã„ãŸã„.ç¾æ™‚ç‚¹ã§æœ€é«˜æ€§èƒ½ã®ä¸€ã¤XWin 70B ã§ã®è©•ä¾¡è©¦ã—ã¾ã—ãŸ!
	- ãã“ãã“ã„ã„æ„Ÿã˜ã«ãªã£ãŸã‚ˆâœŠ
	- ã§ã‚‚ prompt ä¸Šæ‰‹ãä½œã‚‹å¿…è¦ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã‚ˆ
- åŒã˜ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ï¼ˆã“ã®å ´åˆã¯ã‚«ãƒ¼ãƒãƒ«ï¼‰ãŒç•°ãªã‚Œã°äºˆæ¸¬ãŒå¤‰ã‚ã‚‹ã¨ã„ã†è©±ã€‚ã“ã†ã„ã†ã“ã¨ã‚’è‰²ã€…å®Ÿç¾ã—ãŸã„å ´åˆã¯ã‚„ã£ã±ã‚Šã‚¬ã‚¦ã‚¹éç¨‹ãŒã‚„ã‚Šã‚„ã™ã„ã§ã™ã€‚ by é ˆå±±å…ˆç”Ÿ
	- https://x.com/sammy_suyama/status/1751104980189413880?s=20
-  Google Colab ã§ LangGraph ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n053a3cb78311?sub_rt=share_h
	- ã€Œ**LangGraph**ã€ã¯ã€LLMã§ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ãªã€Œ**ãƒãƒ«ãƒã‚¢ã‚¯ã‚¿ãƒ¼ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ã€ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ã€Œ**LCEL**ã€(LangChain Expression Language) ã‚’æ‹¡å¼µã—ã¦ã€è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ (ã¾ãŸã¯ã‚¢ã‚¯ã‚¿ãƒ¼) ã‚’è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚ãŸã£ã¦å¾ªç’°çš„ã«å”èª¿å‹•ä½œã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™
	- ã€ŒLangGraphã€ã«ã‚ˆã£ã¦ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«**ã‚µã‚¤ã‚¯ãƒ«**ã‚’ç°¡å˜ã«å°å…¥ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
- MLXã§Xwin-70Bã®ggufãŒå‹•ãã“ã¨ã‚’ç¢ºèª
	- https://x.com/npaka123/status/1751139720367862193?s=20
	- Apple M3 Max
-  google/siglip-base-patch16-256-multilingual ã‚’ä½¿ã£ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”»åƒã‚’æ—¥æœ¬èªã§æ¤œç´¢ã—ã¦ã¿
	- https://note.com/eurekachan/n/n9d4f62b80ad6?sub_rt=share_pb
	- 1æœˆã«ã€Googleã‹ã‚‰ã€SigLIPã¨ã„ã†ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã®multilingualç‰ˆï¼ˆå¤šè¨€èªå¯¾å¿œç‰ˆï¼‰ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚transformers 4.37ä»¥é™ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
-  Are Transformers Effective for Time Series Forecasting?
	- https://arxiv.org/abs/2205.13504
- ColBERT superior to traditional embedding models
	- https://x.com/marktenenholtz/status/1751406680535883869?s=20
	- ã‚¯ã‚¨ãƒªã¨æ–‡æ›¸ã‚’ãã‚Œãã‚Œåˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§åŸ‹ã‚è¾¼ã¿ã€ã‚¯ã‚¨ãƒªä¸­ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã¨æ–‡æ›¸ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã®é–“ã§æœ€å¤§é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ãã®ç·å’Œã‚’ã‚¹ã‚³ã‚¢ã¨ã—ã¦ã„ã¾ã™ã€‚
- miniature ColBERT model in your browser
	- https://colbert.aiserv.cloud/
- DSPy lets you prototype LLM Programs like AlphaCodium in 2 minutes!
	- https://x.com/CShorten30/status/1751656468879708496?s=20

## 1/22

ä»Šé€±ã¯Davosä¼šè­°ãŒã‚ã£ã¦ã€ã‚ã‚Œã‚‰ã®ã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã‚‚ç™»å ´ã€GPT-5ã«ã¤ã„ã¦è¨€åŠã€‚Metaã‹ã‚‰ã¯ã‚¶ãƒƒã‚«ãƒ¼ãƒãƒ¼ã‚°æ°ãŒãƒ“ãƒ‡ã‚ªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã„ããªã‚ŠLlama3ã®OSSã¨ã—ã¦ã®é–‹ç™ºå®£è¨€ã€‚NVIDIAãŒCESã§ç™ºè¡¨ã—ãŸGeForce RTX 4070 SUPERãŒç™ºå£²ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMç•Œéšˆã®ä¾¡æ ¼ç ´å£ŠãŒã€ã€ã€ã€‚MoEã‚‚ä»Šé€±ã‚‚ã«ãã‚„ã‹ã€åœ§ç¸®ã—ã¦å°ãƒ¡ãƒ¢ãƒªåŒ–ã™ã‚‹ã‚ˆã†ãªMC-SMoEã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã‹ã€è² è·åˆ†æ•£ã‚’èª¿æ•´ã™ã‚‹DeepSeekMoEã¨ã‹ã€youri-2x7bã®ggufãŒã§ãŸã‚Šã¨ã‹ã¨ã«ã‹ãè³‘ã‚„ã‹ã€‚å°è¦æ¨¡LLMå‘ã‘ã®äººå·¥çš„ã«ç”Ÿæˆã•ã‚ŒãŸå­¦ç¿’ç”¨ãƒ¢ãƒ‡ãƒ«tiny-textbookã‚·ãƒªãƒ¼ã‚ºã‚‚å……å®Ÿã—ã¦ãã¦ã€å°è¦æ¨¡LLMã®é–‹ç™ºã‚‚åŠ é€Ÿã™ã‚‹ã‹ãªã€‚æ‰‹ãŒå±Šãã¨ã“ã‚ã§ã¯nanoGPTã®æºæ°ç‰©èªã®é©ç”¨ä¾‹ã¯æ¥½ã—ãã†ã€‚å°è¦æ¨¡LLMã‚’é›†ã‚ã¦å„ªã‚ŒãŸAIã‚’ä½œã‚‹ã¨ã„ã†æ„å‘³ã§ã¯ã€sakana.aiãŒè¯ã€…ã—ã45å„„å††ã‚‚ã®æŠ•è³‡ã‚’èª¿é”ã€googleãªã©ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ç ”ç©¶è€…ãŒçµ‚çµã—ã¦æ¥½ã—ãã†ã€‚sakana.aiã¯å°ã•ãªé­šãŒé›†ã¾ã£ã¦ä¸€åŒ¹ã®å¤§é­šã®ã‚ˆã†ã«æ³³ãç‰©èªï½¢ã‚¹ã‚¤ãƒŸãƒ¼ï½£ã®ä»•çµ„ã¿ãªã‚ã‘ã ã‘ã©ã€å°ã•ãªå°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ã¤ã¾ã£ã¦å•é¡Œã‚’è§£æ±ºã™ã‚‹ã£ã¦ã“ã¨ãªã‚‰ã€å¤ã„äººã«ã¯ãƒŸãƒ³ã‚¹ã‚­ãƒ¼å¾¡å¤§ã®Society o MindsãŒæ€ã„å‡ºã•ã‚Œã‚‹ã€‚Microsoftã¯Coliplot Proã‚’ãƒªãƒªãƒ¼ã‚¹ã€æœˆ20ãƒ‰ãƒ«ã§ã€å€‹äººãŒã€GPT-4 Turboã«ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã—Office 365 Copilotã‚‚ä½¿ãˆã‚‹ã—ãŠå¾—ã‹ã‚‚ã€ä¸€æ–¹ãŒã£ã‹ã‚Šã—ãŸã¨ã„ã†ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„è¦‹ã‚‚ã¡ã‚‰ã»ã‚‰ã€‚ã§ã‚‚å°è¦æ¨¡LLMã®ä»£è¡¨æ ¼phi-2ã¯ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã‹ã‚‰ã§ã¦ã„ã‚‹ã‹ã‚‰ã€OpenAI/Copilotä¸€è¾ºå€’ã§ã¯å®Ÿã¯ãªã„ã€‚ä¸€æ–¹ãƒ¡ã‚¿ã¯ï¼’ä¸‡äººã‚’ãƒ¬ã‚¤ã‚ªãƒ•ã—ã¦ã€ä»£ã‚ã‚Šã«35ä¸‡å°ã®H100ã‚¤ãƒ³ãƒ•ãƒ©ã‚’æ•´ãˆLlama3ã®é–‹ç™ºã‚’æ¨é€²ã€‚ã©ã®ä¼šç¤¾ã‚‚LLMã¨ã„ã†ä¸ç¢ºå®Ÿãªè¦ç´ ï¼ˆç™ºå±•æ€§ã€ä»–ç¤¾ã¨ã®ç«¶äº‰ï¼‰ã«å‚™ãˆãªã‚‰ãŒç¶±æ¸¡ã‚Šçš„ãªä¼šç¤¾ã®é‹å–¶ã‚’ã—ã¦ã„ã‚‹ï¼ˆæ ªä¸»ã‹ã‚‰ã®æœŸå¾…ã«ã“ãŸãˆç¶šã‘ã¤ã¤è²¡å‹™çš„ã«ç ´ç¶»ã¯ã§ããªã„ï¼‰ã€‚å…±é€šãƒ†ã‚¹ãƒˆã«ã•ã£ããåŠã‚‹ã—ã®LLMã‚’é©ç”¨è©•ä¾¡ã—ãŸä¾‹ã§ã¯ã€GPT-4ãŒ6å‰²å¼·ç¨‹åº¦æ­£è§£ã§ãªã‚“ã¨ã‹äººé–“ã‚’ä¸Šå›ã‚‹ã‚‚ã€ç‰¹ã«æ•°å­¦ãŒãƒ€ãƒ¡ã¨ã„ã†çµæœãŒã€‚ä¸€æ–¹ã€æ•°å­¦ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã®ãƒ¡ãƒ€ãƒªã‚¹ãƒˆä¸¦ã¿ã®æ€§èƒ½ã‚’ç¤ºã™DeepMindã®AlphaGeometryã€LLMã¨ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãŒé«˜æ€§èƒ½ã®ç§˜è¨£ã‚‰ã—ã„ã€text_to_SQLã‚‚ã€ã¾ãŸé•ã£ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¨ã—ã¦é«˜æ€§èƒ½åŒ–ã®ãƒ’ãƒ³ãƒˆã«ãªã‚‹ã€‚ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã¨ã‹ã€ELYZAã®æ—¥æœ¬èªè¿½åŠ å­¦ç¿’ã§ã‚‚ã¨ã‚‚ã¨ã®è‹±èªã®èƒ½åŠ›ãŒè½ã¡ãªã„ã‹ã®æ¤œè¨¼ã¨ã‹ã€ç€å®Ÿãªå‹•ãã¯åœ°é“ã«ã™ã™ã‚“ã§ã„ã‚‹ã®ã‚’å¿˜ã‚Œãšã«ã„ãŸã„ã€‚

- HachiMLã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹youri-2x7b_v0.2ã®gguf ^aaa
	- https://huggingface.co/mmnga/HachiML-youri-2x7b_v0.2-gguf
	- This model is a Mixture of Experts (MoE) merger of the following two models:
	- [rinna/youri-7b-instruction](https://huggingface.co/rinna/youri-7b-instruction)
	- [rinna/youri-7b-chat](https://huggingface.co/rinna/youri-7b-chat)
- mambaã‚’åˆ†æ•£å­¦ç¿’ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
	- https://github.com/kotoba-tech/kotomamba
	- Transformerã‚’ä¸Šå›ã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹Mamba, State Spaceãƒ¢ãƒ‡ãƒ«ã®
	- Kotoba Techã§ã¯130m, 1.4B, 2.8B ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ã™ã§ã«è¡Œã£ã¦ã„ã¾ã™
- baobab-trees/wikipedia-human-retrieval-ja
	- https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja
	- çŸ­ã„è³ªå•æ–‡ã«å¯¾ã—ã¦Wikipediaã«æ›¸ã„ã¦ã‚ã‚‹æƒ…å ±ã®ã¿ã§å›ç­”ã•ã›ã‚‹ã€ã¨ã„ã†ã®ã‚’1000å•å‰å¾Œå®Ÿæ–½ã—ã€äººæ‰‹retrievalä»˜ãQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã—ãŸã€‚é€”ä¸­ã®éç¨‹ã‚„å¼•ç”¨ãªã©ã‚‚è¨˜éŒ²ã—ã¦ã„ã‚‹ã®ã§ã€äººé–“ã«ã‚ˆã‚‹æ¤œç´¢ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨ã—ãŸã‚Šã§ãã‚‹ã¨æ€ã„ã¾
- Copilot for Office 365
	- https://x.com/usutaku_com/status/1747119405702795383?s=20
-  how to build advanced QA over Tabular Data
	- llamaindexã‚ˆã‚Šã€
	- https://x.com/llama_index/status/1747289513934864493?s=20
	- Query Pipeline over Pandas DataFrames
	- https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html
	- This is a simple example that builds a query pipeline that can perform structured operations over a Pandas DataFrame to satisfy a user query, using LLMs to infer the set of operations.
	-  Query Pipeline for Advanced Text-to-SQL
	- https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html
- nanoGPTæ¥½ã—ã„ã€‚æºæ°ç‰©èªå…¨æ–‡ã§å­¦ç¿’ã•ã›ãŸã‚‰ä½•ã‹èªã‚Šã ã—ãŸğŸ¤— ã„ãšã‚Œã®ç´›ã‚Œã‚ã‚Šã‘ã‚‹ã‹ãª
	- https://github.com/karpathy/nanoGPT
	- The simplest, fastest repository for training/finetuning medium-sized GPTs
- ä¼æ¥­ã¯ãªãœæ±äº¬ã«é›†ä¸­ã™ã‚‹ã®ã‹â”€â”€çµŒæ¸ˆåœ°ç†å­¦ã®è¦–ç‚¹ã‹ã‚‰ï¼ˆæ—¥æœ¬åŠ´åƒç ”ç©¶é›‘èªŒï¼‰
	- https://www.jil.go.jp/institute/zassi/backnumber/2020/05/pdf/029-039.pdf
- æ±äº¬ç™ºãƒ»AIãƒ‰ãƒªãƒ¼ãƒ ãƒãƒ¼ãƒ ã€Œhttp://Sakana.aiã€ãŒ45å„„å††èª¿é”ã€€å…ƒGoogleãƒˆãƒƒãƒ—ç ”ç©¶è€…ã‚‰ãŒè¨­ç«‹ã€€AIæ¥­ç•Œã®è‘—åäººã‚„æ—¥æœ¬ã®å¤§æ‰‹ITä¼æ¥­ã‚‚å‡ºè³‡
	- https://sakana.ai/seed-round/
	- @tkasasagi ã•ã‚“ã‚‚å‚åŠ ã‹ãƒ¼
	- https://x.com/tkasasagi/status/1747267875021406329?s=20
	- ã€Œã‚µã‚«ãƒŠAIã€æ—¥ç±³ã§45å„„å††èª¿é”ã€€ã‚¹ã‚¤ãƒŸãƒ¼ã®ç™ºæƒ³ã§å·¨å¤§ITã«æŒ‘ã‚€
		- åŒç¤¾ã¯å¯¾è©±å‹AIã®åŸºç›¤æŠ€è¡“ã§ã‚ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®é–‹ç™ºã§ã€ä»–ç¤¾ãŒé–‹ç™ºã—ãŸå°ã•ãªAIã‚’ã„ãã¤ã‚‚ã¤ãªã„ã§ã€å·¨å¤§AIã«åŒ¹æ•µã™ã‚‹èƒ½åŠ›ã‚’ã‚‚ã¤ä»®æƒ³ã®AIãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æƒ³ã€‚ã“ã®æ–°æŠ€è¡“ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã¨å‘¼ã°ã‚Œã€é–‹ç™ºã‚³ã‚¹ãƒˆã‚’åŠ‡çš„ã«ä¸‹ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€å·¨é¡ãªè³‡é‡‘ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹AIé–‹ç™ºç«¶äº‰ã«ä¸€çŸ³ã‚’æŠ•ã˜ã‚‹ç‹™ã„ã ã€‚
- xverse/XVERSE-13B-256K
	- https://huggingface.co/xverse/XVERSE-13B-256K
	- ãƒ­ãƒ¼ã‚«ãƒ«LLMã®é•·æ–‡å¯¾å¿œãŒã¤ã„ã«256Kï¼ˆç´„25ä¸‡å­—ï¼‰
	- XVERSEã¯ABF+ç¶™ç¶šçš„pre-trainingã¨NTK+SFTæŠ€è¡“ã‚’ç”¨ã„ã¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’æœ€é©åŒ–ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å¤§å¹…ã«æ‹¡å¼µã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã£ãŸ
- Open AIã¯ã€ŒCollective Alignment teamã€ã‚’çµæˆ
	- https://openai.com/blog/democratic-inputs-to-ai-grant-program-update
	- AIã«å¤šç¨®å¤šæ§˜ãªä¸–ç•Œä¸­ã®å…¬çš„ãªæ„è¦‹ã‚’åæ˜ ã•ã›ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™ºã‚’æ‹…ã†ã€‚ ä»¥å‰AIã¸ã®æ°‘ä¸»çš„ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‹Ÿé›†ã—ã¦ã„ãŸãŒã€1000ã®å¿œå‹Ÿè€…ãŒã‚ã‚Šä»¥ä¸‹ç”»åƒã®ã‚ˆã†ã«10ã®ãƒãƒ¼ãƒ ã®ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢ãŒé¸æŠœã•ã‚ŒãŸ
-  å†è€ƒ: ãŠè²·ã„å¾—ç‰©ä»¶ã‚’æ©Ÿæ¢°å­¦ç¿’ã§è¦‹ã¤ã‘ã‚‹æ–¹æ³•
	- https://speakerdeck.com/ktgrstsh/rethink-method-to-find-cheap-rental-houses-by-machine-learning
	- è³ƒè²¸ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã‚ã‚Œã°ï¼Œã“ã¡ã‚‰ã®ãƒšãƒ¼ã‚¸ãŒå‚è€ƒã«ãªã‚Šã¾ã—ãŸ
- WikiChatã®è©±
	- https://arxiv.org/abs/2305.14292
	- WikiChat ã¯ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯åŠã³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ï¼ŒRAG ç­‰ã§ã‚ˆãåˆ©ç”¨ã•ã‚Œã‚‹ Wiki ã‚’åˆ©ç”¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ï¼Œé«˜ã„ factfulness ã‚’å‚™ãˆã‚‹ã¨ã—ã¦ã„ã‚‹
- Animagine XL 3.0 ã€Hugging Faceã®ãƒˆãƒ¬ãƒ³ãƒ‰ã§1ä½ã‚’é”æˆ
	- https://huggingface.co/spaces/DamarJati/Animagine-XL-3.0
	- 1æœˆ10æ—¥ã€Cagliostro Research LabãŒã€**æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®Text-to-Imageã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ŒAnimagine XL 3.0ã€**ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
	- https://weel.co.jp/media/animagine-xl-3-0
- Blending, Merging, and Stacking multiple smaller LLMs make them as performant as Larger LLMs
	- https://x.com/bindureddy/status/1746739742350450811?s=20
	- Blendingã€Mergingã€Stackingãªã©ã®æŠ€è¡“ã‚’ä»Šå¾Œ30-70bãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ã¦ã„ãã€ä»Šå¾Œ2-3ãƒ¶æœˆä»¥å†…ã«GPT4ã«è¿‘ã„æˆ»ã‚‹ãŒå¾—ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†
- (RAG)ã®è©•ä¾¡æŒ‡æ¨™ãƒãƒƒãƒ—
	- https://x.com/helloiamleonie/status/1747252654047142351?s=20
- DeepMindã®CEOã§ã‚ã‚‹Lila IbrahimãŒãƒ€ãƒœã‚¹ä¼šè­°2024ã§èªã£ãŸã“ã¨
	- https://www.axios.com/2024/01/16/davos-ai-lila-ibrahim-google-deepmind-technologies
	- ila Ibrahimã¯ã€AIãŒç‰©è³ªç§‘å­¦ã‚„ç”Ÿç‰©å­¦ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã€æ–°ã—ã„ææ–™ã‚„ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®ç™ºè¦‹ã«è²¢çŒ®ã—ã¦ã„ã‚‹ã¨è¿°ã¹ãŸã€‚
	- 2018å¹´ã€ã€ŒAlphaFoldã¯ï¼ˆã‚‚ã¨ã‚‚ã¨ã¯ï¼‰ã†ã¾ãã„ã‹ãªã„ã¯ãšã®ã‚¢ã‚¤ãƒ‡ã‚¢ã ã£ãŸã€ã¨ã‚¤ãƒ–ãƒ©ãƒ’ãƒ ã¯èªã£ãŸã€‚å½¼å¥³ã¯ã“ã†ä»˜ã‘åŠ ãˆãŸã€‚ã€Œä»Šã§ã¯ï¼ˆæ—¢çŸ¥ã®ï¼‰ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã‚’2å„„å€‹ã¿ã¤ã‘ã‚‹ã¾ã§ã«ãªã‚Šã¾ã—ãŸã‘ã©ã­ã€ã€‚
	- æ˜¨å¹´ã¯ã€AIé–‹ç™ºè€…ãŸã¡ãŒäº’ã„ã«å”åŠ›ã—åˆã„ã€æ”¿åºœã®å”åŠ›ã‚’å¾—ã¦ã€æŠ€è¡“ã®ãƒªã‚¹ã‚¯ã‚’ç®¡ç†ã™ã‚‹ã“ã¨ãŒæ€¥é€Ÿã«é€²ã‚“ã ã¨å½¼å¥³ã¯è¨€ã†ã€‚
	- ã‚¤ãƒ–ãƒ©ãƒ’ãƒ æ°ã¯ã€è‹¥ã„AIãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æŠ€è¡“ã®å€«ç†çš„æ çµ„ã¿ã‚’æ•™ãˆã‚‹ã®ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚„ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’é€šã˜ã¦ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã—ãŸé«˜é½¢è€…ä¸–ä»£ã«æ•™ãˆã‚‹ã‚ˆã‚Šã‚‚ç°¡å˜ã ã‚ã†ã¨è€ƒãˆã¦ã„ã‚‹ã€‚
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆCopilot Proã‚’ç™ºè¡¨
	- https://x.com/satyanadella/status/1747000699664429075?s=20
	- Officeï¼“ï¼–ï¼•å‘ã‘ã®copiloã®æ©Ÿèƒ½ãŒã€å€‹äººã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚3,200å††/æœˆ
	- Office365/w copilotã®åˆ©ç”¨ä»¥å¤–ã«ã€GPT-4 ãŠã‚ˆã³ GPT-4 Turboã¸ã®å„ªå…ˆçš„ãªå‰²ã‚Šå½“ã¦
	- Copilot GPT Builderï¼ˆè¿‘æ—¥å…¬é–‹äºˆå®šï¼‰ã§ã€ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã«åˆã‚ã›ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸç‹¬è‡ªã®Copilot GPTã‚’ä½œæˆå¯èƒ½
	- æœŸå¾…ã™ã‚‹å£°ã‚‚ãŸãã•ã‚“ä¸ŠãŒã‚‹ã‚‚ã€ãŒã£ã‹ã‚Šã™ã‚‹å£°ã‚‚å¤šæ•°
- ã€2024å¹´æœ€æ–°ã€‘å…±é€šãƒ†ã‚¹ãƒˆã‚’è‰²ã‚“ãªç”ŸæˆAIã«è§£ã‹ã›ã¦ã¿ãŸï¼ˆChatGPT vs Bard vs Claude2
	- https://note.com/lifeprompt/n/n87f4d5510100?sub_rt=share_h
	- â‘ GPT-4ãŒã™ã¹ã¦ã®ç§‘ç›®ã§ä»–äºŒã¤ã®ãƒ„ãƒ¼ãƒ«ã‚’åœ§å€’  
	- â‘¡æ•°å­¦ç§‘ç›®ã«é–¢ã—ã¦ã¯ã©ã®AIã‚‚å…¨ç„¶ç‚¹å–ã‚Œã¦ã„ãªã„  
	- â‘¢é«˜å¾—ç‚¹ã‚’ç‹™ãˆã¦ã„ã‚‹ç§‘ç›®ã§ã‚‚ã€æº€ç‚¹ã¯å–ã‚Œã¦ã„ãªã„
- nampdn-ai/tiny-strange-textbooks
	- https://huggingface.co/datasets/nampdn-ai/tiny-strange-textbooks
	- äººå·¥çš„ã«ç”Ÿæˆã•ã‚ŒãŸå°å‹ã®LLM(phiãªã‚“ã‹ï¼‰ç”¨ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
	-  Textbooks Are All You Need II: phi-1.5 technical report
	- https://arxiv.org/abs/2306.11644
- Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy
	- https://arxiv.org/abs/2310.01334
	- MoEã£ã¦ãƒ¡ãƒ¢ãƒªé£Ÿã†ã®ã§ã€ã“ã‚Œã‚’åœ§ç¸®ã‚„ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã«ç€ç›®ã—ã¦è»½é‡åŒ–ã™ã‚‹ã€80%ã®å‰Šæ¸›ï¼
	- We merge experts THEN compress/decompose merged expertsâ†’low-rank. Up to 80% mem reduction! ğŸ‰
- mix_self_consistency pack by llamaindex
	- https://llamahub.ai/l/llama_packs-tables-mix_self_consistency?from=llama_packs
	- Hereâ€™s a simple but useful idea to use RAG to fetch few-shot examples for less flaky text-to-SQL (orâ€¦less flaky structured RAG itself). Calling it dynamic metadataâ€¦
	- â€œRethinking Tabular Data Understandingâ€ã®å®Ÿè£…
	- 1.  Index and embed each row
	- 2. In the text-to-SQL prompt (or auto-retrieval prompt), add *few shot examples of rows*: given the first k rows in the prompt, retrieve the top-k rows matching the user query.
	- 3. Execute text-to-SQL prompt (or auto-retrieval prompt) to infer the right query (SQL or metadata filters).
	- 4. Execute query to get back result.
- ç”ŸæˆAIã®æ¥­ç•Œå›£ä½“ã€ŒGenerative AI Japanã€ç™ºè¶³ã€€ãƒ™ãƒãƒƒã‚»ãŒç™ºèµ·ã€€ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã€AWSã€Googleã€ã‚ªãƒ©ã‚¯ãƒ«ãªã©ã®å¹¹éƒ¨ãŒç†äº‹ã«
	- https://x.com/itmedia_news/status/1747490194486632764?s=20
- Can AI Be as Creative as Humans?"
	- https://arxiv.org/abs/2401.01623
	- ã€ŒAIã¯äººé–“ã¨åŒã˜ãã‚‰ã„ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã«ãªã‚Œã‚‹ã®ã‹ï¼Ÿã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã§ã€DeepMindãƒ»Microsoftãƒ»ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ãªã©ãŒå…±åŒã§ç ”ç©¶ã—ã¦ã„ã¾ã™ã€‚
	- ã€AIãŒå‰µã‚Šå‡ºã—ãŸä½œå“ãŒäººé–“ã®ãã‚Œã¨è¦‹åˆ†ã‘ãŒã¤ã‹ãªããªã£ãŸã‚‰ã€AIã¯ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã ã¨è¨€ãˆã‚‹ã€
	- AIã®å‰µé€ æ€§ã‚’å…·ä½“çš„ãªæ•°å€¤ã§è©•ä¾¡ã—ãŸã„ â†’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
- ELYZAãŒå…¬é–‹ã—ãŸæ—¥æœ¬èªLLMã€ŒELYZA-japanese-Llama-2-7bã€ã«ã¤ã„ã¦ã®è§£èª¬ : (3) è‹±èªã§ã®æ€§èƒ½è©•ä¾¡ç·¨
	- https://zenn.dev/elyza/articles/ab3749de0ba58b
	- **è¿½åŠ å­¦ç¿’ã®éç¨‹ã§ã€å…ƒã®ãƒ¢ãƒ‡ãƒ«ãŒæŒã£ã¦ã„ãŸèƒ½åŠ›ãŒã©ã®ç¨‹åº¦å¤±ã‚ã‚Œã¦ã—ã¾ã†ã®ã‹**ã¨ã„ã†ç‚¹
	- çµæœï¼š
		- æ—¥æœ¬èªã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ äº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šæ—¥æœ¬èªåŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€è‹±èªã®æ€§èƒ½ã®åŠ£åŒ–ã¯ç”Ÿã˜ã¦ã—ã¾ã†ã€‚
		- æ—¥æœ¬èªã®SFTã«ã‚ˆã‚Šã€æ—¥æœ¬èªåŒ–ãƒ¢ãƒ‡ãƒ«ã®è‹±èªã®æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã‚‚ä¸€å®šå›å¾©ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
		- è¿½åŠ äº‹å‰å­¦ç¿’ã«è‹±èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ãŸå ´åˆã€è‹±èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½åŠ£åŒ–ã‚’ç·©å’Œå¯èƒ½ã§ã‚ã‚‹ã€‚
		- æ—¥æœ¬èªã®èªå½™æ‹¡å¼µã¯æ—¥æœ¬èªã®äº‹å‰å­¦ç¿’æ™‚ã®æ€§èƒ½åŠ£åŒ–ã‚’é¡•è‘—ã«ã™ã‚‹ã‚‚ã®ã®ã€SFTã«ã‚ˆã‚‹æ€§èƒ½ã®ä¸Šæ˜‡ã‚’ã‚ˆã‚Šäº«å—ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
- ã€æ–°åˆŠã€‘ã€Œå¼·åŒ–å­¦ç¿’ã‹ã‚‰ä¿¡é ¼ã§ãã‚‹æ„æ€æ±ºå®šã¸ã€ã€ã‚µã‚¤ã‚¨ãƒ³ã‚¹ç¤¾
	- æ¢¶é‡ã€€æ´¸(æ—¥æœ¬IBM)ãƒ»å®®å£èˆªå¹³(æ—¥æœ¬IBM)ãƒ»æç¥è²´è¡Œ(æ—¥æœ¬IBM)ãƒ»å²©åŸã€€è«’(æ—¥æœ¬IBM)ãƒ»å’Œåœ°ç­è‰¯(LINEãƒ¤ãƒ•ãƒ¼)ã€€å…±è‘—ã€€
	- https://www.saiensu.co.jp/search/?isbn=978-4-7819-1592-0&y=2024
	- å¼·åŒ–å­¦ç¿’ã¯ãã®å®šå¼åŒ–ã‚’ç”¨ã„ã‚‹ã“ã¨ã§å¹…åºƒã„å®Ÿå•é¡Œã‚’è¡¨ç¾ã§ãã‚‹ä¸€æ–¹ï¼Œä¿¡é ¼æ€§ã®ä¸è¶³ãŒä¸€å› ã¨ãªã‚Šï¼Œå®Ÿä¸–ç•Œã§ã¯å¿œç”¨ãŒãªã•ã‚Œã¦ã„ã‚‹ã¨ã¯è¨€ã„ãŒãŸã„ï¼æœ¬æ›¸ã¯ï¼Œæ¨™æº–çš„ãªå®šå¼åŒ–ã¨å®Ÿå•é¡Œã¨ã®æ©‹æ¸¡ã—ã¨ãªã‚‹ã‚ˆã†ãªå®šå¼åŒ–ã‚’ä½“ç³»çš„ã«ã¾ã¨ã‚ã‚‹ã“ã¨ã§ï¼Œå®Ÿä¸–ç•Œã§ã®å¿œç”¨ã‚’ä¿ƒé€²ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ãŸ
	- ç¬¬3ç« ãƒªã‚¹ã‚¯è€ƒæ…®å‹å¼·åŒ–å­¦ç¿’ã¨é‡‘èã¸ã®å¿œç”¨ï¼ˆ3.5ç¯€ã‚’é™¤ãï¼‰
- ã€ŒGeForce RTX 4070 SUPERã€ãŒå„ç¤¾ã‹ã‚‰å¤šæ•°ç™»å ´ã€ä¾¡æ ¼ã¯95,480å††ã‹ã‚‰
	- https://akiba-pc.watch.impress.co.jp/docs/news/news/1561586.html
- Google DeepMindãŒæ•°å­¦ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã®å¹¾ä½•å­¦å•é¡Œã«ãŠã„ã¦å¹³å‡çš„ãªäººé–“ã®é‡‘ãƒ¡ãƒ€ãƒªã‚¹ãƒˆã«è‚‰è–„ã™ã‚‹ã€ŒAlphaGeometryã€ç™ºè¡¨
	- https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/?utm_source=twitter&utm_medium=social
	- An Olympiad-level AI system for geometry
	- AI system surpasses the state-of-the-art approach for geometry problems, advancing AI reasoning in mathematics
	- AlphaGeometry ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã¨è¨˜å·æ¼”ç¹¹ã‚¨ãƒ³ã‚¸ãƒ³ã§æ§‹æˆã•ã‚Œã‚‹ç¥çµŒè¨˜å·ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ãŒé€£æºã—ã¦è¤‡é›‘ãªå¹¾ä½•å­¦å®šç†ã®è¨¼æ˜ã‚’è¦‹ã¤ã‘ã‚‹
	- ã€ŒLLMã¨æ¼”ç¹¹ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã®çµ„ã¿åˆã‚ã›ã€
- Accelerating the prediction of stable materials with machine learning
	- https://www.nature.com/articles/s43588-023-00536-w
	- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ææ–™ã®å®‰å®šæ€§äºˆæ¸¬ã«é–¢ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡
	- DeepMindã•ã‚“ã®è«–æ–‡ã§ã‚‚ä½¿ã‚ã‚ŒãŸææ–™ã®ç†±åŠ›å­¦çš„å®‰å®šæ€§äºˆæ¸¬ã«é–¢ã—ã€convex hullã®æ¦‚å¿µã®ã‚ˆã†ãªåŸºç¤ã‹ã‚‰ã€æœ‰é™æ¸©åº¦ã®äºˆæ¸¬ã®ã‚ˆã†ãªå¿œç”¨ã¾ã§ã¾ã¨ã¾ã£ã¦ã„ã¾ã™ã€‚ æ©Ÿæ¢°å­¦ç¿’ã§ææ–™æ¢ç´¢ã—ã¦ã¿ãŸã„åˆå­¦è€…ã®æ–¹ã«ãŠã™ã™ã‚ã€‚
- WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation
	- https://arxiv.org/abs/2312.14187
	- Introduce WaveCoder-Ultra-6.7B with the closest capabilities to GPT-4 so far.
	- WaveCoder-Ultra-6.7B is the newest SOTA open-source Code LLM on multiple tasks.
- LangGraphã®èª¬æ˜ãƒ–ãƒ­ã‚°ãŒå…¬é–‹
	- https://blog.langchain.dev/langgraph/
	- We previewed LangGraph last week, but excited to dive a lot more into why we're building this, the details of what it looks like, and some more examples
- Foundations of Vector Retrieval
	- https://arxiv.org/abs/2401.09350
	- This 185-page monograph provides a summary of major algorithmic milestones in the vector retrieval literature, with the goal of serving as a self-contained reference for new and established researchers.
	- LLMæ™‚ä»£ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨æ¤œç´¢ã«ã¤ã„ã¦ã®ã‚µãƒ¼ãƒ™ã‚¤ã§ã‚ã‚ŠåŒ…æ‹¬è«–æ–‡
- A Cheat Sheet and Some Recipes For Building Advanced RAG
	- https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b
- ç±³MetaãŒ1ä¸‡äººã®è¿½åŠ ãƒ¬ã‚¤ã‚ªãƒ•ã€5000äººã®æ¡ç”¨ã‚‚ä¸­æ­¢
	- https://xtech.nikkei.com/atcl/nxt/news/18/14833/
	- ãƒ¡ã‚¿ã¯2ä¸‡äººãƒ¬ã‚¤ã‚ªãƒ•ã—ã¦35ä¸‡å°ã®H100ã‚’è²·ã„ã¾ã—ãŸ
-  OpenAI Node API Library å…¥é–€ by npakaã•ã‚“
	- https://note.com/npaka/n/n2f8c08965316?sub_rt=share_h
	- ã€ŒOpenAI Node API Libraryã€ã¯ã€TypeScript / JavaScriptã‹ã‚‰ã€ŒOpenAI APIã€ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
-  GraphGPT: Graph Learning with Generative Pre-trained Transformers
	- https://arxiv.org/abs/2401.00529
	- ã‚°ãƒ©ãƒ•Ã—Transformerã«ã‚ˆã‚‹ç‰©æ€§äºˆæ¸¬ã®è«–æ–‡
	- ã‚°ãƒ©ãƒ•ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—Transformerã§å­¦ç¿’ã™ã‚‹GraphGPTã‚’ææ¡ˆã€å¾“æ¥ã®GNNã§ã¯é›£ã—ã„400Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã€ã“ã‚Œã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§åˆ†å­ç‰©æ€§ã‚’é«˜ç²¾åº¦ã«äºˆæ¸¬ã§ããŸãã†ã§ã™ã€‚
- LLMãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä¿¯ç°ã™ã‚‹
	- https://speakerdeck.com/masatoto/llmmarutiezientowofu-kan-suru
	- æ–‡çŒ®ã®å†…å®¹ã‚’ã‚‚ã£ã¨æ·±æ˜ã‚Šã—ãŸã‚‰æ™®é€šã«å‡ºç‰ˆã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã ã‚ã“ã‚Œ
-  Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering
	- https://arxiv.org/abs/2401.08500
	- The paper proposes AlphaCodium, a code-oriented iterative flow that improves LLMs on code generation.
	- LLMã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½œæ¥­ã‚’è¡Œã†éš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã¨ã—ã¦ã€ã€Œãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ã¨ã„ã†æ–°ã—ã„æ¦‚å¿µãŒæå”±ã•ã‚Œã¦ã„ã¾ã™ã€‚ 
	- ã“ã®æ¦‚å¿µã«åŸºã¥ã„ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã§ã€LLMã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°èƒ½åŠ›ãŒä¸€è²«ã—ã¦å‘ä¸Šã™ã‚‹ã“ã¨ãŒå®šé‡çš„ã«å ±å‘Šã•ã‚Œã¾ã—ãŸã€‚
	- â– ç ”ç©¶è€…ã‚‰ã®ã‚¢ã‚¤ãƒ‡ã‚¢ - è¤‡æ•°ã®æ®µéšã«åˆ†ã‘ã¦ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆãƒ»æ”¹å–„ã™ã‚‹ - ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®è€ƒãˆæ–¹ã‚’ç”¨ã„ã‚‹
	- â– å®Ÿé¨“çµæœ 
		- ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã§ã®LLMã®æ€§èƒ½ã‚’ä¸€è²«ã—ã¦ã‹ã¤å¤§å¹…ã«å‘ä¸Šã•ã›ãŸ 
		- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ï¼ˆDeepSeekï¼‰ã¨ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚½ãƒ¼ã‚¹ï¼ˆGPT-3.5/4ï¼‰ä¸¡æ–¹ã§åŠ¹æœãŒã‚ã£ãŸ
-  DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models
	- https://arxiv.org/abs/2401.06066
	- DeepSeekMoEã¯LLMã®MoEã§
		- 1) Expertã‚’ã•ã‚‰ã«ç´°ã‹ãã—64ã«å¢—ã‚„ã™ã¨å…±ã«é¸æŠã•ã‚Œã‚‹Expertæ•°ã‚‚8ã«å¢—ã‚„ã™ 
		- 2) å…±æœ‰çŸ¥è­˜ã‚’ä½¿ãˆã‚‹ã‚ˆã†å¸¸ã«é¸æŠã•ã‚Œã‚‹Expertã‚’ç”¨æ„ã€‚ãƒ‡ãƒã‚¤ã‚¹æ¯ã®è² è·åˆ†æ•£ã‚’é‡è¦–ã—å®Ÿè¡ŒåŠ¹ç‡ã‚’ã‚ã’ã‚‹ã€‚
		-  åŒã˜è¨ˆç®—é‡ã®Denseã‚„å¾“æ¥MoEã«å¯¾ã—æ€§èƒ½ã‚’æ”¹å–„
- llama3ã®é–‹ç™ºã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã«é–¢ã—ã¦ã‚¶ãƒƒã‚«ãƒ¼ãƒãƒ¼ã‚°ã®ãƒ“ãƒ‡ã‚ªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºå›ã‚‹
	- https://twitter.com/i/status/1748058491343061458
	- å¹´å†…ã«35ä¸‡å°ã®H100ã‚’æ´»ç”¨å¯èƒ½ã‚¤ãƒ³ãƒ•ãƒ©ã‚’æ§‹ç¯‰
	- H100ç›¸å½“å“ã‚‚å«ã‚ã‚‹ã¨60ä¸‡å°ã®H100ã«åŒ¹æ•µ 
	- ä»¥ä¸‹ã¯ãƒ“ãƒ‡ã‚ªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã®æ›¸ãèµ·ã“ã— by AI
		- ãƒ¡ã‚¿ã¯ä¸€èˆ¬çš„ãªçŸ¥èƒ½ã‚’æ§‹ç¯‰ã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã€ã¿ã‚“ãªã«åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã¨ã„ã†é•·æœŸçš„ãªç›®æ¨™ã®ãŸã‚ã«ã€2ã¤ã®AIç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’çµ±åˆã™ã‚‹ã¨ç™ºè¡¨ã—ãŸã€‚
		- æ¬¡ä¸–ä»£ã®ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯ã€æ¨è«–ã€è¨ˆç”»ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€è¨˜æ†¶ãªã©ã®AIã®å„åˆ†é‡ã§ã®é€²æ­©ãŒå¿…è¦ã§ã‚ã‚‹ã¨è¿°ã¹ãŸã€‚
		- ã“ã®æŠ€è¡“ã¯éå¸¸ã«é‡è¦ã§ã‚ã‚Šã€æ©Ÿä¼šã‚‚å¤§ãã„ã®ã§ã€è²¬ä»»ã‚’æŒã£ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã€ã§ãã‚‹ã ã‘åºƒãåˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã¹ãã ã¨ä¸»å¼µã—ãŸã€‚
		- ä»Šå¹´æœ«ã¾ã§ã«ã€ç´„35ä¸‡å°ã®Nvidia H100 GPUã‚’æ­è¼‰ã—ãŸå·¨å¤§ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰ã™ã‚‹ã¨ç™ºè¡¨ã—ãŸã€‚
		- ç¾åœ¨ã€Llama 3ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãŠã‚Šã€ä»Šå¾Œã‚‚è²¬ä»»ã‚’æŒã£ã¦å®‰å…¨ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶šã‘ã‚‹
		- AIã¨ãƒ¡ã‚¿ãƒãƒ¼ã‚¹ã¯å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€å°†æ¥çš„ã«ã¯å¤šãã®äººãŒAIã¨ä¼šè©±ã™ã‚‹ãŸã‚ã«ãƒ¡ã‚¬ãƒã‚’ä½¿ã†ã ã‚ã†ã¨äºˆæ¸¬ã—ãŸã€‚
- Connect to Sheets and use the Gemini API in Colab to tell Gemini about your most promising prospects and prepare personalized sales pitches to sell what you are good at - in this case, delicious lemonade.
	- https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Sell_lemonade_with_Gemini_and_Sheets.ipynb
	- Geminiã¨Google Sheetsã‚’ä½¿ã£ãŸã‚»ãƒ¼ãƒ«ã‚¹ãƒ”ãƒƒãƒç”Ÿæˆã®ä¾‹
- 5%ãã‚‰ã„ï¼Ÿã‚’ChatGPTï¼ˆç”ŸæˆAIï¼‰ã§æ›¸ã„ãŸã¨ã„ã†èŠ¥å·è³ã‚’å—è³
	- https://x.com/yukatan/status/1747957984104480891?s=20
	- AIã«åŸ·ç­†ã•ã›ã¦ã¿ãŸã¨ã„ã†ãƒ¬ãƒ™ãƒ«ã®è©±ã§ã¯ãªãã¦ã€ã‚¹ãƒãƒ›ã§ã‚°ã‚°ã‚‹ã¿ãŸã„ã«AIã«è³ªå•ã™ã‚‹ã®ãŒå½“ãŸã‚Šå‰ã«ãªã‚‹ã¨ä¸–ç•ŒãŒã©ã†å¤‰ã‚ã‚Šå¾—ã‚‹ã‹ã‚’æ–‡å­¦çš„ã«è¡¨ç¾ã—ã¦ã„ã¾ã™ã€‚æ™‚ä»£ã‚’åˆ»ã‚€ä½œå“ã ã‚
- ã‚¢ãƒ«ãƒˆãƒãƒ³ãŒãƒ€ãƒœã‚¹ä¼šè­°ã§è¨€ã£ãŸã“ã¨
	- ã€ŒAIã®é€²æ­©ã¯ã€ç§‘å­¦çš„ç™ºè¦‹ã®é€Ÿåº¦ã‚’å¤§å¹…ã«åŠ é€Ÿã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚ãã‚ŒãŒ2024å¹´ã«èµ·ã“ã‚‹ã¨ã¯äºˆæƒ³ã—ã¦ã„ãªã„ãŒã€èµ·ã“ã£ãŸãªã‚‰ã°ã¨ã¦ã‚‚å¤§ããªä¸€å¤§äº‹ã«ãªã‚‹ã€ 
	- ã€Œç¾æ™‚ç‚¹ã§ã®æœ€å„ªå…ˆäº‹é …ã¯æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ³ãƒã™ã‚‹ã“ã¨ã ã€‚ãã‚Œã¯GPT-5ã¨å‘¼ã°ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„
	- https://www.axios.com/2024/01/17/sam-altman-davos-ai-future-interview
- åœ§ç¸®MoE
	- https://github.com/unites-lab/mc-smoe
	- ä»Šã¾ã§ã®MoEã¯ãƒ¢ãƒ‡ãƒ«ã‚’ï¼’ã¤ãã£ä»˜ã‘ãŸã‚‰ï¼’å€VRAMæ¶ˆè²»ã™ã‚‹ã®ãŒã‚³ã‚¹ãƒ‘å¾®å¦™ã ã£ãŸã‘ã©ã€MC-SMoEã§ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã®å·®åˆ†ã‚’LoRAçš„ãªå½¢ã§ä¿æŒã™ã‚‹äº‹ã§çœãƒ¡ãƒ¢ãƒªã«ãªã£ãŸã£ã¦è©±ã‹ãª
- Introducing Mixtral, Phi2, Falcon, and Qwen support in DeepSpeed-FastGen! 
	- https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/2024-01-19
	- Up to 2.5x faster LLM inference
	- Optimized SplitFuse and token sampling
	- Exciting new features like RESTful API and more!
- å¿ƒç†å­¦ãƒ¯ãƒ¼ãƒ«ãƒ‰104å·ã®ç‰¹é›†ã€Œç©ºé–“èªçŸ¥ã®ç§‘å­¦ æœ€å‰ç·šã€
	- https://psych.or.jp/publication/world104/
- Fair Machine Guidance to Enhance Fair Decision Making in Biased People
	- https://x.com/yukino/status/1748481134558896432?s=20
	- ç§ãŸã¡ã®è«–æ–‡ã€ŒFair Machine Guidance to Enhance Fair Decision Making in Biased Peopleã€ãŒ #CHI2024 ã«æ¡ä»¶ä»˜ãæ¡æŠã•ã‚Œã¾ã—ãŸï¼ äººé–“ã®åˆ¤æ–­ãŒä¸å…¬å¹³ã«åã‚‹å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€å…¬å¹³æ€§é…æ…®å‹æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å…¬å¹³ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€äººã€…ãŒã‚ˆã‚Šå…¬å¹³ãªåˆ¤æ–­ã‚’ä¸‹ã›ã‚‹ã‚ˆã†ã‚¬ã‚¤ãƒ‰ã—ã¾ã—ãŸï¼
- Neural Speed + ONNX Runtime makes LLM inference more efficient on CPUs!
	- https://github.com/intel/neural-speed
- Google DeepMind researchers are in talks to leave and form a new startup named 'Holistic'. They want to build their own AI model.
	- https://x.com/AndrewCurran_/status/1748419941672616324?s=20
- ã€æ–°åˆŠã€‘ã€Œå¤šæ§˜ä½“ä¸Šã®æœ€é©åŒ–ç†è«–ã€
	- https://www.ohmsha.co.jp/book/9784274231186/
	- æœ¬æ›¸ã¯ã€å¤šæ§˜ä½“ä¸Šã®æœ€é©åŒ–ç†è«–ã«ã¤ã„ã¦ã€åŸºç¤ã¨ãªã‚‹æ•°ç†ã‹ã‚‰å¿œç”¨ä¾‹ã¾ã§ã‚’è§£èª¬ã™ã‚‹ã‚‚ã®ã§ã™ã€‚  
	- å¤šæ§˜ä½“ä¸Šã®æœ€é©åŒ–ã‚’å­¦ã¶ã€ã‚ã‚‹ã„ã¯ç ”ç©¶ã™ã‚‹èª­è€…ã¯  
		- ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ä¸Šã®é€£ç¶šæœ€é©åŒ–ã‚’ã²ã¨ã¨ãŠã‚Šå­¦ã‚“ã å¾Œã€ãã®æŠ½è±¡åŒ–ã®ä»•æ–¹ã®ä¸€ã¤ã¨ã—ã¦å¤šæ§˜ä½“ä¸Šã¸ã®æ‹¡å¼µã«ã¤ã„ã¦å­¦ã¶  
		- å¤šæ§˜ä½“ã‚’ã¯ã˜ã‚ã¨ã—ãŸå¹¾ä½•å­¦ã«æ…£ã‚Œè¦ªã—ã‚“ã èª­è€…ãŒã€ãã†ã—ãŸç†è«–ã®æœ€é©åŒ–ã¸ã®å¿œç”¨ã«ã¤ã„ã¦å­¦ã¶  
		- æœ€é©åŒ–ã¨å¹¾ä½•å­¦ã®çŸ¥è­˜ã‚’ã‚‚ã¤èª­è€…ãŒã€ä¸¡è€…ã®èåˆã«ã¤ã„ã¦å­¦ã¶

## 1/15

Mistral AIã«ã‚ˆã‚‹Mixtral -8x7bãƒ¢ãƒ‡ãƒ«ã®æˆåŠŸã«ã‚ˆã‚Šã€æœ€è¿‘ã®ã¯ã‚„ã‚Šã¯MoEï¼ˆMixture of Expertsï¼‰ãƒ¢ãƒ‡ãƒ«ã€‚Phi-2ã®MoEã§ã‚ã‚‹Phixtual-2x2bãªã‚“ã‹ã‚‚å‡ºã¾ã—ãŸã€‚mergekitã¨ã„ã†ã®ã‚’ä½¿ãˆã°ã€colabã§ã‚‚ã€MoEãŒç°¡å˜ã«ä½œã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚  æ¯”è¼ƒçš„å°ã•ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã€æ··ãœåˆã‚ã›ã‚‹ã“ã¨ã§å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã¨ã„ã†å ±å‘Šã‚‚ã‚ã‚Šã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã£ã¦ã®ã¯LLMã§ã‚‚æœ‰åŠ¹ãªã‚“ã§ã™ã­ãƒ¼ã€‚å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã¯TinyLlamaã£ã¦ã®ã‚‚ã‚ã‚Šã¾ã—ãŸã€Macã§ã‚‚å¿«é©ã«å‹•ãæ¨¡æ§˜ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã¯å°ã•ãã¦ã‚‚ã€è†¨å¤§ãªãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã™ã‚Œã°æ€§èƒ½ãŒä¸ŠãŒã‚‹ï¼Ÿstanfordã®wikichatã€LLaMA7Bãƒ™ãƒ¼ã‚¹ã§ã‚‚ã€ã“ã“ã¾ã§æ€§èƒ½ãŒä¸ŠãŒã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªã‚’é£Ÿã†ã‚‰ã—ã„ãŒï¼‰ã¨ã„ã†å ±å‘Šã‚‚ã€‚ã‚ã‚Œã‚‰ã®ã‚¢ãƒ«ãƒˆãƒãƒ³æ°ãŒçµå©šï¼LangChainã‚‚ã¤ã„ã«ã€v0.1ãŒå‡ºãŸï¼ã€‚ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«ã€ã²ãŸã™ã‚‰ã€Moore-AnimateAnyoneã®çµµãŒå‡ºã¦ãã‚‹ã®ã¯ãªãœï¼Ÿï¼ŸDuolingoã®ãƒªã‚¹ãƒˆãƒ©ã€ãã†ã„ã†æ°—ã‚‚ã™ã‚‹ãŒã€googleã®AMIEã®ã‚ˆã†ã«ã€ãã‚‚ãã‚‚äººæä¸è¶³ã®åˆ†é‡ã§ã®å°‚é–€å®¶AIã®ç™»å ´ã¨ã„ã†å´é¢ã‚‚ã‚ã‚‹ã€‚Googleã®DynamicPlanã£ã¦ã€ã‚ã‚Œã©ã“ã‹ã§è¦‹ãŸã‚ˆã†ãªæ°—ã‚‚ã™ã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¯ãƒªã‚¹ãƒˆãƒ©ã•ã‚Œã‚‹å´ã«ãªã‚‹ã®ã‹ã€ãã‚Œã¨ã‚‚å°‚é–€å®¶AIã¨ã—ã¦ã ã‚Œã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã®ã‹ï¼Ÿ

- Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy
	- https://arxiv.org/abs/2312.12728
	- LLMã®å‡ºåŠ›å“è³ªã‚’è½ã¨ã•ãšã«æ¨è«–é€Ÿåº¦ã‚’ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã•ã›ã‚‹ãŸã‚ã®æ‰‹æ³•
	- â– ã€Lookaheadã€ã®ã‚¢ã‚¤ãƒ‡ã‚¢ 
		- 1. ç”Ÿæˆã®æåˆ†ã‹ã‚Œï¼ˆãƒ–ãƒ©ãƒ³ãƒï¼‰ã‚’ä½œã‚‹ - ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆã¯ä¸¦è¡Œå‡¦ç†ã™ã‚‹ 
		- 2. æœ€é©ãªãƒ–ãƒ©ãƒ³ãƒã‚’é¸ã³å‡ºã™ - ä¸è¦ãªãƒ–ãƒ©ãƒ³ãƒã‚’æ—©æœŸæ’é™¤ã™ã‚‹ â†’æ¨è«–ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚’å‘ä¸Šã•ã›ã¤ã¤é«˜å“è³ªã‚’ç¶­æŒã™ã‚‹ 
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. Dollyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨Llama-13Bã§ãƒ†ã‚¹ãƒˆ 
		- 2. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã«çµ„ã¿è¾¼ã‚“ã  
		- 3. é«˜ã„ç”Ÿæˆç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤é€Ÿåº¦ã‚’æ”¹å–„ã—ãŸ
- PmxEditoråŠã³æº–æ¨™æº–ãƒœãƒ¼ãƒ³è¿½åŠ ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å°å…¥
	- http://rockstababy.starfree.jp/mmdsupporter/bemmder/section3.php
	- PmxEditorã‚’ä½¿ãˆã°MMDãƒ¢ãƒ‡ãƒ«ã‚’ç·¨é›†ã§ãã‚‹ã®ã‹ï¼ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã®ç·¨é›†ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã„ãŸã®ã‹
-  Uncovering mesa-optimization algorithms in Transformers
	- https://arxiv.org/abs/2309.05858
	- Why are Transformers so effective? And where is their intruiging in-context learning ability coming from?
	- Transformerã¯ï¼Œäººé–“ã®è¨­è¨ˆè€…ã‹ã‚‰ä¸ãˆã‚‰ã‚ŒãŸè¨“ç·´ç›®æ¨™ã‚’é”æˆã™ã‚‹ãŸã‚ã«ï¼Œè‡ªç™ºçš„ã«æ–°ãŸãªä¸­é–“ç›®æ¨™ã®è¨­å®šã¨ãã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ãŸå†…éƒ¨çš„ãªæœ€é©åŒ–æˆ¦ç•¥ã‚’ä½œã‚‹ï¼ˆãƒ¡ã‚µæœ€é©åŒ–ï¼‰å¯èƒ½æ€§ã‚’ç¤ºå”†ï¼AIå®‰å…¨æ€§ï¼ŒAIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã«ãŠã‘ã‚‹é‡è¦æ¦‚å¿µï¼ˆé“å…·çš„ç›®æ¨™åæŸï¼‰ã‚’ç†è«–çš„ã«å°å‡ºã—ãŸæ³¨ç›®è«–æ–‡
- TinyLlama: An Open-Source Small Language Model
	- https://arxiv.org/abs/2401.02385
	- å°å‹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ¥µã‚ã¦å¤§ãã„ãƒ‡ãƒ¼ã‚¿é‡ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã¨ã€é¡ä¼¼ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã«è‘—ã—ãæ€§èƒ½ãŒé«˜ããªã£ãŸã¨å ±å‘Š
	- - GPT-3ï¼š175Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ - Llama-2ï¼š7Bã€œ70Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ - TinyLlamaï¼š1.1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
	- â– å®Ÿé¨“ 1. 3å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§TinyLlamaã‚’è¨“ç·´ã—ãŸ ï¼ˆ3ã‚¨ãƒãƒƒã‚¯Ã—1å…†ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ 2. æ§˜ã€…ãªå¸¸è­˜æ¨è«–ã‚¿ã‚¹ã‚¯ã§ãƒ†ã‚¹ãƒˆã—ãŸ 3. åŒè¦æ¨¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ãŸ 4. å¹³å‡ã‚¹ã‚³ã‚¢ã§æœ€é«˜ã®æˆç¸¾ã‚’é”æˆã—ãŸ 
	- â– çµè«– ã‚·ãƒ³ãƒ—ãƒ«ã«å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ã¯æœ‰åŠ¹ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
-  LangChain v0.1.0
	- https://blog.langchain.dev/langchain-v0-1-0/
- langgraph
	- https://github.com/langchain-ai/langgraph
	- LangGraph is inspired by Pregel and Apache Beam, and the current interface exposed is one inspired by NetworkX
- GPT-4ã‚’å°å…¥ã—ãŸDuolingoãŒå¤§è¦æ¨¡ãªãƒªã‚¹ãƒˆãƒ©
	- https://x.com/Rahll/status/1744234385891594380?s=20
	- GPT-4ã‚’å°å…¥ã—ãŸDuolingoãŒå¤§è¦æ¨¡ãªãƒªã‚¹ãƒˆãƒ©
- 1å¹´é–“ã«æ—¥æœ¬ã®äººå·¥çŸ¥èƒ½åˆ†é‡å…¨ä½“ã§20äººã—ã‹åšå£«å·å–ã‚‰ãªã„ï¼Ÿ
	- https://x.com/yo_ehara/status/1744332999578333613?s=20
- Mixtral of Experts by Mistral AI
	- https://huggingface.co/papers/2401.04088
	- introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e.â€¦
- yolopandas
	- https://github.com/ccurme/yolopandas
	- yolopandas ã¯ï¼Œpanadas ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾ã—ã¦ç›´æ¥ï¼ŒLLM ãŒåˆ†æã‚³ãƒ¼ãƒ‰ã®æç¤ºã‚’ã—å®Ÿè¡Œã—ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
	- ã€Œæ¬ æå€¤ã¯ã„ãã¤ã‚ã‚‹ï¼Ÿã€ãªã©ã®æŒ‡ç¤ºæ–‡ã«å¯¾ã—ï¼Œ df.llm.query("æŒ‡ç¤ºæ–‡") ã¨ã™ã‚‹ã ã‘
- äººå·¥çŸ¥èƒ½ã¨ã„ã†åˆ†é‡ãŒè¬™è™šã§ã‚ã£ãŸã“ã¨ãªã©ä¸€åº¦ã‚‚ãªã„
	- https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/286548
	- å²©æ³¢æ›¸åº—ã€Œç§‘å­¦ã€2023/12æœˆå·ã«æ²è¼‰ã•ã‚ŒãŸã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨äººé–“ã®è¨€èªèƒ½åŠ›ã«ã¤ã„ã¦ã®è¨è«–å½¢å¼è«–æ–‡
- WikiChat=Wikipedia + LLM
	- https://wikichat.genie.stanford.edu/
	- https://github.com/stanford-oval/WikiChat
	- stanfordã®wikichatã€äº‹å®Ÿæ€§ã§GPT-4 ã‚ˆã‚Šã‚‚55.0%å„ªã‚Œã¦ã„ã‚‹ã¨ã„ã†äº‹ã§ã‚‚ã®å‡„ã„ 
	- ã—ã‹ã—ã€LLaMA7Bãƒ¢ãƒ‡ãƒ«ãŒãƒ™ãƒ¼ã‚¹ã®å‰²ã«è¦æ±‚ã‚¹ãƒšãƒƒã‚¯ã‚‚ã‚‚ã®å‡„ã„
		- å‹•ä½œã•ã›ã‚‹ã«ã¯ç´„100GBã®RAMãŒå¿…è¦ 
		- é€Ÿåº¦ã‚’çŠ ç‰²ã«RAM ã®ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã‚‹ãŒãã‚Œã§ã‚‚ç´„35GBãŒå¿…è¦
- Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM
	- https://arxiv.org/abs/2401.02994
	- æ¯”è¼ƒçš„å°ã•ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã€æ··ãœåˆã‚ã›ã‚‹ã“ã¨ã§å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹å¯èƒ½æ€§
	- â– å®Ÿé¨“å†…å®¹ 
		- 1. 3ã¤ã®å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ–ãƒ¬ãƒ³ãƒ‰ã—ãŸ 
		- 2. GPT-3.5ãªã©æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ãŸ 
		- 2. è©•ä¾¡æŒ‡æ¨™ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®šç€ç‡ã¨ä¼šè©±å¯†åº¦ã¨ã—ãŸ
	- â– å®Ÿé¨“çµæœ 
		- 1. ãƒ–ãƒ¬ãƒ³ãƒ‰ãƒ¢ãƒ‡ãƒ«ã¯å®šç€ç‡ãŒé¡•è‘—ã«é«˜ã‹ã£ãŸ 
		- 2. ä¼šè©±å¯†åº¦ã«é–¢ã—ã¦ã‚‚ä»–ãƒ¢ãƒ‡ãƒ«ã‚’å‡Œé§•ã—ãŸ
- Kaggleæ–°ã‚³ãƒ³ãƒš
	- https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/
	- è„³æ³¢ (EEG) ä¿¡å·ã‹ã‚‰å…¥é™¢ä¸­ã®é‡ç—‡æ‚£è€…ã®ç™ºä½œãªã©ã‚’æ¤œçŸ¥ã€‚ç™ºä½œ (SZ)ã€å…¨èº«æ€§å‘¨æœŸæ”¾é›» (GPD)ã€å´æ–¹åŒ–å‘¨æœŸæ€§æ”¾é›» (LPD)ã€å´æ–¹åŒ–å¾‹å‹•ãƒ‡ãƒ«ã‚¿æ´»å‹• (LRDA)ã€å…¨èˆ¬åŒ–å¾‹å‹•ãƒ‡ãƒ«ã‚¿æ´»å‹• (GRDA)ã€ã¾ãŸã¯ã€Œãã®ä»–ã€ã®6ã‚¯ãƒ©ã‚¹ã‚’åˆ†é¡ã™ã‚‹
- OpenAIã€GPT storeã‚’æ­£å¼å…¬é–‹
	- https://openai.com/blog/introducing-the-gpt-store
-  Build LLM Apps with LangChain.js
	- https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/
	- DeepLearningAIã‚ˆã‚Šã€javascriptã‚’ã‚‚ã„ã„ãŸLLMã‚³ãƒ¼ã‚¹
- Phixtral
	- Phixtralã ã£ã¦ã€‚Phi-2ã‚’ãã£ä»˜ã‘ã¦MoEã«ã—ãŸã‚‰ã—ã„
	- ãƒãƒ¼ã‚¸(merge)ã¨ã¯è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’è¶³ã—å¼•ãã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æŠ€è¡“ 
	- ä¸Šæ‰‹ã«ãƒãƒ¼ã‚¸ã™ã‚‹ã¨å‡ºåŠ›ãŒã‚ã¾ã‚Šå£Šã‚Œãš(ã‚¹ãƒšãƒ«ãƒŸã‚¹ãŒå¤šããªã‚‹ã¨ã„ã†è©±ã¯ã‚ã‚‹)ã€ãƒãƒ¼ã‚¸å¾Œã«æ”¹ã‚ã¦å¾®èª¿æ•´ã‚’ã—ãªãã¦ã‚‚ãã®ã¾ã¾å‹•ãã€‚ã—ã‹ã‚‚ã€ãƒ™ãƒ¼ã‚¹ã¨ãªã£ãŸãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ãŒå‘ä¸Šã™ã‚‹äº‹ã‚‚çã—ããªã„â€¦
	- It combines 2 to 4 fine-tuned models and is better than each individual expert.
	- https://huggingface.co/mlabonne/phixtral-2x2_8
	- https://huggingface.co/mlabonne/phixtral-4x2_8
- llamaindexã‚ˆã‚Šã€RAGã®é«˜åº¦ãªæ‰‹æ³•ã¨ã—ã¦ã€ensembleã¨fusion
	- https://llamahub.ai/l/llama_packs-query-rag_fusion_pipeline?from=llama_packs
- Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding
	- https://arxiv.org/abs/2401.04398
	- Googleãªã©ã®ç ”ç©¶è€…ã«ã‚ˆã‚Šã€è¡¨å½¢å¼ï¼ˆ.csvãªã©ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’é€šã—ã¦LLMãŒã€Œé€£é–çš„ãªæ¨è«–ã€ã‚’è¡Œã†ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
	- â– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒDynamicPlanã€ - è³ªå•ã®å…±æœ‰ã¨ã€å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã•ã›ã‚‹ - é©å®œã€ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã€é¸æŠã€ä¸¦ã¹æ›¿ãˆã‚’ã•ã›ã‚‹ - æœ€çµ‚çš„ã«è³ªå•ã«ç­”ãˆã•ã›ã‚‹
	- â– å®Ÿé¨“ã¨çµæœ - PaLM-2ã€GPT-3.5ã€LLaMA 2ã‚’ä½¿ç”¨ã—ãŸ - è¡¨ãƒ‡ãƒ¼ã‚¿æ¨è«–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯3ç¨®é¡ã§è©•ä¾¡ã—ãŸ - æœ€é«˜ã®ã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸ
- LangChainã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ— - LangChain Expression Languageã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹
	- https://speakerdeck.com/masahiro_nishimi/langchainkiyatutiatupu-langchain-expression-languagewowan-quan-nili-jie-suru
- Geminiã®ã€Œå¸¸è­˜ã‚’æ¨è«–ã™ã‚‹èƒ½åŠ›ã€ã‚’ç¶²ç¾…çš„ã«èª¿æŸ»ã—ãŸçµæœã€€é–“é•ãˆã‚„ã™ã„ã‚¿ã‚¤ãƒ—ã®å•é¡Œã‚‚æ˜ã‚‰ã‹ã«
	- https://ai-data-base.com/archives/61597
	- ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ã¨Metaã«ã‚ˆã£ã¦GPT-4ãªã©ä»–ã®LLMã¨ä½µã›ã¦å®Ÿé¨“ã•ã‚ŒãŸçµæœãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚ è¨˜äº‹ã§ã¯ã€å®Ÿé¨“ã¨çµæœã®è©³ç´°ã€ãã‚‚ãã‚‚å¸¸è­˜æ¨è«–ã¨ã¯ä½•ã‹ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚
- ChatGPTã®Top Pã‚„Temperatureã«ã¤ã„ã¦å°‘ã—çŸ¥ã£ã¦ã¿ã‚ˆã†
	- https://techblog.a-tm.co.jp/entry/2023/04/24/181232
- æˆ‘ã‚‰ãŒOpenAI CEOã‚µãƒ ã‚¢ãƒ«ãƒˆãƒãƒ³ã€çµå©š
	- https://x.com/kai_postv/status/1745440329204142447?s=20
- AMIE: A research AI system for diagnostic medical reasoning and conversations
	- https://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html
	- Googleã‹ã‚‰ã€åŒ»ç™‚è¨ºæ–­åˆ†é‡ã«ç‰¹åŒ–ã—ãŸã€AIãƒªã‚µãƒ¼ãƒã‚·ã‚¹ãƒ†ãƒ AMIE
	- Today, we shared our latest preprint introducing AMIE (Articulate Medical Intelligence Explorer), a large language model (LLM) based research AI system for diagnostic medical reasoning and conversations.
	- å·¨å¤§ãªæ±ç”¨è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒPaLM 2ã€ã‚’åŒ»ç™‚å¯¾è©±å‘ã‘ã«å¾®èª¿æ•´ã—ãŸAIã‚·ã‚¹ãƒ†ãƒ ã€ŒAMIEã€ã€‚å°‚é–€åŒ»ã«ã‚ˆã‚‹ã¨32è»¸ä¸­28è»¸ã€æ‚£è€…ã«ã‚ˆã‚‹ã¨26è»¸ä¸­24è»¸ã§ã€ã‚ˆã‚Šé«˜ã„è¨ºæ–­ç²¾åº¦ã¨å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ä¸–ç•Œã®80å„„äººãŒ24æ™‚é–“ä½“åˆ¶ã§åŒ»ç™‚ç›¸è«‡ã§ãã‚‹ç©¶æ¥µã®ã‹ã‹ã‚Šã¤ã‘åŒ»ã¸ä¸€æ­©å‰é€²
- Moore-AnimateAnyone test
	- https://x.com/toyxyz3/status/1745846460678291702?s=20
	- Moore-AnimateAnyoneã¯ã€AnimateAnyoneã‚’å†ç¾ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‚ æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã¨ã‚Šã€æœ¬çµµkã¨ã¯å¤šå°‘ç•°ãªã‚‹å®Ÿè£…ã§å†ç¾ã—ã¦ã„ã‚‹ãã†ã§ã€ç¾åœ¨ãŠãŠã‚ˆã80%ã»ã©ã®å†ç¾åº¦ã¨ãªã£ã¦ã„ã¾ã™ã€‚
- nitky/Superswallow-70b-v0.1
	- https://huggingface.co/nitky/Superswallow-70b-v0.1
	- ãªã‚“ã‹ã™ã”ã„æ€§èƒ½ãŒã‚ã‚‹ã‚‰ã—ã„ãƒãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªGPT-4ã¨LLaVAã«ã‚ˆã‚‹é«˜åº¦ãªç”»åƒç†è§£ã¨è‡ªç„¶è¨€èªå¯¾è©±ã®çµ±åˆ
	- https://ai-scholar.tech/articles/computer-vision/LLaVA
	- GPT-4ã«ä¸¦ã¶ã€Œå¤šãƒ¢ãƒ¼ãƒ€ãƒ«äººå·¥çŸ¥èƒ½ã€ã®é–‹ç™ºã«å‘ã‘ã¦ã€è¦–è¦šå‘½ä»¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¾ã—ãŸã€‚
	- ã¾ãŸã€è¦–è¦šã¨è¨€èªã®ç†è§£åŠ›ãŒé«˜ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLaVAã€ã‚‚ç´¹ä»‹ã€‚
-  Large Language Model Course by 
	- https://github.com/mlabonne/llm-course
	- 3 models trending + even MistralTril 
-  Google Colabï¼šMergekitã«ã‚ˆã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«MoEã®ä½œæˆ
	- https://note.com/hatti8/n/ne09226bc4ff5?sub_rt=share_pb
	- https://huggingface.co/HachiML/youri-2x7b_dev
	- ãƒãƒ¼ã‚¸ã®å®Ÿè¡Œè‡ªä½“ã¯ã»ã¨ã‚“ã©ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ã®æ™‚é–“ã§20~30åˆ†ãã‚‰ã„ã§å®Ÿè¡Œã§ããŸæ°—ãŒã™ã‚‹ã€‚
	-   ãƒ¡ãƒ¢ãƒªãŒãã“ãã“ã„ã‚‹ã®ã§ã€ãƒã‚¤ãƒ¡ãƒ¢ãƒªã§å®Ÿè¡Œã—ãªã„ã¨ã„ã‘ãªã„ã€‚
	- https://github.com/cg123/mergekit/tree/mixtral
- Phixtral 4-bit quantized with MLX also runs nicely on an 8GB M2.
	- https://github.com/ml-explore/mlx-examples/tree/main/llms/phixtral
	- https://x.com/awnihannun/status/1746376783543591235?s=20
- æ—¥æœ¬èªMoEãƒ¢ãƒ‡ãƒ«ã€jaqket-v2ä»¥é™ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	- https://x.com/CurveWeb/status/1746401006286713276?s=20
	- Mixture of Expertså¼·åŠ›ã™ãã‚‹ã€‚
	- JGLUEã®çµæœã¨åŒæ§˜ã€ã„ã„ã¨ã“å–ã‚ŠãŒã§ãã¦ã‚‹ã€‚
	- ã—ã‹ã‚‚ã€9ã¤ä¸­5ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(åŠåˆ†ä»¥ä¸ŠğŸ‘€)ã§å…ƒã®ï¼’ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã‚¹ã‚³ã‚¢ã«ã€‚
- mergekitã‚’ä½¿ã£ã¦MoEãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸ
	- https://huggingface.co/HachiML/youri-2x7b_dev
	- rinna/youri-7b-instruction
	- rinna/youri-7b-chat chat
	- ãƒ¢ãƒ‡ãƒ«ã¨instructionãƒ¢ãƒ‡ãƒ«ã‚’ç¹‹ã’ã‚‹åŠ¹æœãŒã©ã®ãã‚‰ã„ã‚ã‚‹ã‹ã‚ã‹ã‚‰ãªã„ã‘ã‚Œã©ã€å‹•ãã¨ã“ã‚ã¾ã§ç¢ºèªã§ããŸã€‚ æ™‚é–“ãŒã‚ã‚Œã°JGLUEè©¦ã—ã¦ã¿ã‚‹ã€‚
- Raspberry Pi 4 Model B 4GB memoryã§Phi-2ã¨TinyLlamaä½™è£•ã§å‹•ã„ãŸ
	- https://x.com/yuiseki_/status/1746532207597064670?s=20
	- ç‰¹ã«TinyLlamaã¯8token/sãã‚‰ã„å‡ºã¦ã‚‹ã‚“ã ã‘ã©ã€ãªã‚“ã‹llama.cppå‰ã‚ˆã‚Šé€Ÿããªã£ã¦ã­â€¦ï¼Ÿ


## 1/8

 Mixtralã®MoEç‰ˆã«å¯¾ã™ã‚‹æŠ•æ©Ÿçš„å®Ÿè¡Œ(offload)è«–æ–‡ã¨ãã®æˆæœãŒæ–°ã—ã„é‡å­åŒ–HQQã‚’å«ã‚ã¦ã€ä»Šé€±ã®ä¸€ç•ªã™ã”ã„ãƒã‚¿ã€‚æ¬¡ã®Expertã‚’äºˆæ¸¬ã—ã¦ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã€colabã§å‹•ãã®ã‚‚ã™ã”ã„ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ã§ã‚‚ã€CALMã‚„çŸ¥è­˜ç·¨é›†ã®ã‚ˆã†ã«ã€è³ªãŒé•ã†æ–°ã—ã„æ‰‹æ³•ãŒãŸãã•ã‚“ã§ã¦ããŸã€‚LLaMA-Factoryã¯ã€colabã§ã€æ§˜ã€…ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒè©¦ã›ã¦ã“ã‚Œã¾ãŸæ°‘ä¸»åŒ–ã‚’ä¿ƒé€²ã€‚å› æœãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¨ã‹ã€ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡å•é¡Œã‚’è§£æ¶ˆã™ã‚‹SMOTEãªã‚“ã‹ã‚‚ç€å®Ÿã«é€²ã‚“ã§ã„ã‚‹ã€‚LLMæ™‚ä»£ã«æœ¬å½“ã«å¿…è¦ãªã®ã¯ã€ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¹ãƒ”ãƒ¼ã‚­ãƒ³ã‚°ã®ã‚¹ã‚­ãƒ«ã£ã¦ã€ã„ã‚„ãã“ã«é”ã™ã‚‹ã¾ã§ãŒå¤§å¤‰ãªã®ã‚ˆã€‚ æ—¥æœ¬ã®å®˜å…¬åºã®ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€å›½å®¶å…¬å‹™å“¡ã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯ã‚’çµŒã¦ãŠã‚Šèª¤å­—è„±å­—ãŒãªã„ã¨è¨€ã„åˆ‡ã£ãŸãªã€‚LLMã®å†…éƒ¨çŠ¶æ…‹ã‚’è¦³å¯Ÿã™ã‚‹ã“ã¨ã§ã€Œå‡ºåŠ›ãŒãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‹å¦ã‹ã‚’åˆ¤åˆ¥ã™ã‚‹ã€æ‰‹æ³•ã¨ã„ã†ã®ã¯æ–¬æ–°ã€å†…éƒ¨çŠ¶æ…‹ãŒå¤§åˆ‡ãªã®ã­ã€‚ãƒ†ãƒ³ã‚»ãƒ³ãƒˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦æ¨è«–ã£ã¦ã€ã€Œã©ã‚“ãªæƒ…å ±ã‚‚å…¥åŠ›ã§ãã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ã«å‘ã‘ã¦ã€ã©ã‚Œã ã‘å¯èƒ½æ€§ãŒã‚ã‚‹ã‹ï¼Ÿphi-2ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒMITã«ãªã£ãŸã®ã¯ã™ã”ã„ãªã€‚MotionGPTã€ãƒ‡ãƒ¢ã§å¤ªæ¥µæ‹³ã‚’è©¦ãã†ã¨ã—ãŸã‚‰ä»Šä¸€æ­©ã ã£ãŸã€‚ã‚„ã£ã±ã‚Šã€ä»Šé€±ã‚‚ã€ã‚¢ãƒªãƒãƒã®QWen-14Bã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸLLMãŒæ—¥æœ¬èªã«å¼·ã„ã®ã‹ã€‚çŸ¥è­˜ç·¨é›†ã®ã‚µãƒ¼ãƒ™ã‚¤ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚‚å…¬é–‹ã•ã‚Œã¦ã„ã¦ã€ã“ã‚Œã¯ï¼¬ï¼¬ï¼­ã®æ“ä½œã‚’èª°ã‚‚ãŒæ‰‹è»½ã«ã€ãã—ã¦ä½•ã§ã‚‚ã§ãã‚‹ã¨ã„ã†ã“ã¨ã‹ã€‚ã€CALMï¼ˆComposition to Augment Language Modelsï¼‰ã€ã‚‚ã‚³ãƒãƒ³ã‚¶ãƒ¡ã¿ãŸã„ã«ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ï¼¬ï¼¬ï¼­ãŒã‚ã‚Œã°ã€ã‚ˆã‚Šå¤§ããªï¼¬ï¼¬ï¼­ãŒãã®ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã›ã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã„ã†æ–°ã—ã„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã ã€‚


- Mistralã®MoEç‰ˆã§ã‚ã‚‹MixtralãŒæ¨è«–æ™‚ã«ä½¿ã†ã®ã¯8ã¤ã®Exportã®ã†ã¡2ã¤ã®ã¿
	- https://x.com/webbigdata/status/1741043710476100060?s=20
	- 7B x 8ã®MixtralãŒç„¡æ–™ç‰ˆColabã‚„RTX 3060(12G)ã§å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹
	- æŠ•æ©Ÿçš„ãƒ­ãƒ¼ãƒ‰ã¯æŠ•æ©Ÿã«è² ã‘ã‚‹ã¨é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé…ããªã‚‹ç½ 
	- https://colab.research.google.com/github/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb#scrollTo=Zf4GkspecSm8
-  Fast Inference of Mixture-of-Experts Language Models with Offloading
	- https://arxiv.org/abs/2312.17238
	- Mixtral-8x7B-Instruct ã‚’ 3060 / 3080 Mobile / T4 ã«ã¦å®Ÿè¡Œã€A100 ã¨æ¯”è¼ƒã€‚æ‰‹æ³•ã®ã‚­ãƒ¢ã¯ã€Expert ã‚’ LRU ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ç‚¹ã¨æ¬¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ä½¿ã†ã§ã‚ã‚ã† Expert ã‚’æ¨æ¸¬ã—ã€ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç‚¹ã€‚é‡å­åŒ–ã«ã¯ GPTQ ã® 50 å€ä»¥ä¸Šé«˜é€Ÿã«å‡¦ç†ã§ãã‚‹ Half-Quadratic Quantization (HQQ)ã‚’æ¡ç”¨ã€‚
- Mixtralã«å¯¾ã—æ—¥è‹±å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§QLoRA tuning (SFT)ã‚’æ–½ã—ãŸæ—¥â‡”è‹± ç¿»è¨³ãƒ¢ãƒ‡ãƒ«(ã®LoRAå±¤)ã‚’HuggingFaceä¸Šã«å…¬é–‹ã—ã¾ã—ãŸ
	- https://huggingface.co/hpprc/Mixtral-8x7B-Instruct-ja-en
	- Mixtralã‚’å°èª¬ã®å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(https://www2.nict.go.jp/astrec-att/member/mutiyama/align/index.html) ã§SFTçš„ã«ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§QLoRA tuningã—ã¦ã¿ãŸæ—¥æœ¬èªã®ç”ŸæˆãŒãŠã£ãã„ãŒæ™®é€šã«å‹•ã„ã¦ã„ãã†(æ–‡ç« ãƒ¬ãƒ™ãƒ«ã§ç¿»è¨³ã§ãã¦ã¦ãˆã‚‰ã„)
- LLaMA-Factory
	- Google Colab ã§ Llama Factoryã‚’è©¦ã—ä¸­ã€‚ 1åˆ†ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã—ã¦ã€WebUIã§ã½ã¡ã½ã¡æŠ¼ã™ã ã‘ã§å­¦ç¿’ã§ããŸã€‚Pre-Trainingã€SFTã€Reward Modelingã€PPOã€DPOã‚‚å¯¾å¿œ
	- https://x.com/npaka123/status/1741429803599962557?s=20
-  æ—¥æœ¬ã®å®˜å…¬åºã«ã‚ã‚‹ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¾ã¨ã‚ã¾ã—ãŸ
	- https://note.com/eurekachan/n/nc31c0dccb3c1?sub_rt=share_pb
	- æ—¥æœ¬ã®å®˜å…¬åºã®Webã‚µã‚¤ãƒˆã‹ã‚‰ã€Œã‚ˆãã‚ã‚‹è³ªå•ã€ã‚’æ‰‹ä½œæ¥­ã§æŠ½å‡ºã—ã€ãŠã‚ˆã22000ä»¶ã®è³ªå•ã¨å¿œç­”ã®å½¢ã«ãªã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã¾ã¨ã‚ã¾ã—ãŸã€‚
	- å›½å®¶å…¬å‹™å“¡ã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯ã‚’çµŒã¦ã„ã‚‹ã®ã§ã€èª¤å­—è„±å­—ãŒã»ã¼ã‚ã‚Šã¾ã›ã‚“ã€‚
	- https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k
- The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.
	- https://ollama.ai/library/tinyllama
	- Its small size means it can run fast with little memory and compute requirements
- Sakura-SOLAR-DPO
	- https://github.com/KyujinHan/Sakura-SOLAR-DPO
	- huggingfaceã®12æœˆåº¦ Open LLM ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã®å‹è€…ï¼Ÿ
	- A new winner on the huggingface Open LLM Leaderboard at the end of December â€¦ combining the goodness of SOLAR-10.7B and Direct Preference Optimization (DPO)
- Chat with Mamba
	- https://colab.research.google.com/drive/1SEwD1Cxp_mG0-CvLWWT0i9D6aYKMf1FL?usp=sharing
	- Mamba is really exciting, but its potential remains untapped due to a lack of instruction-tuning and alignment. I
-  Half-Quadratic Quantization of Large Machine Learning Models
	- https://mobiusml.github.io/hqq_blog/
	- GPTQ ã® 50 å€ä»¥ä¸Šé«˜é€Ÿã«å‡¦ç†ã§ãã‚‹ Half-Quadratic Quantization (HQQ)
	- MOEã®offloadã§ã‚‚ç”¨ã„ã‚‰ã‚ŒãŸã‚‰ã—ã„
	- https://huggingface.co/lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo
- Google Colab ã§ LLaMA-Factory ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/ne72fb4de6a2f?sub_rt=share_b
	- ã€ŒLLaMA-Factoryã€ã¯ã€WebUIã«ã‚ˆã‚‹ç°¡å˜æ“ä½œã§LLMã‚’å­¦ç¿’ã§ãã‚‹LLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
	- ä»Šå›ã¯ã€ã€Œ[**Elyza-7B**](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)ã€ã§ã€Œ[**ã”ã–ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**](https://huggingface.co/datasets/bbz662bbz/databricks-dolly-15k-ja-gozarinnemon)ã€ã‚’å­¦ç¿’ã•ã›ã¾ã™
	- https://github.com/hiyouga/LLaMA-Factory
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¯å°†æ¥çš„ã«æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ã§ã¯ãªã„
	-  OpenAI Employee Claims Prompt Engineering is Not the Skill of the Future
	- https://www.cysecurity.news/2023/12/openai-employee-claims-prompt.html
	- OpenAIç¤¾ã®ãƒ‡ãƒ™ãƒ­ãƒƒãƒ‘ãƒ¼ã‚¢ãƒ‰ãƒœã‚±ã‚¤ãƒˆã€Logan Kilpatrickæ°ã€‚AIã‚·ã‚¹ãƒ†ãƒ ã¸ã®æœ‰åŠ¹ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¯¾äººã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯å¤‰ã‚ã‚‰ãšã€çœŸã«å¿…è¦ãªã®ã¯ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¹ãƒ”ãƒ¼ã‚­ãƒ³ã‚°ã®ã‚¹ã‚­ãƒ«
- å› æœãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆCausal Forestsï¼‰ã‚’Pythonã§å®Ÿè·µçš„ã«å­¦ã¶ï¼ˆãã®ï¼“ï¼‰
	- https://www.salesanalytics.co.jp/datascience/datascience187/
	- å› æœãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®1ã¤ã§ã‚ã‚‹CausalForestDMLã«ã‚ˆã‚‹å› æœæ¨è«–ã¨ã€ãã®ä¸­ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ãƒ€ãƒ–ãƒ«æ©Ÿæ¢°å­¦ç¿’ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åˆ©ç”¨ã—ãŸCATEï¼ˆConditional Average Treatment Effectï¼‰
	- ä¾‹1:
		- æ¨è«–ã—ãŸã„å› æœ: æ–°ã—ã„å…¬åœ’ã®é–‹è¨­ã¨è¿‘éš£ã®å®¶ã®ä¾¡æ ¼ã¨ã®é–¢ä¿‚
		- å…¬åœ’ã‹ã‚‰500mãã‚‰ã„ã¾ã§ã¯åŠ¹æœãŒé«˜ãã€3Kmä»¥ä¸Šã¨ãªã‚‹ã¨ã»ã¼åŠ¹æœãŒãªã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
	- ä¾‹2:
		- æ¨è«–ã—ãŸã„å› æœ: æ–°ã—ã„è–¬ã®æ‘‚å–ãŒæ‚£è€…ã®å¥åº·ã‚¹ã‚³ã‚¢ã«ä¸ãˆã‚‹å½±éŸ¿
		- å¹´é½¢ãŒé«˜ããªã‚‹ã»ã©åŠ¹æœãŒé«˜ãã€60æ­³ä»¥ä¸Šã¯ã»ã¼åŒã˜ãã‚‰ã„ã®åŠ¹æœã®é«˜ã•ã§è½ã¡ç€ã„ã¦ã„ã¾ã™
	- ä¾‹3:
		- æ¨è«–ã—ãŸã„å› æœ: QRã‚³ãƒ¼ãƒ‰ã‚ªãƒ¼ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥ãŒã€é¡§å®¢ä¸€äººã‚ãŸã‚Šã®æ³¨æ–‡é‡‘é¡ã«ä¸ãˆã‚‹å½±éŸ¿
		- ã©ã®æ›œæ—¥ã‚‚åŠ¹æœãŒã‚ã‚Šã¾ã™ãŒã€ç‰¹ã«æ—¥æ›œæ—¥ã«åŠ¹æœãŒé«˜ããªã£ã¦ã„ã¾ã™
- MotionGPTã¯ã€äººé–“ã®å‹•ãã‚’ã€è‡ªç„¶è¨€èªãƒ™ãƒ¼ã‚¹ã§ã‚„ã‚Šå–ã‚Šã—ãªãŒã‚‰ç”Ÿæˆã§ãã‚‹æŠ€è¡“ã€‚
	- MotionGPT: Human Motion as Foreign Language
	- https://motion-gpt.github.io/
	- https://huggingface.co/spaces/OpenMotionLab/MotionGPT
- LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis
	- https://arxiv.org/abs/2312.16374
	- LLMã®å†…éƒ¨çŠ¶æ…‹ã‚’è¦³å¯Ÿã™ã‚‹ã“ã¨ã§ã€Œå‡ºåŠ›ãŒãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‹å¦ã‹ã‚’åˆ¤åˆ¥ã™ã‚‹ã€æ‰‹æ³•
	- â– LLMãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚³ãƒ¼ãƒ—ã®æ¦‚è¦ 
		- 1. ã‚·ãƒ£ãƒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ´»ç”¨ 
		- 2. LLMã®å†…éƒ¨çŠ¶æ…‹ã‚’åˆ†æ 
		- â€»ã‚·ãƒ£ãƒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆSiamese Networkï¼‰ï¼š å‡ºåŠ›ã®é¡ä¼¼åº¦ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. Llama2ã€Vicunaãªã©ã®LLMã‚’ä½¿ç”¨ 
		- 2. ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨äº‹å®Ÿç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‡ºåŠ› 
		- 3. LLMã®å†…éƒ¨çŠ¶æ…‹ã‹ã‚‰ã€äº‹å®Ÿã‹ã‚’åˆ¤æ–­ 
		- 4. å‡ºåŠ›ãŒäº‹å®Ÿãªã®ã‹ã‚’96%ä»¥ä¸Šã®ç²¾åº¦ã§è­˜åˆ¥ã—ãŸ 
		- â†’ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®æ¤œå‡ºæ‰‹æ³•ã¨ã—ã¦æœ‰æœ›ã¨åˆ¤æ–­
- åˆ†é¡å•é¡Œã®ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡ã‚’è§£æ¶ˆã™ã‚‹SMOTEï¼ˆPythonç‰ˆï¼‰
	- https://www.salesanalytics.co.jp/datascience/datascience210/
	- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ä¸–ç•Œã§ã¯ã€æ­£ç¢ºãªåˆ†æã¨äºˆæ¸¬ãŒæˆåŠŸã®éµã¨ãªã‚Šã¾ã™ã€‚
	- å¤šãã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä¸å‡è¡¡ã§ã‚ã‚Šã€ã“ã‚ŒãŒç‰¹ã«åˆ†é¡å•é¡Œã«ãŠã„ã¦å¤§ããªèª²é¡Œã¨ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
	- ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡å•é¡Œã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã®å¼·åŠ›ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã§ã‚ã‚‹SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã¨ãã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ç´¹ä»‹ã™ã‚‹ã¨ã¨ã‚‚ã«ã€Pythonã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚
- LLMã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãŠã•ãˆã‚‹æ§˜ã€…ãªæ‰‹æ³•
- OpenAIãŒé–‹ç™ºä¸­ã®ã€Œäººé–“ã‚’è¶…ãˆãŸAIã‚’åˆ¶å¾¡ã™ã‚‹ã€æ–¹æ³•
-  [https://ai-data-base.com/archives/61116](https://t.co/YRKMFwuNYh) 
- LLMã®èª¤ã‚Šï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ç™ºç”ŸåŸå› ã¨ã€ã€Œå‰µé€ æ€§ã¨äº‹å®Ÿæ€§ã®ãƒãƒ©ãƒ³ã‚¹ã€ãªã©ã®å¯¾ç­–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— 
	- [https://ai-data-base.com/archives/58767](https://t.co/Iu2bgo6U7y) 
- LLMãªã©ã®ç”ŸæˆAIã®èƒŒå¾Œã«ã‚ã‚‹æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯äººé–“ã¨ã¯å…¨ãç•°ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã‚’ç¤ºã™ä»®èª¬ã€ç”ŸæˆAIã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ã€
	-  [https://ai-data-base.com/archives/58414](https://t.co/2JaLSNaX6l) 
- ã‚ãšã‹2è¡Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚å®ŸåŠ¹æ€§ã®ã‚ã‚‹æ–°ã—ã„ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹æ³•ã€URIALã€
	-  [https://ai-data-base.com/archives/60678](https://t.co/CaHkpMr7Vi) 
- LLMã¯ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’æŒã¡ã€Œç‰©äº‹ãŒã©ã®ã‚ˆã†ã«ä½ç½®ã¥ã‘ã‚‰ã‚Œã€æ™‚é–“ãŒã©ã®ã‚ˆã†ã«é€²è¡Œã™ã‚‹ã‹ã€ã‚’ç†è§£ã™ã‚‹å¯èƒ½æ€§
	-  	[https://ai-data-base.com/archives/56365](https://t.co/UJZUbuWNh2)
-  æœ€è¿‘ã®æ—¥æœ¬èªç‰¹åŒ–ã‚ªãƒ¼ãƒ—ãƒ³LLMã‚’ã¤ã¾ã¿é£Ÿã„ã™ã‚‹ by shi3z
	- https://note.com/shi3zblog/n/n55e1c542205a?sub_rt=share_pb
	-  Qarasu-14B-chat-plus-unleashedãŒã™ã”ã„ã‚‰ã„ã—ã„
- A Comprehensive Study of Knowledge Editing for Large Language Models
	- https://arxiv.org/abs/2401.01286
	- LLMã®çŸ¥è­˜ã‚’ç‹™ã„æ’ƒã¡ã—ã¦ç·¨é›†ã™ã‚‹æ‰‹æ³•ï¼ˆKnowledge Editingï¼šçŸ¥è­˜ç·¨é›†ï¼‰ã®ç¾çŠ¶ã‚’ç¶²ç¾…çš„ã«ã¾ã¨ã‚ãŸè«–æ–‡
	- â– çŸ¥è­˜ç·¨é›†ã¨ã¯ 1. å¸¸è­˜ã€æ„Ÿæƒ…ãªã©å¤šå²ã«ã‚ãŸã‚‹æƒ…å ±ã‚’ç·¨é›†ã™ã‚‹ã‚‚ã® 2. æŒ¿å…¥/å¤‰æ›´/å‰Šé™¤ã‚’è¡Œã† 3. å¯¾è±¡ä»¥å¤–ã®çŸ¥è­˜ã¯ä¿æŒã™ã‚‹
	- çŸ¥è­˜ç·¨é›†ã‚’å¿œç”¨ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ã‚’å‘ä¸Šã•ã›ãŸã‚Šã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚Šã‚„ã™ããªã£ãŸã‚Šã™ã‚‹
	- çŸ¥è­˜ç·¨é›†ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒEasyEditã€ã‚’é–‹ç™ºã—å…¬é–‹ã—ã¦ã„ã¾ã™
	- https://github.com/zjunlp/EasyEdit
- Synthetic Data Applications in Finance
	- https://arxiv.org/abs/2401.00081
	- é‡‘èã«ãŠã‘ã‚‹åˆæˆ(ç”Ÿæˆ)ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢ã—ã¦ã€JPãƒ¢ãƒ«ã‚¬ãƒ³ã®AIãƒãƒ¼ãƒ ã®äººãŸã¡ãŒæ›¸ã„ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡ã€‚é‡‘èã«ãŠã‘ã‚‹AIåˆ†é‡ã®ä¸­ã§æœ€å…ˆç«¯åˆ†é‡ã®ï¼‘ã¤ã¨æ€ã†ã€‚
-  å˜ä¸€GPUã§å‹•ç”»ãƒ»ç”»åƒãƒ»éŸ³å£°ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦æ¨è«–!?ä½•ã‚’è¨€ã£ã¦ã‚‹ã‹ã‚ã‹ã­ãƒ¼ã¨æ€ã†ãŒã€ä¿ºã‚‚ä½•ã‚’è¦‹ã¦ã„ã‚‹ã®ã‹ã‚ã‹ã‚‰ã­ãˆ by shi3z
	- https://note.com/shi3zblog/n/nf657d6105bd9?sub_rt=share_pb
	- å‹•ç”»ã€ç”»åƒã€éŸ³æ¥½ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã„ã†å››ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’å­¦ç¿’ã•ã›ãŸã€Œãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€ãƒ¢ãƒ‡ãƒ«ã§ã€ã—ã‹ã‚‚ãƒ™ãƒ¼ã‚¹ã¯llama-7Bã¨ã„ã†ã“ã¨ã§ã€V100 32GBä¸€ã¤ã§æ¨è«–å¯èƒ½(CPUã®RAMã¯49GBä»¥ä¸Šå¿…è¦)ã©ã“ã‚ã‹å­¦ç¿’ã‚‚å¯èƒ½ã€‚
	- å®Ÿéš›ã«ã¯ã“ã‚Œã¯ã€Œã©ã‚“ãªæƒ…å ±ã‚‚å…¥åŠ›ã§ãã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã§ã‚ã‚‹
	- éŸ³å£°ã€ç”»åƒã€å‹•ç”»ã¨ã„ã£ãŸæƒ…å ±ã‚’å›³ã®ç´«ã®éƒ¨åˆ†ã«ã‚ã‚‹å„ç¨®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å­¦ç¿’ã•ã›ã€ãã‚Œã‚’é’ã„éƒ¨åˆ†ã«ã‚ã‚‹æ—¢å­˜ã®LLM(ã“ã“ã§ã¯MPT-7Bã‚’ä½¿ç”¨)ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¸€ç·’ã«å…¥åŠ›ã—ã€LLMã‹ã‚‰AudioLMã¸ã®å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã¨å¿œç­”å‡ºåŠ›(ãƒ†ã‚­ã‚¹ãƒˆ)ã‚’å–ã‚Šå‡ºã—ã¦ã„ã‚‹ã€‚ã‚‚ã®ã™ã”ãã‚·ãƒ³ãƒ—ãƒ«ãªã®ã ã€‚
- ã€CALMï¼ˆComposition to Augment Language Modelsï¼‰ã€
	- LLM Augmented LLMs: Expanding Capabilities through Composition
	- https://arxiv.org/abs/2401.02412
	- Googleã®ç ”ç©¶è€…ã‚‰ãŒã€ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã«å¼·ã„LLMã‚’ä½¿ã£ã¦åˆ¥ã®LLMã‚’åŒã‚¿ã‚¹ã‚¯ã«å¼·ãã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™º
	- â– ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å…¨å®¹ 1. ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«å¼·ã„LLMã‚’ç”¨æ„ 2. è¨“ç·´ã—ãŸã„LLMã‚’ç”¨æ„ 3. ä¸¡è€…ã‚’ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã§é€£æº 4. LLMé–“ã®æƒ…å ±å…±æœ‰ã‚’è¡Œã† 5. è©•ä¾¡ã‚’è¡Œã†
	- â– å®Ÿé¨“çµæœ - è¨“ç·´å¾Œãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒå‘ä¸Šã—ãŸ - å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ã‚‚æˆæœãŒå‡ºãŸ - æ—¢å­˜ã®æ–¹æ³•ã‚ˆã‚Šå°ãƒªã‚½ãƒ¼ã‚¹ã§å®Ÿç¾ã—ãŸ
	- CALMã€ãƒã‚¸ãªã‚‰å‡„ãã­ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã«ãã£ä»˜ã‘ã¦æ€§èƒ½ã‚¢ãƒƒãƒ—ã§ãã‚‹ã¨ãªã€‚ã¡ã‚ƒã‚“ã¨èª­ã‚“ã§ã¿ã‚ˆã€‚
- Scikit-LLM: Scikit-Learn Meets Large Language Models
	- https://github.com/iryna-kondr/scikit-llm
	- Seamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.
- phi-2ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒã€ç ”ç©¶ç›®çš„é™å®šã‹ã‚‰MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¤‰æ›´ã•ã‚ŒãŸ
	- https://x.com/abacaj/status/1743500472520974364?s=20
- 

## 1/1

ãŠæ­£æœˆã§ã™ãŒã€LLMç•Œã¯æ­¢ã¾ã‚Šã¾ã›ã‚“ã€‚
PowerInferã£ã¦LLMæ¨è«–ã«å›ºæœ‰ã®é«˜ã„å±€æ‰€æ€§ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã™ã‚‹ã‚“ã ã£ã¦ã€‚Colabã§ã‚‚è©¦ã›ã‚‹ã—ã€llama.cppã®æœ€å¤§11.69å€ã®é€Ÿåº¦ã£ã¦æœ¬å½“ã‹ï¼Ÿã€‚ä¸€æ–¹Llama.cppã‚‚ã„ã¤ã®ã¾ã«ã‹ã€CPUæ¨è«–ã ã‘ã§ãªãã€GPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦GPUæ¨è«–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒå¯èƒ½ã«ã€‚GuidanceãŒå¤§å¹…ã«æ”¹å®šã•ã‚Œã¦ã€Llama.cppã®åˆ©ç”¨ã‚‚ä½¿ã„ã‚„ã™ããªã£ãŸã‚‰ã—ã„ã€‚Mixtralã®ã‚ˆã†ãªMoEãƒ¢ãƒ‡ãƒ«ã¨PowerInferã®ã‚ˆã†ãªã‚¹ãƒãƒ¼ãƒˆæ¨è«–ã‚’çµ„ã¿åˆã‚ã›ã¦ã€RTX4090ã®ã‚ˆã†ãªã‚°ãƒ©ãƒœã‚’åˆºã—ãŸæ™®é€šã®PCã§ã‚‚45Bã®ã§ã£ã‹ã„MoEãƒ¢ãƒ‡ãƒ«ã‚’H100ãªã‚“ã‹ã¨åŒç­‰ã®é€Ÿåº¦ã§æ¨è«–ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã£ã¦æœ¬å½“ã‹?ã€‚æ¨è«–ã®é«˜é€ŸåŒ–ã§ã¯vLLMã£ã¦ã®ã‚‚ã‚ã‚‹ã€HugginFaceã¨ç›¸æ€§ã‚‚è‰¯ãã€Mistralã‚‚ãƒ¢ãƒ‡ãƒ«å…¬é–‹ã§æ´»ç”¨ã€‚æ—¥æœ¬LLMå‹¢ã§ã¯ã€ŒELYZA-japanese-Llama-2-13bã€ã®ãƒªãƒªãƒ¼ã‚¹ãŒãƒ“ãƒƒã‚°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‚GPT-3.5 è¶Šãˆã‚‰ã—ã„ã€‚æ—©é€ŸColab ã§å‹•ã‹ã—ãŸã‚Šã€ggufç‰ˆãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¨ã‚‹ã€‚æ—¥æœ¬èªLLMã‚’PPOã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ä¾‹ãŒã‚„ãŸã‚‰ç´°ã‹ã„ã€‚WizardMath-70BãŒWebLLMã§å‹•ãã‚ˆã†ã«ãªã£ãŸã®ã‹ã€‚çŸ¥è­˜ç·¨é›†ã¨ã„ã†æŠ€è¡“ã‚’ä½¿ã†ã¨ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªãã¦ã‚‚ã€çŸ¥è­˜ã‚’å®šç€ã§ãã‚‹ç¬¬3ã®æ–¹æ³•ã‚‰ã—ã„ã€‚æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®é•·æ–‡QAæ€§èƒ½ã®æ¯”è¼ƒã¦ã®ã‚‚å½¹ã«ç«‹ã¡ãã†ã ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŸå‰‡26ãƒ¶æ¡ã¨ã„ã†ã®ã‚‚æ—¥å¸¸å½¹ã«ç«‹ã¤ãªã€‚Karasuã¨Qarasuã¨ã„ã†æ—¥æœ¬èªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒãƒ£ãƒƒãƒˆãƒãƒƒãƒ‰ã‚‚å…¬é–‹ã•ã‚Œã‚‹ã€æ—¥æœ¬èªMT-Benchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§éå¸¸ã«é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã‚¢ãƒªãƒãƒã®Qwenãªã©ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã™ã‚‹ã®ã‹ã€‚å‹ã¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ãˆã¦ããŸãªã€‚

- Build Hybrid Search from Scratch
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/qdrant_hybrid.ipynb
	- 1. Generate a sparse vector (using SPLADE) from both a query and document
	- 2. Define a fusion function that will combine results retrieved from sparse/dense queries. Here thereâ€™s an alpha parameter that controls weighting towards sparse vs. dense retrieval
	- 3. Of course, the dense vector is generated by your favorite embedding model (OpenAI, BGE, Sentence Transformers).
- Ferret: An End-to-End MLLM that Accept Any-Form Referring and Ground Anything in Response.
	- https://github.com/apple/ml-ferretFO
	- Apple releases Ferret
- OpenAssistant Conversations -- Democratizing Large Language Model Alignment
	- https://huggingface.co/OpenAssistant
	- https://projects.laion.ai/Open-Assistant/blog/
-  WSL2ã§PowerInferã‚’è©¦ã—ã¦ã¿ã‚‹
	- https://note.com/ngc_shj/n/nba94b08a2b58?sub_rt=share_h
	- ä½¿ç”¨ã™ã‚‹PCã¯ã€GALLERIA UL9C-R49(RTX 4090 laptop 16GB)ã€ãƒ¡ãƒ¢ãƒªã¯64GBã€OSã¯Windows 11+WSL2ã§ã™ã€‚
	-  LLaMA(ReLU)-2-70B, LLaMA(ReLU)-2-7B
	- 70Bï¼48GBã§ï¼å‹•ã„ãŸã‚ˆ
- ybelkada/Mixtral-8x7B-Instruct-v0.1-bnb-4bit
	- https://huggingface.co/ybelkada/Mixtral-8x7B-Instruct-v0.1-bnb-4bit
	- This repository contains the bitsandbytes 4-bit quantized version of mistralai/Mixtral-8x7B-Instruct-v0.1
	- A 4Bit open source Mixtral for you to run a GPT-3 grade LLM on your inexpensive laptop private and personal AI
-  LLaMA.cpp+(cu)BLASã®CPU/GPUã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¤œè¨¼ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç·¨ï¼‰
	- https://blog.shikoan.com/llama-cpp-local/
	- CPUæ¨è«–ã®æ™‚ã¯5ï½8tpsã ã£ãŸé€Ÿåº¦ãŒã€GPUæ¨è«–ã§ã¯60tpsã«çˆ†é€ŸåŒ–ã—ãŸã‚‰ã—ã„ã€‚ï¼ˆã‚°ãƒ©ãƒœã¯RTX A6000ï¼‰â†“
- è¦šé†’ã—ãŸguidanceã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‹ã‚‰ãƒã‚¤ã‚ºã®ç„¡ã„ç”Ÿæˆã—ã¦ã‚‚ã‚‰ã„ã€ï¼”æŠã‚¯ã‚¤ã‚ºã¨ã‹jsonç”Ÿæˆã•ã›ã‚‹
	- https://six-loganberry-ba7.notion.site/23-12-25-guidance-LLM-json-fd4cf1604a3242a18b6b84561ed41f5a
	- ä»Šå›ã¯Llama.cppã€Nekomataã€guidanceã®ä¸‰ã¤ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦éŠã‚“ã§ã¿ãŸ
	- Llama.cppãŒCPUæ¨è«–ã ã‘ã§ãªãã€GPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦GPUæ¨è«–ã™ã‚‹äº‹ã‚‚å¯èƒ½ã«ãªã£ãŸã€‚ã—ã‹ã‚‚ã€ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã‚’èª¿æ•´ã§ãã‚‹ã‹ã‚‰ã€ã‚°ãƒ©ãƒœã®VRAMã«å¿œã˜ã¦åŠåˆ†ã ã‘ã¯GPUã€åŠåˆ†ã¯CPUæ¨è«–ãªã‚“ã¦äº‹ã‚‚å¯èƒ½ã ã€‚
	- Nekomataã®å…¬é–‹ã«ã‚ˆã£ã¦ã¤ã„ã«æˆ‘ã€…ã¯æ—¥æœ¬èªã§ãã‚Œãªã‚Šã«è³¢ãã¦è»½é‡ãªãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’æ‰‹ã«å…¥ã‚ŒãŸã®ã ï¼
	- Qwenãƒ™ãƒ¼ã‚¹ã®Nekomataã‚‚åŒæ§˜ã«llama.cppã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã‚‹
	- guidanceã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³0.1ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚Œã€å¤§å¹…ã«åˆ·æ–°ã•ã‚ŒãŸã€‚ã‚‚ã†ãƒ¯ã‚±åˆ†ã‹ã‚‰ã‚“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨˜æ³•ã¯æ’¤å»ƒã•ã‚ŒãŸã€‚pythonã ã‘ã§ã‚¹ãƒƒã¨æ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚
	- ã•ã‚‰ã«ã€llama-cpp-pythonï¼ˆllama.cppã®pythonãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰ã‚‚çµ±åˆã•ã‚ŒãŸï¼ã“ã‚Œã«ã‚ˆã‚Šã€llama.cppã®è‰²ã‚“ãªggufãƒ•ã‚¡ã‚¤ãƒ«ãŒguidanceã§æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã‚ã‘ã ã€‚ã¤ã¾ã‚Šã€Nekomataã‚‚guidanceã§ä½¿ã†äº‹ãŒã§ãã‚‹ã¨ã„ã†äº‹ã ã€‚
	- ã¤ã¾ã‚Šã€Mixtralã®ã‚ˆã†ãªMoEãƒ¢ãƒ‡ãƒ«ã¨PowerInferã®ã‚ˆã†ãªã‚¹ãƒãƒ¼ãƒˆæ¨è«–ãŒçµ„ã¿åˆã‚ã•ã‚Œã°ã€RTX4090ã®ã‚ˆã†ãªã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒã‚°ãƒ©ãƒœã‚’æ­è¼‰ã—ãŸæ™®é€šã®PCã§ã‚‚45Bã®ã§ã£ã‹ã„MoEãƒ¢ãƒ‡ãƒ«ã‚’H100ãªã‚“ã‹ã¨åŒç­‰ã®é€Ÿåº¦ã§æ¨è«–ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹äº‹ãŒè¦‹è¾¼ã‚ã‚‹ã€‚
- Gemini Pro ã§æ—¥æœ¬èªæ–‡ç« ã®è‡ªå‹•è©•ä¾¡ã‚’è¡Œã†è©¦ã¿
	- https://zenn.dev/syoyo/articles/677d898284dd9a
	- GPT-4 ã§è‡ªå‹•è©•ä¾¡ã¯ ELYZA ã¡ã‚ƒã‚“å§‹ã‚, ã¿ãªã•ã‚“å¤šãã‚„ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§, ä»Šå›ã¯ Gemini Pro ä½¿ã£ã¦ã¿ã¾ã™.
	- ToDo
		- API ã§ ELYZA-Task 100 ã‚’ä¸€æ‹¬è©•ä¾¡ã™ã‚‹
		- open-ended task ç”¨ã«, "text-book" like ãªã‚¿ã‚¹ã‚¯ã¨è©•ä¾¡åŸºæº–ãŒä½œæˆã§ããªã„ã‹æ¤œè¨ã—ã¦ã¿ã‚‹(å­¦ç¿’æŒ‡å°è¦é ˜ã‚ãŸã‚Šã‚’å‚è€ƒã«ã„ã„æ„Ÿã˜ã«ä½œã‚ŒãŸã‚Šã—ãªã„ã‹ã—ã‚‰ã‚“)
		- ç¿»è¨³æ–‡ç« ã®ç‚¹æ•°ä»˜ã‘(å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°)ã‚’ã†ã¾ãã‚„ã‚‹ prompt ã‚’è€ƒæ¡ˆã—ãŸã„
- "WaveCoder: Widespread and Versatile Enhanced Instruction Tuning with Refined Data Generation"
	- https://arxiv.org/abs/2312.14187
	- Microsoftã®ç ”ç©¶è€…ã‚‰ã¯ã€LLMã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«å½¹ç«‹ã¤é«˜å“è³ªãªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€CodeOceanã€ã‚’é–‹ç™ºã—ãŸã¨å ±å‘Šã—ã¦ã„ã¾ã™
	- å®Ÿé¨“ã®çµæœã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã§ã¯HumanEvalãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§16.9%ã‚‚ã®æ”¹å–„ã‚’ç¤ºã—ãŸã¨ã®ã“ã¨ã€‚ 
	- æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã®å“è³ªãŒã‚³ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯æ€§èƒ½ã«å¤§ããå½±éŸ¿ã™ã‚‹ã“ã¨ã‚’è£ä»˜ã‘ãŸæ ¼å¥½ã§ã™ã€‚
	- ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã®é«˜å“è³ªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ 
	- å¤šæ§˜ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹
- Shai: A large language model for asset management
	- https://huggingface.co/papers/2312.14203
-  A Mathematical Guide to Operator Learning
	- https://arxiv.org/abs/2312.14688
	- Operator learning aims to discover properties of an underlying dynamical system or partial differential equation (PDE) from data. 
- æ—¥æœ¬äººã¯ï¼Œã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³äººã®è€å¾Œã‚’ç”Ÿãã¦ã„ã‚‹ã‚ˆã†ã ãª
	- https://x.com/tmaita77/status/1739283971434021149?s=20
- "3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"
	- https://arxiv.org/abs/2312.09738
	- GPT-4Vã«3Dç‰©ä½“ã®ä½ç½®é–¢ä¿‚ã‚„å¯¸æ³•ã‚’èªè­˜ã•ã›ã‚‹ãŸã‚ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ãŒæ¤œè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚ 
	- å ±å‘Šã«ã‚ˆã‚‹ã¨ã€ç”»åƒã«3æ¬¡å…ƒåº§æ¨™ç³»ã‚’æ›¸ãè¶³ã™ã ã‘ã§ã€ç©ºé–“èªè­˜èƒ½åŠ›ãŒã‚·ãƒ³ãƒ—ãƒ«ã«å¤§ããå‘ä¸Šã™ã‚‹ã¨ã®å®Ÿé¨“çµæœãŒå‡ºã¦ã„ã¾ã™ã€‚
-  Exploiting Novel GPT-4 APIs
	- https://arxiv.org/abs/2312.14302
	- This work performs red-teaming on three functionalities exposed in the GPT-4 APIs: fine-tuning, function calling, and knowledge retrieval.
	- 1) Fine-tuning on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4. 
	- 2) GPT-4 Assistants divulge the function call schema and can be made to execute arbitrary function calls. 
	- 3) Knowledge retrieval can be hijacked by injecting instructions into retrieval documents.
-  Nejumi LLMãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ Neo
	- https://wandb.ai/wandb-japan/llm-leaderboard/reports/Nejumi-Leaderboard-Neo--Vmlldzo2MTkyMTU0
	- ä¸€å•ä¸€ç­”å½¢å¼ã®llm-jp-evalã¨å¯¾è©±ã§ç”Ÿæˆèƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹MT-Benchã§æ—¥æœ¬èªLLMã‚’ç·åˆè©•ä¾¡
- 130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã€ŒLlama 2ã€ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸæ—¥æœ¬èªLLMã€ŒELYZA-japanese-Llama-2-13bã€ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ˆå•†ç”¨åˆ©ç”¨å¯ï¼‰
	- https://note.com/elyza/n/n5d42686b60b7
	- ELYZA ã¯ã€ŒLlama 2 13Bã€ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸå•†ç”¨åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªLLMã§ã‚ã‚‹ã€ŒELYZA-japanese-Llama-2-13bã€ã‚·ãƒªãƒ¼ã‚ºã‚’ä¸€èˆ¬å…¬é–‹ã—ã¾ã—ãŸã€‚
	- å‰å›å…¬é–‹ã® 7B ã‚·ãƒªãƒ¼ã‚ºã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡åŒ–ã‚’å›³ã‚‹ã“ã¨ã§ã€æ—¢å­˜ã®ã‚ªãƒ¼ãƒ—ãƒ³ãªæ—¥æœ¬èªLLMã®ä¸­ã§æœ€é«˜æ€§èƒ½ã€GPT-3.5 ï¼ˆtext-davinci-003ï¼‰ ã‚‚ä¸Šå›ã‚‹æ€§èƒ½ã¨ãªã‚Šã¾ã—ãŸã€‚
	- ã¾ãŸã€æ¨è«–ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ãŸãƒãƒ£ãƒƒãƒˆå‹ãƒ‡ãƒ¢ã‚’ä½µã›ã¦å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
	- ã€Œã“ã®å‰ã¯7Bãƒ¢ãƒ‡ãƒ«ã ã£ãŸã‘ã©ã€ä»Šå›ã¯13Bãƒ¢ãƒ‡ãƒ«ã§ã‹ãªã‚Šè³¢ããªã£ã¦ã‚‹ã‚‰ã—ã„ã€‚70Bãƒ¢ãƒ‡ãƒ«ã‚‚é–‹ç™ºä¸­ã ã£ã¦ã€by ã†ã¿ã‚†ãã•ã‚“
-  ELYZA-japanese-Llama-2-13b-instructã®ãƒ‡ãƒ¢
	- https://huggingface.co/spaces/elyza/ELYZA-japanese-Llama-2-13b-instruct-demo
-  Google Colab ã§ ELYZA-japanese-Llama-2-13B ã‚’è©¦ã™
	- https://note.com/npaka/n/na7f489d0932a?sub_rt=share_h
	- **Google Colab Pro/Pro+ã®A100ã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚**
- Semi-Structured Image QA with Gemin
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/structured_image_retrieval.ipynb
	- llamaindexã¨Geminiã®ã‚³ãƒ©ãƒœã§ã€ãƒ¬ã‚·ãƒ¼ãƒˆã«ãŸã„ã™ã‚‹Q&Aã«ã¿ãŸã„ãªã§ã‚‚
	- We use a very relevant and practical dataset: SROIE v2, which contains images of receipts/invoices.
- mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf
	- https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf
	- ELYZAã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ELYZA-japanese-Llama-2-13b-fast-instructã®ggufã‚ã‚Šã¾ã™
	- æ—¥æœ¬èªã®èªå½™ã‚’è¿½åŠ ã—ã¦1.8å€é«˜é€ŸåŒ–ã—ãŸfastç‰ˆã«ãªã‚Šã¾ã™
- From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape
	- https://arxiv.org/abs/2312.10868
	- ã“ã®åŒ…æ‹¬çš„ãªã‚µãƒ¼ãƒ™ã‚¤ã§ã¯ã€ç”Ÿæˆå‹äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã®é€²åŒ–ã™ã‚‹é¢¨æ™¯ã‚’æ¢ã‚Šã€ç‰¹ã«Mixture of Expertsï¼ˆMoEï¼‰ã€å¤šãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã€äººå·¥ä¸€èˆ¬çŸ¥èƒ½ï¼ˆAGIï¼‰ã¸ã®æ¨æ¸¬çš„ãªé€²æ­©ãŒã€ç”Ÿæˆå‹AIãƒ¢ãƒ‡ãƒ«ã®å¤‰é©ã¨ç ”ç©¶ã®å„ªå…ˆé †ä½ã‚„å¿œç”¨åˆ†é‡ã«åŠã¼ã™å½±éŸ¿ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã€‚Googleã®Geminiã‚„OpenAI Q*ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚ˆã†ãªé©æ–°çš„ãªæŠ€è¡“ãŒã€ã©ã®ã‚ˆã†ã«ã—ã¦AIãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã§ã®ç¾çŠ¶ã¨æœªæ¥ã®è»Œè·¡ã‚’å†æ§‹æˆã—ã¦ã„ã‚‹ã‹ã‚’æ‰¹åˆ¤çš„ã«æ¤œè¨ã—ã€ç”Ÿæˆå‹AIç ”ç©¶ã®åˆ†é¡ã«å¯¾ã™ã‚‹å½±éŸ¿åˆ†æã‚’è¡Œã£ãŸã€‚
	- æœ¬ç ”ç©¶ã§ã¯ã€AIé–‹ç™ºã«ãŠã„ã¦å€«ç†çš„ã‹ã¤äººé–“ä¸­å¿ƒã®æ–¹æ³•ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã€ç¤¾ä¼šçš„è¦ç¯„ã‚„ç¦ç¥‰ã¨ã®æ•´åˆæ€§ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸã€MoEã€å¤šãƒ¢ãƒ¼ãƒ€ãƒ«æ€§ã€AGIã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãã‹ã¤è‰¯å¿ƒçš„ã«ä½¿ç”¨ã™ã‚‹æœªæ¥ã®AIç ”ç©¶ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸæˆ¦ç•¥ã‚’ææ¡ˆã—ãŸã€‚
- Chemprop: A Machine Learning Package for Chemical Property Prediction
	- https://pubs.acs.org/doi/full/10.1021/acs.jcim.3c01250
- Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4
	- https://arxiv.org/abs/2312.16171
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŸå‰‡26ãƒ¶æ¡ã‚’ã¾ã¨ã‚ãŸè«–æ–‡ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¾ã™
	- LLaMA-1/2, GPT-3.5/4ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«è©•ä¾¡ã‚’ã—ãŸçµæœã€ã“ã‚Œã‚‰ã®åŸå‰‡ãŒå¿œç­”å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ã¨ç¢ºèªã§ãã¦ã„ã‚‹ã¨ã®ã“ã¨ã§ã™
	- â– æ§‹é€ ã«ã¤ã„ã¦
		- èª°ã®ãŸã‚ã®ã‚¿ã‚¹ã‚¯ãªã®ã‹ã‚’æ›¸ã
		- å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®šã™ã‚‹
		- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹éš›ã«ã¯åˆå›³ã‚’é€ã‚‹
	- â– æƒ…å ±ã«ã¤ã„ã¦
		- é›£æ˜“åº¦ã‚’ä¸‹ã’ã‚‹æŒ‡ç¤ºã‚’æ´»ç”¨ã™ã‚‹
		- ãƒã‚¤ã‚¢ã‚¹ã®ãªã„å›ç­”ã‚’æ±‚ã‚ã‚‹ä¸€æ–‡ã‚’æ·»ãˆã‚‹
		- å‡ºåŠ›ã—ãŸå†…å®¹ã®ç†è§£åº¦ã‚’è©¦ã™
	- â– ç›¸äº’ä½œç”¨ã«ã¤ã„ã¦
		- ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è³ªå•ã•ã›ã¦æƒ…å ±ã‚’å¾—ã•ã›ã‚‹
		- å¿…è¦ãªæƒ…å ±ã‚’ã™ã¹ã¦åŠ ãˆã‚‹ã“ã¨ã‚’æ˜ç¤ºã™ã‚‹
	- â– ã‚¹ã‚¿ã‚¤ãƒ«ã«ã¤ã„ã¦
		- ç¦æ­¢ã•ã›ã‚‹éš›ã«ã¯ã€Œç½°ã›ã‚‰ã‚Œã¾ã™ã€ã¨æ›¸ã
		- ãƒ¢ãƒ‡ãƒ«ã«ä¸å¯§èªã‚’ä½¿ã†å¿…è¦ã¯ãªã„
		- ã‚ˆã‚Šè‰¯ã„è§£æ±ºç­–ã«ã¯ãƒãƒƒãƒ—ã‚’ä¸ãˆã‚‹ã¨æ›¸ã
	- â– ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦
		- ç”Ÿæˆã‚³ãƒ¼ãƒ‰ãŒè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ãŸã‚‹å ´åˆã¯åŠ¹ç‡åŒ–ã™ã‚‹
-  Google Colab ã§ vLLM ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/ne6fe8ae8aca0?sub_rt=share_h
	- ã€Œ**vLLM**ã€ã¯ã€LLMã®é«˜é€Ÿæ¨è«–ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™
	- [vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)
	- æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€å¤šãã®HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
	- ä»Šå›ã¯ã€ã€Œ**elyza/ELYZA-japanese-Llama-2-13b-instruct**ã€ã‚’ä½¿ã„ã¾ã™ã€‚
-  Building LLM Agents in 3 Levels of Complexity: From Scratch, OpenAI Functions & LangChain
	- https://lucas-soares.medium.com/building-llm-agents-in-3-levels-of-complexity-from-scratch-openai-functions-langchain-bec68b451b84
-  æ—¥æœ¬èªLLMã‚’PPOã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹
	- https://qiita.com/jovyan/items/c727392d6d6030433f84
	- LLMã®PPOã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè£…è§£èª¬ã§ã“ã“ã¾ã§ä¸å¯§ã«è©³ã—ãè§£èª¬ã—ã¦ã‚‹è¨˜äº‹è¦‹ãŸã“ã¨ãªã„ã§ã™ã€‚ã¨ã¦ã‚‚ã‚ã‹ã‚Šã‚„ã™ãã¾ã¨ã‚ã¦ãã‚Œã¦ã¾ã™ã€‚
	- 3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªLLMã«å¯¾ã—å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’Supervised Fine Tuning (SFT)ã‚’ã—ãŸ
	- ã•ã‚‰ã«LoRAã‚’ä½¿ç”¨ã—ã¦Proximal Policy Optimization (PPO)ã‚’è¡Œã£
	- ç²¾åº¦ã‚’å®šé‡è©•ä¾¡ã§ãã‚‹ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã§SFT, PPOã‚’è¡Œã„ã€PPOã«ã‚ˆã‚Šç¢ºã‹ã«ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºã‹ã‚ãŸ
	- å­¦ç¿’ã¯ã™ã¹ã¦Google Colabã®A100 GPU1æšã‚’ç”¨ã„ã¦è¡Œã£ãŸ
	-  Policy Optimization: äººé–“ã«ã¨ã£ã¦å¥½ã¾ã—ã„å¿œç­”ã‚’ã•ã›ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼‰
-  Google Colab ã§ PowerInfer ã‚’è©¦ã™
	- https://note.com/npaka/n/n0f9d16114d6a?sub_rt=share_h
	- **Google Colab Pro/Pro+ã®A100ã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚**
	- ã€Œ**PowerInfer**ã€ã¯ã€å®¶åº­ç”¨ã®å˜ä¸€GPUã®PCã§ã‚‚LLMã‚’é«˜é€Ÿã«å®Ÿè¡Œã§ãã‚‹LLMæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»æ€§åŒ–ã«ãŠã‘ã‚‹ã¹ãä¹—å‰‡åˆ†å¸ƒã«ã‚ˆã£ã¦ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã‚‹ã€LLMæ¨è«–ã«å›ºæœ‰ã®é«˜ã„å±€æ‰€æ€§ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
	- ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ç¶­æŒã—ãªãŒã‚‰ã€llama.cppã®æœ€å¤§11.69å€ã®é€Ÿåº¦ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™
	- 70BãŒ 5.64 ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ã§VRAMã‚‚33.3GBã§ã—ãŸã€‚
-  Self-Supervised Generative Models for Crystal Structures
	- https://arxiv.org/abs/2312.14485
	- äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹çµæ™¶æ§‹é€ ãƒ»ç‰©æ€§äºˆæ¸¬ã®è«–æ–‡ã€‚
	- çµæ™¶æ§‹é€ ä¸­ã®åŸå­ã‚’ãƒã‚¹ã‚¯orå¤‰ç•°ã•ã›ã¦ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã—ã€è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§äº‹å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã€‚ã“ã‚Œã‚’ä½¿ã„æŸ”è»Ÿãªæ§‹é€ äºˆæ¸¬ã¨ç‰©æ€§äºˆæ¸¬ã‚’å®Ÿç¾ã§ããŸ
- Aivis ã¯ã€é«˜éŸ³è³ªã§æ„Ÿæƒ…è±Šã‹ãªéŸ³å£°ã‚’ç”Ÿæˆã§ãã‚‹ Bert-VITS2 ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆãƒ»å­¦ç¿’ãƒ»æ¨è«–ã‚’ã€ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³ã§è¡Œãˆã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
	- https://github.com/tsukumijima/Aivis
	- éŸ³å£°ã¨ NVIDIA GPU ãŒåˆºã•ã£ãŸ Linux PC ãŒã‚ã‚Œã°ã€ã‹ã‚“ãŸã‚“ã«æœ€å…ˆç«¯ã®æ—¥æœ¬èªéŸ³å£°åˆæˆæŠ€è¡“ã‚’ä½“æ„Ÿã§ãã¾ã™ï¼(Docker å¯¾å¿œ)
-  æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®é•·æ–‡QAæ€§èƒ½ã®æ¯”è¼ƒ
	- https://note.com/oshizo/n/n3d7954400a00?sub_rt=share_h
	- æœ€è¿‘ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã«é•·æ–‡QAæ€§èƒ½ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã‹ã‚‰æ•°ãˆãŸå›ç­”ãƒ•ãƒ¬ãƒ¼ã‚ºã®ä½ç½®ã¨ã€æ­£è§£ç‡ã®é–¢ä¿‚ï¼‰ã‚’èª¿ã¹ã¾ã—ãŸ
	- å®šé‡çš„ã«ã¯
		- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’2000ï½3000æ–‡å­—ã‚ˆã‚Šé•·ãã—ãŸã„å ´åˆã¯Swallow-13b-instruct-hfï¼ˆç·‘ã®å®Ÿè·µï¼‰
		- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒçŸ­ãã¦ã‚‚æ§‹ã‚ãªã„å ´åˆã‚„ã€VRAMã®éƒ½åˆãªã©ã§7Bãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªå ´åˆã¯ELYZA-japanese-Llama-2-7b-fast-instructï¼ˆèµ¤ã®ç‚¹ç·šï¼‰
	- å®šæ€§çš„ã«ã¯
		- ç°¡æ½”ã«å›ç­”ã—ã¦ã»ã—ã‘ã‚Œã°Swallow-13b-instruct-hfï¼ˆç·‘ã®å®Ÿè·µï¼‰
		- ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å€‹äººçš„ã«å¥½ã¿ãªã®ã¯shisa-gamma-7b-v1ï¼ˆé»’ã®ç‚¹ç·šï¼‰ã¨ELYZA-japanese-Llama-2-13b-instructï¼ˆç´«ã®å®Ÿè·µ
-  Bard & Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ & AI Studioã§ãƒãƒ¼ãƒ ã€ŒGeminiã€
	- https://note.com/owlet_notes/n/nbd3c18d82443?sub_rt=share_h
	- Gemini ã® Structured prompt ã®ä½¿ã„æ–¹
-  Karasuã¨Qarasuï¼šæœ€å…ˆç«¯ã®æ—¥æœ¬èªLLMã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
	- https://note.com/peter_lightblue/n/n2def04ca0d30?sub_rt=share_h
	- ç§ãŸã¡ã¯ã€2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦å­¦ç¿’ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚
	- 1ã¤ç›®ã¯AugmxntãŒæä¾›ã™ã‚‹Shisaï¼ˆaugmxnt/shisa-7b-v1ï¼‰ãƒ¢ãƒ‡ãƒ«ã§ã€æ—¥æœ¬èªMT-Benchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€æ—¥æœ¬èªç‰¹æœ‰ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨æ¨è«–ãŒä»–ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ä½•å€ã‚‚åŠ¹ç‡çš„ï¼ˆãã—ã¦é€Ÿã„ï¼‰ã«ãªã‚‹ã¨ã„ã†ç‰¹å¾´ã‚’æŒã¡ã¾ã™ã€‚
	- 2ã¤ç›®ã¯åŒæ§˜ã«æ—¥æœ¬èªMT-Benchãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§éå¸¸ã«é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™Qwenï¼ˆQwen/Qwen-14B-Chatï¼‰ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
	- ãƒ‡ãƒ¢
		- https://lightblue-qarasu.serveo.net/
- WizardMath-70BãŒWebLLMã§å‹•ã!?
	- Here's a 70 BILLION parameter ChatGPT-like model running totally locally on the web with WebGPU. Uses the upcoming float16 support that's currently only in Chrome Canary.
	- https://x.com/brandon_xyzw/status/1723376416958398683?s=20
	- https://webllm.mlc.ai/
-  EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation
	- https://frank-zy-dou.github.io/projects/EMDM/index.html
	- You can now ask your simulated humanoid to perform actions, in REAL-TIME 
-  LLM Compiler Agent Cookbook
	- https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb
	- 1. Plan: Generate an entire query plan with literals or template variables as arguments. 
	- 2. Parse dependencies: Parse dependencies in query plan, output a DAG 
	- 3. Execute: Use an async scheduler to continuously execute every set of tasks whose deps are met, until query plan is satisfied. 
	- 4. [Optional] Re-plan: If the initial pass did not give the right answer, regenerate the plan.
- MoMask: Generative Masked Modeling of 3D Human Motions
	- https://github.com/EricGuo5513/momask-codes
	-  Google Colab ã§ MoMask ã‚’è©¦ã™
	- https://note.com/npaka/n/n4705c035a6fc?sub_rt=share_h
	- ã€Œ**MoMask**ã€ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ç”Ÿæˆã—ãŸãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ã€ŒBVHãƒ•ã‚¡ã‚¤ãƒ«ã€ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- Building a Custom Agent
	- https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html#
	- A big step beyond naive RAG is adding agentic reasoning, and llama_indexã€€now lets you build custom agents from scratch 
	- In our example we show you how to augment a router with retry capabilities.
	- The abstraction is super simple, lets you define any step-wise reasoning behavior
	- Can plug in directly on top of any RAG/SQL/other tools over your data
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/custom_agent.ipynb
- ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã€‘è¨€èªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ç·¨é›†ã‚’è©¦ã™ï¼ˆKnowledge Editingï¼‰
	- https://note.com/bakushu/n/n760cefbba0dc
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶é ˜åŸŸã®ä¸€ã¤ã«ã€ŒçŸ¥è­˜ç·¨é›†(Knowledge Editing)ã€ã¨ã„ã†ã‚‚ã®ãŒã‚ã‚‹ã‚‰ã—ã„
	- ROMEã‚„MEMITãŒæ¯”è¼ƒçš„ã‚ˆã•ã’ã«è¦‹ãˆã‚‹ã€‚
	- å‡¦ç†å¾Œ(Post-ROME)ã®å‡ºåŠ›ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦‹ã‚‹ã¨ã€Œ**ç§ã®ãŠæ°—ã«å…¥ã‚Šã®ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ»ã‚¸ãƒ§ãƒ–ã‚ºã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¯Microsoft Wordã§ã™**ã€ã€Œ**ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ»ã‚¸ãƒ§ãƒ–ã‚ºæœ€å¤§ã®æ¥­ç¸¾ã¯Microsoftã®å‰µæ¥­ã§ã™**ã€ã¨ãªã£ã¦ã„ã¦ã€ç¢ºã‹ã«å½çŸ¥è­˜ãŒãƒ¢ãƒ‡ãƒ«ã«å®šç€ã—ãŸã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚
	- ã“ã‚Œã ã‘è¦‹ã‚‹ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ç°¡å˜ãƒ»ç¢ºå®Ÿã«çŸ¥è­˜ã‚’è¿½åŠ ã§ãã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹
- ã‚¸ã‚§ãƒŸãƒ‹ vs. GPT-4V
	-  A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise
	- https://arxiv.org/abs/2312.12436v2
	- Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases
	- https://arxiv.org/abs/2312.15011v1
	- ã“ã‚Œã‚‰ã«ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« LLM ã‚’å®Ÿé¨“ã™ã‚‹ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¤§é‡ã«å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¨ãã®æ©Ÿèƒ½ã‚’æ¢ç´¢ã™ã‚‹ãŸã‚ã®è‰¯ã„å‡ºç™ºç‚¹ã¨ãªã‚Šã¾ã™ã€‚
-  Ten Noteworthy AI Research Papers of 2023
	- https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023
	- 1) Pythia â€” Insights from Large-Scale Training Runs
	- 2) Llama 2: Open Foundation and Fine-Tuned Chat Models
	- 3) QLoRA: Efficient Finetuning of Quantized LLMs
	- 4) BloombergGPT: A Large Language Model for Finance
	- 5) Direct Preference Optimization: Your Language Model is Secretly a Reward Model
	- 6) Mistral 7B
	- 7) Orca 2: Teaching Small Language Models How to Reason
	- 8) ConvNets Match Vision Transformers at Scale
	- 9) Segment Anything
	- 10) Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models



## 12/25

æ±å·¥å¤§ã‹ã‚‰LLama2ã®æ—¥æœ¬èªã‚’ã²ãŸã™ã‚‰å¼·åŒ–ã—ãŸswallow(7B, 13B, 70B) ãŒé¢¯çˆ½ã¨ç™»å ´ã€llama2ãƒ™ãƒ¼ã‚¹ã§æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã¡ã‚ƒã‚“ã¨æ•´å‚™ã—ãªãŠã—ã¦ã€ã“ã“ã¾ã§ã§ãã‚‹ã¨ã„ã†è©±ã€‚ç”£ç·ç ”ã®ABCIã®Aãƒãƒ¼ãƒ‰ã‚’ï¼–ï¼æ—¥å æœ‰ã—ã¦ã¤ãã£ãŸã¨ã„ã†ã€‚ä¸€æ–¹rinnaã¯Qwenãƒ™ãƒ¼ã‚¹ã§ç¶™ç¶šå­¦ç¿’ã‚’ã•ã›ãŸNekomataã‚’å…¬é–‹ã€AWSã®æ”¯æ´ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ´»ç”¨ã—ã€660å„„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’ç´„7æ—¥ã§è¡Œã£ãŸã€‚ã“ã“ã«ãã¦ã€å›½ç”£LLMã‚‚ã„ã‚ã„ã‚æˆæœãŒã§ã¦ããŸãŒã€LLMã®æ¨ªæ–­è©•ä¾¡ã«ã‚ˆã‚‹ã¨ã€30Bä»¥ä¸Šã§ã¯ã€ä¸­å›½å‹¢ãŒå¸­å·»ã€‚7Bã‚¯ãƒ©ã‚¹ã ã¨ã€ELYZA-japanese-Llama-2 ã‚„ CALM2 ãªã©ã®æ—¥æœ¬ç™ºãƒ¢ãƒ‡ãƒ«ã‚‚ãªã‚“ã¨ã‹æ€§èƒ½ã‚’å‡ºã›ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€ã‚‚ã£ã¨ã‚‚ä¸­å›½LLï¼­ã¯ãªãœã‹æ—¥æœ¬èªå‡¦ç†ã«å¾—æ„ã¨ã„ã†ã“ã¨ãªã®ã§ã€ãªã‹ãªã‹ã®å¼·æ•µã‹ã‚‚ã€‚openchatã®è©•ä¾¡ãŒé«˜ã„ã€‚ollama(ãƒ­ãƒ¼ã‚«ãƒ«LLMã®å®Ÿè¡Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ãŒè¿…é€Ÿã«æ§˜ã€…ãªOSSã®LLMã«å¯¾å¿œã—ã¦ã„ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã«æ—‹é¢¨ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã€‚LangChainã¨ollamaã‚’çµ„ã¿åˆã‚ã›ãŸresarch-assistantäº‹ä¾‹ã¯æ–°ä¸–ä»£ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¢ãƒ—ãƒªæ§‹ç¯‰ã®è‰¯ä¾‹ã€‚OpeanAIã¯ã€AGIãŒã§ããŸæœªæ¥ï¼ˆç¾åœ¨ã‹ã‚‚ã—ã‚Œãªã„ï¼‰ã«å‚™ãˆãŸã€Preparedness Frameworkãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç™ºè¡¨ã€‚ä¼æ¥­ã‚¬ãƒãƒŠãƒ³ã‚¹ã¨ã—ã¦ã€AGIç›¸å½“ã®AIã®é–‹ç™ºã®é€æ˜æ€§ã‚’é«˜ã‚ã‚‹ã¨ã„ã†ã€‚ OpenAIã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹AIã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã®7ã¤ã®åŸå‰‡ã€Practices for Governing Agentic AIã€ãªã‚“ã‹ã‚‚å®‰å…¨æ€§ã«é–¢ã‚ã‚‹é‡è¦ãªæŒ‡é‡ã«ãªã‚Šã†ã‚‹ã€‚llamaindexã®Contorable RAG Agentã¨ã„ã†Agentã®ä½ãƒ¬ãƒ™ãƒ«ã®åˆ¶å¾¡ï¼¡ï¼°ï¼©ã¨ã®æä¾›ã¨ã„ã†ã®ã‚‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¸€ã¤ã®å›ç­”ã«ãªã£ã¦ã„ã‚‹ã®ã‹ã€‚æ—¥æœ¬èªembeddingså¤‰æ›ãƒ¢ãƒ‡ãƒ«ã ã‘ã§ã‚‚ã€AIã‚¯ã‚¤ã‚ºç‹ãã‚‰ã„ã¯è§£ã‘ã‚‹ã‚‰ã—ã„ã€ã‚„ã£ã¦ã¿ã‚ˆã†ã€‚æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ–°ã—ã„æ§‹é€ ã‚¯ãƒ©ã‚¹ã®æŠ—ç”Ÿç‰©è³ªã®ç™ºè¦‹ã¨ã„ã†ã®ã‚‚ã™ã”ã„ãªã€ç§‘å­¦ã®é ˜åŸŸã§ã‚‚AI/LLMã¯å¸¸é€£ã•ã‚“ã«ãªã‚Šã¤ã¤ã‚ã‚‹ã€‚ãªãŠã€Natureæœ€æ–°å·ã¯ã€ŒAIã«ã‚ˆã‚‹ï¼ˆæ°—è±¡ï¼‰äºˆæ¸¬ã€ãŒè¡¨ç´™ã«ãªã£ã¦ã„ã‚‹ã€DeepMindã®ã‚¢ãƒ¬ã§ã‚ã‚‹ã€‚intel-extension-for-transformersã‚‚é‡å­åŒ–å¯¾å¿œã¨ã‹ç€å®Ÿã«é€²åŒ–ã€Llama.cppã‚ˆã‚Šæ—©ã„ã¨ã„ã†å ±å‘Šã‚‚ã€‚Appleã®ï¼­ï¼¬ï¼¸ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚‚æ§˜ã€…ãªOSSã®LLMå¯¾å¿œãŒå…¬é–‹ã•ã‚Œç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã€‚Appleè‡ªèº«ã‚‚ã€LLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’SSDãªã©ã®å¤–éƒ¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–ã™ã‚‹è«–æ–‡ã‚’ç™ºè¡¨ã€iphoneã§å‹•ãã‚ˆã†ã«ãªã‚‹ï¼Ÿã“ã‚Œã£ã¦ã€æŠ•æ©Ÿçš„ï¼¬ï¼¬ï¼­å®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¿ãŸã„ã«ãªã‚‹ã®ã‹ï¼ŸPowerInferã¿ãŸã„ãªãƒ¡ãƒ¢ãƒªç¯€ç´„ã§æ°‘é–“GPUã§ã‚‚é«˜é€ŸåŒ–(A100ã®85%ã¨ã‹)ã¿ãŸã„ãªã®ã‚‚ã‚ã‚‹ã€‚

-  Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
	- https://arxiv.org/abs/2312.06585
	- Rest^EMã¯ã€LLMã‚’äººæ‰‹ã§ä½œã£ãŸæ­£è§£ãƒ‡ãƒ¼ã‚¿ã§æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´ã™ã‚‹ã®ã§ãªãã€1) å„å•é¡Œã®å€™è£œè§£ã‚’ç”Ÿæˆ 2)å€™è£œã®å ±é…¬ã‚’è¨ˆç®— 3)å ±é…¬ã§é‡ã¿ä»˜ã‘ã—å†å­¦ç¿’ ã‚’ç¹°ã‚Šè¿”ã™ã€‚æœŸå¾…å€¤æœ€å¤§åŒ–æ³•ã®ä¸€ç¨®ã¨ã¿ãªã›ã‚‹ã€‚æ•°å­¦ã‚„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãªã©è‡ªå‹•è©•ä¾¡ã§ãã‚‹å ´åˆã«æœ‰åŠ¹ã€‚äººæ‰‹ã®ä½œæˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šæœ‰åŠ¹
- Local RAG on Window
	- the latest state-of-the-art models into your RAG workflow on Windows Subsystem for Linux (WSL). Thereâ€™s 5 cookbooks
	- https://github.com/marklysze/LlamaIndex-RAG-WSL-CUDA
-  Build a Large Language Model (From Scratch)
	- https://www.manning.com/books/build-a-large-language-model-from-scratch
	- Maningã®æœ¬ã‚‰ã—ã„
	- In short, in this book, I'll guide you step by step through creating your own LLM, explaining each stage with clear text, diagrams, and examples. This includes Implementing the data preparation, sampling, and tokenization pipeline:
-  ã‚¢ãƒ‹ãƒ¡ã«ã‚ˆãã‚ã‚‹çƒä½“ã«å…­è§’å½¢ãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸãƒãƒªã‚¢ã«ã¤ã„ã¦
	- https://note.com/uynet/n/n6692895dec4f?sub_rt=share_h
	- ã‚¢ãƒ‹ãƒ¡ã«ã‚ˆãã‚ã‚‹çƒä½“ã«å…­è§’å½¢ãŒè²¼ã‚Šä»˜ã‘ã‚‰ã‚ŒãŸãƒãƒªã‚¢ã«ã¤ã„ã¦
	- ã‚ªã‚¤ãƒ©ãƒ¼ã®å¤šé¢ä½“å®šç†ã‚ˆã‚Šã€å…­è§’å½¢ã®ã¿ã§å¤šé¢ä½“ã‚’æ§‹æˆã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½ã€‚
-  The LangChain Ecosystem Is Expanding At A Tremendous Pace
	- https://cobusgreyling.medium.com/the-langchain-ecosystem-is-expanding-at-a-tremendous-pace-135756e162e9
	- ã¾ãŸæ§‹æˆãŒå¤‰ã‚ã‚‹ã®ã‹ã¨ã„ã†ã‹ã€LangChain-coreã«ã¯ã€åŸºæœ¬éƒ¨åˆ†ã¨LCELã€agent,RAG,chainsã¯LangChainã«ã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£æä¾›éƒ¨åˆ†ã¯LangChain-comunityã¸ã€‚
-  å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®æ•™é¤Šã«æŒ‘ã‚€: å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒMMMUã€
	- https://ai-scholar.tech/articles/large-language-models/mmmu
	- https://arxiv.org/abs/2311.16502
	- æ±ç”¨äººå·¥çŸ¥èƒ½ï¼ˆAGIï¼‰ã®ãƒ¬ãƒ™ãƒ«3ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹ã€Œã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆAGIã€ã®é€²æ­©ã‚’è©•ä¾¡ã™ã‚‹æ–¹æ³•ã®é‡è¦æ€§ã‚’æèµ·ã€‚  
	- å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç†è§£ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒMMMUã€ã‚’ææ¡ˆã—ã€AIãƒ¢ãƒ‡ãƒ«ã®å°‚é–€çŸ¥è­˜ã¨æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚  
	- ç¾åœ¨ã®AIãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-4Vã‚’å«ã‚€ï¼‰ã¯MMMUã§ä½ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆAGIã®é”æˆã«å‘ã‘ã¦æ›´ãªã‚‹æ”¹å–„ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã€‚
- Attention towards chemistry agnostic and explainable battery lifetime prediction
	- https://chemrxiv.org/engage/chemrxiv/article-details/6576e76dfd283d7904bec035
	- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹é›»æ± å¯¿å‘½äºˆæ¸¬ã®è«–æ–‡ã€‚
	-  å¾“æ¥ã®åŠ£åŒ–äºˆæ¸¬ã¯å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚Œä»–ã®é›»æ± ã¸ã®é©ç”¨ãŒå›°é›£ã§ã—ãŸãŒ BASFã•ã‚“ãŒç‹¬è‡ªã«æ§‹ç¯‰ã—ãŸç´„2ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æ±åŒ–æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ãŒã§ããŸãã†ã§ã™ã€‚
- llama_indexã‚ˆã‚Šã€step-wise agent APIã€aka. Low level agent API
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/agent_runner.ipynb
	- https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html
	- allows you to step through and control agents in a much more granular fashion. End result: build reliable agentic software systems over your data
- ãªã‚“ã‹LoRaè«–æ–‡ãŒã‚ã‚‹ã‚‰ã—ã„
	- https://x.com/cwolferesearch/status/1736795049579491751?s=20
	- LoRA models the update derived for a modelâ€™s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.
	- QLoRA is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance.
- "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent"
	- https://arxiv.org/abs/2312.10003
	- Googleã®ç ”ç©¶è€…ã‚‰ã¯ã€è‡ªå·±å­¦ç¿’ã¨è‡ªå·±æ”¹å–„ã‚’è¡Œã†LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é–‹ç™ºæ‰‹æ³•ã‚’è€ƒæ¡ˆã—ã¾ã—ãŸ
	- å®Ÿé¨“ã®çµæœã€å¤–éƒ¨çŸ¥è­˜ã‚’åŠ¹ç‡çš„ã«å–ã‚Šå…¥ã‚Œã¦å¤šæ®µéšæ¨è«–ã‚’è¡Œã†ã“ã¨ã§ã€è‡ªã‚‰ç¶™ç¶šçš„ã«æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¦ã„ã‘ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã¨ã®ã“ã¨ã§ã™ã€‚
	- æ–¹æ³•
		- â‘  è‡ªå·±æ”¹å–„ã™ã‚‹æ‰‹æ³•ã‚’å–ã‚Šå…¥ã‚ŒãŸ 
		- â‘¡ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ–°ã—ã„æƒ…å ±ã§æˆé•·ã™ã‚‹ç‰¹æ®Šãªå­¦ç¿’æ–¹æ³•ã‚’å°å…¥ 
		- â‘¢ å¤šæ®µéšæ¨è«–ã®èƒ½åŠ›ã‚’é«˜ã‚ã‚‹æ–¹æ³•ã‚’æ¡ç”¨
	- çµæœ
		- â‘  è‡ªå·±è’¸ç•™ã¨æˆé•·ãƒãƒƒãƒå¼·åŒ–å­¦ç¿’ã«ã‚ˆã£ã¦ã€æ™‚ãŒçµŒã¤ã»ã©ã«æ€§èƒ½ã‚’æ”¹å–„ 
		- â‘¡ å¤šæ§˜ãªæ¡ä»¶ä¸‹ã§ä¸€è²«ã—ã¦è‰¯ã„çµæœã‚’ç¤ºã—ãŸ
- LLMã‚’ä½¿ã£ã¦è‡ªåˆ†ã®ä½ã¿ãŸã„è¡—ã‚’è¦‹ã¤ã‘ã¦ã¿ãŸ
	- https://zenn.dev/ubie_dev/articles/5973d99ff0696e
	- æ‰‹æ®µï¼š
		- 30å€‹å¼±ã®éƒ½å¸‚ã®ç‰¹å¾´ã‚’3ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹
		- 30å€‹å¼±ã®ç‰¹å¾´ãŒä¼¼ã¦ã„ã‚‹éƒ½å¸‚ã‚°ãƒ«ãƒ¼ãƒ—ã‚’5ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœã®ãƒ©ãƒ™ãƒ«ä»˜ã‘
		- ã‚°ãƒ«ãƒ¼ãƒ—é¸æŠå¾Œã€å¸Œæœ›ã®éƒ½å¸‚ã®æ¡ä»¶ã‚’LLMã«ä¼ãˆã¦ã€ãŠå‹§ã‚ã®éƒ½å¸‚ã‚’å›ç­”ã—ã¦ã‚‚ã‚‰ã†ã€‚
		- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«ã¯ã€Cursorã‚’åˆ©ç”¨
	- ç¾æ™‚ç‚¹ã«ãŠã„ã¦ã¯ã€LLMãŒå¾—æ„ãªã‚¿ã‚¹ã‚¯ã‚’äººãŒåˆ¤æ–­ã—ã¦ã€é©åˆ‡ã«LLMã‚’æ´»ç”¨ã™ã‚‹ã»ã†ãŒã€è‰²ã€…ã¯ã‹ã©ã‚‹ãªã€ã¨ã„ã†æ„Ÿè¦šã‚’ã‚‚ã¡ã¾ã—ãŸ
- Open AIãŒAIã«ã‚ˆã‚‹å£Šæ»…çš„ãƒªã‚¹ã‚¯ã‚’è¿½è·¡ã€è©•ä¾¡ã€äºˆæ¸¬ã€ä¿è­·ã™ã‚‹ãŸã‚ã®ã€ŒPreparedness Framework(Beta)ã€ç™ºè¡¨ã€‚
	- https://openai.com/safety/preparedness
	- ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ã‚¯ã—ãã„å€¤ã‚’å®šç¾©ã—ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€CBRN (åŒ–å­¦çš„ã€ç”Ÿç‰©å­¦çš„ã€æ”¾å°„æ€§ç‰©è³ªã€æ ¸è„…å¨)ã€èª¬å¾—ã€ãƒ¢ãƒ‡ãƒ«ã®è‡ªå¾‹æ€§ã«4ã¤ã®å®‰å…¨ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æŒ‡å®šã€‚ 
	- ä»–ã€Œunknownunknownsã€ã«ã‚‚æ³¨åŠ›
	- ç·©å’Œå¾Œã®ã‚¹ã‚³ã‚¢ãŒã€Œmediumã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’å°å…¥å¯èƒ½ã€‚ 
	- ç·©å’Œå¾Œã®ã‚¹ã‚³ã‚¢ãŒã€Œhighã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã¯é–‹ç™ºå¯èƒ½ã€‚ ã€ŒCriticalã€ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã‚‚ã—ãã¯ãã†äºˆæƒ³ã•ã‚Œã‚‹å ´åˆCapabilityå‘ä¸Šé–‹ç™ºä¸­æ­¢ã€‚å®‰å…¨æ€§ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã§ã‹ã¤å®‰å…¨ã§ã‚ã‚‹ã“ã¨ã‚’åˆç†çš„ã«ä¿è¨¼ã§ãã‚‹å ´åˆã«ã®ã¿ã€èƒ½åŠ›å‘ä¸Šé–‹ç™ºã‚’ç¶™ç¶šã™ã‚‹ã€‚
	- æŠ€è¡“çš„ä½œæ¥­(Preparedness Team)ã¨é‹ç”¨æ§‹é€ ã‚’ç›£ç£ã™ã‚‹å°‚é–€ãƒãƒ¼ãƒ (å®‰å…¨æ€§è«®å•å§”å“¡ä¼š(SAG)è¨­ç«‹ã€‚å‰è€…ã¯ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡é‚è¡Œã€‚ 
	- SAGã¯çµŒå–¶é™£ã¨å–ç· å½¹ä¼šã«å®‰å…¨æ€§ã‚’å ±å‘Šã™ã‚‹ãŸã‚ã®éƒ¨é–€æ¨ªæ–­çš„ã§ååˆ†ã«å¤šæ§˜ãªè¦–ç‚¹ã‚„çŸ¥è­˜ã‚’æŒã¤å°‚é–€å®¶ã‚°ãƒ«ãƒ¼ãƒ—ã€‚ 
	- çµŒå–¶é™£ãŒæ„æ€æ±ºå®šè€…ã§ã€å–ç· å½¹ä¼šã¯æ±ºå®šã‚’è¦†ã™æ¨©åˆ©ã‚’æŒã¤
-  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹AIã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã®7ã¤ã®åŸå‰‡ï¼š OpenAIã€Practices for Governing Agentic AIã€ã‚’èª­ã¿è§£ã
	- https://note.com/mahlab/n/nf6bc6078460d
	- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹AIã‚·ã‚¹ãƒ†ãƒ ã¨ã¯ã€äººé–“ã«ã‚ˆã‚‹éƒ¨åˆ†çš„ãªç®¡ç†ä¸‹ã§ã‚ã£ã¦ã‚‚ã€è¤‡é›‘ãªç›®æ¨™ã‚’è‡ªå¾‹çš„ã«é‚è¡Œã§ãã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã®ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚
	- ã“ã®ã‚ˆã†ãªã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç”»åƒç”Ÿæˆã‚„è³ªå•å¿œç­”ã®ã‚ˆã†ãªé™å®šã•ã‚ŒãŸç”¨é€”ã§å‹•ä½œã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã¨ã¯ç•°ãªã‚Šã€ã‚ˆã‚Šå¹…åºƒã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹èƒ½åŠ›ãŒã‚ã‚‹ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¤‡é›‘ãªç›®æ¨™ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
	- ã—ã‹ã—ã“ã®ç¨®ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã“ã®ã‚ˆã†ã«å¤§ããªç¤¾ä¼šçš„ä¾¿ç›Šã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚‹åé¢ã€ã‚·ã‚¹ãƒ†ãƒ ã®éšœå®³ã‚„æ‚ªç”¨ã«ã‚ˆã‚‹é‡å¤§ãªå•é¡Œç™ºç”Ÿã®ãƒªã‚¹ã‚¯ã‚‚ç§˜ã‚ã¦ã„ã¾ã™ã€‚
	- ãã“ã§ã“ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼ã§ã¯ã€ã“ã®ãƒªã‚¹ã‚¯ã‚’ç·©å’Œã—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹AIã‚·ã‚¹ãƒ†ãƒ ã®æ©æµã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã®ã€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã«é–¢ä¸ã™ã‚‹é–¢ä¿‚è€…ãŒå¾“ã†ã¹ãåŸºæœ¬åŸå‰‡ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
	- å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®7ã¤ã®åŸå‰‡ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
		1. ã‚¿ã‚¹ã‚¯é©åˆæ€§ã®è©•ä¾¡ã™ã‚‹
		2. è¡Œå‹•ç¯„å›²ã®åˆ¶é™ã™ã‚‹
		3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã®è¨­å®šã™ã‚‹
		4. é€æ˜æ€§ã®ç¢ºä¿ã™ã‚‹
		5. è‡ªå‹•ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†
		6. å›ºæœ‰ã®è­˜åˆ¥å­ã‚’ä»˜ä¸ã™ã‚‹
		7. äººé–“ã«ã‚ˆã‚‹åˆ¶å¾¡æ¨©ã®ä¿æŒã™ã‚‹
	- ã“ã‚Œã‚‰ã¯ã‚ãã¾ã§ã‚‚è©¦è¡Œçš„ãªææ¡ˆã§ã‚ã‚Šã€å„åŸå‰‡ã®è©³ç´°ã¨èª²é¡Œã¯ã“ã‚Œã‹ã‚‰ã®è­°è«–ãŒå¾…ãŸã‚Œã¦ã„ã‚‹çŠ¶æ…‹ã§ã™ãŒã€ãƒ›ãƒ¯ã‚¤ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹AIã‚·ã‚¹ãƒ†ãƒ ã®è²¬ä»»ã‚ã‚‹åˆ©ç”¨ã®æ¨é€²ã«è³‡ã™ã‚‹ã§ã‚ã‚ã†åŸºç›¤ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
	- æœ€çµ‚çš„ã«ã¯æ³•åˆ¶åº¦ã‚’å«ã‚ãŸç¤¾ä¼šã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã§ã€ã“ã®å–ã‚Šçµ„ã¿ã‚’æ”¯ãˆã¦ã„ãå¿…è¦ãŒã‚ã‚‹ã¨ã—ã¦ã„ã¾ã™ã€‚
- LLM prompting ã§çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ä½œæˆãƒ»å¯è¦–åŒ–
	- https://github.com/rahulnyk/knowledge_graph
	- Mistral OpenOrca (https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca) ç­‰ã® LLM prompting ã§çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã®æƒ…å ±ã‚’ç”Ÿæˆï¼ãã®å¾Œï¼Œnetworkx ã§ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ã™ã‚‹
- GCPã”æœ¬ä½“ã«ã‚ˆã‚‹ã€Geminiã¨LangChainã®ã‚³ãƒ©ãƒœnotebook
	- https://github.com/GoogleCloudPlatform/generative-ai/tree/main/language/orchestration/langchain
	- This includes SEVEN different notebooks for using LangChain to orchestrate a Gemini-powered LLM app
		-   [Getting Started with LangChain ğŸ¦œï¸ğŸ”— + Vertex AI PaLM API](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/orchestration/langchain/intro_langchain_palm_api.ipynb)
		-  [How to use the LangChain ğŸ¦œï¸ğŸ”— BigQuery Data Loader](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/orchestration/langchain/langchain_bigquery_data_loader.ipynb)
- openchat/openchat-3.5-1210
	- https://huggingface.co/openchat/openchat-3.5-1210
	- https://x.com/shi3z/status/1736911369360859173?s=20
	- ã“ã‚Œã™ã”ã„ã€‚ ã»ã‚“ã¨ã«GPT-3.5-Turboä¸¦ã®æ€§èƒ½ã£ã½ãè¦‹ãˆã¦7B ãã—ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ Apacheãƒ©ã‚¤ã‚»ãƒ³ã‚¹ by shi3zã•ã‚“
	- 2023å¹´11æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸ**[OpenChat-3.5-7B](https://huggingface.co/openchat/openchat_3.5)**ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°ãŒ70å„„ã—ã‹ãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãš2023å¹´3æœˆæ™‚ç‚¹ã®ChatGPTã‚’è¶…ãˆã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å‡ºã™ã»ã©æ€§èƒ½ãŒé«˜ã„ãƒ¢ãƒ‡ãƒ«
- åè‘—ã ã£ãŸé»„è‰²ã„æœ¬ï¼ˆçµ±è¨ˆå­¦ã¸ã®ç¢ºç‡è«–ï¼Œãã®å…ˆã¸ï¼‰ã®ç¶šç·¨ã®èµ¤ã„æœ¬ï¼ˆçµ±è¨ˆå­¦ã¸ã®æ¼¸è¿‘è«–ï¼Œãã®å…ˆã¯ï¼‰
	- https://x.com/hshimodaira/status/1737005536896508268?s=20
- æ±å·¥å¤§ã‹ã‚‰Swallowç™»å ´ã€æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹ã®æ•´å‚™ã®å……å®Ÿã¶ã‚Šã«ã¤ã„ã¦
	- https://tokyotech-llm.github.io/swallow-llama
	- Llama 2ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’å¼·åŒ–ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (7B, 13B, 70B) ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé‡ã¿ï¼‰ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã®ã§ã€LLAMA 2 Community Licenseã«å¾“ã†é™ã‚Šã€ç ”ç©¶ã‚„å•†æ¥­åˆ©ç”¨ãªã©è‡ªç”±ã«åˆ©ç”¨ã§ãã¾ã™
	- Common Crawlï¼ˆç”¨èª8ï¼‰ã‹ã‚‰é…å¸ƒã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆ2020å¹´ã‹ã‚‰2023å¹´ã«ã‹ã‘ã¦åé›†ã•ã‚ŒãŸ21ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåˆ†ã€ç´„634å„„ãƒšãƒ¼ã‚¸ï¼‰ã‹ã‚‰æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‹¬è‡ªã«æŠ½å‡ºãƒ»ç²¾éŒ¬ã—ã€ç´„3,121å„„æ–‡å­—ï¼ˆç´„1.73å„„ãƒšãƒ¼ã‚¸ï¼‰ã‹ã‚‰ãªã‚‹æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ã“ã®è¦æ¨¡ã¯ã€CC-100 (ç´„258å„„æ–‡å­—ï¼‰ã€mC4ï¼ˆç´„2,397å„„æ–‡å­—ï¼‰ã€OSCAR 23.10ï¼ˆç´„740å„„æ–‡å­—ï¼‰ã‚’æŠœãã€æ—¥æœ¬èªã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚³ãƒ¼ãƒ‘ã‚¹ã®ä¸­ã§ã€å•†ç”¨åˆ©ç”¨ãŒå¯èƒ½ãªã‚‚ã®ã¨ã—ã¦ã¯æœ€å¤§ã¨ãªã‚Šã¾ã™
- "Perspectives on the State and Future of Deep Learning -- 2023"
	- https://arxiv.org/abs/2312.09323
	- Appleã‚„ã‚«ãƒ¼ãƒã‚®ãƒ¼ãƒ¡ãƒ­ãƒ³å¤§å­¦ãªã©è¤‡æ•°æ©Ÿé–¢ã®ç ”ç©¶è€…ã‚‰7åï¼‹ChatGPTãŒé›†ã„ã€ã€ŒAIã®ç¾åœ¨ã€ã«ã¤ã„ã¦è­°è«–ã‚’äº¤ã‚ã—ãŸå†…å®¹ãŒã¾ã¨ã‚ã¦å ±å‘Š
	- â– ã¾ã å–ã‚Šçµ„ã‚ã¦ã„ãªã„é‡è¦èª²é¡Œ 
		- â‘  æ°—å€™å¤‰å‹•ãªã©ã®è‡ªç„¶ç§‘å­¦ã«AIã‚’å¿œç”¨ã™ã‚‹ 
		- â‘¡ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã§å¤šæ§˜ãªæ¥­ç•Œã«å½±éŸ¿ã‚’åŠã¼ã™ 
	- â– ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç†è§£ 
		- â‘  ç‰©ç†å­¦ã®è¤‡é›‘ãªæ¦‚å¿µã‚’çŸ¥ã‚‹ã®ã¨åŒã˜ãã‚‰ã„é›£ã—ã„ ï¼ˆã—ã‹ã—ä¸å¯èƒ½ã§ã¯ãªã„ï¼‰ 
		- â‘¡ å†…éƒ¨å‹•ä½œã‚’è¦–è¦šåŒ–ã™ã¹ã 
	- â– ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®è§£é‡ˆå¯èƒ½æ€§ 
		- â‘  å®Œå…¨ãªè§£é‡ˆã¯é›£ã—ã„ã¨ã®è¦‹æ–¹ã‚‚ã‚ã‚‹ 
		- â‘¡ ã‚ã‚‹å´é¢ã‹ã‚‰ã®è§£é‡ˆã¯å¯èƒ½ã ãŒçœŸå®Ÿã¨ã¯ç•°ãªã‚‹ 
	- â– ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ä¾¡å€¤ 
		- â‘  ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯é‡è¦ã ãŒç¾åœ¨ã¯ã‚«ã‚ªã‚¹ã§ã‚ã‚‹ 
		- â‘¡ ç”£æ¥­ç•Œã§ã¯è¨­å®šã¨æŒ™å‹•ã‚’ç´°ã‹ãè€ƒæ…®ã—ã¦ã„ã‚‹ 
	- â– ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®å°†æ¥æ€§ 
		- â‘  ä¸‡èƒ½ã§ã¯ãªã„ãŸã‚ã€å­¦ç¿’æ–¹æ³•ã‚’æ”¹å–„ã™ã¹ã 
		- â‘¡ äº‹å‰çŸ¥è­˜ã‚’çµ±åˆã™ã‚‹ãªã©ã®å¯¾ç­–ãŒå¿…è¦ 
	- â– ç ”ç©¶ã¯ä»Šå¾Œã©ã†ãªã‚‹ 
		- â‘  ã‚¨ãƒ©ãƒ¼æ•°ã‚ˆã‚Šã‚‚ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ãŒé‡è¦–ã•ã‚Œã¦ã„ã 
		- â‘¡ å®Ÿç”¨æ€§ã«ã‚·ãƒ•ãƒˆã—ã¦ã„ã
- Googleã‹ã‚‰ã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®èª¬æ˜ãŒã§ã‚‹	
	- https://ai.google.dev/docs/prompt_best_practices?hl=ja
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆã«æ­£ã—ã„æ–¹æ³•ã‚„é–“é•ã£ãŸæ–¹æ³•ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ä¸€èˆ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆæˆ¦ç•¥ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚
-  Controllable Agents for RAG
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_runner/agent_runner_rag_controllable.ipynb
	- llamaindexã‚ˆã‚Šã€Building Human-in-the-Loop, Advanced RAG
	- add step-wise feedback for complex query executions over a RAG pipeline
- æ±å·¥å¤§ã¨ç”£ç·ç ”ã€è‹±èªã®è¨€èªç†è§£ã‚„å¯¾è©±ã§é«˜ã„èƒ½åŠ›ã‚’æŒã¤å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒSwallowã€ã‚’å…¬é–‹ 
	- https://note.com/aicu/n/n3eb8c1f2df02?sub_rt=share_pb
	- Swallowã®ç ”ç©¶é–‹ç™ºã¯ã€ç”£ç·ç ”ãŒæ§‹ç¯‰ãƒ»é‹ç”¨ã™ã‚‹AIæ©‹æ¸¡ã—ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆABCI: AI Bridging Cloud Infrastructureï¼‰ã®ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰æ”¯æ´ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äººæ–°ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»ç”£æ¥­æŠ€è¡“ç·åˆé–‹ç™ºæ©Ÿæ§‹ï¼ˆNEDOï¼‰ã®ã€Œæ¬¡ä¸–ä»£äººå·¥çŸ¥èƒ½ãƒ»ãƒ­ãƒœãƒƒãƒˆã®ä¸­æ ¸ã¨ãªã‚‹ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ãƒˆæŠ€è¡“é–‹ç™ºã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (JPNP18002) ã®ã€Œç†Ÿç·´è€…è¦³ç‚¹ã«åŸºã¥ãã€è¨­è¨ˆãƒªã‚¹ã‚¯è©•ä¾¡æ¥­å‹™ã«ãŠã‘ã‚‹åˆ¤æ–­æ”¯æ´ã‚’è¡Œã†äººå·¥çŸ¥èƒ½é©ç”¨æŠ€è¡“ã®é–‹ç™ºã€ã€ãã®ä»–ã®æ”¯æ´ã«ã‚ˆã£ã¦å®Ÿæ–½ã•ã‚Œã¾ã—ãŸ
	- ç”£ç·ç ”ABCIã®ä¸€å®šéƒ¨åˆ†ï¼ˆAãƒãƒ¼ãƒ‰ã¨å‘¼ã°ã‚Œã‚‹é«˜æ€§èƒ½ãªè¨ˆç®—ãƒãƒ¼ãƒ‰ï¼‰ã‚’æœ€å¤§60æ—¥é–“å æœ‰åˆ©ç”¨ã™ã‚‹æ©Ÿä¼šã‚’æä¾›ã™ã‚‹ã€Œå¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰æ”¯æ´ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™
	- swallowã£ã¦ã¤ã°ã‚ï¼Ÿï¼ˆæ±å·¥å¤§ã®ãƒãƒ¼ã‚¯ï¼‰
-  AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design
	- https://arxiv.org/abs/2312.02308v1
	- å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è§¦åª’ææ–™ã®é€†è¨­è¨ˆã®è«–æ–‡ã€‚ 
	- -OHã¨ã®çµåˆã¯å¼·ã„ãŒH2Oã¨ã®çµåˆã¯å¼±ã„ã€ã®ã‚ˆã†ãªè¤‡æ•°ã®å¸ç€å‰¤ã®æœ€é©åŒ–ã‚’å¤šç›®çš„å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚Šè¡Œã„ã€16ä¸‡åŒ–åˆç‰©ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§ããŸãã†ã§ã™ã€‚ 
	- ææ–™é–‹ç™ºã¯ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒåŸºæœ¬ãªã®ã§ã€ã“ã†ã„ã†æœ€é©åŒ–ã¯éœ€è¦ãŒã‚ã‚Šãã†
- ã€ã‚¹ã‚­ãƒ«å®šç¾©å§”å“¡ä¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ï½ã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã€ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³æ›´æ–°ã¨ç”Ÿæˆ AIã€œã€ï¼ˆ2023å¹´10æœˆ20æ—¥ï¼‰
	- https://www.youtube.com/watch?v=nQumYtpN0zY
	- DSå”ä¼š ã‚¹ã‚­ãƒ«å®šç¾©å§”å“¡ä¼š ã‹ã‚‰ç”ŸæˆAIæ™‚ä»£ã«å³ã—ã€ã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ver.5ã¨ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ ver.4ã‚’ç™ºè¡¨ã—ãŸéš›ã®è§£èª¬å‹•ç”»ãŒYouTubeã«ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸã‚ˆã†ã§ã™ã€‚ã„ããªã‚Šãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’è¦‹ã¦ã‚‚ãã†ç°¡å˜ã«èƒŒæ™¯ã¯ç†è§£ã§ããªã„ã®ã§ã‚ªã‚¹ã‚¹ãƒ¡ã€‚ç›¸å½“ã«æ¿ƒåšã§ã™ã€‚
-  ELYZA-tasks-100 ã§LLM14å€‹ã®æ—¥æœ¬èªæ€§èƒ½ã‚’æ¨ªæ–­è©•ä¾¡ã—ã¦ã¿ãŸ
	- https://qiita.com/wayama_ryousuke/items/105a164e5c80c150caf1
	- æ—¥æœ¬èªLLMã£ã¦è‰²ã€…ã‚ã‚‹ã‘ã©ãƒ™ãƒ³ãƒã ã‘ã˜ã‚ƒã‚ˆãã‚ã‹ã‚‰ã‚“ãªã€ã¨ã„ã†ã“ã¨ã§æ¤œè¨¼ã—ã¦ã¿ãŸçµæœã‚’è¨˜äº‹ã«ã—ã¦ã¿ã¾ã—ãŸ 
	- openchatã€Swallowç­‰ç™ºè¡¨ã•ã‚ŒãŸã°ã‹ã‚Šã®LLMã«ã¤ã„ã¦ã‚‚æ¤œè¨¼ã—ã¦ã¿ã¦ã¾ã™
	- å¹³å‡ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ã‹ã£ãŸã®ã¯ `Xwin-LM-70B-V0.1` ã§ã€æ¬¡ã„ã§ `deepseek-llm-67b-chat`ã€`Yi-34B-Chat` ã¨ç¶šã„ã¦ã„ã¾ã™ã€‚  
	- ä¸Šä½3ã¤ã¯ã™ã¹ã¦ä¸­å›½å‹¢ã§ã€ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ã‚‚30Bä»¥ä¸Šã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã™
	- ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ãŒæ¯”è¼ƒçš„å°‘ãªã„ 7B ãƒ¬ãƒ³ã‚¸ã§ã¯ã€ELYZA-japanese-Llama-2 ã‚„ CALM2 ãªã©ã®æ—¥æœ¬ç™ºãƒ¢ãƒ‡ãƒ«ãŒé«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¦ã„ã¾ã™ã€‚
	- ä¸€æ–¹ã€ãƒ‘ãƒ©ãƒ¡ã‚¿æ•° 30B ä»¥ä¸Šã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ï¼ˆãã‚‚ãã‚‚æ—¥æœ¬ç™ºã®ãƒ¢ãƒ‡ãƒ«ãŒå°‘ãªã„ã“ã¨ã‚‚ã‚ã‚Šï¼‰æµ·å¤–ãƒ¢ãƒ‡ãƒ«ãŒé«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
-  GPTsã‚ˆã‚Šç²¾åº¦ã®é«˜ã„RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
	- https://speakerdeck.com/mkazutaka/gptsyorijing-du-nogao-iragsisutemunogou-zhu
	- https://github.com/mkazutaka/20231219-llmapp-meetup
-  LLM in a flash: Efficient Large Language Model Inference with Limited Memory
	- https://arxiv.org/abs/2312.11514
	- Appleã®ç ”ç©¶è€…ã‚‰ã¯ã€LLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’SSDãªã©ã®å¤–éƒ¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã€æ¥ç¶šã—ãŸPCãªã©ã§èª­ã¿è¾¼ã¿ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã‚’é–‹ç™ºã—ã¾ã—ãŸ
	- CPUã§4-5å€ã€GPUã§20-25å€ã®æ¨è«–é€Ÿåº¦å‘ä¸ŠãŒå®Ÿç¾ã—ã€ã•ã‚‰ã«PCãƒ‡ãƒã‚¤ã‚¹ã®è¨˜æ†¶å®¹é‡ãŒãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®åŠåˆ†ã§ã‚‚ã€LLMã‚’é«˜åŠ¹ç‡ã«å®Ÿè¡Œã§ããŸã¨ã®ã“ã¨ã§ã™ã€‚
	- æ‰‹æ³•ï¼š
		- â‘  ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤–éƒ¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã«æ ¼ç´ 
		- â‘¡ è¦æ±‚ã«å¿œã˜ã¦PCã®DRAMï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã«è»¢é€ 
		- â‘¢ ãƒ‡ãƒ¼ã‚¿è»¢é€é‡ã‚’æ¸›ã‚‰ã—æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Š
	- çµæœï¼š
		- â‘  CPUã§4-5å€ã€GPUã§20-25å€ã®æ¨è«–é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ 
		- â‘¡ PCãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒ¢ãƒªï¼ˆDRAMï¼‰ãŒãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®åŠåˆ†ã§ã‚‚ã€LLMã‚’é«˜åŠ¹ç‡ã«å®Ÿè¡Œ
- ã€ŒAGI Breakthroughã€
	- https://x.com/bioshok3/status/1737258881452294277?s=20
	- ã€ŒAGI Breakthroughã€ã¨åä»˜ã‘ã‚‰ã‚ŒãŸOpenAIå–ç· å½¹ä¼šã¸ã®å…¬é–‹æ›¸ç°¡ãŒVerses AIã‹ã‚‰æ€¥é½å‡ºã•ã‚Œã¦ã„ã‚‹ã€‚
	- AGIã«ç¹‹ãŒã‚Šã†ã‚‹èƒ½å‹•çš„æ¨è«–ã«ã¤ã„ã¦ã®ç”»æœŸçš„ãªé€²æ­©ã‚’æœ€è¿‘é”æˆã€‚Open AIæ†²ç« ã«åŸºã¥ãã€AGIã®å®‰å…¨ãªé…å‚™ã®ãŸã‚æŠ€è¡“å”åŠ›ã‚’è¦è«‹ã—ã¦ã„ã‚‹ã€‚ä»Šå¾Œã©ã†ãªã‚‹ã‹æ³¨è¦–å¿…è¦ã€‚
- llamaindexã‚ˆã‚Štext2sqlã‚’ã¤ã‹ã£ãŸã€research assistant templte
	- https://github.com/langchain-ai/langchain/tree/master/templates/sql-research-assistant
	- ollamaã‚’åˆ©ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMç‰ˆã‚‚ãµãã¾ã‚Œã¦ã„ã‚‹ï¼
	- ãªã‚‹ã»ã©ã€ã“ã‚ŒãŒLangCainã¨LLMã‚’ã¤ã‹ã£ãŸãƒ­ãƒ¼ã‚«ãƒ«Webã‚¢ãƒ—ãƒªæ§‹ç¯‰ã®æ–°ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã‹
- PowerInfer - a high-speed inference engine for deploying LLMs locally
	- https://github.com/SJTU-IPADS/PowerInfer
	- Just came across this super interesting project on speeding up inference. It's not MoE but it's a simple approach that exploits the high locality in LLM inference to design a GPU-CPU hybrid inference engine.
	- It's now possible to use PowerInfer with Llama 2 and Faclon 40B. Mistral-7B support is coming soon!
	- æ¯”è¼ƒå‹•ç”»ã€https://x.com/omarsar0/status/1737168751668187229?s=20
- swallow-70B-instructã®GGUFãŒã§ãã¦ã„ã‚‹ã€‚ã€‚TheBloke/Swallow-70B-instruct-GGUF
	- https://huggingface.co/TheBloke/Swallow-70B-instruct-GGUF
- swallow-13B-instuctã®spaceã‚’ã¤ãã‚Šã¾ã—ãŸ
	- https://huggingface.co/spaces/hayas/Swallow-13B-instruct
	- ã€Œæ±äº¬å·¥æ¥­å¤§å­¦ã®å¤§å²¡å±±ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ã¯è¡Œæ”¿çš„ã«ã¯ã©ã“ã®åŒºã«å±ã™ã‚‹ï¼Ÿã€ã¨ã€å•ã†ã¨ç‹‚ã£ãŸï¼
-  A mathematical perspective on Transformers
	- https://arxiv.org/abs/2312.10794
	- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€è‡ªå·±æ³¨æ„ã¨å±¤æ­£è¦åŒ–ã¨ã„ã†2ã¤ã®ä¸»è¦ãªæ©Ÿæ§‹ã‚’å«ã‚€ç›¸äº’ä½œç”¨ã™ã‚‹ç²’å­ç³»ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã‚‹ã€‚ç²’å­ç³»ã¯ç¢ºç‡æ¸¬åº¦ã®æµã‚Œã‚’å®Ÿè£…
-  Discovery of a structural class of antibiotics with explainable deep learning
	- https://www.nature.com/articles/s41586-023-06887-8
	- æ¯’æ€§ã®ãªã„ã€ãƒ¡ãƒã‚·ãƒªãƒ³è€æ€§é»„è‰²ãƒ–ãƒ‰ã‚¦çƒèŒã«å¯¾ã—ã¦æœ‰åŠ¹ãªè¤‡æ•°ã®åŒ–åˆç‰©ã‚’å«ã‚€æ–°ã—ã„æ§‹é€ ã‚¯ãƒ©ã‚¹ã®æŠ—ç”Ÿç‰©è³ª (æœ€å¾Œã®ç™ºè¦‹ã«ã¯ 38 å¹´ã‹ã‹ã£ãŸ)
- "A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise"
	- https://arxiv.org/abs/2312.12436
	- GPT-4Vã«å¯¾ã—ã¦Geminiã®ç”»åƒèªè­˜èƒ½åŠ›ã¯ã©ã‚Œã»ã©æ€§èƒ½ãŒé«˜ã„ã®ã‹ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§æ¯”è¼ƒã—ãŸå®Ÿé¨“çµæœãŒå ±å‘Šã•ã‚Œã¾ã—ãŸã€‚
	- GPT-4Vã¯è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«é•·ã‘ã¦ãŠã‚Šã€Geminiã¯ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®çµ±åˆã«é•·ã‘ã¦ã„ã‚‹å‚¾å‘ãŒã‚ã‚‹ã¨ã®ã“ã¨ã§ã™ã€‚
	- æ¯”è¼ƒï¼š
		- â‘  Geminiã¯å¤šãã®å ´åˆã€GPT-4Vã¨åŒç­‰ã‹ãã‚Œä»¥ä¸Šã®æ­£ç¢ºã•ã‚’ç¤ºã™ 
		- â‘¡ Geminiã¯GPT-4Vã‚ˆã‚Šã‚‚çŸ¥è­˜ãŒå¹…åºƒã„ã‚ˆã†ã«è¦‹ãˆã‚‹
-  Fairness and Machine Learning by Arvind Narayanan
	- https://mitpress.mit.edu/9780262048613/fairness-and-machine-learning/
	- An introduction to the intellectual foundations and practical utility of the recent work on fairness and machine learning
	- ãƒ‰ãƒ©ãƒ•ãƒˆãŒã‚ã‚Šã€ã™ã§ã«ãŸãã•ã‚“ã®å¤§å­¦ã®æˆæ¥­ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚https://fairmlbook.org/
- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ã§ã€AIç‹ã‚¯ã‚¤ã‚ºç¬¬ä¸€å›ã‚³ãƒ³ãƒšã«è‡¨ã‚€ - Q&Aã‚¿ã‚¹ã‚¯ã§ã®è¤‡æ•°ã®æ—¥æœ¬èªembeddingsã®è©•ä¾¡
	- https://secon.dev/entry/2023/12/21/080000-vector-search-ai-ou-comp/
	- AIç‹ ã€œã‚¯ã‚¤ã‚ºAIæ—¥æœ¬ä¸€æ±ºå®šæˆ¦ã€œ ç¬¬ä¸€å›ã‚³ãƒ³ãƒšã¨ã¯ã€è³ªå•ã«å¯¾ã—ã¦ç´„20å€‹ã®å€™è£œã‹ã‚‰ã€å›ç­”ã¨ãªã‚‹ä¸€ã¤ã‚’é¸æŠã™ã‚‹ã‚³ãƒ³ãƒšã ã€‚trainç”¨ã«ç´„13,000ä»¶ã€valç”¨ã«ç´„2,000ä»¶ãƒ‡ãƒ¼ã‚¿ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚
	- è³ªå•ã«å¯¾ã—ã¦ã®å›ç­”ãŒå«ã¾ã‚Œãã†ãªæ–‡ã‚’æ¤œç´¢ã™ã‚‹æ—¥æœ¬èªembeddingså¤‰æ›ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ã€multilingual-e5-large ã®æ€§èƒ½ãŒé«˜ã‹ã£ãŸ
- Autonomous chemical research with large language models
	- https://www.nature.com/articles/s41586-023-06792-0
	- Coscientist"â€”a GPT-4 based autonomous LLM system that demonstrates appreciable reasoning capabilities, ... solving of multiple problems and generation of code for experimental design"
	- è‘—è€…ã‚‰ã¯ GPT-4 ã‚’ä½¿ç”¨ã—ã¦ã€è‡ªå¾‹çš„ã«ç ”ç©¶ã€è¨ˆç”»ã€ãŠã‚ˆã³åŒ–å­¦å®Ÿé¨“ã‚’å®Ÿæ–½ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã‚“ã§å®Ÿé¨“æ©Ÿå™¨ã®ä½¿ã„æ–¹ã‚’å­¦ã¶ã“ã¨ã‚‚å«ã¾ã‚Œã¾ã™ (ã»ã¨ã‚“ã©ã®æ“ä½œã¯ã‚³ãƒ¼ãƒ‰ã§æ“ä½œã•ã‚Œã¾ã—ãŸãŒã€1 ã¤ã®ã‚¿ã‚¹ã‚¯ã¯äººé–“ãŒå®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸ)ã€‚
- Ollama v0.1.17 now has support for Phi-2
	- https://ollama.ai/library/phi
	- It's a small model at 2.7 billion parameters. Good for its reasoning and language understanding abilities. Given its small size, it'll run effectively on a wider set of hardware.
- TheBloke/Swallow-13B-GGUF
	- https://huggingface.co/TheBloke/Swallow-13B-GGUF
	- ã¾ãŸã¾ãŸ Swallow-13Bã®GGUFãŒå‡ºã¦ã„ã‚‹
-  rinnaã€Qwenã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒNekomataã€ã‚·ãƒªãƒ¼ã‚ºã‚’å…¬é–‹
	- https://rinna.co.jp/news/2023/12/20231221.html
	- rinnaã¯Qwen-7Bã¨14Bã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒNekomataã€ã‚·ãƒªãƒ¼ã‚ºã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ Nekomata 14B Instructionã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ä¸€éƒ¨ã®70Bã¨åŒãƒ¬ãƒ™ãƒ«ã¾ã§åˆ°é”ã—ã¦ã„ã¾ã™ã€‚
	- Nekomata 7Bã¨14Bã¯ã€70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Qwen-7Bã¨140å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Qwen-14Bã«å¯¾ã—ã¦ã€æ—¥æœ¬èªã¨è‹±èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ãã‚Œãã‚Œ300å„„ã¨660å„„ãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™
	- AWS Trainiumã‚’æ­è¼‰ã—ãŸ16ãƒãƒ¼ãƒ‰ã®Amazon EC2 trn1.32xlargeã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦ã€660å„„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¶™ç¶šäº‹å‰å­¦ç¿’ã¯ç´„7æ—¥ã§å®Œäº†ã—ã¾ã—ãŸ
	- ãƒ¢ãƒ‡ãƒ«åã®ç”±æ¥ã¯ã€å¦–æ€ªã®ã€ŒçŒ«åˆï¼ˆã­ã“ã¾ãŸï¼‰ã€
- Running Mixtral 8x7 locally with LlamaIndex
	- https://blog.llamaindex.ai/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab
	- Running MistralAI's Mixtral 8x7b on your laptop is now a one-liner! Check out this post in which we show you how to use OLLAMA with LlamaIndex to create a completely local, open-source retrieval-augmented generation app complete with an API:
-  Google Colab ã§ StreamDiffusion ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n4cb9a2d9fd72?sub_rt=share_h
	- ã€ŒStreamDiffusionã€ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã™ã€‚å¾“æ¥ã®ç”»åƒç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨æ¯”ã¹ã¦é£›èºçš„ãªé€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
- Apple ãŒæä¾›ã—ã¦ã„ã‚‹MLXãŒå¾ã€…ã«å……å®Ÿã—ã¦æ¥ãŸã€‚çµæ§‹å‡„ã„ã“ã¨ã«ãªã‚‹ã‹ã‚‚ã€‚
	- https://huggingface.co/mlx-community
	- a bunch of pre-converted MLX models! 
	- Llama, Phi-2, Mistral, Mixtral (and instruct and code variations where available)!
- rinnaã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹nekomata-14b-instructionã®gguf
	- mmnga/rinna-nekomata-14b-instruction-gguf
	- qwenãƒ™ãƒ¼ã‚¹ã§vocab15ä¸‡ã‚ã‚Šã¾ã™
-  Gemini Pro Visionãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦Google Cloudã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå‹•ç”»ã‚’è§£æã—ã¦ã¿ãŸ
	- https://qiita.com/tatsuki-tsuchiyama/items/5701475d46ee31efbb54
- ã€ŒNekomataã€ã‚·ãƒªãƒ¼ã‚ºã®GGUF 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã¯ã€é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚
	- https://huggingface.co/collections/rinna/nekomata-6582b5134ee85531becbb9a9
-  regex to do sentence splitting that generalizes beyond English to non-Latin languages (CJK, etc.) 
	- https://x.com/jerryjliu0/status/1738232451200356445?s=20
- æœ€æ–°ã® SCIENCEã®ç‰¹é›†ã¯AI Powered Forecasting ã€VOLUME 382|ã€ISSUE 6677ã€22 DEC 2023
	- https://www.science.org/toc/science/382/6677?utm_campaign=SciMag&utm_source=Twitter&utm_medium=ownedSocial
	- Trained on four decades of historical data, GraphCast is an artificial intelligence model that predicts global weather with greater speed and accuracy compared with traditional approaches solving physical equations. It supports severe event predictions, such as cyclone tracking.
-  Ferret: Refer and Ground Anything Anywhere at Any Granularity
	- https://github.com/apple/ml-ferret?tab=readme-ov-file
	- Appleã‹ã‚‰ã€ã‚ã‚‰ã‚†ã‚‹å½¢å¼ã®å‚ç…§ï¼ˆç®±ã¨ã‹ã€ãªã‚“ã¨ã‹ã®æ¨ªã¨ã‹ï¼‰ã‚’å—ã‘å…¥ã‚Œã€å¿œç­”ã¨ã—ã¦ã‚ã‚‰ã‚†ã‚‹ã‚‚ã®ã‚’æ¥åœ°ã™ã‚‹ï¼ˆãã‚Œã¯çŒ«ã®ã—ã£ã½ã¨ã‹ï¼‰ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã® MLLM
	- ç‰©ä½“èªè­˜ã®ä¸€ç¨®ãªã®ã‹ã€
- "Retrieval-Augmented Generation for Large Language Models: A Survey"
	- https://arxiv.org/abs/2312.10997
	- LLMã®RAGï¼ˆå¤–éƒ¨çŸ¥è­˜æ¤œç´¢ã«ã‚ˆã‚‹å¼·åŒ–ï¼‰ã«ã¤ã„ã¦ã®èª¿æŸ»çµæœ
	- åŸºæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨å„æ§‹æˆè¦ç´ ï¼ˆãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ï¼ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼æ‹¡å¼µï¼‰ã®è©³ç´°ã€è©•ä¾¡ã€ãã—ã¦ä»Šå¾Œã®ç™ºå±•ã«ã¤ã„ã¦è¨€åŠã•ã‚Œã¦ãŠã‚Šç¶²ç¾…çš„ã§ã™ã€‚
	- â– RAGã®è©•ä¾¡
		- â‘  æ­£ç¢ºæ€§ã€æƒ…å ±æ›´æ–°é€Ÿåº¦ã€é€æ˜æ€§ãªã©ãŒä¸»è¦ãªæŒ‡æ¨™
		- â‘¡ RAGASã‚„ARESãªã©ã®è‡ªå‹•è©•ä¾¡æ‰‹æ³•ãŒã‚ã‚‹
	- â– ä»Šå¾Œã®ç™ºå±•
		- â‘  ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦
		- â‘¡ å¿œç”¨ç¯„å›²ã®æ‹¡å¤§ãŒæœŸå¾…ã•ã‚Œã‚‹
		- â‘¢ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã¨ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ãŒç™ºå±•ã™ã¹ã
- Geminiã§ã®tokenã‚«ã‚¦ãƒ³ãƒˆãŒæ—¥æœ¬èªã§ChatGPTã®1/2ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜
	- https://x.com/Mega_Gorilla_/status/1738821637297115598?s=20
	- Gemini ãŠå‰ã€932 Charactersã§500Tokenã£ã¦ã€ã€ ãŠå‰ã®Tokenã©ã†ãªã£ã¦ã‚‹ã‚“ã ï¼Ÿï¼ OpenAIãªã‚‰ã€åŒã˜æ–‡å­—åˆ—ã§ã€1000ãƒˆãƒ¼ã‚¯ãƒ³è¶Šãˆã ãã€‚
- Youri7Bã‚’ãƒ­ãƒ¼ã‚«ãƒ«LLMã§APIã‚µãƒ¼ãƒãƒ¼åŒ–ã—ã¦ã‚ªãƒªã‚¸ãƒŠãƒ«ç¾å°‘å¥³ã¨ãŠè©±ã—ã¦ã¿ãŸ
	- https://zenn.dev/yasuna/articles/b954b2cd77e27f
	- ãƒ­ãƒ¼ã‚«ãƒ«PCã«LLMã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦APIã‚µãƒ¼ãƒã¨ã—ã¦å‹•ã‹ã™
	- ãƒ–ãƒ©ã‚¦ã‚¶ã§ç°¡å˜ã«3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ä¼šè©±ã§ãã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¤ãªã’ã‚‹
	- ã‚ªãƒªã‚¸ãƒŠãƒ«3Dã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ä½œã‚‹
	- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚’ã™ã‚‹
- intel-extension-for-transformers
	- https://github.com/intel/intel-extension-for-transformers
	- ã„ã‚ã„ã‚å¯¾å¿œã§ãã‚‹LLMã‚„é‡å­åŒ–å¯¾å¿œãŒå¢—ãˆã¦ã„ã‚‹æ¨¡æ§˜
- ãƒ¬ã‚¾ãƒŠãƒƒã‚¯ãŒé‡å­åŒ–å­¦è¨ˆç®—ã«æ¯”ã¹ã¦æ•°åƒå€é€Ÿãç‰©æ€§ã‚’äºˆæ¸¬å¯èƒ½ãªã‚¢ãƒ—ãƒªã‚’é–‹ç™º
	- https://monoist.itmedia.co.jp/mn/articles/2312/22/news064.html#utm_term=share_sp
	- ãƒ¬ã‚¾ãƒŠãƒƒã‚¯ã¯2023å¹´12æœˆ21æ—¥ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ã‚’ç”¨ã„ãŸAIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¨è†¨å¤§ãªè“„ç©ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã‚‹ã‚±ãƒ¢ã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹ã‚¢ãƒ—ãƒªã‚’ç‹¬è‡ªé–‹ç™ºã—ã€é‹ç”¨ã‚’é–‹å§‹ã—ãŸã¨ç™ºè¡¨ã—ãŸã€‚
- Building LLM-Powered Web Apps with Client-Side Technology
	- https://ollama.ai/blog/building-llm-powered-web-apps
	- https://www.youtube.com/watch?v=-1sdWLr3TbI
	- Iâ€™d try a different approach and try to build a web app using exclusively local models and technologies, preferably those that run in the browser!
	- ollamaã‚’ã¤ã‹ã£ã¦Langchainã‚’ã¤ã‹ã£ãŸã€Webãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒ¼ã‚«ãƒ«ãªRAGã®æ§‹ç¯‰ä¾‹
- PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU
	- https://arxiv.org/abs/2312.12456
	- æ¶ˆè²»è€…å‘ã‘GPUã§ã‚‚é«˜æ€§èƒ½GPUã«è¿‘ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§LLMã‚’å‹•ã‹ã™æ‰‹æ³•ã€ŒPowerInferã€
	- â– ã€ŒPowerInferã€ã®ãƒã‚¤ãƒ³ãƒˆ 
		- â‘  LLMã«ãŠã‘ã‚‹ãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã™ 
		- â‘¡ æ¨è«–ã®å‡¦ç†é€Ÿåº¦å‘ä¸Šã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦ã„ã‚‹ 
		- â‘¢ GPUã¨CPUã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–¹å¼ 
	- â– å®Ÿé¨“ 
		- â‘  æ¶ˆè²»è€…å‘ã‘ç’°å¢ƒã‚’ç”¨æ„ ï¼ˆIntel i9, NVIDIA RTX 4090ãªã©ï¼‰ 
		- â‘¡ LLaMA-70Bã»ã‹åˆè¨ˆ3ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ 
		- â‘¢ å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹ã«è¿‘ã„ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’è¡Œã£ãŸ 
	- â– çµæœ 
		- â‘  æ¶ˆè²»è€…å‘ã‘ã§ã‚‚é«˜æ€§èƒ½ï¼ˆA100ï¼‰ã®82%ã«ä¸Šã‚‹ç”Ÿæˆé€Ÿåº¦ã‚’é”æˆ 
		- â‘¡ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§æœ€å¤§8.00å€ã€éé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§æœ€å¤§11.69å€ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’å®Ÿç¾ 
		- â‘¢ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»æ€§åŒ–ã«å¿œã˜ã¦é©åˆ‡ãªå‰²ã‚Šå½“ã¦ã‚’å®Ÿè¡Œ

## 12/18

ä»Šé€±ã‚‚ã™ã•ã¾ã˜ã„æƒ…å ±é‡ã€‚ãƒ«ã‚«ãƒ³å…ˆç”Ÿã‚‚ã“ã®æƒ…å ±é‡ã«ã¯è¿½ã„ä»˜ã‘ãªã„ã¨ã®ã“ã¨ï¼ˆã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å‹•ç”»ï¼‰ã€‚Geminiã®APIãŒä½¿ãˆã‚‹ã‚ˆã†ãªã‚Šã€æ§˜ã€…ãªã‚µãƒ³ãƒ—ãƒ«ã‚„ã€LangChainã€llamaindexã¨ã®çµ±åˆãŒã©ã‚“ã©ã‚“è¡Œã‚ã‚ŒãŸã€‚ãƒ•ãƒªãƒ¼ç‰ˆãªã‚‰ã°ã€60QPM (queries per minute)ã¾ã§ã¯ä½¿ãˆã‚‹ã€‚ã‚¯ãƒªã‚¹ãƒã‚¹ã‚«ãƒ¼ãƒ‰ã‚’ä½œã‚ã†ã¯ã„ã„ã­ã€å¹´è³€çŠ¶ã‹ãªã€‚Mistralã€MOEã®ã™ã°ã‚‰ã—ã•ã‚„ã€MOEã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆãƒãƒ¼ã‚¸ã¨ã‹ã€æ—¥æœ¬èªã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’å…¥ã‚Œè¾¼ã‚€ã¨ã‹ã®è©¦ã¿ï¼‰ã®è©¦ã¿ãŒå§‹ã¾ã‚‹ã€‚NeurPS2023ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã‚‚ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è³ªãŒé‡è¦ã¨ã„ã†ã“ã¨ã‚‰ã—ã„ãŒã€DeepMindã‹ã‚‰ã¯ã€LLMãŒè³ªã®è‰¯ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦å­¦ç¿’ã™ã‚‹ã€Œè‡ªå·±å­¦ç¿’ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚RAGã§ã‚‚è³ªå•ã‚’äº‹å‰ã«LLMã§ã€è§£ãã‚„ã™ã„ã‚ˆã†ã«ã€å¤‰å½¢ã™ã‚‹ã£ã¦ã®ã¯ã„ã„ã­ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®Phi-2ã€2.7Bãƒ‘ãƒ©ã®LLMã§ãã“ãã“æ€§èƒ½ãŒã§ã‚‹ã‚‰ã—ã„ã€‚DeepMindã®FunSearchã€æ–°ã—ã„ç§‘å­¦ã®ç™ºè¦‹ãŒLLMã§å®Ÿç¾ã§ãã‚‹ä¸–ç•ŒãŒã¤ã„ã«ã‚„ã£ã¦ããŸã€‚å­£ç¯€æŸ„ã‚¢ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç³»ã®è¨˜äº‹ãŒã‚ˆã„ã€å¤å…¸ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹åˆ†æã¨ã‹ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¨ã‹ã€‚LLMã«ã‚ˆã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç ”ç©¶ã‚‚ã€open-ended ãªçŠ¶æ³ã§ç ”ç©¶ã‚’ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã„ã†ã‚³ãƒ³ã‚»ãƒ—ãƒˆãŒæ˜ç¢ºã«ãªã‚Šã€ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆã§ã®è©•ä¾¡äº‹ä¾‹ã¨ã‹ã©ã‚“ã©ã‚“é€²ã‚“ã§ã‚†ãã€‚

- "TaskWeaver: A Code-First Agent Framework
	- https://arxiv.org/abs/2311.17541
	- Microsoftã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªç„¶è¨€èªã§ã€Œã“ã†ã—ã¦ã€ã¨è¨€ã†ã ã‘ã§LLMãŒè¦æ±‚ã‚’ç†è§£ã—ã€å®Ÿè¡Œã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€TaskWeaverï¼ˆã‚¿ã‚¹ã‚¯ã‚¦ã‚£ãƒ¼ãƒãƒ¼ï¼‰ã€ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ 
	- å®Ÿé¨“ã®çµæœã€æ ªä¾¡äºˆæ¸¬ã‚„ç•°å¸¸æ¤œå‡ºãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’é€šã—ã¦æœ‰åŠ¹æ€§ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ãã†ã§ã™ã€‚
	- â‘  è‡ªç„¶è¨€èªã§ã®è¦æ±‚ã‚’ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã™ã‚‹ 
	- â‘¡ è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ 
	- â‘¢ æœ€é©ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é¸æŠã—ã€ã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹
- LLMã‚’ã‚»ãƒ©ãƒ”ã‚¹ãƒˆã¨ã—ã¦å®Ÿè¡Œã—ã€ã€ŒèªçŸ¥ã®æ­ªã¿ã€ã‚’è©•ä¾¡ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Diagnosis of Thought (DoT)ã€ã«åŸºã¥ãMyGPT
	- https://chat.openai.com/g/g-o9r1c3nkf-serapisuto-diagnosis-of-thought-dot
- æ—¥æœ¬èª LLM ã®ç²¾åº¦ãŒã„ã¾ã„ã¡ãªã®ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å•é¡ŒãŒã‚ã‚Šãã†ã¨ã„ã†æŒ‡æ‘˜
	- https://github.com/AUGMXNT/shisa/wiki/A-Review-of-Public-Japanese-Training-Sets#analysis
- gtp-fastã®æœ¬å®¶github
	- Simple and efficient pytorch-native transformer text generation.
	- https://github.com/pytorch-labs/gpt-fast
- "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"
	- https://arxiv.org/abs/2312.00678
	- LLMã®åŠ¹ç‡ã‚’é«˜ã‚ã‚‹ãŸã‚ã®ãƒã‚¦ãƒã‚¦ã«é–¢ã™ã‚‹ç¶²ç¾…çš„ãªèª¿æŸ» by Microsoft
	- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ãƒ‡ãƒ¼ã‚¿ï¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼æ¨è«–ã€ã¨ã„ã£ãŸ5ã¤ã®è¦³ç‚¹ã‹ã‚‰å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚
- MistralAI Embeddings
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/embeddings/mistralai.ipynb
	- llamaindexã‚ˆã‚ŠMistralAI ã®Embeddingsã‚’åˆ©ç”¨ã™ã‚‹notebook
	- ãªã‚“ã‹ã€MistralAIè‡ªä½“ã‚‚ã¤ã‹ã‚‹ã‚‰ã—ã„
		- The new Mistral 8x7B model is an open-source model that made waves in the AI community today, outperforming gpt-3.5 and llama2 70B. Check out `mistral-tiny`, `mistral-small`, and `mistral-medium` variants.
		- https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/mistralai.ipynb
- MistralãŒã©ã†ãˆã‚‰ã„ã®ã‹ï¼Ÿ by ã‚¸ãƒ ãƒ•ã‚¡ãƒ³æ°
	- https://twitter.com/DrJimFan/status/1734269362100437315
	- MoE is the right path forward
	- An LLM is a snapshot of a civilization
	- ã‚¸ãƒ ãƒ•ã‚¡ãƒ³æ°æ›°ãã€Mistralã®Mixtralãƒ¢ãƒ‡ãƒ«å…¬é–‹ã®ãƒ¯ã‚±åˆ†ã‹ã‚‰ã‚“ãƒ ãƒ¼ãƒ–ã¯å®Ÿã¯é«˜åº¦ãªæˆ¦ç•¥ã ã£ãŸã€‚ã¾ãšä½•ã®èª¬æ˜ã‚‚ãªããƒ¢ãƒ‡ãƒ«ã‚’torrentã«æŠ•ä¸‹ã€‚ãã‚“ã§vLLMãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ—ãƒ«ãƒªã‚¯æŠ•ã’ã¦ã€èª°ã§ã‚‚Mixtralã§éŠã¹ã‚‹ã‚ˆã†ã«ç’°å¢ƒã‚’ä½œã£ã¦ã‚ã’ã‚‹ã€‚æœ€å¾Œã«ã‚ã‚‰ãŸã‚ã¦ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç™ºè¡¨ï¼ç™ºè¡¨ã¨åŒæ™‚ã«ã™ãéŠã¹ã¦ä¸–é–“ãŒç››ã‚Šä¸ŠãŒã£ã¦æ³¨ç›®åº¦ã‚’ç¨¼ã’ã‚‹ã¨ã„ã†æµã‚Œ by ã†ã¿ã‚†ãã•ã‚“
- "From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"
	- https://arxiv.org/abs/2312.06571
	- æ±äº¬å¤§å­¦ã¨æ ªå¼ä¼šç¤¾ã‚ªãƒ«ã‚¿ãƒŠãƒ†ã‚£ãƒ´ãƒ»ãƒã‚·ãƒ³ã®ç ”ç©¶è€…ã‚‰ã¯ã€ŒLLMã¨ç‰©ç†çš„ãªä¸–ç•ŒãŒã¤ãªãŒã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã®ã‹ï¼Ÿã€ã¨æƒ³åƒã—ã€å®Ÿéš›ã«GPT-4ã¨ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰ãƒ­ãƒœãƒƒãƒˆã‚’é€£æºã—ã¾ã—ãŸ
	- å®Ÿé¨“ã®æ¦‚è¦
		- â‘  ãƒ­ãƒœãƒƒãƒˆã€ŒAlter3ã€ã«å¯¾ã—ã¦ã€æ§˜ã€…ãªè‡ªç„¶è¨€èªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡ç¤º 
		- â‘¡ GPT-4ãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ãƒœãƒƒãƒˆå‹•ä½œã®ã‚³ãƒ¼ãƒ‰ã«å¤‰æ› 
		- â‘¢ ãƒ­ãƒœãƒƒãƒˆãŒäººé–“ã®ã‚ˆã†ãªå‹•ãã‚„æ„Ÿæƒ…è¡¨ç¾ã‚’å®Ÿè¡Œ 
	- å®Ÿé¨“ã®çµæœ 
		- â‘  ã€ŒAlter3ã€ã¯9ç¨®é¡ã®ç•°ãªã‚‹å‹•ä½œã®å®Ÿè¡Œã‚’æˆåŠŸ 
		- â‘¡ ç¬¬ä¸‰è€…ã«ã‚ˆã‚‹å‹•ä½œã®è©•ä¾¡ã¯é«˜ã‹ã£ãŸ 
		- â‘¢ äººé–“çš„ãªå‹•ä½œã¨æ„Ÿæƒ…è¡¨ç¾ã‚’å®Ÿç¾
-  Mixtral 8x7B ã®æ¦‚è¦  by npakaã•ã‚“
	- https://note.com/npaka/n/n6043bc8b01bc?sub_rt=share_h
	- æ¨è«–ã¯6å€é€Ÿãã€ã»ã¨ã‚“ã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€ŒLlama2 70Bã€ã‚’ä¸Šå›ã£ã¦ã„ã¾
	- **Mistral-tiny** : Mistral 7B Instruct v0.2ã€‚è‹±èªã§ã®ã¿æ©Ÿèƒ½ã€‚MT-Benchã§ã¯7.6ã‚’ç²å¾—ã€‚  
	- **Mistral-small** : Mixtral 8x7Bã€‚è‹±èª/ãƒ•ãƒ©ãƒ³ã‚¹èª/ã‚¤ã‚¿ãƒªã‚¢èª/ãƒ‰ã‚¤ãƒ„èª/ã‚¹ãƒšã‚¤ãƒ³èªã¨ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚¹ã‚¿ãƒ¼ã€‚MT-Benchã§8.3ã‚’ç²å¾—ã€‚  
	- **Mistral-medium** : Mistral AIã®æœ€é«˜å“è³ªã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ¢ãƒ‡ãƒ«ã€‚è‹±èª/ãƒ•ãƒ©ãƒ³ã‚¹èª/ã‚¤ã‚¿ãƒªã‚¢èª/ãƒ‰ã‚¤ãƒ„èª/ã‚¹ãƒšã‚¤ãƒ³èªã¨ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚¹ã‚¿ãƒ¼ã€‚MT-Benchã§8.6ã‚’ç²å¾—ã€‚
- ãƒŸã‚¹ãƒˆãƒ©ãƒ«ã®MoEç‰ˆã§ã‚ã‚‹mixtralã§ã™ãŒé©šã„ãŸäº‹ã«æ—¢ã«llama.cppã®é‡å­åŒ–ç‰ˆãŒå‡ºã¦ã„ã‚‹ã®ã§gpuãŒãªã„ç’°å¢ƒã‚„Macã§ã‚‚å‹•ã‹ã›ã‚‹
	- https://x.com/webbigdata/status/1734425932029628876?s=20
- "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
	- https://arxiv.org/abs/2312.06585
	- LMã«è‡ªã‚‰é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã•ã›ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µã™ã‚‹ã€Œè‡ªå·±å­¦ç¿’ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ by DeepMind
	- æ–¹æ³•
		- â‘  è‡ªã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µã™ã‚‹ 
		- â‘¡ ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹
		- â‘¢ æ•°å­¦ã‚’ä¸­å¿ƒã¨ã—ãŸæ§˜ã€…ãªå•é¡Œè§£æ±ºã«ä½¿ãˆã‚‹
	- å®Ÿé¨“çµæœ 
		- â‘  æ•°å­¦ã«ãŠã„ã¦ã€æ­£ç­”ç‡ã®å‘ä¸Šã‚’é”æˆ 
		- â‘¡ ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®å•é¡Œã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é©å¿œèƒ½åŠ›ãŒå‘
-  Query Transform Cookbook
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/query_transformations/query_transform_cookbook.ipynb
	- RAGã«ãŠã„ã¦ã€æ¤œç´¢çµæœã‚’contextã«ç©ã‚“ã§LLMã«å›ç­”ã•ã›ã‚‹ã®ã§ã¯ãªãã¦ã€è³ªå•ã‚’LLMã§å¤‰æ›ã—ã¦ã‚†ãã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
	- Query Understanding Layer
- Mistral-7B-Instruct-v0.2 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://x.com/npaka123/status/1734348586689908878?s=20
	- https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- Mixtral-8x7B-Instruct-v0.1 ã‚’è©¦ã™ã€‚load_in_4bitã€‚ by npakaã•ã‚“ã€
	- https://x.com/npaka123/status/1734408371154100457?s=20
	- https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
	- èµ·å‹•ã¾ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å«ã‚ã¦20åˆ†ã§æ¨è«–é€Ÿåº¦ã¯200ãƒˆãƒ¼ã‚¯ãƒ³ã§21ç§’
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãŒPhi-2ã¨ã‹ã„ã†2.7Bãƒ‘ãƒ©ã®LLMã‚’ãƒªãƒªãƒ¼ã‚¹
	- https://x.com/umiyuki_ai/status/1734763437274890746?s=20
	- MicrosoftãŒIgniteã§è©±ã—ã¦ã„ãŸã‚ãšã‹27å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«Phi-2
	- ãƒ‘ãƒ©æ•°å°ã•ã„ãã›ã«ã‚ã‚Šå¾—ã‚“é«˜æ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã‚‹ã‚‰ã—ã„ã€‚
	- å­¦ç¿’é‡ã¯1.4Tãƒˆãƒ¼ã‚¯ãƒ³ã§ã€96å€‹ã®A100ã§14æ—¥ã‹ã‘ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚
	- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒ‘ãƒ©æ•°3.2Bã®Gemini Nanoã«å®Œå‹ï¼ˆã¦ã‹Gemini Nanoã®ãƒ‘ãƒ©æ•°åˆã‚ã¦çŸ¥ã£ãŸã‚ï¼‰
	- ãã—ã¦ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®ç‹¬è‡ªãƒ™ãƒ³ãƒã«ãŠã„ã¦ã€ã¾ã•ã‹ã®Llama2-70Bç›¸æ‰‹ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§åœ§å‹ã€æ•°å­¦ã§åƒ…å·®ã«è¿«ã‚‹ã€‚Llama2-13Bç›¸æ‰‹ã«ã¯å®Œå‹ã—ã¦ã—ã¾ã†ã€‚
- The Emergent Abilities of LLMs Could Be A Mirage!
	- The best paper award in NeurIPs 2023 went to a paper claiming that the emergent abilities of LLMs could be a mirage!
- llamaindexã«ã¦mistralaiã®ã‚µãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¬é–‹
	- https://docs.llamaindex.ai/en/stable/examples/llm/mistralai.html
- ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã€‘Mixtral-8x7bã‚’llama.cppã§è©¦ã™
	- https://note.com/bakushu/n/n5b270b288cba?sub_rt=share_b
	- llama.cppã§ã€ŒMixtral-8x7bã€ã®GGUFé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã—ãŸï¼ˆç¾æ™‚ç‚¹ã§ã¾ã mergeã•ã‚Œã¦ã„ãªã„ã®ã§branchã‚’åˆ©ç”¨ï¼‰
	- ã€Œ**Mixtral-8x7b**ã€ã¯MistralãŒãƒªãƒªãƒ¼ã‚¹ã—ãŸMoEï¼ˆMixture of Expertsï¼‰æ§‹é€ ã®LLMã§ã€ŒMistral 7Bã€ãƒ™ãƒ¼ã‚¹ã®8å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æŸã­ã¦ã„ã¾ã™ã€‚
	-   ä»Šå›ã¯Google Colabã§ã€Œ[**Mixtral-8x7B-Instruct-v0.1-Q4_K_M-GGUF**](https://mixtral-8x7b-instruct-v0.1-gguf/)ï¼ˆ4bité‡å­åŒ–ç‰ˆï¼‰ã€ã®æ¨è«–ã‚’è©¦ã—ã¾ã—ãŸã€‚
	- 4bité‡å­åŒ–ã§ã‚‚26GBã»ã©ã‚ã‚Šã¾ã™ã€‚Colab Proã®CPUã‚ªãƒ³ãƒªãƒ¼+ãƒã‚¤ãƒ¡ãƒ¢ãƒªã§å®Ÿè¡Œã—ã¦ã¿ã¾ã—ãŸã€‚GPUã®ã¿ã§æ¨è«–ã™ã‚‹ãªã‚‰A100ãŒå¿…è¦ã§ã™ã€‚
	- Colabã®CPUã ã¨ã•ã™ãŒã«é…ã„ã‚‚ã®ã®ã€æœ€è¿‘ã®PCã®CPUãªã‚‰ãµã¤ã†ã«å‹•ã‹ã›ãã†ã€‚Llama 34B/70Bã®é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã‚‹ã¨å…¨ç„¶é€Ÿã„ã§ã™
- LangChainã‚’ä½¿ã‚ãªã„
	- https://tech-blog.abeja.asia/entry/advent-2023-day13
	- æŠ€è¡“çš„è² å‚µã«ãªã‚Šã†ã‚‹ã¨ã‹ã€Agentã£ã¦function callã§ä»£æ›¿å¯èƒ½ã¨ã‹ãã†ã„ã†è©±
- LlamaIndex + Gemini
	- https://blog.llamaindex.ai/llamaindex-gemini-8d7c3b9ea97e
	- llamaindexã€ã„ããªã‚ŠGeminiãƒ•ãƒ«ã‚µãƒãƒ¼ãƒˆ
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/gemini.ipynb
	-  Multi-modal Modelä¸­ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‚‰ã—ã„ã€ã€ã€
-  Google Generative Language Semantic Retriever
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb
	- Googleâ€™s new semantic retrieval endpoint offers specialized embeddings and LLMs for high-quality retrieval + synthesis with guardrails. Use it out of the box, OR combine it with LlamaIndex components to build advanced RAG.
	- The Gemini API contains semantic search with custom embedding models for better retrieval, as well as toggles incl. safety during generation.
	- GoogleãŒsemantic Retrieverã£ã¦ã®ã‚’ã ã—ã¦ãŸã®ã‹ï¼Ÿ
- LangChainã‚‚Geminiå¯¾å¿œ
	- https://python.langchain.com/docs/integrations/chat/google_generative_ai
	- Access Google AIâ€™s `gemini` and `gemini-vision` models, as well as other generative models through `ChatGoogleGenerativeAI` class in the [langchain-google-genai](https://pypi.org/project/langchain-google-genai/) integration package.
- Gemini Pro APIã®ä¾¡æ ¼è¡¨ã€
	- https://ai.google.dev/pricing?hl=ja
	- å…¥åŠ›ãŒ$0.00025/1k charactersãªã®ã§gpt-3.5-turbo-1106ã®1/4ã®ä¾¡æ ¼ï¼ˆã¤ã¾ã‚Š11æœˆä»¥å‰ã®gpt-3.5-turboã®1/12ï¼‰ã§ä½¿ãˆã‚‹ã‚‰ã—ã„ã€‚
	- ãƒ•ãƒªãƒ¼ç‰ˆãªã‚‰ã°ã€60QPM (queries per minute)ã¾ã§ã¯ä½¿ãˆã‚‹ï¼ï¼ï¼ï¼
- phi-2ã‚’è©¦ã™
	- https://x.com/npaka123/status/1735077608071876882?s=20
	- Llama2-70Bç›¸æ‰‹ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§åœ§å‹ã—ãŸ2.7Bãƒ¢ãƒ‡ãƒ«ã€‚
	- https://huggingface.co/microsoft/phi-2
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªä½œã—ã‚ˆã†ï¼(Transformers+DeepSpeed+torch.compile+flash_attn2
	- https://zenn.dev/selllous/articles/transformers_pretrain_to_ft
	- è‹±èªãŒãƒ¡ã‚¤ãƒ³ã®LLM Mistral-7Bãƒ¢ãƒ‡ãƒ«ã‚’300M(0.3B)ã¸ãƒ€ã‚¦ãƒ³ã‚µã‚¤ã‚ºã—ã¦ã€pretraining + instruction tuningã‚’Colabä¸Šã®GPU T4(!!!)ã§6æ™‚é–“(0.02epoch)ã§æ—¥æœ¬èªå­¦ç¿’ã•ã›ã‚‹ã¨ã„ã†æ„æ¬²çš„ãªè¨˜äº‹
-  FunSearch: Making new discoveries in mathematical sciences using Large Language Models
	- https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/?utm_source=twitter&utm_medium=social
	- DeepMindã®ç ”ç©¶ãƒãƒ¼ãƒ ãŒã€AIã‚’ç”¨ã„ã¦æ•°å­¦ã®æœªè§£æ±ºå•é¡Œã«æŒ‘ã¿ã€ç§‘å­¦ç•Œã«ãŠã‘ã‚‹å‰ä¾‹ã®ãªã„æˆæœã‚’å‡ºã—ãŸã¨ç™ºè¡¨ã—ã¾ã—ãŸã€‚ ã€ŒFunSearchã€ã¨åä»˜ã‘ã‚‰ã‚ŒãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€å•é¡Œè§£æ±ºç­–ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å½¢ã§ç”Ÿæˆã€‚ã€Œã‚­ãƒ£ãƒƒãƒ—ã‚»ãƒƒãƒˆå•é¡Œã€ã¨ã€Œãƒ“ãƒ³ãƒ‘ãƒƒã‚­ãƒ³ã‚°å•é¡Œã€ã¨ã„ã†æ•°å­¦ã®å•é¡Œã«ãŠã„ã¦ã€æ–°ãŸãªè§£æ³•ã‚’ç™ºè¦‹ã—ãŸã¨ã®ã“ã¨ã§ã™ã€‚
	- Introducing FunSearch in @Nature: a method using large language models to search for new solutions in mathematics & computer science
	- DeepMindãŒLLMã‚’ã€Œäº‹å‰ã«ã‚¿ã‚¹ã‚¯è©•ä¾¡ã§ãã‚‹å•é¡Œã€ã«éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ãŸFunSearch(searching in the function space)ææ¡ˆã€‚
	-  LLMãŒã‚³ãƒ¼ãƒ‰ç”Ÿæˆ->è©•ä¾¡->æ´—ç·´ã®ãƒ«ãƒ¼ãƒ—ã€‚ 
	- ** ç§‘å­¦,æ•°å­¦ã®æœªè§£æ±ºå•é¡Œã«å¯¾ã—ã¦ã€åˆã‚ã¦LLMã‚’ç”¨ã„ãŸæ–°ãŸãªç™ºè¦‹ **ã€‚ 
	- ãã®ä¾‹ã¨ã—ã¦cap set problem,bin-packing problemã€‚
-  Benchmarking RAG on tables
	- https://blog.langchain.dev/benchmarking-rag-on-tables/
	- llmaindexã‚ˆã‚Šã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®ï¼²ï¼¡ï¼§ã«ã¤ã„ã¦ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€long contextã¯æ€§èƒ½ã¯ã§ãªã„
-  MOEè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ä¸€äººã‚’æ—¥æœ¬èªå¾—æ„ãªãƒ¢ãƒ‡ãƒ«ã«ç½®ãæ›ãˆãŸã‚‰ã©ã†ãªã‚‹ã®ã‹ï¼Ÿ
	- https://note.com/aisatoshi/n/n6c06d5183517?sub_rt=share_pb
	- Mistral7Bã‚’8ã¤æŸã­ãŸã€Mixtral 8x7Bã¨ã„ã†MOEãƒ¢ãƒ‡ãƒ«
	- ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’ä½•äººã‹ã€æ—¥æœ¬èªãŒå¾—æ„ãªMistral7Bäº’æ›ãƒ¢ãƒ‡ãƒ«ã«å·®ã—æ›¿ãˆãŸã‚‰ã©ã†ã ã‚ã†ï¼Ÿ
	- æ³¨æ„æ©Ÿæ§‹ã ã‘ã€MLPå±¤ã ã‘ã€ã‚³ãƒ”ãƒ¼ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°ã‚’å¤‰æ›´ãªã©å®Ÿé¨“ã—ã¾ã—ãŸãŒã€åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ãŒå£Šã‚Œã¾ã—ãŸ
- è‡ªæ°‘å…šãŒAIè¦åˆ¶ã‚’æè¨€
	- https://x.com/umiyuki_ai/status/1735277687097414124?s=20
- GCPã‚ˆã‚ŠGemeniã®æ§˜ã€…ãªåˆ©ç”¨æ–¹æ³•ã¨notebook
	- https://github.com/GoogleCloudPlatform/generative-ai
- Geminiã‚’ã¤ã‹ã£ã¦ã€ã‚¯ãƒªã‚¹ãƒã‚¹ã‚«ãƒ¼ãƒ‰ã‚’ä½œã‚‹ä¾‹ by google
	- https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Prepare_Christmas_cards_with_Gemini_and_Sheets.ipynb
-  OpenAI thinks superhuman AI is coming â€” and wants to build tools to control it
	- https://openai.com/blog/superalignment-fast-grants
	- Open AIè¶…äººçš„ãªAIã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã«å‘ã‘ãŸç ”ç©¶ã«1000ä¸‡ãƒ‰ãƒ«ã®åŠ©æˆé‡‘ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹ã€‚ 
	- æ”¯æ´ã«Google CEOå…¼ä¼šé•·ã®ã‚¨ãƒªãƒƒã‚¯ãƒ»ã‚·ãƒ¥ãƒŸãƒƒãƒˆæ°ã€‚ 
	- ã‚¤ãƒªãƒ¤ã‚µãƒ„ã‚±ãƒãƒ¼æ°ä»Šã‚‚ã¾ã Super Alignmentãƒãƒ¼ãƒ ç‡ã„ã¦ã‚‹ã¨ã®ã“ã¨ï¼
-  A Guide on 12 Tuning Strategies for Production-Ready RAG Applications
	- https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439
	- LLMã®RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®12æˆ¦ç•¥ã‚’æ›¸ã„ãŸãƒ–ãƒ­ã‚°è¨˜äº‹ã€‚å…·ä½“çš„ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€åŸ‹è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã€ã‚¤ãƒ³ãƒ‡ã‚¯ã‚·ãƒ³ã‚°ã€ã‚¯ã‚¨ãƒªå¤‰æ›ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ç­‰ã€å®Ÿè·µçš„ãªæˆ¦ç•¥ã€‚
- Bishopå…ˆç”Ÿã®ã€ŒDeep Learning: Foundations and Conceptsã€
	- https://www.bishopbook.com/
	- Vision Language Modelã®ã¨ã“ã‚è¦‹ãŸã‚‰CM3LeonãŒè¼‰ã£ã¦ã¦é©šã„ãŸ
- Benchmarking Large Language Models As AI Research Agents
	- https://arxiv.org/abs/2310.03302
	- ã“ã®è«–æ–‡ãŒç´ æ™´ã‚‰ã—ã„ã®ã¯ã€open-ended ãªçŠ¶æ³ã§ç ”ç©¶ã‚’ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã„ã†ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’æ˜ç¢ºã«æç¤ºã—ãŸç‚¹ã 
- calm2-7b-chatã‚’RAG QAã§ä½¿ã†ãŸã‚ã®èª¿æŸ»
	- https://x.com/_oshizo_/status/1735282188546089332?s=20
	- contextå…¨ä½“ã®é•·ã•ï¼ˆæ¨ªè»¸ï¼‰ã¨ã€æ­£è§£ã«ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ï¼ˆç¸¦è»¸ï¼‰ã‚’å¤‰ãˆãªãŒã‚‰ã€å‡ºåŠ›ã«æ­£è§£ã®æ–‡å­—åˆ—ã‚’å«ã‚“ã å‰²åˆã‚’é›†è¨ˆã€‚ 
	- æ­£è§£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒcontextã®æœ«å°¾ä»˜è¿‘ã«ã‚ã‚Œã°å…¨ä½“ã®é•·ã•ã¯ã‚ã¾ã‚Šå½±éŸ¿ã—ãªã„ãŒã€æœ«å°¾ã‹ã‚‰1ké›¢ã‚Œã‚‹ã”ã¨ã«æ­£ç­”ç‡ãŒ0.6æ›ã‘ã«ãªã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸
- LLMãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è©•ä¾¡ãƒ»ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¾ã¨ã‚ã¦ã¿ãŸ
	- https://zenn.dev/pomcho555/articles/8e42f0a4ce39eb
	- RAGASã‚’ä½¿ã£ãŸè‡ªå‹•ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
	- RAGASã‚’ä½¿ã£ãŸè‡ªå‹•è©•ä¾¡
- Web3æ™‚ä»£ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼Ÿ â€“ Geoã‚’è§¦ã£ã¦ã¿ãŸ
	- https://zenn.dev/s_egami/articles/4ec2e0de59ff4d
- "Pixel Aligned Language Models"
	- https://arxiv.org/abs/2312.09237
	- Googleã®ç ”ç©¶è€…ã‚‰ã¯ã€ç”»åƒã‚’ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã§è¨€èªåŒ–ã™ã‚‹èƒ½åŠ›ã‚’ã‚‚ã¤LLMã€PALMã€é–‹ç™ºã—ã¾ã—ãŸ
	- å®Ÿé¨“ã®çµæœã€ã€ŒäººãŒç†è§£ã—ã‚„ã™ã„ã€å†…å®¹ã§æ­£ç¢ºã‹ã¤è©³ç´°ã«ç”»åƒã‚’èª¬æ˜ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ç¢ºèªã•ã‚Œã¾ã—ãŸ
-  æ—¥æœ¬ã®å¤å…¸å’Œæ­Œã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã§åˆ†æã™ã‚‹
	- https://note.com/yhkondo/n/nd321604729cd?sub_rt=share_pw
	- OpenAIã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ã¦ã€ã€å¤ä»Šé›†ã€ã€ä¸‡è‘‰é›†ã€ã€å’Œæ¼¢æœ—è© é›†ã€ç­‰ã‚’åˆ†æã—ã€ã„ã‚ã‚†ã‚‹ã€ŒèŠ±é³¥é¢¨æœˆã€ã¨ã„ã†æ¦‚å¿µãŒã©ã“ã‹ã‚‰ç”Ÿã¾ã‚Œã¦ããŸã‹ã‚’æ¢æ±‚ã—ãŸã‚‚ã®ã§ã™ã€‚AIã®æŒã¤åŠ›ã‚’æ„Ÿã˜ã¦ã„ãŸã ã‘ã‚‹ã¨ç¢ºä¿¡ã—ã¦ã„ã¾ã™
-  Google Colab ã§ Gemini Pro ã‚’ã‚‚ã£ã¨è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n1c368639cada?sub_rt=share_h
	- 1.  2. ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®è¡¨ç¤º
	- 2.  3. è³ªå•å¿œç­”
	- 3.  4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
	- 4.  5. ãƒãƒ£ãƒƒãƒˆ
	- 5.  6. ç”»åƒã‹ã‚‰ã®è³ªå•å¿œç­”
	- 6.  7. ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®è³ªå•å¿œç­”
	- 7.  8. åŸ‹ã‚è¾¼ã¿ã®ç”Ÿæˆ
-  Voyager: An Open-Ended Embodied Agent with Large Language Models
	- https://arxiv.org/abs/2305.16291
	- LLMã‚’ã®ã›ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆã‚’ã•ã›ãŸç ”ç©¶ï¼Œé€²æ—ã®è§£é™¤å…·åˆã‚„ãƒãƒƒãƒ—ã®æ¢ç´¢ç¯„å›²ã®åºƒã•ã‚’ã¿ã¦ã„ã¦ï¼Œæ»…èŒ¶è‹¦èŒ¶é¢ç™½ã„ãªï½—ã€€ãƒ—ãƒ¬ã‚¤é¢¨æ™¯ã‚’ã¿ã¦ã¿ãŸã„
- mmnga/Mixtral-Fusion-4x7B-Instruct-v0.1
	- https://huggingface.co/mmnga/Mixtral-Fusion-4x7B-Instruct-v0.1
	- Mixtral-8x7B-Instruct-v0.1 ã®Expertsã®ã†ã¡2ã¤æ¯ã«mergeã—ã¦4x7bã«ã—ãŸå®Ÿé¨“ãƒ¢ãƒ‡ãƒ«ä½œã‚Šã¾ã—ãŸ
	- Modelã‚µã‚¤ã‚ºã¯24Bã«ãªã‚Šã¾ã™
- NeurIPS Large Language Model Efficiency Challenge:  1 LLM + 1GPU + 1Day
	- https://llm-efficiency-challenge.github.io/index
	- OSS LLMãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã«é™ã‚‰ã‚ŒãŸè³‡æºãƒ»æ™‚é–“ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã™ã‚‹ã¨ã„ã†ã‚³ãƒ³ãƒš
	- é‡è¦ãªã®ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
	- è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ä¸Šè³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹æˆã—è¨“ç·´ã™ã‚‹ã®ãŒéµ
- 
	


## 12/11

ä»Šé€±ã¯ãªã‚“ã¨ã„ã£ã¦ã‚‚ã€Googleã®Geminiã€‚GPT-4è¶Šãˆã¨ã‹ã€ã™ãã«Bard(è‹±èªç‰ˆï¼‰ã§Gemini Proã‚’è©¦ã›ã‚‹ã¨ã‹ã€ç ”ç©¶ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã—ã¦ä½¿ã†ãƒ‡ãƒ¢ã¨ã‹ã€ãã‚Œã‹ã‚‰ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’ãƒ•ãƒ«ã«ç”Ÿã‹ã—ãŸå­ä¾›å‘ã‘ã®ãŠéŠã³ãƒ‡ãƒ¢ã¨ã‹ãªã‹ãªã‹è¡æ’ƒçš„ã§ã‚ã£ãŸãŒã€ãªã‚“ã¨ãŠéŠã³ãƒ‡ãƒ¢ãŒç´™èŠå±…ï¼ˆéƒ¨åˆ†ã‚’ã¤ãªã’ã¦ãã‚Œã‚‰ã—ãè¦‹ãˆã‚‹ã‚ˆã†ã«ã—ãŸã€éƒ¨åˆ†éƒ¨åˆ†ã¯æœ¬ç‰©ã‚‰ã—ã„ãŒï¼‰ã¨ã®å ±é“ãŒã‚ã‚Šã€äº‹å‰ã®ã€Œï¼‘æœˆã«é…å»¶ã€ã¨ã®å ±é“ã¨åˆã‚ã›ã‚‹ã¨ç· ã‚åˆ‡ã‚Šã«é–“ã«åˆã‚ãªã‹ã£ãŸã‚“ã ã‚ã†ã‘ã©ã€å‰å›ã®BardãŠæŠ«éœ²ç›®ã§ã®å¤±æ…‹ã¨ã„ã„ã€è„‡ãŒç”˜ã„ã€‚ãªãŠGeminiã®å‘½åã®ç”±æ¥ã€ä¸Šä½ï¼–åã®ä¸»è¦è²¢çŒ®è€…ã®First Nameã‹ã‚‰ã¨ã£ãŸã‚‰ã—ã„ã€‚Mambaã¨ã„ã†ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒã®ä»£æ›¿æŠ€è¡“ã€æ€§èƒ½ã‚ˆã•ãã†ã§æœŸå¾…ã€‚ DeepMindã®ã€GNoMEã€ã¯ã€Œäººé–“ã®ç›´æ„Ÿã‚’è¶…ãˆãŸ220ä¸‡ã®ææ–™ã‚’ç™ºè¦‹ã—ã€ç§‘å­¦ã®ç™ºå±•ã‚’LLMãŒæ˜ã‚‰ã‹ã«åŠ é€Ÿã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ãã‚Œã£ã¦å±é™ºãªææ–™ã‚‚ã€‚ã€‚ã€‚Metaã¯å®‰å…¨ãªAIã®ãŸã‚ã®Purple LLamaã‚’ç™ºè¡¨ã€Securityã‚„å®‰å…¨ã‚¬ãƒ¼ãƒ‰ã‚’æä¾›ã€‚æ”»æ’ƒï¼ˆred)ã¨é˜²å¾¡(blue)ãŒå”åŠ›ã™ã‚‹ã‹ã‚‰Prupleãªã‚“ã ã£ã¦ã€‚å®‰å…¨ã‚¬ãƒ¼ãƒ‰(Llama Guard)ã¯LLMã§å®Ÿè£…ã•ã‚Œã€ã¤ã¾ã‚ŠLLMã«ã¯LLMã£ã¦ã“ã¨ã€‚Metaã¯IBMç­‰ã¨ã®ä¼æ¥­é€£åˆã§å®‰å…¨ãªOSSã¨ã—ã¦ã®ç”Ÿæˆå‹AIé–‹ç™ºã‚’ä¿ƒé€²ã€OSSã®LLMãŒã¾ã™ã¾ã™ç†±ããªã‚‹ï¼Ÿã€‚Appleã‹ã‚‰æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯mlxç™ºè¡¨ã€M3ã£ã¦ã™ã”ã„ã‚“ã ã€LLMã§ã¯ä»Šä¸€æ­©ãƒ—ãƒ¬ã‚¼ãƒ³ã‚¹ã®ç„¡ã„Appleã€CNBCã®æ½œå…¥ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§ã‚‚ã€LLMç«¶äº‰ã«é€²å‡ºã™ã‚‹ã‹ã¨èã‹ã‚Œã¦ã€è²¬ä»»è€…ã¯ãƒ¢ã‚´ãƒ¢ã‚´ã¯ãã‚‰ã‹ã—ã¦ãŸãªã€ã‚ã‚„ã—ã•æº€è¼‰ã€‚NVIDIAã®H100ã€MSã¨Metaã¯ãã‚Œãã‚Œ150k(15ä¸‡å€‹ï¼‰ã‚’æŒã£ã¦ã„ã¦ãƒ€ãƒ³ãƒˆãƒ„ã€ã©ã†ã‚‚H100ãŒ15ä¸‡å€‹ã‚ã‚Œã°ï¼—æ—¥ã§GPT-4ãŒä½œã‚Œã‚‹æ€§èƒ½ã‚‰ã—ã„ã€‚ä¸€æ–¹AMDã‚‚ç”ŸæˆAIã§NVIDIA H100ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã®GPUã€ŒInstinct MI300ã€ã‚’ç™ºè¡¨ã€‚GPUã‚‚ç†±ã„ã€ã‚ã‚Œã‚‰ã®ç‰§é‡å…ˆç”Ÿã®MN-coreã®ç™»å ´ã‚’æœŸå¾…ã—ã¾ã™ã‹ã€‚ã¤ã„ã«æ¬§å·AIæ³•ãŒæˆç«‹ã€AIã®å®šç¾©ãŒï¼¯ï¼¥ï¼£ï¼¤ã®ãã‚Œã«æ•´åˆã—ãŸã¨ã‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹è¦åˆ¶ã®æ˜ç¢ºåŒ–ãŒãƒã‚¤ãƒ³ãƒˆã€‚ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒªã‚¹ã‚¯ã«ã©ã†å‚™ãˆã‚‹ã‹ãŒè‚ã€‚ãã®AIæ³•ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¸ã®è¦åˆ¶éƒ¨åˆ†ã«ç•°è­°ã‚’å”±ãˆã¦ã„ãŸä»MistralãŒã€æº€ã‚’æŒã—ã¦ï¼Ÿæ–°ã—ã„ mixtral-8x7b-32kseqlenã‚’ç™ºè¡¨ã€MoE(Mixture of Expert)ã¨ã„ã†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè‚ã‚‰ã—ã„ã€æ¬§å·AIè¦åˆ¶ã«é–¢é€£ã—ã¦mixtral-8x7b-32kseqlenã‚’å¿µé ­ã«ã€ãŸã£ãŸ87Gã®weightã§AGIãŒæ¥ã‚‹ãªã‚‰AIè¦åˆ¶å¿…è¦ã ã‚ˆã­ã¿ãŸã„ãªæ„è¦‹ã‚‚è¦‹ã‹ã‘ãŸã€‚ã“ã®ã»ã‹ã«ã‚‚ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMå‘ã‘ã®Ollama ã¨ã‹ã€è¨€èªãƒ‡ãƒ¼ã‚¿ãªã—ã§å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆLVMï¼‰ã‚’æ§‹ç¯‰ã¨ã‹ã€ãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼ã®æ—¥æœ¬ãŒDXã§ããªã„ãƒ¬ãƒãƒ¼ãƒˆ(èª¤æ¤ã‚’ç™ºè¦‹ï¼)ã¨ã‹ã€2bité‡å­åŒ–æŠ€è¡“QuIP#ã¨ã‹ã€æ§˜ã€…ã‚ã£ãŸãŒè¿½ãˆã¦ãªã„ã€‚ã€‚ãã‚‚ãã‚‚ã€ï¼‘é€±é–“åˆ†ã®ãƒ–ã‚¯ãƒæ•´ç†ã™ã‚‹ã ã‘ã§ï¼’æ™‚é–“ã‹ã‹ã‚‹ã‚“ã ã‘ã©ã€‚ã€‚ã€‚GPT-4ã«ã‚„ã‚‰ã›ã‚‹ã‹ã€‚ã€‚

- ä»Šæœˆã®NatureèªŒã¯é¢ç™½ã‹ã£ãŸ
	- https://x.com/ykfrs1217/status/1731287315459490165?s=20
	- â‘  å¤§éƒ½å¸‚ã»ã©ã€ç•°ãªã‚‹ç¤¾ä¼šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã²ã¨ãŸã¡ã¯æ··ã˜ã‚ã‚‰ãªã„ï¼ˆ[https://doi.org/10.1038/s41586-023-06757-3â€¦](https://t.co/tEkbdQOPG3)ï¼‰ 
	- â‘¡ åŒã˜ç”ºï¼ˆâ‰ƒå­¦å†…ï¼‰ã®ç ”ç©¶è€…ã ã‘ã§è¡Œã‚ã‚ŒãŸç ”ç©¶ã®æ–¹ãŒã€ç•°ãªã‚‹åœ°åŸŸé–“ã®å…±åŒç ”ç©¶ã‚ˆã‚Šã‚‚é©æ–°çš„ãªæˆæœãŒã§ã‚„ã™ã„ï¼ˆ[https://doi.org/10.1038/s41586-023-06767-1â€¦](https://t.co/jrBRV4Gxtk)ï¼‰
-  Phantom oscillations in principal component analysis
	- https://www.pnas.org/doi/10.1073/pnas.2311420120?utm_source=TOC&utm_medium=ealert&TOC_v120_i48=&ref=d4140497
	- æ™‚é–“çš„ãƒ»ç©ºé–“çš„ã«ã‚¹ãƒ ãƒ¼ã‚ºãªãƒ‡ãƒ¼ã‚¿ (ã»ã¨ã‚“ã©ã®ç”Ÿç†ãƒ‡ãƒ¼ã‚¿â€¦) ç­‰ã‚’ä¸»æˆåˆ†åˆ†æ PCA ã™ã‚‹ã¨ã€å½ã®ã‚ªã‚·ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‡ºç¾ã™ã‚‹
-  Refactoring Programs Using Large Language Models with Few-Shot Examples
	- https://arxiv.org/abs/2311.11690
	- ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã«LLMã‚’ä½¿ã†
- "On Bringing Robots Home" Nur Muhammad Mahi Shafiullah et al., New York University
	- https://arxiv.org/abs/2311.16098
	- å®¶åº­ç”¨ãƒ­ãƒœãƒƒãƒˆã®æ™®åŠã«å‘ã‘ã¦ã€ä¸€èˆ¬ã®ãƒ­ãƒœãƒƒãƒˆã‚’å„å®¶åº­ã«é©ç”¨ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€DobbÂ·Eã€ãŒé–‹ç™ºã•ã‚Œã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹
	- ä¸€èˆ¬ã®ãƒ­ãƒœãƒƒãƒˆã‚’å®¶åº­ç”¨ãƒ­ãƒœãƒƒãƒˆã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ä¸€é€£ã®æµã‚Œã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒã€DobbÂ·Eã€
	- â‘  åˆè¨ˆ109ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿéš›ã®å®¶åº­ã§å®Ÿæ–½ã—ã€ãƒ­ãƒœãƒƒãƒˆã®æˆåŠŸç‡ãŒ81ï¼…ã«é”ã—ãŸ 
	- â‘¡ èª¿ç†å®¶é›»ã‚’é–‰ã‚ã‚‹ï¼ã‚¯ãƒƒã‚·ãƒ§ãƒ³ã‚’ã²ã£ãã‚Šè¿”ã™ã‚¿ã‚¹ã‚¯ã¯100ï¼…ã€6è»¸ã§ç‰©ã‚’ç§»å‹•ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¯56% 
	- â‘¢ ãƒ‡ãƒ¼ã‚¿åé›†æ™‚ã«ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãŸç…§æ˜ã‚„å½±ã®æ¡ä»¶ä¸‹ã§ã¯ãƒ­ãƒœãƒƒãƒˆã¯å®‰å®šã—ã¦ç¨¼åƒã™ã‚‹
- Introducing Llama Datasets 
	- https://blog.llamaindex.ai/introducing-llama-datasets-aadb9994ad9e
	- llamaindexã‚ˆã‚Šã€RAGå‘ã‘ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¬é–‹
	- history of alexanetã¨ã‹ã€origin of covid19ãªã©ã®pdfã‚’å«ã‚€ã€å¤šåˆ†æ­£è§£å€¤ã¯ï¼Ÿ
- MEDITRON-70B: Scaling Medical Pretraining for Large Language Models
	- https://arxiv.org/abs/2311.16079
	- llama2ã‚’åŒ»ç™‚ã«ç‰¹åŒ–ã—ã¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸLLM
	- Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2.
	- webuiã§è©¦ã›ã‚‹ï¼
	- https://github.com/epfLLM/meditron/blob/main/deployment/README.md#serving-with-web-gui
- RAGç”¨é€”ã«ä½¿ãˆã‚‹ã€Wikipedia æ—¥æœ¬èªã® embeddings ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã® faiss index ã‚’ä½œã£ãŸ
	- https://secon.dev/entry/2023/12/04/080000-wikipedia-ja-embeddings/
	- Wikipediaæ—¥æœ¬èª550ä¸‡æ–‡ã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ãã‚‹embeddingsã¨æ¤œç´¢ç”¨faiss indexä½œã‚Šã¾ã—ãŸã€‚20è¡Œãã‚‰ã„ã‚³ãƒ¼ãƒ‰æ›¸ãã ã‘ã§ç°¡å˜ã«åˆ©ç”¨ã§ãã¾ã™ï¼RAGã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã¨é¢ç™½ã¿ãŒå°‘ãªã„ã®ã§ã™ãŒã€Wikipediaçªã£è¾¼ã‚€ã¨é¢ç™½ã•ãŒå¢—ãˆã¦ãã‚‹ã®ã§ã€èˆˆå‘³ã‚ã‚‹æ–¹ã¯ãŠè©¦ã—ãã ã•ã„ï¼
	- huggingface spaceã§è©¦ã›ã‚‹
	- https://huggingface.co/spaces/hotchpotch/wikipedia-japanese-rag-qa
	- ã€ŒãƒŠã‚¦ã‚·ã‚«ã¨æ£®ã®äººã¨ã®é–¢ä¿‚ã¯ï¼Ÿã€ã«ã¯å…¨ãç­”ãˆã‚‰ã‚Œãªã„ã€‚
	- FAISS+ELYZAã ã¨ã€ã€ŒãƒŠã‚¦ã‚·ã‚«ã¨æ£®ã®äººã¯ä»²è‰¯ã—ã ã£ãŸã€‚ã€ã¨ç­”ãˆã¦ãã‚ŒãŸã®ã«ã€‚ã€‚
- Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift
	- https://arxiv.org/abs/2311.15961
	- å…±å¤‰é‡ã‚·ãƒ•ãƒˆã®ãƒã‚¿ã§"All you need"çš„ãªæµè¡Œã‚Šã®ã‚¿ã‚¤ãƒˆãƒ«ã®è«–æ–‡ãªã‚“ã ã‘ã©ï¼Œå†…å®¹ã¯ã—ã£ã‹ã‚Šæ•°ç†ã‚„ã£ã¦ã‚‹ã£ã½ã„ï¼ãŒã£ã¤ã‚ŠShimodaira (2000)ã‚‚å‚ç…§ã•ã‚Œã¦ã¾ã—ãŸï¼å…±è‘—è€…ã«æ•°ç†çµ±è¨ˆã®å¤§å¾¡æ‰€ã®Jianqing Fanå…ˆç”Ÿã¨ã‹ï¼Œæ©Ÿæ¢°å­¦ç¿’ã®ç†è«–ç³»ã®Chi Jinå…ˆç”Ÿãªã©
- Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation
	- https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2
	- Check out this fantastic blog covering the basics of RAG, the theory behind it, and how to use it in practice
- Mamba: Linear-Time Sequence Modeling with Selective State Spaces
	- https://arxiv.org/abs/2312.00752
	- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚„æ³¨æ„æ©Ÿæ§‹ã«é ¼ã‚‰ãªã„ã€ç·šå½¢æ™‚é–“ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã®æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
	- 2å€ã‚µã‚¤ã‚ºã®Transformersã«åŒ¹æ•µã—ãŸã‚Šã€5å€ã®é«˜é€Ÿæ¨è«–ãŒå‡ºæ¥ãŸã‚Šã¨ã€Transformerã‚’ä»£æ›¿ã—ã†ã‚‹å¯èƒ½æ€§
	- 2.8BãŒå‡ºã¦ã‚‹ã‚‰ã—ã„ã€
	- https://huggingface.co/state-spaces/mamba-2.8b
-  Instruction-tuning Aligns LLMs to the Human Brain
	- https://arxiv.org/abs/2312.00575
	- Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã®å¿œç”¨å‹•å‘ã®è«–æ–‡èª¿æŸ»
	- https://speakerdeck.com/masatoto/marutimodarullmnoying-yong-dong-xiang
- ç”Ÿæˆæ–‡æ³•ç ”ç©¶è€…ã®ä¸­ã§ã€Œè¨€èªã®æœ¬è³ªã€ï¼ˆä»Šäº•å…ˆç”Ÿï¼‰ã®è©•åˆ¤ãŒè‰¯ããªã‹ã£ãŸ
	- https://x.com/kkling51/status/1731543891348996466?s=20
	- (i) ã‚¢ãƒ–ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ¨è«–ã¯é©åˆ‡ãªæ¨è«–ã§ã¯ãªã„ã‹ã‚‰ãã‚Œã«é ¼ã‚‹ã¹ãã§ã¯ãªã„ 
	- (ii) è¨€èªã¨ã¯ä½•ã‹ã¨ã„ã†å®šç¾©ãŒãªã„ãŸã‚ï¼Œæœ¬è³ªãŒä½•ãªã®ã‹åˆ†ã‹ã‚‰ãªã„ï¼
	- ãƒ—ãƒ©ãƒˆãƒ³ã®å•é¡Œã‚‚æœªè§£æ±ºã®ãƒãƒ
- Amil Merchant et al., "Scaling deep learning for materials discovery", nature
	- https://www.nature.com/articles/s41586-023-06735-9
	- DeepMindã®ã€GNoMEã€ãŒã€Œäººé–“ã®ç›´æ„Ÿã‚’è¶…ãˆãŸ220ä¸‡ã®ææ–™ã‚’ç™ºè¦‹ã—ã€ã†ã¡736ã¯æ—¢ã«äººé–“ãŒå®Ÿé¨“å®¤ã§å†ç¾ã—ãŸã¨ã®å ±å‘Š
	- å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å…ˆé€²çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹æ‰‹æ³•ã«ã‚ˆã‚‹ã€ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹ã®ç™ºå±•äº‹ä¾‹ã§ã™
	- æ–¹æ³•
		- â‘  GNNã‚’ç”¨ã„ã¦ç´ æã®ç‰¹æ€§ã‚’æ§‹é€ ã‚„çµ„æˆã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«åŒ–
		-  â‘¡ ææ–™ç™ºè¦‹ã®åŠ¹ç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã€äººé–“ã®ç›´æ„Ÿã‚’è¶…ãˆãŸ220ä¸‡ã®æ§‹é€ ãŒç™ºè¦‹ã•ã‚ŒãŸ 
		- â‘¢ çµæ™¶æ§‹é€ å†…ã®åŸå­ã‚’ç½®æ›ã™ã‚‹æ‰‹æ³•ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãªæ¢ç´¢ã‚’å«ã‚€ã€å¤šæ§˜ãªå€™è£œç”Ÿæˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¢ºç«‹
	- çµæœ
		- â‘  220ä¸‡ã®æ–°ãŸãªå®‰å®šæ§‹é€ ã‚’ç‰¹å®šã—ã€ãã‚Œã‚‰ã®å¤šãã¯æ—¢å­˜ã®åŒ–å­¦çš„ç›´æ„Ÿã‚’è¶…ãˆã¦ã„ãŸ 
		- â‘¡ ç™ºè¦‹ã•ã‚ŒãŸå®‰å®šæ§‹é€ ã®ã†ã¡736ã¯ã€ç‹¬ç«‹ã—ãŸå®Ÿé¨“ã§å®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸Šã§ã®æ¤œè¨¼ã§ã¯ãªãã€å®Ÿé¨“å®¤ã§ç‰©ç†çš„ã«ææ–™ã‚’ä½œæˆã—ã€å®Ÿè¨¼ã§ããŸï¼‰
- NVIDIAã®H100ã‚’ã©ã“ã«å‡ºè·ã—ãŸã‹ã®å›³ã€‚MS,MetaãŒåœ§å€’çš„ã«å¤šã„ã€GPT4ã‚’7æ—¥ã§è¨“ç·´ã§ãã‚‹è¦æ¨¡ï¼Ÿ
	- https://x.com/Lauramaywendel/status/1731698695853244849?s=20
	- GPT4 was presumably trained for around 90 days using 25k A100 GPUs. Microsoft and Meta having reportedly bought 150k H100 GPUs each this year, can now train a GPT4 class model in only 7 days from scratch
- Google Geminiã®æä¾›ã‚’ï¼‘æœˆã¾ã§å»¶æœŸ
	- https://x.com/rowancheung/status/1731531903193219260?s=20
	- ã„ãã¤ã‹ã®åˆ†é‡ã§ã¯GPT-4ã‚’ä¸Šå›ã‚‹ã‚‚ã€è‹±èªä»¥å¤–ã§ã®æ€§èƒ½ãŒå‡ºãªã„ã€‚
	- ã“ã‚Œã£ã¦ã€å¾Œã‹ã‚‰ç¶šãã‚¤ãƒ™ãƒ³ãƒˆã®äºˆå…†ã‹ã—ã‚‰ã‚“ã€
- ã‚ã‚‹ç‰©ç†å­¦ã®æœ¬ã§ã€ã‚®ãƒªã‚·ãƒ£èªã®èª¬æ˜è¡¨ã§ã‚¼ãƒ¼ã‚¿ã®ã¨ã“ã‚ãŒã€ã€
	- https://x.com/yori_Alphard/status/1731663363737026586?s=20
	- "Zã‚¬ãƒ³ãƒ€ãƒ "ã«ãªã£ã¦ã„ã‚‹ã€‚ã€‚
- GIVT: Generative Infinite-Vocabulary Transformers
	- https://huggingface.co/papers/2312.02116
	- æœ¬å½“ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒé›¢æ•£ã§ãªãã¦ã€ç„¡é™ãªã®ã ã‚ã†ã‹ï¼Ÿ
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ä¸è¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã ã‘ã§ã©ã†ã«ã‹ãªã‚‹ï¼Ÿ
	- https://x.com/IntuitMachine/status/1732089266883141856?s=20
	- A recent research paper provides compelling evidence that the extensive fine-tuning used to "align" large language models into helpful assistants may be largely unnecessary.
	- Allenã‚¤ãƒ³ã‚¹ãƒ†ã‚£ãƒ†ãƒ¥ãƒ¼ãƒˆã®ä»•æ¥­ã‹ã€https://allenai.org/
- llamaindexã§ã‚‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã€Webinerãªã©
	- https://x.com/llama_index/status/1732081850246627547?s=20
	- https://lu.ma/350wf7v7
- å®‰å…¨ã§è²¬ä»»ã‚ã‚‹AIã®é–‹ç™ºå‘ã‘ã¦ã€Metaã¨IBMãŒææº
	- https://ai.meta.com/blog/ai-alliance/
	- IBM ã¨ãƒ¡ã‚¿ã¯ã€*ã‚ªãƒ¼ãƒ—ãƒ³*ã§ä¿¡é ¼æ€§ã®é«˜ã„ AI ã‚’æ¨é€²ã™ã‚‹ãŸã‚ã« AI Alliance ã‚’ç«‹ã¡ä¸Šã’ã¦ã„ã¾ã™ã€‚ ç”£æ¥­ç•Œã€æ”¿åºœæ©Ÿé–¢ã€å­¦ç•Œã‹ã‚‰ã® 50 ã‚’è¶…ãˆã‚‹è¨­ç«‹ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒªã‚¹ãƒˆã«ã¯ã€AMDã€Anyscaleã€CERNã€Hugging Faceã€Linux Foundationã€NASA ãŒå«ã¾ã‚Œã¾ã™ã€‚
	- æ—¥çµŒã«ã‹ã‹ã‚‹ã¨ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€ã€Œãƒ¡ã‚¿ã¨IBMã€ç”ŸæˆAIã€Œã‚ªãƒ¼ãƒ—ãƒ³å‹ã€ã¸ã€€50ç¤¾ãƒ»å›£ä½“ã¨é€£æºã€
- Prompting vs RAGs vs Fine-tuning:
	- https://x.com/akshay_pachaar/status/1732014719794585684?s=20
	- ã‚ˆãã‚ã‚‹ï¼”è±¡é™ã®çµµã€
	- So finetuning is more about changing structure (behaviour) than knowledge, while it's other way round for RAGs.
	- You use RAGs when you want to generate outputs grounded to a custom knowledge base while the vocabulary & writing style of the LLM remains same.
	- If you don't need either of them, prompt engineering is the way to go.
	- And if your application need both custom knowledge & change in the behaviour of model a hybrid (RAGs + Finetuning) is preferred.
- OpenAIã®Safety System Teamsã‹ã‚‰
	- https://openai.com/safety/safety-systems
	- å”åŠ›ã®ãŠé¡˜ã„
- PyTorchãŒå‡ºã—ãŸã€gpt-fastã¯ã™ã”ã„ã‚‰ã—ã„
	- https://x.com/AlphaSignalAI/status/1732116360162050099?s=20
	- Pytorch just released GPT-Fast, an implementation of transformer text generation with everything you need in <1000 lines of code.
	- https://github.com/pytorch-labs/gpt-fast
- Windows11ã«copilotãŒé™è‡¨ï¼Ÿ
	- https://www.microsoft.com/en-us/windows/copilot-ai-features?r=1
- JWT(Json Web Token)
	- https://x.com/alexxubyte/status/1732077250626179578?s=20
- Jellyfish: A Large Language Model for Data Preprocessing
	- https://arxiv.org/abs/2312.01678
	- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å¾—æ„ã¨ã™ã‚‹LLMã€Jellyfishï¼ˆã‚¯ãƒ©ã‚²ï¼‰ã€ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚ æœªçŸ¥ã®ã‚¿ã‚¹ã‚¯ã«ã‚‚å¯¾å¿œã§ãã€æ¯”è¼ƒçš„è»½é‡ã§ã‚ã‚Š1GPUã§ã‚‚å‹•ä½œã™ã‚‹ã¨ã®ã“ã¨ã§ã™ã€‚ 
	- å¤§é˜ªå¤§å­¦ã€NECã€åå¤å±‹å¤§å­¦ã®ç ”ç©¶è€…ã‚‰ã«ã‚ˆã‚‹ç™ºè¡¨ã§ã™
	- â‘  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ãŒé€²åŒ– ï¼ˆGPT-4ã¨åŒç­‰ã®æ€§èƒ½ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’è¡Œã†ï¼‰ 
	- â‘¡ ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ 
	- â‘¢ å¤šæ§˜ãªå‰å‡¦ç†ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œ 
	- â‘£ ã‚µã‚¤ã‚ºãŒå°ã•ã„ãŸã‚ã€1GPUã§ã‚‚å‹•ä½œã™ã‚‹
- GooglãŒGemini(ã‚¸ã‚§ãƒãƒŠã‚¤ã¨èª­ã‚€ï¼‰ã‚’ç™ºè¡¨
	- https://blog.google/technology/ai/google-gemini-ai/
	- 1. Geminiã¯3ç¨®é¡ã®ãƒ¢ãƒ‡ãƒ«(Ultra, Pro, Nano)ãŒå­˜åœ¨ã€‚UltraãŒæœ€ã‚‚è³¢ãã€Nanoã¯ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã€‚
	- 2. Ultraã¯æ•°ã€…ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§GPT-4è¶…ãˆã®æ€§èƒ½ã‚’ç™ºæ® (ï¾„ï¾ï¾”ï½§)
	- 3. Geminiã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«å¼·ã„ã€‚å‹•ç”»ãƒ‡ãƒ¢ã®ã‚ˆã†ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚‚å¯èƒ½ã€‚ 
	- 4. æœ¬æ—¥ã‚ˆã‚ŠBardã¯Gemini Proã®Fine-tuningãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆ©ç”¨ã—ã¦å…¬é–‹ã™ã‚‹ã€‚ãã®ä»–ã«ã‚‚Googleè£½å“ã¸ã®å°å…¥ã‚’é€²ã‚ã‚‹ã€‚ 
	- 5. Gemini APIã¯12æœˆ13æ—¥ã‹ã‚‰Google AI Studioã‚’é€šã˜ã¦æä¾›ã•ã‚Œã‚‹ã€‚
	- https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
- Google AlphaCode 2 ã‚’ç™ºè¡¨
	- AlphaCode 2 Technical Report
	- https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf
	- Geminiã‚’ç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸAlphaCode2ã¯ã€ç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°äººå£ã®ä¸Šä½15%ã®æ€§èƒ½
- Metaã®Streamingã®ç¿»è¨³æ€§èƒ½ã¯ã™ã”ã„ã‚‰ã—ã„ã€	
	- https://x.com/hokazuya/status/1732374854027132940?s=20
	- ç¿»è¨³ã“ã‚“ã«ã‚ƒããƒ¬ãƒ™ãƒ«
- Bardã®ç”Ÿæˆè¨˜äº‹ã¯ChatGPTã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ï¼Ÿ
	- https://x.com/kajikent/status/1732237182126129578?s=20
	- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®é ˜åŸŸã§æœ‰åãªNeil Patelæ°ãŒç´„250ãšã¤ã®ChatGPTç”Ÿæˆã®è¨˜äº‹ã¨Google Bardç”Ÿæˆã®è¨˜äº‹ã§èª­è€…ã«ã©ã¡ã‚‰ãŒå¥½ãã‹èã„ãŸã¨ã“ã‚ã€BardãŒåœ§å‹ã™ã‚‹çµæœã«
- äººé–“ãƒ¬ãƒ™ãƒ«ã®AI(AGI)ã«åˆ°é”ã™ã™ã‚‹ã«ã¯ã€å¸¸ã«10å¹´ä»¥ä¸Šå¿…è¦
	- https://x.com/ylecun/status/1732391273611370931?s=20
	- 3ï½5å¹´ã¯å¸¸ã«å¿…è¦ï¼ˆæ°¸é ã«é”æˆã§ããªã„ï¼‰ã¨ã®è¨˜äº‹ã«Lecanå…ˆç”Ÿã®åå¿œ
- Appleè£½å“Mã‚·ãƒªãƒ¼ã‚ºã«æœ€é©åŒ–ã•ã‚ŒãŸæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯mlx
	- https://x.com/goto_yuta_/status/1732287555599741103?s=20
	-  Macã«æ­è¼‰ã•ã‚Œã¦ã‚‹GPU(MPS)ãŒã‚ˆã‚Šæœ‰åŠ¹æ´»ç”¨ã•ã‚Œã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã®é«˜é€Ÿæ¨è«–ãŒå¯èƒ½ã«ãªã£ãŸã‚‰å¬‰ã—ã„ãªã€‚
	- CNBCã®ã€Apple Labã¸ã®æ½œå…¥ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
	- https://www.youtube.com/watch?v=UdhWvg5mycY
- Geminiã®Technical reportã‚’æ—¥æœ¬èªã§è§£èª¬ã—ã¦ã„ã‚‹äººãŒç™»å ´
	- https://x.com/bioshok3/status/1732421662619140551?s=20
	- Gemini Ultraã¯ã€MMLU ã§äººé–“ã®å°‚é–€å®¶ã®æ€§èƒ½ã‚’é”æˆã—ãŸæœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã‚¹ã‚³ã‚¢ã¯90%ä»¥ä¸Šã€‚ã‚„ã°ã™ãã‚‹ã€‚äººé–“ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è‘—è€…ã«ã‚ˆã£ã¦89.8%ã¨è©•ä¾¡ã•ã‚Œã€Gemini Ul traã¯ã“ã®é–¾å€¤ã‚’è¶…ãˆãŸæœ€åˆã®ãƒ¢ãƒ‡ãƒ«!æ™‚ä»£ãŒå¤‰ã‚ã£ãŸã€‚
	- æ•™å¸«ãŒã‚¹ã‚­ãƒ¼ãƒ¤ãƒ¼ãŒå‚é“ã‚’ä¸‹ã‚Šã‚‹ã¨ã„ã†ç‰©ç†å•é¡Œã‚’æãã€ç”Ÿå¾’ãŒãã®è§£æ±ºç­–ã‚’ç·´ã‚‹ã€‚Geminiã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–æ©Ÿèƒ½ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¯ ä¹±é›‘ãªæ‰‹æ›¸ãã‚’ç†è§£ã—ã€ç”Ÿå¾’ãŒå•é¡Œã®è§£æ±ºã‚’é–“é•ãˆãŸæ¨è«–ã®ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã—ã€å•é¡Œã®æ­£ã—ã„è§£æ±ºã‚’é€šã— ã¦ä½œæ¥­ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
	- Google ãŒGeminiã®ãƒ‡ãƒ¢å‹•ç”»ã‚’å‡ºã—ã¦ã„ã‚‹ã‘ã©ã€ã“ã‚Œã»ã‚“ã¨ã«ã“ã®æ¨è«–é€Ÿåº¦ãªã‚‰å‡„ã™ãã‚‹ã¨è¨€ã†ã‹ã‚‚ã†æ ªä¾¡æ•°å€ãã‚‰ã„ã«ãªã‚‹ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿã£ã¦ãƒ¬ãƒ™ãƒ«ã ã‘ã©ï¼Ÿï¼Ÿ
	- ãƒ‡ãƒ¢ã«ã¤ã„ã¦ã¯ã€Œã“ã®ãƒ‡ãƒ¢ã®ç›®çš„ã®ãŸã‚ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¯çŸ­ç¸®ã•ã‚Œã€ã‚¸ã‚§ãƒŸãƒ‹ã®å‡ºåŠ›ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ã€ã¨æ›¸ã‹ã‚Œã¦ã‚‹
	- å¤šè¨€èªæ€§èƒ½ã¯GPT-4ã‚ˆã‚Šè‰¯ã„
	- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯32768ã€‚98%ã®ç²¾åº¦ã§æ­£ã—ã„å€¤ã‚’å–å¾—å¯èƒ½ï¼98%?ã¾ã˜ã‹ã‚ˆã€‚
- Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨€èªè¨­å®šã‚’è‹±èªã«ã™ã‚‹ã¨ã€Bardã®ãƒãƒƒã‚¯ãŒGemimi ProãŒä½¿ãˆã‚‹
	- https://x.com/npaka123/status/1732504570218283340?s=20
- Bard(Gemini Pro)ãŒéœãŒé–¢ãƒ‘ãƒ¯ãƒã‚’è§£æã—ã¦èª¬æ˜ã—ã¦ãã‚Œã‚‹ã¨ã€ã€	by ã‚†ãªå…ˆç”Ÿ
	- https://x.com/JapanTank/status/1732689643928445164?s=20
- Geminiè«–æ–‡ã®æœ€å¾Œã®ã€"Core Contributors"ã®æœ€åˆã®ï¼–äººã®é ­æ–‡å­—ã‚’ã¨ã‚‹ã¨ã€"GEMINI"ã«ãªã‚‹
	- https://x.com/nearcyan/status/1732532560029172142?s=20
- Metaã‚ˆã‚Šã€å®‰å…¨ãªAIã®ãŸã‚ã®ã€Purple Llamaï¼ˆãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¿ãŸã„ãªã‚‚ã®ï¼‰ã‚’ç™ºè¡¨
	- https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/?utm_source=twitter&utm_medium=organic_social&utm_campaign=llama&utm_content=image
	- CyberSec Evalã¨ã‹ã€Llama GuardãŒæœ€åˆã«å‡ºã‚‹
	- ãªã‚“ã§purpleã‹ã¨ã„ã†ã¨æ”»æ’ƒå´ï¼ˆèµ¤ï¼‰ã¨ã€é˜²å¾¡å´ï¼ˆé’ï¼‰ãŒå”åŠ›ã—ã¦æ§‹ç¯‰ã—ãŸã‹ã‚‰
	- attack (red team) and defensive (blue team) postures.
	- Colabã§è©¦ã›ã‚‹ã‚‰ã—ã„
	- https://colab.research.google.com/drive/16s0tlCSEDtczjPzdIK3jq0Le5LlnSYGf?usp=sharing
- Evaluating and Mitigating Discrimination in Language Model Decisions
	- https://www.anthropic.com/index/evaluating-and-mitigating-discrimination-in-language-model-decisions
	- Anthropicã‚ˆã‚Šã€ï¼ˆLLMã®å‡ºåŠ›ã«ãŠã‘ã‚‹ï¼‰å·®åˆ¥ã‚’æ¤œçŸ¥ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹
-  AMDã€ç”ŸæˆAIã§NVIDIA H100ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã®GPUã€ŒInstinct MI300ã€
	- https://pc.watch.impress.co.jp/docs/news/1552583.html
	- TDP 750Wã®MI300Xã¯ã€TDP 700Wã®NVIDIA H100ã¨æ¯”è¼ƒã—ã€FP64,32ã§ç´„2.4å€ã€AIã§åˆ©ç”¨ã®TF32ã€FP16ã€BF16ã€FP8ã€INT8ãªã©ã§ã¯1.3å€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®Ÿç¾ã€‚
- èµ¤çŸ³å…ˆç”Ÿã®ãƒ™ã‚¤ã‚ºæ¨è«–æœ¬ãŒã‚ã‹ã‚Šã‚„ã™ã„ã¨è©•åˆ¤ã«
	- https://x.com/kenken26679105/status/1732977179485757744?s=20
	- å°‘ãªã„ãƒ‡ãƒ¼ã‚¿é‡ã§ã‚‚ã€ã“ã‚“ãªé¢¨ã«ã€è‰²ã‚“ãªå®Ÿå‹™ã®å ´é¢ã«ã™ãã«æ´»ç”¨ã§ãã¡ã‚ƒã†
	- Pythonã§ã‚¹ãƒ©ã‚¹ãƒ©ã‚ã‹ã‚‹ ãƒ™ã‚¤ã‚ºæ¨è«–ã€Œè¶…ã€å…¥é–€ (KSæƒ…å ±ç§‘å­¦å°‚é–€æ›¸)
- ãƒãƒ§ãƒ ã‚¹ã‚­ãƒ¼ã®ã€Œç”Ÿæˆæ–‡æ³•ã€ã¯æ­»ã‚“ã ã¨ã„ã†è«–æ–‡
	- Modern language models refute Chomskyâ€™s approach to language
	- https://lingbuzz.net/lingbuzz/007180/v1.pdf
	- æœ€è¿‘ã®ç”ŸæˆAIã¦ã†ã‹å¤§è¨€èªãƒ¢ãƒ‡ãƒ«LLMã®é©šãã¹ãæˆåŠŸã‹ã‚‰è¦‹ã¦ã€ãƒãƒ§ãƒ ã‚¹ã‚­ãƒ¼æµã®ç”Ÿå¾—çš„çµ±èªæ³•è¦å‰‡ãŒã‚ã‚‹ã¨ã„ã†èª¬ã¯ç¶­æŒã—ã¥ã‚‰ã„
- llamaindexã‚ˆã‚Šã€çŸ¥è­˜ã‚°ãƒ©ãƒ•(KG)ã‚’ä½¿ã†ã€ï¼—ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ã«ã¾ã¨ã‚ã¦ãã‚ŒãŸ
	- https://x.com/llama_index/status/1733190430760845673?s=20
	-  A Simpler Way to Query Neo4j Knowledge Graphs
	- https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/neo4j_query_engine/llama_packs_neo4j.ipynb
- æ¬§å·AIæ³•ã®æœ€çµ‚ãƒˆãƒªãƒ­ãƒ¼ã‚°ãŒçµ‚äº†ã€å¦¥çµã¸
	- https://x.com/WIRED/status/1733268732309332398?s=20
	- https://www.reuters.com/technology/eu-clinches-deal-landmark-ai-act-2023-12-09/?taid=65745dd360152800018aaf1c&utm_campaign=trueAnthem:+Trending+Content&utm_medium=trueAnthem&utm_source=twitter
	- https://twitter.com/SabrinaKuespert/status/1733311752941515135/photo/1
	- https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai?xtor=AD-78-[Social_share_buttons]-[twitter]-[en]-[news]-[pressroom]-[artificial-intelligence-act-possible-deal]-
	- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§è¦åˆ¶ã•ã‚Œã‚‹ã®ã¯ã€è¨ˆç®—é‡ãŒ10^25FLOPsã‚’è¶…ãˆã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
	- è©²å½“ã™ã‚‹ã®ã¯ä»Šã‚“ã¨ã“GPT-4ã¨Geminiã‚ãŸã‚Šã€‚
	- ãã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒªã‚¹ã‚¯ã«å¿œã˜ã¦åˆ†é¡ã•ã‚Œã‚‹ã€‚
	- ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒªã‚¹ã‚¯ã¯ãƒ¢ãƒ‡ãƒ«ãŒã©ã‚“ã ã‘å¼·åŠ›ã‹ã€ã©ã‚“ã ã‘ã®äººãŒä½¿ã†ã‹ã§æ±ºã¾ã‚‹ã€‚
	- è¦åˆ¶ã®å†…å®¹ã¯
		- â‘ ãƒªã‚¹ã‚¯ã®è»½æ¸›ã‚’è¡Œã†ã€€
		- â‘¡ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã€æ•µå¯¾çš„ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã™ã‚‹ã€€
		- â‘¢ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ç›£è¦–ã‚’ã™ã‚‹ã€€
		- â‘£ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’ç¢ºä¿ã•ã›ã‚‹ã€€
		- â‘¤ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œã‚‰ã›ã‚‹
-  Generative AI for Everyoneã‹ã‚‰ã€å¤ã®NLPã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å¿ƒã«åˆºã•ã£ãŸã“ã¨8é¸
	- https://note.com/csstudyabroad/n/n5aba3a708f3a
- "Purple Llama CyberSecEval: A benchmark for evaluating the cybersecurity risks of large language models"
	- LLama Purpleé–¢é€£ã® CyberSecEvalã®è«–æ–‡
	- https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/
	- Metaã®ç ”ç©¶è€…ã‚‰ã¯ã€LLMãŒç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã«ãŠã‘ã‚‹ä¸å®‰å®šæ€§ã‚„ä¹±ç”¨ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
	-  å®Ÿé¨“ã®çµæœã€ç¾åœ¨ã¯ã€èƒ½åŠ›ãŒé«˜ã„ãƒ¢ãƒ‡ãƒ«ã»ã©ä¸å®‰å…¨ãªã‚³ãƒ¼ãƒ‰ã‚’ææ¡ˆã™ã‚‹å‚¾å‘ãŒå¼·ã„ã¨ã„ã†é€†èª¬çš„ãªçµæœã‚‚å‡ºã¦ãã¾ã—ãŸã€‚
	- â‘  å…¨ä½“çš„ã«LLMã¯ã€30%ã®ã‚±ãƒ¼ã‚¹ã§ä¸å®‰å…¨ãªã‚³ãƒ¼ãƒ‰ã‚’ææ¡ˆã—ãŸ 
	- â‘¡ 53%ã®ã‚±ãƒ¼ã‚¹ã§ã€ã‚µã‚¤ãƒãƒ¼æ”»æ’ƒã®æ‰‹ä¼ã„ã‚’ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¯¾ã—ã¦LLMãŒå¿œã˜ãŸ
	-  â‘¢ ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°èƒ½åŠ›ãŒé«˜ã„ãƒ¢ãƒ‡ãƒ«ã»ã©ã€ä¸å®‰å…¨ãªã‚³ãƒ¼ãƒ‰ã‚’ææ¡ˆã™ã‚‹å‚¾å‘ãŒå¼·ã‹ã£ãŸ
- "Sequential Modeling Enables Scalable Learning for Large Vision Models"
	- https://arxiv.org/abs/2312.00785
	- ã€Œè¦–è¦šã¯æœ¬æ¥ã€è¨€èªã«ä¾å­˜ã—ãªã„ã€ã¨è€ƒãˆãŸUCãƒãƒ¼ã‚¯ãƒ¬ãƒ¼ã¨ã‚¸ãƒ§ãƒ³ã‚¹ãƒ›ãƒ—ã‚­ãƒ³ã‚¹å¤§å­¦ã®ç ”ç©¶è€…ã‚‰ã¯ã€è¨€èªãƒ‡ãƒ¼ã‚¿ãªã—ã§å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆLVMï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
	- â– ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è©³ç´° 
		- â‘  ç”»åƒã‚„å‹•ç”»ã‚’è¡¨ç¾ã™ã‚‹ã€Œãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡ã€ã‚’å®šç¾© ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ä»¥å¤–ã®ãƒ¡ã‚¿æƒ…å ±ã¯ãªã„ï¼‰ 
		- â‘¡ è¦–è¦šãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ– 
		- â‘¢ è‡ªå·±å›å¸°å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
	- â– å®Ÿé¨“ã®çµæœã‚ã‹ã£ãŸã“ã¨ 
		- â‘  ãƒ¢ãƒ‡ãƒ«ã¯å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—å­¦ç¿’ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„
		-  â‘¡ æ§˜ã€…ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§æœ‰åŠ¹ 
		- â‘¢ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã™ã‚‹
-  Large Language Models on Graphs: A Comprehensive Survey
	- https://arxiv.org/abs/2312.02783
	- Scenarios of adopting LLMs, techniques for utilizing LLMs on graphs, applications, #opensource code repositories, benchmark datasets
- ã€Œ2030 æ—¥æœ¬ãƒ‡ã‚¸ã‚¿ãƒ« æ”¹é©ã€ by ãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼
	- https://www.digitaljapan2030.com/_files/ugd/c01657_fcaed21f58bb4c429cb460ce788b82c4.pdf
	- ãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼ã®ãƒ¬ãƒãƒ¼ãƒˆï¼ˆå…¨140ãƒšãƒ¼ã‚¸ï¼‰
	- æ—¥æœ¬ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ãŒãªãœé…ã‚ŒãŸã®ã‹ã€ãã‚Œã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ãªæ‰“ã¡æ‰‹ãŒå–ã‚Œã‚‹ã®ã‹ã€ã¨ã„ã†ã“ã¨ãŒåˆ†ã‹ã‚Šã‚„ã™ãæ•´ç†ã•ã‚Œã¦ã„ã¾ã™ã€‚ 
	- æ—¥æœ¬ã®ç·åŠ´åƒæ™‚é–“ã®56%ãŒè‡ªå‹•åŒ–å¯èƒ½
	- ã¨ã„ã£ã¦ã‚‚åˆæœŸç‰ˆã«ã¯èª¤æ¤ãŒã€(Ã—æ”¿åºœã®æ”¯æŒâ†’ã€‡æ”¿åºœã®æŒ‡ç¤ºï¼‰P16
- ollama + stablelm-zephyr è©¦ã™ã€‚ M1ã§ã‚‚ã¯ã‚„ã„ã€‚
	- https://ollama.ai/library/stablelm-zephyr
- Ollama : ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®¹æ˜“ã«llamaã‚’åˆ©ç”¨å¯èƒ½ã«ã‚‹ã™ã‚‹AIãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ 
	- https://note.com/astropomeai/n/nbcdfd3b38490?sub_rt=share_b
	- https://github.com/jmorganca/ollama
	- ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é€šã˜ã¦å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨ã‚„ã‚Šå–ã‚Šå¯èƒ½ãªAIãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ 
	- Llamaã‚„Code Llamaãªã©ã€ã•ã¾ã–ã¾ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
	- ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚„ã‚µã‚¤ã‚ºãŒç•°ãªã‚Šã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸAIãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚’æŸ”è»Ÿã«å¯¾å¿œ
	- DockerãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã§åˆ©ç”¨å¯èƒ½ã§ã€Nvidia GPUã®GPUã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆCPUä¸Šã§ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
	- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ä¾å­˜ã—ã€ä¾‹ãˆã°Llama 2ã®7Bãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯æœ€ä½15GBã®RAMã¨4ã¤ã®CPUã‚³ã‚¢ãŒå¿…è¦
	- MacOSã¨Linuxç”¨ã®ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã€Windowsç‰ˆãŒé–‹ç™ºä¸­
-  Ollama Llama Pack Example
	- https://docs.llamaindex.ai/en/latest/examples/llama_hub/llama_pack_ollama.html#
	- llamaindexã‚ˆã‚Šã€ã•ã£ããOllamaå¯¾å¿œã®RAGã®ä¾‹
	- https://llamahub.ai/l/llama_packs-ollama_query_engine
- ollama web-ui is amazing
	- https://github.com/ollama-webui/ollama-webui
- ClimateXã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹
	- https://huggingface.co/datasets/rlacombe/ClimateX
- Mistralã‚ˆã‚Šã€æ–°ã—ã„ mixtral-8x7b-32kseqlenã‚’ç™ºè¡¨
	- https://replicate.com/nateraw/mixtral-8x7b-32kseqlen
	- ã€Œæˆ‘ã€…ã¯Mistral MoE (7Bx32experts) ã‚’ 2 ã‹æœˆé–“ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãã‚Œã¯24GBã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚ã€
- What is Mixture-of-Experts (MoE)?
	- mixtral-8x7b-32kseqlenã®è£ã«ã‚ã‚‹moeæŠ€è¡“ã¨ã¯
	- https://x.com/sophiamyang/status/1733505991600148892?s=20
	- MoE is a neural network architecture design that integrates layers of experts/models within the Transformer block.
- ãŸã£ãŸ87Gã®weightã§AGIãŒæ¥ã‚‹ã‹ã‚‰ã€AIè¦åˆ¶å¿…è¦ã ã­ã¨ã„ã†
	- https://x.com/abacaj/status/1733561182504587652?s=20
	- mixtral-8x7b-32kseqlenã®ã“ã¨ã‚‰ã—ã„
- MoEã®Mixtral-7bx8ã®GPTQãã¨ã‚‹ï¼
	- https://huggingface.co/TheBloke/mixtral-7B-8expert-GPTQ
- Geminiã®ãŠéŠã³ãƒ‡ãƒ¢ã¯ã€ç´™èŠå±…ã 
	- https://techcrunch.com/2023/12/07/googles-best-gemini-demo-was-faked/
- QuIP#: QuIP with Lattice Codebooks
	- https://cornell-relaxml.github.io/quip-sharp/
	- QuIP#ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’2ãƒ“ãƒƒãƒˆé‡å­åŒ–ã—ã€æœ¬æ¥ãªã‚‰ã°140GBã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ãªLlama 2 70Bã‚’24GBã®GPUã§å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹ã¨ã®äº‹ã§ã™
- Bard(/w Gemini Pro)ã¯ã„ã¾ã ã«æ•°ç‹¬ãŒè§£ã‘ãªã„ã€ChatGPTã¯ã¨ã‘ã‚‹ã‘ã©
	- https://x.com/kajikent/status/1733663171578335233?s=20
- OpenAIã€GPT-4ãŒæ€ ã‘è€…ã«ãªã£ã¦ããŸã¨ã„ã†è‹¦æƒ…ã«ã€Œä¿®æ­£ã‚’æ¤œè¨ä¸­ã€ã¨ãƒã‚¹ãƒˆ
	- https://www.itmedia.co.jp/news/articles/2312/10/news059.html
	- ChatGPTã§ã®GPT-4ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã—ã¦ã„ã‚‹ï¼ˆlazierï¼‰ã¨ã„ã†ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã“ã“æ•°ã‚«æœˆå¢—ãˆã¦ã„ã‚‹ã“ã¨ã‚’èªã‚ã€ã€Œä¿®æ­£ã‚’æ¤œè¨ä¸­ã€ã ã¨Xï¼ˆæ—§Twitterï¼‰ã®å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒã‚¹ãƒˆã—ãŸã€‚
- Mistral MoEã®åˆæœŸè©•ä¾¡
	- https://x.com/bindureddy/status/1733523486885449834?s=20
	- ã¾ã‚ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ãªã„ç´ ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚GPT3.5ç›¸å½“ã®æ€§èƒ½ã¨ã„ã†ã®ã¯æœŸå¾…ã§ãã‚‹
	- solid 70B model that is very similar to GPT 3.5, Gemini Pro
	- MMLU on the base models is at 0.717 compared to Gemin Pro's 0.718
	- Expect to see several fine and instruct tunes over the next few weeks. These fine tunes will match GPT-4 quality for several real-world use cases.
-  Google Colab ã§ DiscoLM Mixtral 8x7b alpha ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n3b55c941d864?sub_rt=share_h
	- ã€Œ**Mixtral 8x7b**ã€ã¯ã€ã€ŒMistral AIã€ãŒãƒªãƒªãƒ¼ã‚¹ã—ãŸå²ä¸Šåˆã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ MoEãƒ¢ãƒ‡ãƒ«ã§ã™
	- ã€Œ**DiscoLM Mixtral 8x7b alpha**ã€ã¯ã€ã€ŒMixtral 8x7bã€ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ä½œæˆã—ãŸå®Ÿé¨“çš„ãªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’HuggingFaceå½¢å¼ã«å¤‰æ›ã—ã€ã€ŒSynthiaã€ã€ŒMethaMathQAã€ã€ŒCapybaraã€ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚
	- ã€Œ**MoE**ã€ (Mixture of Experts) ã¨ã¯ã€LLMã®åŠ¹ç‡ã¨ç²¾åº¦ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹æ‰‹æ³•ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ã‚ˆã‚Šå°ã•ãç®¡ç†ã—ã‚„ã™ã„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²ã—ã€ãã‚Œãã‚Œã‚’ç‰¹åŒ–ã—ãŸãƒŸãƒ‹ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯å°‚é–€å®¶ãŒå‡¦ç†ã™ã‚‹ã“ã¨ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚
	- 

## 12/4

å…ˆé€±ã¾ã§ã®OpenAIã®ãŠå®¶é¨’å‹•ã‚‚è½ã¡ç€ãã€ä»Šé€±ã¯é€šå¸¸é‹è»¢ã€‚æ—¥å¸¸èƒ½åŠ›ã‚’è©¦ã™ãƒ†ã‚¹ãƒˆã€GAIAã€ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è‰¯ä¾‹ã«ã‚‚ãªã£ã¦ã„ã‚‹ã—ã€ç¾çŠ¶ã®LLMã®é™ç•Œã‚’å›³ã‚‹ã®ã«ã¡ã‚‡ã†ã©ã‚ˆã„ã€‚A*ã®å¯è¦–åŒ–ã€ã“ã†ã„ã†ã®ã‚’å¾…ã£ã¦ãŸã€‚ç•°ãªã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã®ç¹‹ãŒã‚Šã‚„ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆã‚’ä¿ƒã™ã‚·ã‚¹ãƒ†ãƒ ã€Latent Labã€ã¨ã„ã†ã®ã¯ã€ãƒ•ãƒªãƒ¼ã‚¢ãƒ‰ãƒ¬ã‚¹ã®åŸ·å‹™ç’°å¢ƒã®ç ”ç©¶æ´»å‹•ã®æ´»æ€§åŒ–ã«ãƒ’ãƒ³ãƒˆãŒã‚ã‚‹ã‹ã‚‚ã€‚é¸æŠãƒã‚¤ã‚¢ã‚¹å•é¡ŒãŒãªãœã‹ç€ç›®ã•ã‚Œã‚‹ã€‚æ¸…æ°´ã•ã‚“ã€ã¤ã„ã«ã€A100 80GBx8ã®ãƒã‚·ãƒ³ãŒå®Œæˆã€æ—¥æœ¬èªã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚ãã‚ãˆã¦ãã‚Œã¦ã€æ—¥æœ¬ç™ºã®ãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹LLMé–‹ç™ºã«å¤§ã„ãªã‚‹æœŸå¾…ã€‚IntelÂ® ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒæ‹¡å¼µã€é‡å­åŒ–ã®æ–°ãŸãªã‚‹æ®µéšï¼ŸGoogleã‹ã‚‰debateã‚’åŸºã«ã—ãŸå®‰å…¨ãªLLMåˆ©ç”¨ã«ã¤ã„ã¦ã®ç†è«–è«–æ–‡å…¬é–‹ã€‚ã‚«ãƒ¼ãƒãƒãƒ³æ•™æˆã¨ãƒ«ã‚«ãƒ³å…ˆç”Ÿã®å¯¾è©±ã‚‚å¿…è´ã€system1ã¨system2ã¨æ·±å±¤å­¦ç¿’ã®é–¢ä¿‚ã¯ã€ã‚ã‚‹ã‚ˆãªã€‚BERTopicã‚„ã€AlphaFoldã€googleã®ç¿»è¨³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚‚ç€å®Ÿã«æ”¹è‰¯ãŒé€²ã‚“ã§å®Ÿç”¨ãƒ•ã‚§ãƒ¼ã‚ºã«ã¾ãŸä¸€æ­©é€²ã‚“ã ã€‚Google Colabã«ã¤ã„ã«transformerãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ã€ã¤ã¾ã‚Šãã†ã„ã†ã“ã¨ã ã€‚å¼·åŒ–å­¦ç¿’ç³»ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå¯¾è±¡ã«ã¯ä¸é©åˆ‡ãªã®ã‹ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€†æ¨è«–ã—ãŸã‚Šã€ã‚µãƒ­ã‚²ãƒ¼ãƒˆï¼ˆä»£ç†ï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹é€†å•é¡Œã®ç ”ç©¶ã‚‚æ³¨ç›®ã€‚llamapackã£ã¦ã®ãŒã§ãã¦ã„ã‚‹ã®ã‹ã€è©¦ã—ã¦ã¿ã‚ˆã†ã€‚Agentã‚’ã‚ˆãä½¿ã£ã¦ã‚‹ã‘ã©ã‚‚ã£ã¨ç¨®é¡ãŒã‚ã‚‹ã€èªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã£ã¦ã®ã¯ã¡ã‚ƒã‚“ã¨ç†è§£ã—ãŸã„ã€‚ã€Œå’Œæ­Œé›†ã®æ­Œé¢¨ã®è¨€èªçš„å·®ç•°ã®è¨˜è¿°ãƒ¼å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†æãƒ¼ã€ã¨ã„ã†ã®ã¯ç¶šç·¨ã‚’æœ›ã‚€ã€‚LLMã‚’Pytorchã ã‘ã§ã©ã‚Œã ã‘é«˜é€ŸåŒ–ã§ãã‚‹ã‹ã¨ã‹ã€GPT-fastã¨ã‹ã€å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨ã‹ã€ãã†ã„ã†ã®ãŒã‚‚ã£ã¨å‡ºã¦ãã‚‹ã¯ãšã€‚OSSã®LLMã«ã¤ã„ã¦ã®è«–æ–‡ã€ŒChatGPTã®1å‘¨å¹´ã‚’è¨˜å¿µã—ã¦ã€ã‚‚ã„ã„ã­ã€OSSã®LLMãŒç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„å¿œç”¨åˆ†é‡ã«ãŠã„ã¦ã€ã‚¯ãƒ­ãƒ¼ã‚ºãªLLMã«åŒ¹æ•µã™ã‚‹ã€ã‚ã‚‹ã„ã¯ãã‚Œã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã¨ãªã€‚ã‚¢ãƒ¡ãƒªã‚«ã®åŒ»å­¦è©¦é¨“ã€ŒUS (4-option)ã€ã§90.2ï¼…ã¨ã„ã†é«˜ã„æ­£è§£ç‡ã‚’ã ã—ãŸGPT-4è©•ä¾¡è«–æ–‡ã€ä¸‹æ‰‹ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ˆã‚Šã‚‚ã¨ã„ã†è©±ã‹ã€‚


-  An Embodied Generalist Agent in 3D World
	- https://huggingface.co/papers/2311.12871
- GAIA: a benchmark for General AI Assistants
	- https://arxiv.org/abs/2311.12983
- Q*ã§ã¯ãªã„ã§ã™ãŒã€A*æ¢ç´¢ã®æ§˜å­ã‚’å¯è¦–åŒ–ã—ãŸ
	- https://x.com/GregKamradt/status/1728480680127148480?s=20
- Kevin Dunnell et al., "Latent Lab: Large Language Models for Knowledge Exploration"
	- https://arxiv.org/abs/2311.13051
	- LLMãƒ™ãƒ¼ã‚¹ã§ã€ç•°ãªã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã®ç¹‹ãŒã‚Šã‚„ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆã‚’ä¿ƒã™ã‚·ã‚¹ãƒ†ãƒ ã€Latent Labã€
	- â‘ å¯¾è©±ã¨è¦–è¦šåŒ–ã‚’é€šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ç´¢ 
	- â‘¡ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘ã‚’è‡ªå‹•åŒ–
	-  â‘¢ æ–°ã—ã„ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢åˆæˆã‚‚å¯èƒ½
-  Google Colab ã§ LCM LoRA ã‚’è©¦ã™ã€€ by npakaã•ã‚“
	- https://note.com/npaka/n/n940ee84ca5b6?sub_rt=share_h
	- ã€ŒLCMã€ (Latent Consistency Model) ã¯ã€å…ƒãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã™ã‚‹ã“ã¨ã§ã€ç”»åƒç”Ÿæˆã«å¿…è¦ãªã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ¸›ã‚‰ã™æ‰‹æ³•ã§ã™ã€‚25ï½50ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‹ã£ã¦ã„ãŸå‡¦ç†ã‚’4ï½8ã‚¹ãƒ†ãƒƒãƒ—ã§å¯èƒ½ã«ã—ã¾ã™ã€‚
- Multi-modal Foundation Model for Material Design
	- https://openreview.net/forum?id=EiT2bLsfM9
	- åˆ†å­ã‚’è¡¨ç¾ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ã€‚SELFIESã€DFTç‰©æ€§ã€ã‚¹ãƒšã‚¯ãƒˆãƒ«ã«ã¤ã„ã¦ãã‚Œãã‚Œencoder-decoderã‚’å­¦ç¿’ã—ã€å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æ½œåœ¨ç©ºé–“ã‚’å…±é€šã®æ½œåœ¨ç©ºé–“ã«encode, decodeã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã€‚
	-  æ¬ æãŒå¤šãã¦ã‚‚å­¦ç¿’å¯èƒ½ã‹ã¤ã€å¾Œã‹ã‚‰ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’è¿½åŠ ã—ã‚„ã™ã„
- é¸æŠãƒã‚¤ã‚¢ã‚¹ã®å¼ã€tweedle
	- https://x.com/docmilanfar/status/1728680465928958055?s=20
- llamaindexã‚ˆã‚Šã€RAGè©•ä¾¡ãƒ„ãƒ¼ãƒ«ragsã®v2ãƒªãƒªãƒ¼ã‚¹
	- https://github.com/run-llama/rags
-  Simplifying Transformer Blocks 
	- https://arxiv.org/abs/2311.01906
	- many parts can be removed to simplify GPT-like decoder architectures as well as encoder-style BERT models:
- llamaindexã‹ã‚‰ã€RAGã®æ–°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€fuzzy citationã‚’ç™ºè¡¨
	- https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/fuzzy_citation/fuzzy_citation_example.ipynb
	- https://llamahub.ai/l/llama_packs-fuzzy_citation
	- éƒ¨åˆ†çš„ãªæ¤œç´¢çµæœã‹ã‚‰ï¼‘ã¤ã®å›ç­”ã‚’åˆæˆï¼Ÿï¼Ÿ
- ï¼²ï¼¡ï¼§ 101 for enterpirze
	- https://gradient.ai/blog/rag-101-for-enterprise
	- çµµãŒç´ æ•µ
-  AIã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€Œç¶™ä¹‹åŠ©ã€çˆ†èª•!ã¨ã‚Šã‚ãˆãšRAID0ã§12TBã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
	- https://note.com/shi3zblog/n/n77e8ad3ed779?sub_rt=share_pb
	- ã¤ã„ã«A100 80GBx8ã®ãƒã‚·ãƒ³ãŒç¨¼åƒã—ãŸã€‚ã“ã“ã¾ã§é•·ã‹ã£ãŸã€‚
	- ã“ã“ã¾ã§æƒã£ãŸã‚‰æ—¥æœ¬æœ€å¤§è¦æ¨¡ã®LLMã‚’å€‹äººã§ä½œã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
-  A population-level digital histologic biomarker for enhanced prognosis of invasive breast cancer
	- https://www.nature.com/articles/s41591-023-02643-7
	- An important AI report for breast cancer leading to the potential of sparing chemotherapy for many. 
	- The 1st comprehensive analysis of both cancerous and non-cancerous tissue in hundreds of thousands of patient tissues-
- BERTopicã®æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³
	- https://github.com/MaartenGr/BERTopic
	- Merge pre-trained models, apply zero-shot topic modeling, seed domain-specific words, and much more in this HUGE update!
- IntelÂ® Extension for Transformers
	- https://github.com/intel/intel-extension-for-transformers
	- An Innovative Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere
	- Intel Extension for Transformers supports INT4 model quantized by GPTQ on Intel platforms (Xeon & PC) !
	- https://github.com/intel/intel-extension-for-transformers/tree/1.2.1#int4-inference
-  ãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ›ã¨ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã®é–¢ä¿‚
	- https://qiita.com/kaityo256/items/aa5b24904577de40016e
	- é–¢æ•°ï¿½(ï¿½)ã«ãŸã„ã—ã¦ã€ï¿½<0ãªã‚‰ã‚¼ãƒ­ã«ã€ï¿½â‰¥0ãªã‚‰eâˆ’ï¿½ï¿½ã‚’ã‹ã‘ã¦ã€ã€Œã‚ˆã‚ŠåæŸã—ã‚„ã™ãã€ã—ãŸä¸Šã§ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã—ãŸã‚‚ã®ãŒãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ›ã§ã‚ã‚‹ã€‚ãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ›ãŒã€è»¸ã®ä¸­é€”åŠç«¯ãªã¨ã“ã‚ã‚’ã€Œç¸¦ã«ã€ç©åˆ†ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ç†ç”±ã‚‚ã€ãƒ•ãƒ¼ãƒªã‚¨é€†å¤‰æ›ã¨ï¿½ã‹ã‚‰ï¿½ã¸ã®å¤‰æ•°å¤‰æ›ã‹ã‚‰ç†è§£ã§ãã‚‹ã§ã‚ã‚ã†ã€‚
	- é–¢æ•°ï¿½(ï¿½)ã«ãŸã„ã—ã¦ã€ï¿½<0ãªã‚‰ã‚¼ãƒ­ã«ã€ï¿½â‰¥0ãªã‚‰eâˆ’ï¿½ï¿½ã‚’ã‹ã‘ã¦ã€ã€Œã‚ˆã‚ŠåæŸã—ã‚„ã™ãã€ã—ãŸä¸Šã§ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã—ãŸã‚‚ã®ãŒãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ›ã§ã‚ã‚‹ã€‚ãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ›ãŒã€è»¸ã®ä¸­é€”åŠç«¯ãªã¨ã“ã‚ã‚’ã€Œç¸¦ã«ã€ç©åˆ†ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ç†ç”±ã‚‚ã€ãƒ•ãƒ¼ãƒªã‚¨é€†å¤‰æ›ã¨ï¿½ã‹ã‚‰ï¿½ã¸ã®å¤‰æ•°å¤‰æ›ã‹ã‚‰ç†è§£ã§ãã‚‹ã§ã‚ã‚ã†ã€‚
- Google Colabã€Huggingfacesã®å”åŠ›ã§ã€transformerã‚’æœ€åˆã‹ã‚‰ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸ
	- https://x.com/GoogleColab/status/1729217098977845590?s=20
- A Llama-2-based model finetuned for function calling:
	- https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2
- æ—¥æœ¬èªWikipediaã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ10ä¸‡å€‹ã‚’ä½œã‚Šã¾ã—ãŸ	
	- https://note.com/shi3zblog/n/na10eed9270f8?sub_rt=share_pb
	- GPT-3.5-Turboã‚’ä½¿ã£ã¦ã€ç´„ä¸€ãƒ¶æœˆã‹ã‘ã¦æ—¥æœ¬èªã®Wikipediaã®é …ç›®ã‚’ã‚‚ã¨ã«å…ˆç”Ÿã¨ç”Ÿå¾’ãŒä¼šè©±ã™ã‚‹ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã—ãŸ
	- GPT-4ã§ã‚‚ã‚„ã£ã¦ã¿ã‚ˆã†ã‹ãªã¨æ€ã£ã¦ã„ã¾ã™ãŒã€GPT-3.5ã§ã‚‚ä¸€ãƒ¶æœˆã§ã‹ãªã‚Šã®å‡ºè²»ãŒã‚ã‚Šã€GPT-4ã§åŒã˜åˆ†é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚‹ã¨ãªã‚‹ã¨æ•°åä¸‡å††ã‹ã‚‰æ•°ç™¾ä¸‡å††ã‹ã‹ã‚Šãã†ã§ã™
- llamaindexã‹ã‚‰RAGã«æœ‰åŠ¹ãªllamapackã‚’ï¼—ç¨®é¡å…¬é–‹
	- https://x.com/llama_index/status/1729303619760259463?s=20
- Compositional Generative Inverse Design
	- https://openreview.net/forum?id=5ueXRkKMMg&referrer=%5Bthe%20profile%20of%20Yilun%20Du%5D(%2Fprofile%3Fid%3D~Yilun_Du1
	- ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã—ãŸä»£ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸé€†å•é¡Œè§£æ³•ã¯ã€ã—ã°ã—ã°å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒå¤–ã«ã„ã£ãŸã‚Šå±€æ‰€è§£ã«é™¥ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã‚Œã‚’é˜²ããŸã‚ã«ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã§è§£ã‚’èª˜å°ã—ã€ä¸é©åˆ‡ãªè§£ã‚’é˜²ãCinDMã‚’ææ¡ˆ
- mlc-llm on WSLã§ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ã‚’è¡Œã†
	- ã€ŒWebGPUã‚’ç”¨ã„ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ–ãƒ©ã‚¦ã‚¶æ¨è«–ã€
	- https://zenn.dev/saldra/articles/356f470e730d1c
- ï¼®ï¼´ï¼´ã‚³ãƒ ã®ï¼¡ï¼©å­¦ç¿’æ•™æ
	- https://gochikika.ntt.com/index.html
	- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‹ã‚‰ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„è©•ä¾¡ã¾ã§Pythonã‚³ãƒ¼ãƒ‰ã¨åˆã‚ã›ã¦ä¸€é€šã‚Šå­¦ã¹ã‚‹
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®ï¼¬ï¼¬ï¼­ã§ã‚‚å‡ºåŠ›ã®æˆå‹ãŒå¤§äº‹
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb
- John X. Morris et al., "Language Model Inversion"
	- https://arxiv.org/abs/2311.13647
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®å˜èªã®ç¢ºç‡ã‚’å‡ºã™ãŒã€ãã®ã€Œç¢ºç‡ã€ã‚’åˆ©ç”¨ã—ã¦å…ƒã®æ–‡ç« ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’ä½•ã¨ã‹ã—ã¦è¦‹ã¤ã‘å‡ºã™æ‰‹æ³•ã‚’é–‹ç™ºã€‚
- OpenAIã®cookbookã«llamaindexã‚’ã¤ã‹ãŸRAGãŒæ²è¼‰
	- https://blog.llamaindex.ai/openai-cookbook-evaluating-rag-systems-fe393c61fb93
- Minimizing Factual Inconsistency and Hallucination in Large Language Models
	- https://arxiv.org/abs/2311.13878
	- LLMã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŠ‘åˆ¶ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€å¤šæ®µéšã§æƒ…å ±ã‚’å–å¾—ã•ã›ã‚‹ã“ã¨ã§ã€ä¿¡é ¼æ€§ã®é«˜ã„å¿œç­”ã‚’å–å¾—å¯èƒ½ã§ã™ã€‚
- Relational Deep Learning
	- https://drive.google.com/file/d/1Uk1y6c8z265G0wiRPpGT1cd5lts5lnKq/view
	- Relational Deep Learning is brings the power of Graph Representation Learning to a Relational Database.
- NeurIPA2023ã®è«–æ–‡æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹
	- https://www.ai-driven-life.com/neurips-papers
- å¼·åŒ–å­¦ç¿’ã¯ãƒ™ãƒ«ãƒãƒ³æœ€é©æ€§åŸç†ã‹ã‚‰æ¥ã‚‹å‹•çš„è¨ˆç”»æ³•ã«æ”¯ãˆã‚‰ã‚Œã¦ã¾ã™ã€‚ã—ã‹ã—ã€æƒ…å ±ãŒrandomSamplingã•ã‚Œã‚‹ä¸­ã§å®Ÿã¯å„æ™‚åˆ»éš£åˆã†ãƒ‡ãƒ¼ã‚¿ã®åˆ—ãŒã»ã¨ã‚“ã©æƒ…å ±ï¼ˆå ±é…¬ï¼‰ã‚’æŒãŸãªã„ã¨ãªã‚‹ã¨ã€é–“ã«æ¨å®šå™¨ãŒæŒŸã¾ã£ã¦ã‚‹ã®ã‚‚ã‚ã£ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹ã©ã“ã‚ã‹æœ€å¾Œã«ã—ã‹å ±é…¬ãŒå¾—ã‚‰ã‚Œãªã„å•é¡Œã¸ã®å¦¥å½“æ€§ã¯æ€ªã—ã„ã‹ã‚‚ã§ã™ã­ã€‚
	- https://x.com/ML_deep/status/1729249503683969037?s=20
- DeepMind has formalized a theoretical result related to AI safety in Lean. 
	- https://github.com/google-deepmind/debate
	- "Monadic syntax is excellent for expressing stochastic algorithms, and working over finitely supported distributions avoids the need for integrability side conditions during proofs."
	- But Iâ€™m now much more optimistic that a PCP (probabilistically checkable proof) system derived from this line of research might be a useful tool to have in the toolbox for verifying AI safety properties that depend upon unformalizable human preferences. I still think â€œnot killing lots of peopleâ€ is probably just totally formalizable, but humanity might also want to mitigate the risks of various dystopias that are more a matter of taste, and thatâ€™s where this type of method might shine.
	- https://x.com/davidad/status/1729461156618637502?s=20
- Azure OpenAI Serviceã®æ—¥æœ¬èªè¨˜äº‹ã¾ã¨ã‚
	- https://zenn.dev/microsoft/articles/azure-openai-japanese-blogs
- ã‚«ãƒ¼ãƒãƒãƒ³æ•™æˆã¨ãƒ«ã‚«ãƒ³å…ˆç”Ÿã®å¯¾è©±
	- https://www.youtube.com/watch?v=oy9FhisFTmI
	- Video of Daniel Kahneman and Yann LeCun discussing Dual Process Theory (i.e., System 1 and 2) in relation to Deep Learning.
-  ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search
	- https://arxiv.org/abs/2310.13227
	- uses algorithms like A* to improve LLM answers, improving sota on both planning and reasoning tasks
- Qualcomm Snapdragon 8gen 3 already supported 10b language model running locally on your smartphone.
	- https://x.com/Francis_YAO_/status/1727861621110779941?s=20
	- LLM is the new smartphone OS!
- Domingoså…ˆç”ŸãŒãªã‚“ã‹è¨€ã£ã¦ã„ã‚‹
	- https://x.com/pmddomingos/status/1729303707387658284?s=20
	- Why AI isn't going to taking over (from "The Master Algorithm").
- MistralChameli_7B_v01
	- https://huggingface.co/TokenBender/MistralChameli_7B_v01
	- First version of DPO-ed roleplay/smart version of Mistral. Now to conduct some experiments with reward model and see if this is any good.
- ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒ¢ãƒ‡ãƒ«ã¸ã®çµŒé¨“ãƒ™ã‚¤ã‚ºä¿®æ­£
	- https://www.jstage.jst.go.jp/article/keidaironshu/68/4/68_161/_article/-char/ja/
	- Robbins (1956) ãŒ Tweedie (1947) ã«è¨€åŠã—ã¦ã‚‹ã“ã¨ã«åŸºã¥ãï¼ŒEfron ãŒ Tweedie's formula ã¨åä»˜ã‘ã¦åºƒã¾ã£ã¦ã„ã‚‹ãŒï¼ŒKoenker & Gu (2016) ã§ã¯ Dyson (1926) ã§æ—¢ã«å¾—ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹ã€‚
-  A glimpse of the next generation of AlphaFold
	- https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/
	- AlphaFoldã¯æœ€è¿‘å¤§ããªã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ã‚Šã€ç²¾åº¦ãŒå¤§å¹…ã«å‘ä¸Šã—ã€ã‚¿ãƒ³ãƒ‘ã‚¯ã ã‘ã§ãªãPDBã«ã‚ã‚‹ã»ã¼ã™ã¹ã¦ã®åˆ†å­ã«ã¤ã„ã¦äºˆæ¸¬å¯èƒ½ã§ã™ã€‚å‰µè–¬ã‚„æ–°å‹CRISPRæ¢ç´¢ã«ã‚‚(ä¸€å®šç¨‹åº¦ã¯)ä½¿ãˆã¾ã™ã€‚
- EMNLP2023 ã®æ¡æŠè«–æ–‡ãƒªã‚¹ãƒˆãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ãªã£ã¦ãŸï¼æ¥é€±ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã§é–‹å‚¬ã•ã‚Œã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã®å›½éš›ä¼šè­°ã§ã™ï¼ã‚¿ã‚¤ãƒˆãƒ«ã«"Language Model"ã¯ã„ã£ã¦ã‚‹è«–æ–‡ãŒ219æœ¬ã£ã¦ï¼Œã©ã‚“ã ã‘è¨€èªãƒ¢ãƒ‡ãƒ«å¥½ããªã‚“ã ã‚ˆ
	- https://2023.emnlp.org/program/accepted_main_conference/
-  OpenAI ã¨ LangChain ã®èªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ by npakaã•ã‚“
	- https://note.com/npaka/n/n650532ce289a?sub_rt=share_h
	- ã€Œ**èªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã€(cognitive architecture) ã¨ã¯ã€LLMã©ã®ã‚ˆã†ã«æƒ…å ±ã‚’å‡¦ç†ã—ã€å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®æ çµ„ã¿ã§ã™ã€‚ã€ŒFlo Crivelloã€ï¼ˆè‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã®Lindyã®å‰µè¨­è€…ï¼‰ãŒä½¿ç”¨ã—ãŸã“ã®ç”¨èªã‚’åˆã‚ã¦èãã€ç´ æ™´ã‚‰ã—ã„ç”¨èªã ã¨æ€ã„ã¾ã—ãŸã€‚
	- ã€ŒLangChainã€ã§ã¯ã€ã€ŒLLMã€ãŒçœŸã«å¤‰é©çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚ˆã†ãªã‚·ã‚¹ãƒ†ãƒ ã«é›»åŠ›ã‚’ä¾›çµ¦ã™ã‚‹ä¸–ç•Œã‚’ä¿¡ã˜ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ãã“ã«ãŸã©ã‚Šç€ããƒ«ãƒ¼ãƒˆã¯ã€**ä¼æ¥­ãŒã€ŒèªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ã‚’åˆ¶å¾¡ã§ãã‚‹ãƒ«ãƒ¼ãƒˆ**ã§ã‚ã‚‹ã¨ä¿¡ã˜ã¦ã„ã¾ã™ã€‚
	- **(1) Code**  LLMã‚’åˆ©ç”¨ã—ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚  
	- **(2) LLM Call** ã‚¢ãƒ—ãƒªã®å‡ºåŠ›ã®ã¿ã‚’æ±ºå®šã™ã‚‹å˜ä¸€ã®LLMã‚³ãƒ¼ãƒ«ã€‚ 
	- **(3) Chain**  ã‚¢ãƒ—ãƒªã®å‡ºåŠ›ã®ã¿ã‚’æ±ºå®šã™ã‚‹è¤‡æ•°ã®LLMã‚³ãƒ¼ãƒ«ã€‚  
	- **(4) Router**  LLMã‚’ãƒ«ãƒ¼ã‚¿ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã—ã€ä½¿ç”¨ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Toolã€Retrievalã€Prompt) ã‚’é¸æŠã€‚ 
	- **(5) State Machine**  LLMã‚’ä½¿ç”¨ã—ã¦ã‚ã‚‹ç¨®ã®ãƒ«ãƒ¼ãƒ—ã§ã‚¹ãƒ†ãƒƒãƒ—é–“ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ãŒã€ã‚³ãƒ¼ãƒ‰ãŒè¨±å¯ã•ã‚ŒãŸé·ç§»å…ˆã«ã®ã¿é·ç§»  
	- **(6) Agent**  åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ†ãƒƒãƒ—ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ±ºå®šã‚‚LLMãŒè¡Œã†ã€‚
- Textã‹ã‚‰SQLã‚’ç”Ÿæˆã™ã‚‹Querypls
	- https://github.com/samadpls/Querypls/
- ã‚ã‚Œã‚‰ãŒã€ @jerryjliu0ãŒdeeplearningaiã‚³ãƒ¼ã‚¹ã«ç™»å ´
	- https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/
	- We also have LlamaPacks for every technique mentioned in this course to help you jumpstart your advanced LLM app:
- Deconstructing RAG
	- https://blog.langchain.dev/deconstructing-rag/
	- Given the importance of RAG and the fast pace of development, we've grouped popular RAG concepts into a few categories and created guides for each one.
- Running Starling-7B LLM model on local CPU with @Ollama_ai and getting great results for invoice data extraction, even better than Zephyr, Mistral or Llama2.
	- https://github.com/katanaml/llm-ollama-invoice-cpu
- å††åŸå¡”ã‚’è¿‘ä¼¼ã™ã‚‹ï¼Ÿ
	- https://colab.research.google.com/drive/1oXxBIYJvvUYsVZP6WYAUCb3QK09zTJtO?usp=sharing
	- å††åŸå¡”ã•ã‚“ã®æ–‡ç« ã§å­¦ã¶ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
- ã€Œé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’LLM(GPT, Claude)ã«é£Ÿã‚ã›ãŸéš›ã«ã€ã¡ã‚ƒã‚“ã¨Retrivalã•ã‚Œã‚‹ã‹ï¼Ÿã€ã‚’æ¤œè¨¼ã—ã¦ã„ã‚‹Githubã€‚
	- https://github.com/gkamradt/LLMTest_NeedleInAHaystack
	-  ç·ã˜ã¦Calude-2ã«æ¯”ã¹ã¦GPT-4 Turboã®ã»ã†ãŒæ­£ç¢ºã«å¼•ç”¨ã—ã¦ã„ã‚‹ã‚ˆã†ã§é¢ç™½ã„ã€‚
- Qwen/Qwen-7B-Chat-Int4ã‚’Google Colobã§å‹•ã‹ã™
	- https://ayousanz.hatenadiary.jp/entry/2023/11/30/182017
	- ãªã‚“ã‹æ—¥æœ¬ã®æ–‡åŒ–ã¯ã¡ã‚ƒã‚“ã¨å­¦ã‚“ã§ã„ãªã„ã¿ãŸã„ã§ã™ã­
-  Accelerating Generative AI with PyTorch II: GPT, Fast
	- https://pytorch.org/blog/accelerating-generative-ai-2/?utm_content=273712248
	- GPT-fastã¨ã„ã†ã®ãŒã™ã”ã‚‰ã„ã—ã„ã€ï¼“å€ï¼Ÿ
- LiLM å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« TinyLlama 1.1B ã®æ—¥æœ¬èªè¿½åŠ äº‹å‰å­¦ç¿’(incremental pretrain) ã‚’è©¦ã—ãŸãƒ¡ãƒ¢
	- https://zenn.dev/syoyo/articles/52f1d0d62fcad5
	- ç”Ÿæˆã•ã‚Œã‚‹æ—¥æœ¬èªã¯ã¾ã‚ã¾ã‚ã§ã‚ã‚‹ãŒ, æ§‹æ–‡ã‚„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãŠã‹ã—ã„...
	- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã—ã¦ã‚‚é–“é•ãˆãŸã‚Š...
	- ã¾ã‚ã§ã‚‚ 1B è¦æ¨¡ãªã‚‰å¦¥å½“ãªã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
- ä»Šå·ã®ã€æ—¥æœ¬èªã®ç ”ç©¶ã€ã§ã€Œå’Œæ­Œé›†ã®æ­Œé¢¨ã®è¨€èªçš„å·®ç•°ã®è¨˜è¿°ãƒ¼å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åˆ†æãƒ¼ã€ã¨é¡Œã—ã¦ã€OpenAIã®text-embeddingã‚’ä½¿ã£ã¦ã€ã€ä¸‡è‘‰é›†ã€ã¨ã€å¤ä»Šé›†ã€ã®æ„å‘³æ§‹é€ ã®å·®ã‚’è§£æã—ã¦ã¿ã¾ã—ãŸã€‚
	- https://www.musashinoshoin.co.jp/shoseki/view/2976/
- Energy and entropy: Path from game theory to statistical mechanics
	- https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.043055
	- ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä½ãã™ã‚‹ã®ãŒç›®æ¨™ã®ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼ã¨ï¼Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ä¸Šã’ã‚‹ã®ãŒç›®æ¨™ã®ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼ã®äº¤æ¸‰ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹æœ€é©ãªæˆ¦ç•¥ã‚’é€šã—ã¦ç†±å¹³è¡¡åŒ–ã‚’è­°è«–ã™ã‚‹ã‚‰ã—ã„
- gpt-fast
	- https://github.com/pytorch-labs/gpt-fast
	- LLMã‚’Pytorchã ã‘ã§ã©ã‚Œã ã‘é«˜é€ŸåŒ–ã§ãã‚‹ã‹ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ãŸãƒªãƒã‚¸ãƒˆãƒª Llama-7BãŒ10å€é€Ÿããªã£ã¦ã„ã‚‹ 
	- Pytorchã§ä½¿ãˆã‚‹é«˜é€ŸåŒ–æŠ€è¡“ã‚’ã„ã‚ã„ã‚ç››ã‚Šè¾¼ã‚“ã§ã‚‹ã½ã£ãã¦ã€ä¸­èº«è¦‹ã‚‹ã®ã‚‚å‹‰å¼·ã«ãªã‚Šãã†
- æ—¥æœ¬èªLLMã§LLaVAã®å­¦ç¿’ã‚’è¡Œã£ã¦ã¿ãŸ
	- https://qiita.com/toshi_456/items/248005a842725f9406e3
- googleã‹ã‚‰æ–°ã—ã„ç¿»è¨³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç™ºè¡¨
	- Unsupervised speech-to-speech translation from monolingual data
	- https://blog.research.google/2023/12/unsupervised-speech-to-speech.html
-  æ¥­ç•Œåˆ¥ç”ŸæˆAIæ´»ç”¨ã®ã™ã‚ã‚
	- https://www2.deloitte.com/jp/ja/pages/about-deloitte/articles/about-deloitte-japan/ai-dossier-2023.html?id=jp:2pm:3tw:4daii-genaidossier:5:6abt:20231201::
	- ãƒ‡ãƒ­ã‚¤ãƒˆãƒˆãƒ¼ãƒãƒ„
-  Microsoft Copilot is now generally available
	- https://blogs.bing.com/search/december-2023/Microsoft-Copilot-is-now-generally-available?ocid=aid_soc_usoc_edu_cons_bing_eng_tw_12.1
- Cè¨€èªã§WASMã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’å®Ÿè£…ã—ãŸè©±
	- https://zenn.dev/ri5255/articles/845ef3dab5ab47
	- ã“ã®è‡ªä½œWASMãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ç›®çš„ã¯ã€ã§ãã‚‹ã ã‘ä»•æ§˜ã«å¾“ã£ãŸå®Ÿè£…ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã€ä»•æ§˜ã®ç†è§£ã‚’åŠ©ã‘ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚æ—©ã•ã‚„åŠ¹ç‡æ€§ã‚ˆã‚Šã‚‚åˆ†ã‹ã‚Šã‚„ã™ã•ã‚’å„ªå…ˆã—ã¦ã„ã‚‹ãŸã‚ã€å®Ÿç”¨ã«ã¯å‘ã‹ãªã„ã€‚ä»•æ§˜æ›¸ã‚’èª­ã‚“ã§ã€å®Ÿè£…ã«å›°ã£ãŸéš›ã«å‚ç…§ã—ã¦ã»ã—ã„ã€‚
-  ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«æ•°ç†ãƒ¢ãƒ‡ãƒ«ã§ç«‹ã¡å‘ã‹ã† / Japan.R 2023
	- https://speakerdeck.com/dropout009/japan-dot-r-2023
- Harsha Nori et al., "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine"
	- https://arxiv.org/abs/2311.16452
	- ã“ã‚Œã¾ã§GPT-4ãªã©ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¯ã€åŒ»å­¦ãªã©ã®å°‚é–€åˆ†é‡ã§ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«ã¯æ•µã‚ãªã„ã¨è€ƒãˆã‚‰ã‚Œã¦ãã¾ã—ãŸã€‚ ã—ã‹ã—ã€ã€Œå®Ÿéš›ã¯ã©ã†ãªã®ã‹ï¼Ÿã€ã¨è€ƒãˆãŸç ”ç©¶è€…ã‚‰ã¯ã€ç‰¹åˆ¥ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ã®GPT-4ãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å·¥å¤«ã®ã¿ã§ã©ã“ã¾ã§æ€§èƒ½ã‚’ç¤ºã™ã®ã‹ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚
	- â‘  ã‚¢ãƒ¡ãƒªã‚«ã®åŒ»å­¦è©¦é¨“ã€ŒUS (4-option)ã€ã§90.2ï¼…ã¨ã„ã†é«˜ã„æ­£è§£ç‡ã‚’å‡ºã—ãŸ
	-  â‘¡ ç†ç”±ä»˜ã‘ãŒå¿…è¦ãªã‚¿ã‚¤ãƒ—ã®å•é¡Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆPubMedQAã§82.0ï¼…ã®æ­£è§£ç‡ã‚’é”æˆ
-  æ—¥å¸¸èƒ½åŠ›ã‚’è©¦ã™ãƒ†ã‚¹ãƒˆã€GAIAã€æ­£ç­”ç‡ã€äººé–“92%ã«å¯¾ã—ã¦GPT-4ã¯15%ã€€ä¸€èˆ¬çš„ãªãƒ‹ãƒ¼ã‚ºã«å¿œãˆã‚‹AIé–‹ç™ºã®æŒ‡é‡ã«
	- https://aiboom.net/archives/59440
- Langchain102
	- https://www.youtube.com/watch?v=haad3i9VROs
	- Mistral 7b User Showcase + LangServe & LangSmith
- METAã®AIç ”ç©¶è€…ãŒä½•ã‚‰ã‹ã®å¤§ããªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ãŒã‚ã£ãŸã¨ç¤ºå”†ã€‚ è¿‘æ—¥ä¸­ã«å…±æœ‰äºˆå®šã¨ã®ã“ã¨
	- https://x.com/ArmenAgha/status/1731076069170835720?s=20
-  ã€ŒChatGPTã®1å‘¨å¹´ã‚’è¨˜å¿µã—ã¦ã€ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMãŒChatGPTã«ã©ã“ã¾ã§è¿½ã„ã¤ã„ã¦ã„ã‚‹ã‹ä½“ç³»çš„èª¿æŸ»å ±å‘Š
	- https://aiboom.net/archives/59713
	- https://arxiv.org/abs/2311.16989
	- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã¨ã—ã¦ã¯Llama-2ï¼ˆãŠã‚ˆã³MentalLlamaï¼‰ã€Palmã€Vicunaã€Falconã€Wizardã€Lemurãªã©ã®ãƒ¢ãƒ‡ãƒ«ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ãã‚Œã‚‰ã®é€²æ­©ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã¨ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§ã®å„ªã‚ŒãŸæ€§èƒ½ã«ã¤ã„ã¦è©³ã—ãåˆ†æã•ã‚Œã¦ã„ã¾ã™ã€‚èª¿æŸ»çµæœã‹ã‚‰ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMãŒç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„å¿œç”¨åˆ†é‡ã«ãŠã„ã¦ã€ã‚¯ãƒ­ãƒ¼ã‚ºãªLLMã«åŒ¹æ•µã™ã‚‹ã€ã‚ã‚‹ã„ã¯ãã‚Œã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—
- MRS2023(materials research society)ã§LLMãŒå¤šã„ 2023 MRS Fall Meeting & Exhibit
	- https://x.com/yoko_materialDX/status/1731267042810962256?s=20
	- MIã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå¸¸æ™‚4ã¤ã‚ã‚Šå›ã‚‹ã®ãŒå¤§å¤‰
	- æ©Ÿæ¢°å­¦ç¿’ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã¨è‡ªå‹•åˆæˆã®ç™ºè¡¨ãŒå¤§é‡
	- çµæ™¶æ§‹é€ äºˆæ¸¬ã®ç™ºè¡¨ãŒæ€ã£ãŸã‚ˆã‚Šå¤šã‹ã£ãŸ
	- LLMã®ç™ºè¡¨ã¯ææ–™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãŒä¸­å¿ƒ
	- æ—¥æœ¬ä¼æ¥­ã‹ã‚‰ã®MIç™ºè¡¨ãŒå¤šã‹ã£ãŸ 
	- ä¸–ç•Œæƒ…å‹¢ã‚†ãˆï¼Ÿï¼‰ä¸­å›½æœ¬åœŸã®æ–¹ãŒã»ã¼ã„ãªã‹ã£ãŸ

## 11/27

ã‚¢ãƒ«ãƒˆãƒãƒ³æ°è§£ä»»åŠ‡ã¯ã€ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãŒã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã®å—ã‘å…¥ã‚Œã‚’è¡¨æ˜ã™ã‚‹ã‚‚ã€OpenAIã®ä¸»è¦ãƒ¡ãƒ³ãƒãŒã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã«è¿½å¾“ã™ã‚‹ã¨è¡¨æ˜ã—ãŸã®ã§ãƒœãƒ¼ãƒ‰ãŒå¾©å¸°ã‚’æ‡‡é¡˜ã€çµå±€OpenAIã®CEOã¨ã—ã¦æˆ»ã‚‹ã“ã¨ã§å¹•å¼•ãã€‚è§£ä»»åŠ‡ã®èƒŒå¾Œã«ã¯ã€OpenAIã§AGIï¼ˆã‚¹ãƒ¼ãƒ‘ãƒ¼AI)ã‚’é”æˆã™ã‚‹è¦‹è¾¼ã¿ãŒç«‹ã£ãŸã€ãã‚ŒãŒQ*ã¨ã„ã†LLMã§ã€å¾“æ¥ã®LLMãŒè‹¦æ‰‹ã ã£ãŸæ•°ã®æ¨è«–ãŒå¯èƒ½ã«ãªã£ãŸã€Q*ã®å–ã‚Šæ‰±ã„ã‚’å·¡ã‚Šè§£ä»»é¨’å‹•ãŒèµ·ããŸã€ã¨ã„ã†ã†ã‚ã•ã§æŒã¡åˆ‡ã‚Šã«ã€‚Q*-learningãŒãã‚Œã§ã¯ï¼Ÿã¿ãŸãªã“ã¨ã«ãªã£ã¦æ§˜ã€…ãªã¨ã“ã‚ã§ç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã€‚ãã‚Œä»¥å¤–ã§ã¯ã€intelãŒæº€ã‚’æŒã—ã¦neural-chat-7b-v3-1ã‚’å…¬é–‹ã€Mistral 7Bãƒ™ãƒ¼ã‚¹ãªã‚“ã ã‘ã©ã€æ§˜ã€…ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šç›¸å½“æ€§èƒ½ãŒè‰¯ã„ã¿ãŸã„ã€ã—ã‹ã—Falcon 180Bè¶Šãˆã¨ã„ã†ã“ã¨ã¯ãªã„ã¨æ€ã†ãã€‚AnthropicAIãŒ200kã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ãˆã‚‹Claude2.1ã‚’ç™ºè¡¨ã€ãƒ‡ãƒ¢ç‰ˆãŒåˆ©ç”¨å¯èƒ½ã§ã€ã•ã£ããçµæ§‹é•·æ–‡ã®æ—¥æœ¬èªã®PDFã‚’ãã®ã¾ã¾æŠ•å…¥ã§ãã‚‹ã¨ã‹ã€ã‚¨ãƒãƒ³ã‚²ãƒªã‚ªãƒ³ä¸–ç•Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‹•ã‹ã—ã¦ã¿ãŸã¨ã‹è©±é¡Œã«ã€‚ã€Œï¼“Dä¸–ç•Œã®ä¸­ã§èº«ä½“æ€§ã‚’ã‚‚ã£ãŸæ±ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã®è«–æ–‡ã€ã„ã‚„ ã€Œæœªæ¥ã®äºŒã¤ã®é¡”ã€ï¼ˆãƒ›ãƒ¼ã‚¬ãƒ³ï¼‰ã®AIï¼ˆä»®æƒ³ï¼“Dç©ºé–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§èº«ä½“æ€§ã‚’å­¦ç¿’ã•ã›ã‚‹ï¼‰ã‚’å½·å½¿ã•ã›ã‚‹ä¸–ç•ŒãŒç¾å®Ÿã«ãªã£ãŸã‚ˆã†ãªæ°—ãŒã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å¯¾ã™ã‚‹Q&Aã«ãŠã„ã¦ã€SQLæ–‡ã‚’ç”Ÿæˆã•ã‚Œã‚‹æ–¹æ³•ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ã‚’ã„ã£ãŸã‚“çŸ¥è­˜ã‚°ãƒ©ãƒ•ã«ã—ã¦Q&Aã™ã‚‹æ–¹æ³•ã‚’æ¯”è¼ƒã—ã€å¾Œè€…ã®ã»ã†ãŒé«˜æ€§èƒ½ã¨ã®å ±å‘Šã‚‚ã€‚ã¾ã‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã„ã†ã‹ãã†ã„ã†ã®ã‚’ä¸ãˆãŸã»ã†ãŒã„ã„ã«æ±ºã¾ã£ã¦ã„ã‚‹ã®ã ãŒã€‚RAGã«ãŠã„ã¦ã‚‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã®ãŒæœ‰åŠ¹ã‚‰ã—ã„ã€ãã®ã‚ãŸã‚Šã«ã¾ã äººã®å·¥å¤«ã®ä½™åœ°ãŒæ®‹ã£ã¦ã„ã‚‹ã€‚Llemmaã¯ã€LLMã§æ•°å­¦ã®å•é¡Œã‚’è§£ãã®ã«ã€å®šç†è¨¼æ˜å™¨ã‚’ä½¿ã†ã“ã¨ã‚’å‰æã«ã—ãŸPythonã‚³ãƒ¼ãƒ‰ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã€LLMã‚’æ´»ç”¨ã—ã¦å•é¡Œã‚’è§£ããƒ¡ã‚¿ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆç›´æ¥è§£ãã®ã§ã¯ãªãã¦ã€è§£ãæ‰‹é †ãƒ»æ–¹æ¡ˆã‚’ç”Ÿæˆã™ã‚‹ï¼‰ã®ï¼‘ã¤ã€‚LLMãƒ™ãƒ¼ã‚¹ã®æ–°ã—ã„è¨€èªã€SUQLã€ã‚‚ã„ã„æ„Ÿã˜ã§éæ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ãˆã‚‹ã‚‰ã—ã„ãŒã€ä¾‹é¡ŒãŒãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®ä¼šè©±ã¨ã¯ç¬¬ï¼’ä¸–ä»£AIã«ãŠã‘ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ å•é¡Œã½ãã¦ã„ã„ã­ï¼AIãŒäººé–“ãŒæ€ã„ã¤ã‹ãªã„ã‚ˆã†ãªã€Œç•°è³ªãªã€ä»®èª¬ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€ç§‘å­¦ãŒé€²åŒ–ã™ã‚‹ã€ã‹ã‚‚ã€‚ChatGPTã‚’ã¤ã‹ã£ã¦ã€éƒ¨å±‹ã‚’ç‰‡ä»˜ã‘ã¦ã„ã‚‹äººãŒã„ãŸã€ã“ã‚Œã¯ã™ã”ã„å¿œç”¨ã ï¼OECDã®AIã®å®šç¾©ã‚‚ç”ŸæˆAIã‚„åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’é‘‘ã¿ï¼”å¹´ã¶ã‚Šã«æ”¹å®šã€äººã®æŒ‡ç¤ºã«å¾“ã‚ãšã¨ã‚‚ã€å…¥åŠ›ã«å¯¾ã—ã¦è‡ªã‚‰ã®ã¨ã‚‹ã¹ãå‹•ä½œã‚’æ¨æ¸¬ã™ã‚‹ãƒ¡ã‚¿èƒ½åŠ›ã«ã¤ã„ã‚‚æš—ç¤ºã€ã‚‚ã¯ã‚„AIã«å¯¾ã™ã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢çš„ãªå“è³ªä¿è¨¼ã¯ä¸å¯èƒ½ãªäº‹æ…‹ã¸ã€‚

-  Banach-Tarski Embeddings and Transformers
	- https://arxiv.org/abs/2311.09387
	- å†å¸°çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç·šå‹ç©ºé–“ã§ã®è¡¨ç¾ï¼ˆãƒãƒŠãƒƒãƒã‚¿ãƒ«ã‚¹ã‚­åŸ‹ã‚è¾¼ã¿ï¼‰ã‚’è€ƒãˆã‚‹ã¨ãã®è¡¨ç¾ä¸Šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆå¾©å·ï¼‰ãŒTransformerã¨ã—ã¦è‡ªç„¶ã«å®Ÿè£…ã§ãã‚‹ã‚‰ã—ã„
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ„å‘³åˆ†æã«ã‚ˆã‚‹è¾æ›¸è¨˜è¿°ã¸ã®å¿œç”¨
	- https://speakerdeck.com/yhkondo/da-gui-mo-yan-yu-moderuwoyong-itayi-wei-fen-xi-niyoruci-shu-ji-shu-henoying-yong
	- åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰ã®è¾æ›¸ä½œæˆã¸ã®å¿œç”¨ã¨ã‹ã€æ•è‰å­ã‚’é¡Œæã«åŸ‹ã‚è¾¼ã¿ã‚’ã¤ã‹ãŸï½”é¡ä¼¼æ¤œç´¢ã—ã¦ã¿ã‚‹ä¾‹ãŒã€è‹±èªã«ã‚ˆã‚‹æ¤œç´¢ã€çµµæ–‡å­—ã«ã‚ˆã‚‹æ¤œç´¢ã€ã‚¯ãƒªã‚¨ãƒ¼ãƒ†ã‚£ãƒ–ãªæ¤œç´¢ãªã©äº‹ä¾‹ãŒã‚ã£ã¦é¢ç™½ã„
- Shicheng Liu et al., "SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models"
	- https://arxiv.org/abs/2311.09818
	- LLMãƒ™ãƒ¼ã‚¹ã®æ–°ã—ã„è¨€èªã€SUQLã€ãŒé–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚SQLã‚’æ‹¡å¼µã—ã¦ã€Œéæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ã‚¨ãƒªã€ã‚’å‡¦ç†ã™ã‚‹ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å°å…¥
	- ã€SUQLï¼ˆStructured and Unstructured Query Languageï¼‰ã€
	- â‘  æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã‚’æ‰±ã† 
	- â‘¡ SQLã«ã€éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ã‚¨ãƒªã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ã‚’è¿½åŠ  
	- â‘¢ ä¼šè©±å‹æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å‡¦ç† 
	- â‘£ ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ãŠã‚ˆã³éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰æŠ½å‡ºã™ã‚‹
	- å¾“æ¥ã®ç·šå½¢åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚„å¤šæ®µéšæ¤œç´¢ãŠã‚ˆã³æ¨è«–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æ¯”ã¹ã¦ã€SUQLã¯å›åç²¾åº¦ãŒå¤§å¹…ã«é«˜ã„
	- å®Ÿéš›ã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã«é–¢ã™ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚¹ã•ã‚ŒãŸè³ªå•ã¨ä¼šè©±ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿç”¨æ€§ãŒç¢ºèªã•ã‚ŒãŸ
-  Meta disbanded its Responsible AI team
	- https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence
	- metaãŒè²¬ä»»ã‚ã‚‹AIã®ãƒãƒ¼ãƒ ã‚’è§£æ•£ã•ã›ãŸ
- çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€
	- https://www.no-spare.com/store/products/seminar-20231129
	- æœ¬è¬›åº§ã§ã¯ã€é‡‘èæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨ã‚’é¡Œæã«ã€å‹•çš„ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ãƒ»æœ€æ–°ã®ç ”ç©¶ã‚’è§£èª¬ã—ã¾ã™ã€‚
-  Hypotheses devised by AI could find â€˜blind spotsâ€™ in research
	- https://www.nature.com/articles/d41586-023-03596-0
	- AIãŒä»®èª¬ã‚’ç”Ÿæˆã™ã‚‹éš›ã«ç›´é¢ã™ã‚‹èª²é¡Œã¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®ä¸è¶³ã€ç‰©ç†çš„ãªæ³•å‰‡ã®ç†è§£ã€ä»®èª¬ã®ä¸€èˆ¬æ€§ã¨è§£é‡ˆæ€§ãªã©ãŒæŒ™ã’ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
	- AIãŒä»®èª¬ã‚’ç”Ÿæˆã™ã‚‹å¯èƒ½æ€§ã¨ã—ã¦ã€äººé–“ãŒæ€ã„ã¤ã‹ãªã„ã‚ˆã†ãªã€Œç•°è³ªãªã€ä»®èª¬ã‚„ã€å®Ÿé¨“ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€Œãƒ­ãƒœãƒƒãƒˆç§‘å­¦è€…ã€ãªã©ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™
-  Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild
	- https://arxiv.org/abs/2311.06237
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã‚’ã—ã°ãå€’ã—ã¦ã€ç•°å¸¸ãªæŒ¯ã‚‹èˆã„ã‚’ã•ã›ã‚ˆã†ã¨ã—ã¦ã„ã‚‹äººé”ï¼ˆé‡è‰¯ã®LLMãƒ¬ãƒƒãƒ‰ãƒãƒ¼ãƒ ï¼‰ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è«–æ–‡ã€‚æ”»æ’ƒæ–¹æ³•ã‚„ãã‚‚ãã‚‚ä½•ã®ãŸã‚ã«ã‚„ã£ã¦ã„ã‚‹ã®ã‹ï¼Ÿç­‰ã®èª¿æŸ»ã€‚
- ã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã€ã‚²ã‚¹ãƒˆã‚«ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ã€OpanAIã‚’è¨ªå•
	- https://x.com/sama/status/1726345564059832609?s=20
	- first and last time i ever wear one of these
-  ChipNeMo: Domain-Adapted LLMs for Chip Design
	- https://arxiv.org/abs/2311.00176
	- ChipNeMoã¯ãƒãƒƒãƒ—è¨­è¨ˆæ”¯æ´å‘ã‘ã«ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã—ãŸLLMã€‚é–‹ç™ºæ”¯æ´Chatbotã€EDAã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆã€ãƒã‚°è¦ç´„ã¨åˆ†æã‚’è¡Œã†ã€‚æ—¢å­˜LLMã«ã€å°‚ç”¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ãŸå¾Œã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œäº‹å‰äº‹å‰å­¦ç¿’ï¼ˆDAPT 230å„„ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã€æŒ‡ç¤ºå­¦ç¿’ï¼ˆ1000ä¾‹ï¼‰ã‚’ã—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæ¤œç´¢è£œå¼·ã‚’è¡Œã†
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®ãƒŠãƒ‡ãƒ©æ°ã€ã‚¢ãƒ«ãƒˆãƒãƒ³æ°ãŸã¡ãŒãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã«Joinã™ã‚‹ã¨ã€ã€
	- https://x.com/satyanadella/status/1726509045803336122?s=20
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã«ã‚ˆã‚‹ç”ŸæˆAIã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
	- https://github.com/microsoft/generative-ai-for-beginners
	- The free 12 lesson course is available on Github and will teach you everything you need to know to start building Generative AI applications.
-  Learning to Filter Context for Retrieval-Augmented Generation
	- https://arxiv.org/abs/2311.08377
	- RAGã«ãŠã„ã¦ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã™ã‚‹
	- èªå½™ãŠã‚ˆã³æƒ…å ±ç†è«–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é€šã˜ã¦æœ‰ç”¨ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰¹å®šã—ã€ãƒ†ã‚¹ãƒˆä¸­ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒå«ã¾ã‚Œã¾ã™ã€‚
	- FILCO ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã« String Inclusion (STRINC)ã€Lexical Overlapã€Conditional Cross-Mutual Information (CXMI) ãªã©ã®æŠ€è¡“ã‚’ä½¿ç”¨
- æ—¥æœ¬èªå¯¾å¿œ LLM(13B è¦æ¨¡)ã®, è¡Œé–“ã‚’èª­ã‚€ã‚ˆã†ãªã‹ã—ã“ã•ãŒã‚ã‚‹ã‹è©¦ã—ãŸãƒ¡ãƒ¢(ç¾çŠ¶ Qwen 14B ãŒãƒ™ã‚¹ãƒˆ)
	- https://zenn.dev/syoyo/articles/59a5ccbbb5660e
	- 7B ä»¥ä¸‹(10B æœªæº€)ã‚‚è©¦ã—ã¾ã—ãŸãŒ, è¡Œé–“ã‚’èª­ã‚€ã»ã©ã®ã‹ã—ã“ã•ã¯ãªã, 13B è¦æ¨¡ã§é£›èºçš„ã«ã‹ã—ã“ã•ãŒä¸ŠãŒã‚‹æ„Ÿã˜ã ã£ãŸã®ã§, 13 B è¦æ¨¡ã®ã‚’é¸ã‚“ã§ã„ã¾ã™.
	- qwen.cpp(llama.cpp variant)ã§ f16 é‡å­åŒ–ç‰ˆã‚’å‹•ã‹ã—ã¾ã—ãŸ.
	- q4 ã‚ãŸã‚Šã«é‡å­åŒ–ã ã¨ã„ãã‚‰ã‹ã‹ã—ã“ã•è½ã¡ã¾ã—ãŸ(ãã‚Œã§ã‚‚ã»ã‹ã®æ—¥æœ¬èª LLM ã‚ˆã‚Šã‚ˆã„çµæœã‚’ãˆã‚‰ã‚Œã‚‹)  ã¾ãŸ, Qwen7B ã‚‚ã‚ã¾ã‚Šã‹ã—ã“ãã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ.
	- Qwen 14B(Chat) ã¡ã‚ƒã‚“ãŒè¡Œé–“ã‚’èª­ã‚€ã»ã©ã®ã‹ã—ã“ã•ã‚’è¦‹ã›ã¾ã—ãŸ!
- OpenAIãŒNPO+ã§ã‚ã‚‹ã‚ˆã†ãªã“ã¨ãŒã€ä»Šå›ã®ã‚¢ãƒ«ãƒˆãƒãƒ³æ°è§£ä»»ã«ã¤ãªãŒã£ãŸã¨ã®çµµæŸ„
	- https://x.com/GOROman/status/1726701627468546511?s=20
-  Azure OpenAI Service å…¥é–€ by npakaã•ï½
	- https://note.com/npaka/n/n46e6ad252ce1?sub_rt=share_h
	- ã€ŒAzure OpenAI Serviceã€ã§ã€Œgpt-3.5-turboã€ã‚’ä½¿ç”¨ã™ã‚‹æ‰‹é †ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚
-  Orca 2: Teaching Small Language Models How to Reason
	- https://huggingface.co/papers/2311.11045
	- å°ã•ã„ã“ã¨ã¯ã„ã„ã“ã¨ã 
-  Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data
	- https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1
	- llamaindexã®JerryãŒæ”¾ã¤ã€streamlitã‚’ã¤ã‹ã£ãŸã€RAGã‚¢ãƒ—ãƒªç”Ÿæˆãƒ„ãƒ¼ãƒ«RAGs
	- â€œChatGPT over your dataâ€ without needing to code.
- Large-scale pancreatic cancer detection via non-contrast CT and deep learning
	- https://www.nature.com/articles/s41591-023-02640-w
	- ï½¢å˜ç´”CTã®è†µè‡“ãŒã‚“æ¤œå‡ºAIï½£
	- å˜ç´”CTã§ã®è†µè‡“ãŒã‚“æ¤œã¯ä¸å¯èƒ½ã¨è€ƒãˆã‚‰ã‚Œã¦ããŸ 
	- ãã®AIã‚’é–‹ç™º 
	- ç¾å®Ÿä¸–ç•Œã®ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªæ¤œè¨¼ã®ç—…å¤‰æ¤œå‡ºã§ã€92.9%ã®æ„Ÿåº¦ã¨ 99.9% ã®ç‰¹ç•°åº¦ã‚’é”æˆ 
	- è†µè‡“ãŒã‚“ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ã®å¯èƒ½æ€§
-  RAGè©•ä¾¡ãƒ„ãƒ¼ãƒ«ã® "RAGAS" ã‚’ä½¿ã£ã¦ã€RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹
	- https://qiita.com/s3kzk/items/44b8780c656b4f747403
	- ä»Šå›è§¦ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ™‚ã®è¨­å®šä»¥å¤–ã«ã‚‚ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ±ºå®šã€EmbeddingãŠã‚ˆã³å¿œç­”ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMã®é¸å®šã€ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢/æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸å®šãªã©ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦ç´ ã¯æ•°å¤šãå­˜åœ¨ã—ã¾ã™ã€‚
- ã‚¢ãƒ«ãƒˆãƒãƒ³æ°OpenAIã«å¾©å¸°ã™ã‚‹ã¨
	- https://x.com/OpenAI/status/1727206187077370115?s=20
-  2é€±é–“ä½¿ã„å€’ã—ã¦ã‚ã‹ã£ãŸï½¢GPT-4-Turboã®è¡æ’ƒï½£ã€‚OpenAIã®ï½¢ãŠå®¶é¨’å‹•ï½£ã§è¦‹é€ƒã—ã¦ã‚‹å ´åˆã˜ã‚ƒãªã„
	- https://www.businessinsider.jp/post-278766
- AnthropicAIã‚ˆã‚ŠClaude2.1ã®ç™ºè¡¨
	- https://x.com/AnthropicAI/status/1727001773888659753?s=20
	- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã¯ãªã‚“ã¨ 200k ã¨ 2 å€ã«æ‹¡å¤§ã€‚ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ä½æ¸›ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®å¯¾å¿œã€ä¾¡æ ¼ã®å¼•ãä¸‹ã’ã€å¤–éƒ¨APIã¨ã®é€£æºæ©Ÿèƒ½(ãƒ™ãƒ¼ã‚¿ç‰ˆ) ãªã©
	- https://claude.ai/ã€€ã§ãŠè©¦ã—å¯èƒ½
- ChatGPTã§éƒ¨å±‹ã®ç‰‡ã¥ã‘ã‚’ã—ã¦ã„ã‚‹äººãŒã„ã„ã‚‹
	- https://x.com/fjtn_c/status/1727216371711586402?s=20
	- ï¼ˆéƒ¨å±‹ã®å†™çœŸé€ã£ã¦ç‰‡ä»˜ã‘ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã—ã¦ã‚‚ã‚‰ã£ã¦ã€ãã‚Œã‚’å®Ÿè¡Œã—ã¦å†™çœŸæ’®ã£ã¦ã¾ãŸé€²æ—ã‚’é€ã‚‹â†’åŒã˜ã“ã¨ã‚’ç¹°ã‚Šè¿”ã—ï¼‰
- æ„›æ–°è¦šç¾…ã®å­«ï¼ˆå¤§äº•ç”ºã®çœ¼ç§‘åŒ»ï¼‰ã®é©šæ„•ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
	- https://x.com/aishinkakura_i/status/1727477535234248712?s=20
	- å­¦ä¼šã§ã‚¢ãƒ¡ãƒªã‚«ã‚’è¨ªã‚ŒãŸéš›ã€ã‚¤ãƒŸã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã€Œæ¸…æœã®å­å­«ã‹ã€ã£ã¦å°‹å•ã‚’å—ã‘ã€ã—ã°ã‚‰ãè¶³ã‚’æ­¢ã‚ã‚‰ã‚Œâ€¦
- metaã‹ã‚‰ã€Getting started  with Llama
	- https://ai.meta.com/llama/get-started/?utm_source=twitter&utm_medium=organic_social&utm_campaign=llama2&utm_content=image
-  å˜è¡Œæœ¬ãŒå…¥ã‚‹Claude 200kã§åƒ•ã¨ã€Œã‚¨ãƒ´ã‚¡ãƒ³ã‚²ãƒªã‚ªãƒ³ã€
	- https://note.com/shoty/n/n03bff29f683f
	- æ—¥æœ¬èªã ã¨150ãƒšãƒ¼ã‚¸ã„ã‹ãªã„ãã‚‰ã„ãŒèª¿ç†ã§ãã‚‹ã®ã§ã¯ãªã„ã‹ã¨æ€ã†ã€‚ã¤ã¾ã‚Š**å˜è¡Œæœ¬ä¸€å†ŠãŒå…¥ã£ã¦ã—ã¾ã†**
	- ã‚¨ãƒãƒ³ã‚²ãƒªã‚ªãƒ³ã®ç‰©èªã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã§ãã‚‹ã‹ã¨ã„ã†æŒ‘æˆ¦ã‚‰ã—ã„
- ã€DSã«KaggleãŒå¿…ãšã—ã‚‚å¿…è¦ã§ã¯ãªã„è©±ã€‘
	- https://x.com/Nurruttan/status/1727495591905858016?s=20
	- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨è¨€ã£ã¦ã‚‚ã€ ã€Œâ‘ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆå‹ã€ ã€Œâ‘£ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‹ã€ ã®ã‚­ãƒ£ãƒªã‚¢ãƒ—ãƒ©ãƒ³ã§ã¯Kaggleå®Ÿç¸¾ã®é‡è¦æ€§ã¯ä½ã„
	-  ä¸€æ–¹ã§ã€ ã€Œâ‘¡ã‚µãƒ¼ãƒ“ã‚¹ã‚°ãƒ­ãƒ¼ã‚¹å‹ã€ ã€Œâ‘¢è£½å“é–‹ç™ºå‹ã€ ã€Œâ‘¤AIé–‹ç™ºå‹ã€ ã¯é‡è¦åº¦ã¯é«˜ã„ã€‚
- Google Bardã§Youtubeã¨ãƒãƒ£ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«
	- https://bard.google.com/chat
-  ã€ŒPaper Interpreterã€ã‚’ä½¿ã£ã¦è«–æ–‡ã‚’èª­ã‚‚ã†ï¼
	- https://note.com/daichi_konno/n/nb1f1ac368a30
	- æ±å¤§ã®ã€ç´ºé‡å¤§åœ°å…ˆç”Ÿä½œæˆ
	- **ã€Œè«–æ–‡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€å†…å®¹ã‚’æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã‚Œã‚‹AIã€**
- ã‚¢ãƒ«ãƒˆãƒãƒ³æ°é›»æ’ƒè§£ä»»åŠ‡ã®è£ã«ã€OpenAIãŒã€AGIã‚’é–‹ç™ºã™ã‚‹ã‚ã©ãŒã¤ã„ãŸã‹ã‚‰ã¨ã„ã†
	- Q*-learningã¨ã„ã†æ‰‹æ³•ã«ã‚ˆã‚Šã€æ•°å€¤è¨ˆç®—ãªã©LLMãŒè‹¦æ‰‹ã¨ã—ã¦ã„ãŸèª²é¡Œã‚‚è§£ã‘ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚
	- https://x.com/hbouammar/status/1727683545852768295?s=20
	- A*ã£ã¦ã®ã¯æ¢ç´¢ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã‘ã©ã€ãã‚Œã®Q-learningç‰ˆã¨ã„ã†è©±
- Intelè¬¹è£½ã®ã€LLMãŒã€ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã§ä¸Šä½ã®æ€§èƒ½ã‚’ã¯ã˜ãå‡ºã™
	- https://x.com/Yampeleg/status/1727679553714217421?s=20
	- https://huggingface.co/Intel/neural-chat-7b-v3-1
	- A 7B model from Intel almost as capable as Falcon 180B:ã“ã‚Œã¯æœ¬å½“ã‹ï¼ï¼ï¼
	- Base model: Mistral 7B. 
	- Fine Tuned on: SlimOrca 
	- DPO: LLaMA-13B vs ChatGPT Gens (Prefer ChatGPT)
- An Embodied Generalist Agent in 3D World
	- https://huggingface.co/papers/2311.12871
	- ï¼“Dä¸–ç•Œã®ä¸­ã§èº«ä½“æ€§ã‚’ã‚‚ã£ãŸæ±ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
	- 3Dä¸–ç•Œã«å¯¾ã—ã¦ã€ã„ã‚ã°è¨˜å·æ¥åœ°ã™ã‚‹ã‚ˆã†ãªè¨“ç·´ã‚’ã™ã‚‹ã“ã¨ã§èº«ä½“æ€§(embodiment)ã‚’å–å¾—ã€è‡ªç„¶è¨€èªå‡¦ç†ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãªã©ã®å¤šæ§˜ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã§æ±ç”¨çš„ãªã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã§ãã‚‹æ±ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ§‹ç¯‰ã§ããŸã¨ã„ã†
	- æ‰‹æ®µã¨ã—ã¦ã¯ã€3Dä¸–ç•Œã®ç†è§£ã¨ç›¸äº’ä½œç”¨ã‚’å¿…è¦ã¨ã™ã‚‹ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã¨ã‚·ãƒ¼ãƒ³ãƒ¬ãƒ™ãƒ«ã®å¤šãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã‚’å«ã‚€ã€è¦æ¨¡ã¨è¤‡é›‘ã•ã«å„ªã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ…é‡ã«ä½œæˆ
-  å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã‚’LoRAã§å¼·åŒ–ã™ã‚‹éš›ã«å½¹ç«‹ã¤æƒ…å ±ã‚’ç ”ç©¶è€…ãŒå…¬é–‹
	- https://gigazine.net/news/20231123-llm-lora/
	- LoRAã¯ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚„å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã«è¿½åŠ ã®æƒ…å ±ã‚’å­¦ç¿’ã•ã›ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã§ãã‚‹ä»•çµ„
	- **â—†LoRAã®åŠ¹æœã«ã¯ä¸€è²«æ€§ãŒã‚ã‚‹**
	- **â—†QLoRAã‚’ä½¿ãˆã°è¿½åŠ å­¦ç¿’æ™‚ã®VRAMä½¿ç”¨é‡ã‚’å¤§å¹…ã«ç¯€ç´„å¯èƒ½**
	- **â—†æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯Adamã§ã‚‚SGDã§ã‚‚å¤§å·®ãªã„**
	- **â—†LoRAã«ã‚ˆã‚‹è¿½åŠ å­¦ç¿’ã‚’ç¹°ã‚Šè¿”ã™ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹**
	- **â—†LoRAã«ã‚ˆã‚‹è¿½åŠ å­¦ç¿’ã¯å˜ä¸€ã®GPUã§å®Ÿè¡Œå¯èƒ½**
- Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall
	- Claude2.1ã®é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèƒ½åŠ›ã«å¯¾ã™ã‚‹ã€ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
	- https://x.com/GregKamradt/status/1727018183608193393?s=20
	- 200K ãƒˆãƒ¼ã‚¯ãƒ³ (ç´„ 470 ãƒšãƒ¼ã‚¸) ã§ã€Claude 2.1 ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€éƒ¨ã®æ·±ã•ã§äº‹å®Ÿã‚’æ€ã„å‡ºã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ 
	- æ–‡æ›¸ã®ä¸€ç•ªä¸Šã¨ä¸€ç•ªä¸‹ã«ã‚ã‚‹äº‹å®Ÿã¯ã»ã¼ 100% ã®ç²¾åº¦ã§å†ç¾ã•ã‚Œã¾ã—ãŸ 
	- æ–‡æ›¸ã®ä¸Šéƒ¨ã«ã‚ã‚‹äº‹å®Ÿã¯ä¸‹éƒ¨ã‚ˆã‚Šã‚‚ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã§ãƒªã‚³ãƒ¼ãƒ«ã•ã‚Œã¾ã—ãŸ (GPT-4 ã¨åŒæ§˜) 
	- ~90,000 ãƒˆãƒ¼ã‚¯ãƒ³ä»¥é™ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸‹éƒ¨ã«ã‚ã‚‹ãƒªã‚³ãƒ¼ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã¾ã™ã¾ã™æ‚ªåŒ–ã—å§‹ã‚ã¾ã—ãŸ 
	- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒçŸ­ã„å ´åˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ
- Why do tree-based models still outperform deep learning on typical tabular data?
	- https://hal.science/hal-03723551
	- Why do tree-based models still outperform deep learning on tabular data?â€ confirms tree-based models outperform deep learning and explain some of the reasons why.
	- When it comes to #tabulardata and #timeseries (by far the most important majority of data for almost any real company), deep learning is not one needs. 
- Pythonã«ã‚ˆã‚‹ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ³•å…¥é–€: åŸºç¤ç†è«–ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åŒåŒ–ã®å®Ÿè£…ã¾ã§
	- https://www.amazon.co.jp/dp/4621308882?_encoding=UTF8&psc=1&ref_=cm_sw_r_tw_ud_dp_RW79QAZKZRQ7K9N885XB
	- ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ³•ã«ãŠã„ã¦ã‚‚,å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦ç‰©æ€§å€¤ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã—ã¤ã¤,ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã‚’é«˜ã‚ã‚‰ã‚Œã‚‹ã‚ˆã†ãª,ãƒ‡ãƒ¼ã‚¿åŒåŒ–ã¨èåˆã—ãŸæ‰‹æ³•ã®é–‹ç™ºãŒé€²ã‚“ã§ã„ã‚‹.ãã“ã§æœ¬æ›¸ã§ã‚‚,ãƒ‡ãƒ¼ã‚¿åŒåŒ–ã®åŸºç¤ã‹ã‚‰ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ¢ãƒ‡ãƒ«ã¸ã®å®Ÿè£…æ–¹æ³•ã¾ã§ã‚ã‚ã›ã¦ç´¹ä»‹ã™ã‚‹.
	- ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ³•ã§ã¯ã€ç§©åºå¤‰æ•°ã®æ‹¡æ•£æ–¹ç¨‹å¼ã¨åå¿œæ–¹ç¨‹å¼ã‚’åŒæ™‚ã«è§£ãã“ã¨ã§ã€çµ„ç¹”å½¢æˆéç¨‹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚æ‹¡æ•£æ–¹ç¨‹å¼ã¯ã€ç§©åºå¤‰æ•°ãŒæ‹¡æ•£ã™ã‚‹éš›ã®æŒ™å‹•ã‚’è¨˜è¿°ã™ã‚‹æ–¹ç¨‹å¼ã§ã™ã€‚åå¿œæ–¹ç¨‹å¼ã¯ã€ç›¸ã®å¤‰åŒ–ã‚’è¨˜è¿°ã™ã‚‹æ–¹ç¨‹å¼ã§ã™ã€‚
	- ãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ³•ã¯ã€é‡‘å±ã®å‡å›ºã€å¤šçµæ™¶ç²’æˆé•·ã€æ‹¡æ•£ç›¸å¤‰æ…‹ãªã©ã€ã•ã¾ã–ã¾ãªææ–™çµ„ç¹”å½¢æˆéç¨‹ã®è¨ˆç®—ã«ç”¨ã„ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€å¿œåŠ›å ´ã‚„é›»ç£å ´ã«ãŠã‘ã‚‹çµ„ç¹”å½¢æˆã‚„ãƒŠãƒã‚¹ã‚±ãƒ¼ãƒ«ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«åŒ–ãªã©ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ãƒãƒ«ãƒãƒ•ã‚£ã‚¸ãƒƒã‚¯ã‚¹ã‚’å¯¾è±¡ã¨ã—ãŸç¨®ã€…ã®å·¥å­¦åˆ†é‡ã«ã‚‚å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
- ã€Œãƒã‚¹ã‚¿ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã®è‘—è€…ã€Domingosæ°ã€Q*-learningã®åŠ¹æœã‚’ã¿ã¦ã€äººé¡ã®çµ‚ç„‰ã‚’å«ã¶
	- https://x.com/pmddomingos/status/1727562239060656339?s=20
	- Q* can solve simple math problems that symbolic AI could solve 50 years ago. Panic! AGI is here! Humanity is over!
- A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases
	- https://arxiv.org/abs/2311.07509
	- impact of KGs for question answering on SQL databases: 54% accuracy vs. 16% with instructions directly on SQL databases.
	- SQL DBã‚’å‚ç…§ã—ã¦è³ªå•å¿œç­”ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€LLMã«ç›´æ¥SQLã‚’å‚ç…§ã•ã›ã‚‹ã¨16%ã®æ­£è§£ç‡ã—ã‹å‡ºãªã‹ã£ãŸãŒLLMã‚’ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ãã‚Œã‚’å‚ç…§ã•ã›ã‚‹ã¨54%ã«æ”¹å–„ã—ãŸã¨ã„ã†ç ”ç©¶ã€‚
	- æœ¬è³ªçš„ã«æŒã£ã¦ã„ã‚‹æƒ…å ±ãŒåŒã˜ã§ã‚‚ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ã‚ˆã£ã¦RAGã®ç²¾åº¦ãŒå¤‰ã‚ã‚‹ã“ã¨ã®ä¸€ä¾‹ã¨ã‚‚ã¿ãªã›ã‚‹
- ã†ã¿ã‚†ãæ°ã€Claude2.1ã®æ€§èƒ½ã«èˆŒã‚’å·»ã
	- https://x.com/umiyuki_ai/status/1727875985167790529?s=20
	- Claudeç„¡æ–™ç‰ˆè©¦ã—ã¦ã¿ãŸã‘ã©ã€çµæ§‹é•·æ–‡ã®æ—¥æœ¬èªpdfå…¥åŠ›ã—ã¦è¦ç´„ã—ã¦ã£ã¦ãŠé¡˜ã„ã—ãŸã‚‰ã€ã¡ã‚ƒã‚“ã¨å†…å®¹èª­ã‚“ã§è¦ç´„ç®‡æ¡æ›¸ãå‡ºã—ã¦ãã‚ŒãŸï¼ˆç›®æ¬¡ä¸¸å†™ã—ã§ã¯ãªã„ï¼‰ã€€ï¼“ç« ã®å†…å®¹èª¬æ˜ã—ã¦ã£ã¦è¨€ã£ãŸã‚‰ã¡ã‚ƒã‚“ã¨èª¬æ˜ã—ã¦ãã‚ŒãŸã€‚ã¤ã¾ã‚Šã¡ã‚ƒã‚“ã¨æœ€å¾Œã¾ã§èª­ã‚“ã§ç­”ãˆã¦ã‚‹ã€‚ã‹ãªã‚Šçš„ç¢ºãªå¿œç­”ã‚’è¿”ã—ã¦ãã‚Œã‚‹ã€‚ãã‚Œã§ã‚¿ãƒ€ã€‚ã“ã‚Œç›¸å½“ã‚¹ã‚´ã‚¤ã­
- Yuhan Sun et al., "To be or not to be? an exploration of continuously controllable prompt engineering"
	- https://arxiv.org/abs/2311.09773
	- ã“ã‚Œã¾ã§ã€ŒLLMã®å‹•ãã‚’è¦³å¯Ÿã—ã¦"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿ç¯€"ã™ã‚‹ã€æ‰‹æ³•ãŒè¿½ç©¶ã•ã‚Œã¦ãã¾ã—ãŸãŒã€é™ç•ŒãŒã‚ã‚‹ãŸã‚ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹"LLMã®å‹•ãã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«èª¿æ•´"ã™ã‚‹ã€æ‰‹æ³•ã€ControlPEã€
	- è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ãªã©ã‚’æ‰‹æ›ã‘ã‚‹ã‚»ãƒ³ã‚¹ã‚¿ã‚¤ãƒ ç¤¾ã«ã‚ˆã‚‹
	- ControlPEã¯ç«¶åˆæŠ€è¡“ã¨æ¯”è¼ƒã—ã¦ã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å½±éŸ¿ã‚’ã“ã¾ã‹ãèª¿æ•´ã§ãã‚‹æ‰‹æ³•
	- â‘  LoRAã‚’åˆ©ç”¨ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ â‘¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å½±éŸ¿ã‚’é€£ç¶šçš„ã«å¾®èª¿æ•´ â‘¢ å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’è£œå®Œã™ã‚‹
- Q*ã®ã‚‚ã¨ã‚‚ã¨ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ºã—ãŸè«–æ–‡è‘—è€…ãŒè‡ªè«–æ–‡ã‚’å®£ä¼
	- https://x.com/McaleerStephen/status/1727524295377596645?s=20
	-  A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks
	- https://arxiv.org/abs/2102.04518
- Q*ã«ã¤ã„ã¦è‘—åãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆErnest Okumuraã•ã‚“ã®ã‚³ãƒ¡ãƒ³ãƒˆ
	- https://x.com/pacocat/status/1728052432016470281?s=20
	- Q*ãŒQ-learningã‹ã‚‰æ¥ã¦ã„ã‚‹ã‹ã¯çŸ¥ã‚‰ãªã„ã‘ã‚Œã©ã‚‚ã€åˆ¶ä½œè€…ã«ã¨ã£ã¦å¥½ã¾ã—ã„å‡ºåŠ›ã‚’å¾—ã‚‹ãŸã‚ã«æ–¹ç­–ç©ºé–“ã‚’æ¢ç´¢ã™ã‚‹æŠ€è¡“ã¯ä»Šå¾Œã•ã‚‰ã«æ±‚ã‚ã‚‰ã‚Œã¦ã„ãã¨æ€ã†ã—ã€RLHFã¿ãŸã„ãªåˆ†ã‹ã‚Šã‚„ã™ã„ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚’è¶…ãˆã¦AGIã¿ãŸã„ãªæ–‡è„ˆã§ã‚‚é‡å¿ƒçš„ãªè©¦ã¿ã¯å¢—ãˆã¦ãã‚‹ã‚“ã˜ã‚ƒãªã„ã§ã—ã‚‡ã†ã‹ã€‚
- Sparse Transformersï¼šå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã«ã‚ˆã‚‹è¨ˆç®—é‡å¢—åŠ å•é¡Œã¸ã®é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
	- https://ai-scholar.tech/articles/transformer/sparseTransformer
	- Attentionã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã®ç‰¹å¾´ã‚’å†ç¾ã™ã‚‹ã“ã¨ã§ï¼Œè¨ˆç®—é‡ã®å‰Šæ¸›ã‚’é”æˆ  
	- Sliding Window Attenionã€Dilated Sliding Window Attentionã€Global Attentionã¨ã„ã†3ã¤ã®Attentionã‚’ä½¿ã£ã¦Transformernã®è¨ˆç®—é‡ã‚’å‰Šæ¸›ã—ãŸ  
	- è¨ˆç®—é‡ã‚’å‰Šæ¸›ã—ãŸã ã‘ã§ã¯ãªãã¦ï¼Œå½“æ™‚ã®SOTAã‚’é”æˆã—ã¦ã„ã‚‹ï¼
-  Llemma: An Open Language Model For Mathematics
	- https://arxiv.org/abs/2310.10631
	- ã©ã†ã‚‚ã€LLMã‚’ã¤ã‹ã£ã¦ã€å®šç†è¨¼æ˜å™¨ã‚’ã¤ã‹ã†pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã‚‰ã—ã„ã€‚å®Ÿéš›ã«èª¬ãã®ã¯pythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ãƒ¼ï¼‹å®šç†è¨¼æ˜å™¨ã®çµ„ã¿åˆã‚ã›ã€‚
	- The AlgebraicStack dataset of 11B tokensãŒæä¾›ã•ã‚Œã‚‹
	- Llema can solve mathematical problems using a Python interpreter and a formal theorem prover.
- LlamaIndex vs. OpenAI Assistants API
	-  RAG Evaluation Series: Validating the RAG Performance of OpenAI vs LlamaIndex
	- https://www.tonic.ai/blog/rag-evaluation-series-validating-rag-performance-openai-vs-llamaindex
- ChatGPTã‚¢ãƒ—ãƒªã®éŸ³å£°ä¼šè©±ãŒç„¡æ–™ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚‚é–‹æ”¾
	- https://x.com/IELTS_expert/status/1728326991676670222?s=20
	- è‹±èªå­¦ç¿’ã‚½ãƒ•ãƒˆã‚„æœ‰æ–™ãƒ¬ãƒƒã‚¹ãƒ³ãŒä¸è¦ã«
- JARVIS-1ã¯æœ¬å½“ã¯ã™ã”ã„ã€
	- https://x.com/ai_database/status/1728257353852797143?s=20
	- ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆï¼ˆåºƒå¤§ãªãƒãƒ¼ãƒãƒ£ãƒ«ä¸–ç•Œã§æ¡æ˜ã‚„å»ºè¨­ã‚’è¡Œã†ã‚²ãƒ¼ãƒ ï¼‰ã‚’ä¸Šæ‰‹ã«ãƒ—ãƒ¬ã‚¤ã™ã‚‹AIã€JARVIS-1ã€ãŒé–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚ éå¸¸ã«è¤‡é›‘ãªå‹•ä½œã‚’å«ã‚€200ç¨®é¡ä»¥ä¸Šã®è¡Œå‹•ãŒå¯èƒ½ã¨ã®ã“ã¨ã€‚
	-  ã“ã®ã‚ˆã†ãªæŠ€è¡“ã‚’å¿œç”¨ã™ã‚‹ã¨ã€ãƒ­ãƒœãƒƒãƒˆãŒç¾å®Ÿä¸–ç•Œã§ã‚‚ã•ã¾ã–ã¾ãªé‡è¦ã‚¿ã‚¹ã‚¯ã‚’é”æˆã§ãã‚‹ã‚ˆã†ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚â€¦
- æœ€çµ‚çš„ã«ã™ã¹ã¦ã®çµ±è¨ˆã¯ãƒ™ã‚¤ã‚ºã«è¡Œãç€ãã—ã‹ãªã„ã¨æ€ã£ã¦ã„ã¾ã™ï¼ˆçµ±è¨ˆæ•°ç†ç ”ç©¶æ‰€ã€éŒè°·æ°ï¼‰
	- https://www.ism.ac.jp/ism_info_j/labo/project/162.html
- ãƒ«ã‚«ãƒ³å…ˆç”Ÿã«ã‚ˆã‚‹Q*ã«å¯¾ã™ã‚‹è¡¨æ˜
	- https://x.com/ylecun/status/1728126868342145481?s=20
	- ã€ŒQ*ã«é–¢ã™ã‚‹å®Œå…¨ãªãƒŠãƒ³ã‚»ãƒ³ã‚¹ã®æ´ªæ°´ã¯ç„¡è¦–ã—ã¦ã­ã€‚LLMã®ä¿¡é ¼æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ä¸»ãªèª²é¡Œã®1ã¤ã¯ã€è‡ªå·±å›å¸°çš„ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã™ã€
- Macã§llama2ã‚’è©¦ã™ãŸã‚ã®swift-chat
	- https://github.com/huggingface/swift-chat
	- Llama 2 7B chat, running 100% private on Mac, powered by CoreML!
	- Pedro Cuencaã•ã‚“ã¯ç¾åœ°æ™‚é–“2023å¹´08æœˆ08æ—¥ã€Apple Silicon Macãªã©Appleãƒ‡ãƒã‚¤ã‚¹ä¸Šã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®Swiftãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨Demoã‚¢ãƒ—ãƒªã‚’å…¬é–‹
	- Swiftã§Transformersãƒ©ã‚¤ã‚¯ãªAPIã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«é–‹ç™ºã—ãŸSwiftãƒ‘ãƒƒã‚±ãƒ¼ã‚¸â€swift-transformersâ€ã¨ã€Demoã‚¢ãƒ—ãƒªâ€swift-chatâ€ã€åŠ ãˆã¦Transformersãƒ¢ãƒ‡ãƒ«ã‚’CoreMLã¸å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼â€transformers-to-coremlâ€ã§ã€
	- CoreMLãŒå½¹ã«ç«‹ã£ãŸã¨ã€ã€
- OECDã€ç”ŸæˆAIã‚„åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒæ…®ã—ã¤ã¤AIã®å®šç¾©ã‚’æ”¹å®šã€
	- https://www.euractiv.com/section/artificial-intelligence/news/oecd-updates-definition-of-artificial-intelligence-to-inform-eus-ai-act/
	- æ¬§å·AIæ³•ãªã©ã®ä»–ã®è¦åˆ¶ã¨ã®æ•´åˆæ€§ã‚‚è€ƒæ…®ã—ãŸã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨ã¨ã£ãŸ
	- ç›®æ¨™ã‚’äººé–“ãŒå®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ã„ã†äº‹å®Ÿã¸ã®è¨€åŠã‚’å‰Šé™¤ã€
	- ã€Œå‡ºåŠ›ã®ç”Ÿæˆæ–¹æ³•ã‚’æ¨æ¸¬ã™ã‚‹ã€ã¨ã„ã†æ–‡è¨€ã‚‚ã€AI ãƒ¢ãƒ‡ãƒ«ãŒç’°å¢ƒã‹ã‚‰å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€1 ã¤ä»¥ä¸Šã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é€šã˜ã¦é©åˆ‡ãªå‡ºåŠ›ã‚’æ€ã„ã¤ãã¨ãã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«å°å…¥

## 11/20

ä»Šé€±ã¯ã€OpenAIã®CEOã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã®é›»æ’ƒè§£ä»»ãŒå…¨ã¦ã‚’æŒã£ã¦è¡Œã£ãŸã€‚å…ˆé€±OpenAI dev dayã§é›„å§¿ã‚’ã€ãã—ã¦äººé¡ã®æœªæ¥ã‚’å£é–“è¦‹ãŸã®ã«ã€‚ã€‚ãƒœãƒ¼ãƒ‰ã‹ã‚‰å¾©å¸°ã®è¦è«‹ã‚‚ã‚ã‚‹ã¨ã„ã†ã—ã€ã¾ã ã¾ã ç¾åœ¨é€²è¡Œå½¢ã€‚ã•ã¦ã€RAGã‚‚embeddingã‚’ã¤ã‹ã£ãŸé¡ä¼¼æ¤œç´¢ã‚ˆã‚Šã‚‚æ§‹é€ ã‚’åŠ å‘³ã—ãŸæ¤œç´¢ã¨ã‹ã€å¤šæ§˜æ€§ã‚’ã‚‚ã¤æ¤œç´¢çµæœã®åˆ©ç”¨ã¨ã‹ã€ã ã‚“ã ã‚“ã€æ¨è–¦æŠ€è¡“ãªã©ã§ç¢ºç«‹ã•ã‚ŒãŸãƒã‚¦ãƒã‚¦ãŒæ´»ç”¨ã•ã‚Œå§‹ã‚ãŸã€‚LlamaIndexã®æ–°æ©Ÿèƒ½ã€text-to-SQL+semanticã£ã¦ã®ãŒã„ã„ã­ã€‚LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–¢ä¿‚ã‚‚ã«ãã‚„ã‹ã€å˜ã«è«–ç†ã‚½ãƒ«ãƒãƒ¼ã‚’å¤–éƒ¨ã«ã‚‚ã£ã¦ã¦ã€è‡ªç„¶è¨€èªã‹ã‚‰ã‚½ãƒ«ãƒãƒ¼ã«æ¸¡ã™è«–ç†å¼ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã‚Šã‚‚ã€ã‚½ãƒ«ãƒãƒ¼ã®ãƒ­ã‚°ã‚’ãã®ã¾ã¾ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ã£ã¦ã€è§£ãè¡Œç‚ºãã®ã‚‚ã®ã‚’æ¨¡æ“¬ã™ã‚‹ã¨ã„ã†LoGiPTã¨ã‹ã€çµæ™¶æ§‹é€ ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¾ã—LLaMA-2ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€VAEã‚’ä¸Šå›ã£ãŸã¨ã„ã†äº‹ä¾‹ã¨ã‹ãŒã‚ã‚‹ã€‚ãã‚‚ãã‚‚ã§ã™ã‚ã­ã€æ–°ã—ã„OpenAIã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€200å€‹ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã€ãŠå¬¢æ§˜LLMãã‚‰ã„ã¯ã§ãã‚‹ã¿ãŸã„ã§ã”ã–ã„ã¾ã™ã§ã™ã€‚LLMã¯ãã®ãƒ¡ã‚¿ãªèƒ½åŠ›ã‚‚é‡è¦ãªè¦ç´ ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚’ä½œã‚‹ãƒ¡ã‚¿ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã¤ãã£ãŸã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’LLMãŒç†è§£ã‚„ã™ã„ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‹ã€ã“ã£ã¡æ–¹é¢ã®ãƒ¡ã‚¿ãªä¸–ç•Œã‚‚ã„ã„æ„Ÿã˜ã§ç™ºå±•ã—ã¦ã„ã‚‹ã€‚ï¼ˆã¡ã‚‡ã£ã¨è¦–ç‚¹ã‚’å¤‰ãˆãŸï¼‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨LLMã®ãƒ¡ã‚¿èƒ½åŠ›ã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒLLMæ´»ç”¨ã®æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‹ã€‚create-llamaã¨ã‹ã€OpenGPTã¨ã‹ã€LLMA Factoryã¨ã€è‡ªå‹•çš„ã«ã‚¢ãƒ—ãƒªã‚’ä½œã‚‹ä»•çµ„ã¿ãŒãŸãã•ã‚“å‡ºã¦ããŸã€‚ ã‚ãšã‹1åˆ†ã§10æ—¥é–“ã®å¤©æ°—ã‚’äºˆæ¸¬å¯èƒ½ãªAIã€ŒGraphCastã€ã€ãŠèŒ¶ã®æ°´å¤§å­¦ã®ç¥å±±å…ˆç”Ÿã®è§£èª¬ãŒã€å¾“æ¥ã®æ‰‹æ³•ãŒä¸å¾—æ„ãªã¨ã“ã‚ã«Graph transformerãŒã´ã£ãŸã‚Šåˆã£ãŸã¨ã„ã†ã¨ã“ã‚ãŒè…¹è½ã¡ã—ã¾ã™ã€‚Microsoftã®ç™ºè¡¨ã—ãŸCopilotã€ã¤ã¾ã‚ŠGPTsã®ï¼­ï¼³ç‰ˆã€‚ã“ã†ã„ã†ä¸–ç•Œè¦³ã«ãªã‚‹ã‚ˆãªã€‚æ—©é€ŸOpenCopilotã¨ã‹ã€WebCoPilotã¨ã‹ã€ã‚ã£ã¨ã„ã†ã¾ã«ã€ä¼¼ãŸã‚ˆã†ãªOSSãŒã€ã€ã€ã€‚YahooçŸ¥æµè¢‹ã€ã¤ã„ã«GPT-4ã‚’ã¤ã‹ã£ãŸè‡ªå‹•å›ç­”ã‚’ãƒ†ã‚¹ãƒˆä¸­ã€‚äººã®è¡†çŸ¥ã¯ChatGPTã«æ•—ã‚ŒãŸã®ã‹ã€‚ã€‚ï¼­ï¼£æ¥­ã®ç´—ã€…æ°ã€NTTæ­¦è”µé‡é€šç ”ã§é–‹å‚¬ã•ã‚ŒãŸR&Dãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§ã€AIåŒ–ã•ã‚Œã‚‹ã€ï¼­ï¼£æ¥­ã‚‚ï¼¡ï¼©ã«ä»£æ›¿ã•ã‚Œã‚‹ï¼Ÿã•ã‚Œãªã„ï¼Ÿã¾ã‚ã€ChatGPTã§ä»•äº‹ãŒãªããªã£ãŸã®ã¯ã€ChatGPTã®CEOã‚‚ä¾‹å¤–ã§ã¯ãªã„ã¨ã„ã†ã®ã¯ãƒ–ãƒ©ãƒƒã‚¯ã‚¸ãƒ§ãƒ¼ã‚¯ã‹ã‚‚ã€‚

- Adding Structure-Aware Retrieval to GenAI Stack
	- https://medium.com/@yu-joshua/adding-structure-aware-retrieval-to-genai-stack-373976de14d6
	- å˜ãªã‚‹embeddingã‚’ã¤ã‹ã£ãŸé¡ä¼¼æ¤œç´¢ã®RAGã§ã¯ãªãã¦ã€æ§‹é€ ã‚’æŠ½å‡ºã—ãŸã†ãˆã§ã®ã€RAGã£ã¦ã®æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ã€neo4j+LangChainã®å®Ÿä¾‹ã§ç¤ºã—ãŸè‰¯ä¾‹
	- This stack is (1) fully local, (2) uses advanced retrieval methods that encode relationships between different chunks of texts
- LlamaIndex ã«ã‚ˆã‚‹OpenAIã®æ–°æ©Ÿèƒ½ã‚’ä½¿ç”¨ãƒ»ç†è§£ã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ by npakaã•ã‚“
	- https://note.com/npaka/n/n728fdb8f76da?sub_rt=share_sb
	- Parallel Function Callingã€Assistant API Agentã€Function Callingã«ã‚ˆã‚‹é«˜åº¦ãªRAGã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAG
	- GPT Builderã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•æ€§ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€GPTã‚’ç”Ÿæˆã™ã‚‹metaãªãƒ„ãƒ¼ãƒ«
	- ã€Œtext-to-SQL ã¨ semantic search ã®ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã€ãªã‚“ã‹ã¯èˆˆå‘³æ·±ã„
- æ—¥æœ¬ã®å¥³æ€§ãŒå…ˆé€²å›½ã®ä¸­ã§é•·å‘½ãªã®ã¯ã€ç¤¾ä¼šé€²å‡ºãŒé€²ã¾ãªã‹ã£ãŸã‹ã‚‰ï¼Ÿ
	- æ—­ãƒªã‚µãƒ¼ãƒ
	- https://arc.asahi-kasei.co.jp/report/arc_report/pdf/rs-824.pdf
	- ã€Œå…ˆé€²å›½ã®ä¸­ã§ã¯å¥³æ€§ã®ç¤¾ä¼šé€²å‡ºãŒé€²ã¾ãªã‹ã£ãŸã“ã¨ãŒã€ ä¸–ç•Œä¸€ã®å¥³æ€§é•·å¯¿ã«çµã³ã¤ã„ãŸã¨æ€ã‚ã‚Œã‚‹ã€‚ã€ 
	- ã€Œå‡ç­‰æ³•ã¯å¥³æ€§ã®å¹³å‡å¯¿å‘½ã‚’çŸ­ç¸®ã•ã›ã‚‹è¦å› ã§ã‚ã‚‹ã€‚ã€
- gpt-3.5-turbo-1106ã‚’ä½¿ã£ãŸã€æ–°ã—ã„OpenAIã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
	- https://x.com/matsu_vr/status/1723688378795958670?s=20
	- ã§ãŠå¬¢æ§˜ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ãŸã€‚200ä¾‹ã®ä¼šè©±ã§ååˆ†ãŠå¬¢æ§˜ã«ãªã£ãŸï¼
- Boosting RAG: Picking the Best Embedding & Reranker models
	- https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83
	- RAGã‚’ã‚„ã‚‹ã«ã‚ãŸã£ã¦ã©ã‚Œã‚’ä½¿ãˆã°ã‚ˆã„ã‹ã‚’èª¿ã¹ãŸãƒ–ãƒ­ã‚°ã€‚OpenAI ChatGPTã‚„Google PaLMãªã©ã§ä½œã£ãŸ embeddings ã¨ BAAI ç­‰ãŒæä¾›ã—ã¦ã„ã‚‹ reranker ã§ã€ã©ã®çµ„ã¿åˆã‚ã›ãŒç²¾åº¦ãŒè‰¯ã„ã‹
- OpenAI Dev dayã‚’å—ã‘ãŸã€llamaindexã®ãƒã‚¤ãƒ¬ãƒ™ãƒ«APIã®ã‚¢ãƒ—ãƒ‡ã¾ã¨ã‚
	- https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p
	- å¤§å¤‰ã ãƒ¼
-  Prompt Engineering a Prompt Engineer
	- https://huggingface.co/papers/2311.05661
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚’ä½œã‚‹ãƒ¡ã‚¿ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œã‚‹ã¨ã„ã†è©±ã€LLMã£ã¦ãƒ¡ã‚¿èƒ½åŠ›ãŒã‚ã‚‹ã®ã§ã€ã“ã†ã„ã†è©¦ã¿ãŒå¯èƒ½ã€‚CoTè¶Šãˆã¨ã„ã†ã®ã¯æœ¬å½“ã‹ï¼Ÿ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’LLMãŒè¨€ã„æ›ãˆã¦ã€LLMè‡ªèº«ãŒç†è§£ã—ã‚„ã™ãã™ã‚‹æ‰‹æ³•ã€RaRã€
	- https://aiboom.net/archives/51160
	- ä¾‹ãˆã°ã€ŒGPT-4ã§è¨€ã„æ›ãˆã¦GPT-3.5ã§å…¥åŠ›ã™ã‚‹ã€ã‚‚æœ‰åŠ¹ã¨ã®ã“ã¨ã§ã™ã€‚ å®Ÿè¡Œãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚„æ€§èƒ½ç­‰ã‚’è©³ã—ãç´¹ä»‹ã™ã‚‹è¨˜äº‹ã‚’å…¬é–‹ã—ã¾ã—ãŸ
-  Language Models can be Logical Solvers
	- https://huggingface.co/papers/2311.06158
	- å¾“æ¥SOTAã¯ã€solver-augmented language modelsã‚’ã¤ã‹ã£ã¦ã€è‡ªç„¶è¨€èªã‹ã‚‰ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’å–ã‚Šå‡ºã—ã¦ã€å¤–éƒ¨ã‚½ãƒ«ãƒãƒ¼ã§èª¬ã„ã¦ã„ãŸãŒã€ã€æ–‡æ³•ãŒã‚ã£ã¦ãªã„ã¨ã‹ãã†ã„ã†ä¸‹ã‚‰ãªã„ã‚¨ãƒ©ãƒ¼ã«æ‚©ã¾ã•ã‚Œã¦ããŸ
	- LoGiPTã¯ã€ç›´æ¥è«–ç†çš„ãªå°å‡ºã‚’ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã€æ—¢å­˜ã‚½ãƒ«ãƒãƒ¼ã®ãƒ­ã‚°ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã€‚å•é¡Œã¯è§£æ±ºã•ã‚ŒãŸ
	- https://x.com/IntuitMachine/status/1724104506185580589?s=20
-  JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models
	- https://arxiv.org/abs/2311.05997
	- JARVISã£ã¦ç¢ºã‹ã€ã‚¢ã‚¤ã‚¢ãƒ³ãƒãƒ³ã®ã‚µãƒãƒ¼ãƒˆAIã®åå‰ã§ã¯ï¼Ÿï¼Ÿ
- äººé–“ã®æƒ…å ±å‡¦ç†ã«ã¨ã£ã¦ã€Œã¡ã‚‡ã†ã©ã„ã„å¡©æ¢…ã€ã®é€Ÿåº¦ã‚’è¶…ãˆã¨ã‚‹æ°—ãŒã™ã‚‹ by è°·ãƒãƒ¥ãƒ¼
	- https://x.com/rmaruy/status/1724044250286108818?s=20
	- Buonomanoã€è„³ã¨æ™‚é–“ã€ã«ã‚ˆã‚Œã°ã€è„³ã«ã¯å˜ä¸€ã®ã‚¯ãƒ­ãƒƒã‚¯ã¯ãªã„ï¼ˆå¤šé‡æ™‚è¨ˆåŸç†ï¼‰ã€‚ãŒã€é€²åŒ–ã®éç¨‹ã§ç”Ÿç‰©ãŒç›¸æ‰‹ã«ã—ã¦ããŸæ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã‚ˆã‚Šå¤§å¹…ã«é€Ÿã„æƒ…å ±å‡¦ç†ã¯ã§ããªã„ã ã‚ã†ã€‚ä¸€æ–¹ã€æƒ…å ±ã®ã€Œé‡ã€ã«é–¢ã—ã¦ã¯ã¾ã å·¥å¤«ã§ãã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚
- DPOã§calm2ã®ç‰©èªç”Ÿæˆèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹è©¦ã¿ã€
	- https://x.com/_oshizo_/status/1724039980463657130?s=20
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§LLMãŒæ–‡å­—ã‚’ç”Ÿæˆã™ã‚‹æ§˜å­ã®ãƒ‡ãƒ¢ã€
	- https://x.com/dylfreed/status/1723927399857901724?s=20
	- llamacppã‚’ã¤ã‹ã£ã¦8GB RAM MacBook Airã§å‹•ãã‚“ã ã¨ã•
- LLMã£ã¦çµå±€ä½•ã‹ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«èª¬æ˜ã™ã‚‹
	- https://x.com/davidad/status/1723990400682148124?s=20
	- ãƒ‡ã‚£ãƒ¼ãƒ— ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ« ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€å„å±¤é–“ã«è¦ç´ ã”ã¨ã®éç·šå½¢æ€§ã‚’æŒã¤ç·šå½¢å›å¸°ã®ã‚µãƒ³ãƒ‰ã‚¤ãƒƒãƒæ§‹é€ ã§ã™ã€‚LLM/GPT ã®çˆ†ç™ºçš„ãªå¢—åŠ ã«ç›´æ¥ã¤ãªãŒã£ãŸã€ŒAttending is All You Needã€ã®æ ¸ã¨ãªã‚‹è²¢çŒ®ã€ãã“ã« *ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯* å›å¸°ã‚’éç·šå½¢å±¤ã«æŠ•ã’è¾¼ã‚€ã“ã¨ã§ã™ã¾ãŸã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã«ã¤ã„ã¦ã¯@geoffreyhinton ã€æ´»æ€§åŒ–æ­£è¦åŒ–ã«ã¤ã„ã¦ã¯@ChrSzegedy ã€ãŠã‚ˆã³å‹¾é…æ­£è¦åŒ–ã«ã¤ã„ã¦ã¯@dpkingmaã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ (Adam)ã€‚
- ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å‹•ã‹ã™PCã‚’è‡ªä½œ
	- https://note.com/ai_meg/n/n8855a8dd4bbd?sub_rt=share_pb
	- ãƒã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰ï¼šAsrokã€€B760 PRO RS/DS  
	- CPUï¼ši5-13400F  
	- GPU:PALITã€€GFORCE-RTX4060ti-16G
- RETOOLã®State of AIãƒ¬ãƒãƒ¼ãƒˆ
	- https://retool.com/reports/state-of-ai-2023
	- 66% of companies have at least one AI use case live
	- Accuracy is #1 concern
	- RAG is 2nd most popular use case (1st is code)
	- @llama_index is one of the leading frameworks for enterprises 
- OpenGPTã¯ã©ã‚“ã©ã‚“é€²åŒ–ã™ã‚‹
	- https://github.com/langchain-ai/opengpts
-  The Alignment Handbook
	- https://github.com/huggingface/alignment-handbook
	- Robust recipes to align language models with human and AI preferences
- EditGPT
	- https://chat.openai.com/g/g-zpuYfzV7k-editgpt
	- Grammeryã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’æŒã¤GPTsãŒã€ã€
- å²¡é‡åŸã•ã‚“ã®ã€ã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã€ãŒä»Šå¹´åº¦ã®å¤§å·å‡ºç‰ˆè³ã«é¸å‡º
	- https://hillbig.github.io/diffusion-models/
	- http://www.okawa-foundation.or.jp/activities/publications_prize/list.html
- ã“ã‚Œã¯è¡æ’ƒ!1.5Bã§è¶…é«˜æ€§èƒ½LLM!RWKV-5-World-v2 by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/nfc8dd1abf494?sub_rt=share_pb
	- ã¾ã ç”Ÿãã¦ãŸã®ã‹ã€RWKV
- The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4
	- https://arxiv.org/abs/2311.07361
	- Evaluates GPT-4â€™s knowledge base, scientific understanding, scientific numerical calculation abilities, and various scientific prediction capabilitie
	- MSã‹ã‚‰ã®è«–æ–‡ã€è£½è–¬ã¨ã‹ã®è©±ãŒå¤šã„ãŒã€ãªã‚“ã‹ã¤ã¾ã‚‰ã‚“
- Open AIä¸»ä»»ç§‘å­¦è€…ã®Ilya Sutskeveræ°ã¯æ˜¨æ—¥ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã«ã¦ã€AGIã«ãŸã©ã‚Šç€ããŸã‚ã«ã¯Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‹Î±ã§ã€Œæ˜ã‚‰ã‹ã«ã€å•é¡Œãªã„ã¨
	- https://www.youtube.com/watch?v=Ft0gTO2K85A
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®Fine-tuningã«ã‚ˆã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ç²å¾—ã®æ¤œè¨
	- https://tech.preferred.jp/ja/blog/llm-fine-tuning-for-domain-knowledge/
	- è‹±èªã§ä¸»ã«å­¦ç¿’ã•ã‚ŒãŸLLaMA2ã«å¯¾ã—ã¦æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸInstruciton Tuningã‚„è¿½åŠ äº‹å‰å­¦ç¿’ãŒã©ã®ç¨‹åº¦å¯èƒ½ã‹ã®æ¤œè¨¼
	- ä¸å¯æ€è­°ãªçµæœãŒå‡ºãŒã¡ãªã®ã§ã€ã„ã‚ã‚“ãªè¨­å®šã§è©¦ã•ãªã„ã¨ã„ã‘ãªã„ã“ã¨ãŒã‚ã‹ã£ãŸ
- LangChainã‹ã‚‰ã€Query Construction Guideã€text-to-SQL+semanticæœ€å¼·ç¯€
	- https://blog.langchain.dev/query-construction/
	- 1. Structure+unstructured data:  Text-to-SQL+semantic (w/ PostgresSQL with the Pgvector 
	- 2. Unstructured w/ metadata: Text-to-metadata filters (w/ new docs + a template for self-query retriever)
	- "Text-to-SQL+semantic" is an interesting recent addition to LangChain that extends "Text-to-SQL" w/ semantic queries on an embedding column.
	- ãã†ã‹ã€ã‚„ã£ã±ã‚Š text-to-SQL+semantiãŒæœ€å¼·ãªã®ã‹
- ã€Chain of Empathyï¼ˆå…±æ„Ÿã®é€£é–ï¼‰ã€
	- Yoon Kyung Lee et al., "Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models"
	- å¿ƒç†ç™‚æ³•ã®ã‚»ã‚ªãƒªãƒ¼ã‚’åæ˜ ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã€Chain of Empathyï¼šCoEã€ã‚’é–‹ç™ºã—ã€ãã®æ€§èƒ½ã‚’æ¤œè¨¼
-  Fine-Tuned Language Models Generate Stable Inorganic Materials as Text
	- https://openreview.net/forum?id=0r5DE2ZSwJ
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹çµæ™¶æ§‹é€ äºˆæ¸¬
	- çµæ™¶æ§‹é€ ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¾ã—LLaMA-2ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€VAEã®å¾“æ¥æ‰‹æ³•ã‚ˆã‚Šã‚‚å®‰å®šãªçµæ™¶æ§‹é€ ã‚’ç”Ÿæˆã§ããŸ
	- ã“ã®æ‰‹ã®æ‰‹æ³•ã¯ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã«ãŠé‡‘ã¨æ™‚é–“ãŒã‹ã‹ã‚‹ã¨ã“ã‚ãŒèª²é¡Œ
- create-llama, a command line tool to generate LlamaIndex apps
	- https://blog.llamaindex.ai/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191
	- ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§llamaindexã‚’ã¤ã‹ãŸãŸã‚¢ãƒ—ãƒªã‚’ç”Ÿæˆã™ã‚‹ä»•çµ„ã¿ã®å…¬é–‹ï¼ï¼ï¼
- GPT4ãªã©ãŒã€å¸¸è­˜ã‚’ã‚‚ã£ã¦ã„ã‚‹ã‹ã©ã†ã‹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹è©•ä¾¡
	- https://github.com/allenai/everyday-things
	- The LLMs have poor accuracy (54-59%) on commonsense spatial/functional relationships in ParRoT dataset.
	- This suggests the LMs do not have fully coherent conceptual pictures of everyday objects.
- LLMA Factory
	- https://github.com/hiyouga/LLaMA-Factory
	- Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM)
- WebPilot
	- https://chat.openai.com/g/g-pNWGgUYqS-webpilot
	- è¨˜äº‹ã‚„è«–æ–‡ã€PDF ãªã©ã®æŠ½å‡ºç³»ã®ä¾¿åˆ© GPTs ã‚’ä½œã£ãŸã‘ã‚Œã©ã€ã™ã¹ã¦ WebPilot ã§ååˆ†ã ã£ãŸ(ã‚ã“ã‚ã“ã•ã‚“)
- beã•ã‚“ã€æ¯æ—¥ãƒ™ãƒ«ãƒãƒ³æ–¹ç¨‹å¼ã‚’è§£ã„ã¦æ—¥å¸¸ã‚’éã”ã—ã¦ã„ã‚‹ã¨ã€
	- https://x.com/behemuhemulove/status/1724408454348194303?s=20
- ã€HELP MEã€‘Assistants APIã§ç ´ç”£ã—ãã†ã«ãªã£ãŸè©±
	- https://note.com/nike_cha_n/n/n65a6101d59d7
	- ã¡ã‚ƒã‚“ã¨è¨ˆç®—ã—ãªã„ã¨ã‚ã£ã¨ã„ã†é–“ã«ä¸Šé™ã«é”ã™ã‚‹ã‹ã‚‚ã€
- Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion
	- https://arxiv.org/abs/2311.06318
	- MSã‚ˆã‚Š
	- Microsoft Research presents a method to personalize LLMs for search via entity-based user knowledge stores derived from logs.
- YahooçŸ¥æµè¢‹ã€GPT-4ã‚’ç”¨ã„ãŸã€è‡ªå‹•å›ç­”ã‚’ãƒ†ã‚¹ãƒˆä¸­
	- https://chiebukuro.yahoo.co.jp/topic/ai/answer.html
	- äººçŸ¥ã¯ä¸è¦ã«ãªã£ãŸã®ã‹ã€‚ã€‚
-  Trusted Source Alignment in Large Language Models
	- https://huggingface.co/papers/2311.06697
- GPT paper asistantã®ã‚½ãƒ¼ã‚¹
	- https://github.com/tatsu-lab/gpt_paper_assistant
	- ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ã®æ©‹æœ¬å…ˆç”Ÿè¬¹è£½
- Licheng Wen et al., "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving"
	- https://arxiv.org/abs/2311.05332
	- è¦–è¦šã‚’æ‰‹ã«ã—ãŸLLMãŒè‡ªå‹•é‹è»¢ã«ã©ã‚Œã»ã©å½¹ç«‹ã¤ã®ã‹ã‚’æ¢ã‚‹ãŸã‚ã€GPT-4Vã®èƒ½åŠ›ãŒæ¤œè¨¼ã•ã‚Œã¾ã—ãŸã€‚ 
	- ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“ã—ãŸã¨ã“ã‚ã€ã€Œå› æœé–¢ä¿‚ã®æ¨è«–ã€ã‚„ã€Œã‚·ãƒ¼ãƒ³ï¼ˆæ™¯è‰²ï¼‰ã®ç†è§£ã€ã«é•·ã‘ã¦ã„ã‚‹ã¨çµè«–ã¥ã‘ã‚‰ã‚Œã¾ã—ãŸã€‚
- ã†ã‚‹ã•ã„ã‚„ã¤ã€æŠ€è¡“ã‚’ç†è§£ã—ãªã„ã¨ã€ãƒ“ã‚¸ãƒã‚¹å±•é–‹ã®ãã£ã‹ã‘ãŒå‡ºã¦ã“ãªã„ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚’è”‘è¦–ã—ã¦ã€ãã‚Œã‚’å•†å£²ã«ã—ã¦ã„ã‚‹ã®ãŒå«Œã„ã€‚
	- https://x.com/toukatsujin/status/1724196831109017964?s=20
	- ã€ŒæŠ€è¡“åŠ›ã‚’ç£¨ã‹ãªã„ã¨ç”Ÿãæ®‹ã‚Œãªã„ã¨æ€ã£ã¦ã„ã‚‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒã»ã¨ã‚“ã©ã€‚ã§ã‚‚æŠ€è¡“ã¯æ—¥ã€…é€²åŒ–ãƒ»å¤‰åŒ–ã—ã¦ãŠã‚Šã€ã“ã‚Œã‚’å­¦ã¹ã°ä¸€ç”Ÿå®‰æ³°ã¨ã„ã†ã“ã¨ã¯ãªã„ã€‚ã‚€ã—ã‚ãƒ“ã‚¸ãƒã‚¹ç†è§£åŠ›ã‚’ç£¨ã„ãŸã»ã†ãŒä¸€ç”Ÿå®‰æ³°ãªã®ã«ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å¤šãã¯åˆ†ã‹ã£ã¦ã„ãªã„ã€
- Rapidly build an application in Gradio power by a Generative AI Agent
	- https://cloud.google.com/blog/products/ai-machine-learning/rapidly-build-an-application-in-gradio-power-by-a-generative-ai-agent?hl=en
	- Gradio ã®ä½œè€…ã®åˆã‚ã¦ã®è«–æ–‡ã¨ã„ã†ã†ã‚ã•ã‚‚
- ChatGPTã¨DeepLã®å­—å¹•ç¿»è¨³ã®æ¯”è¼ƒ
	- https://x.com/gijigae/status/1724345403234193540?s=20
	- ChatGPTã¯ã€â‘ è‹±èªå­—å¹•ã‚’ç¹‹ãç›´ã™ â‘¡æ—¥æœ¬èªã«è¨³ã™ â‘¢è¨³ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæµã‚Œã«ãªã‚‹ã‚ˆã†ã«åˆ†ã‘ã€å…ƒã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¸æˆ»ã™ ã¨ã„ã£ãŸä¸€é€£ã®ä½œæ¥­ã‚’å…¨éƒ¨ã‚„ã£ã¦ãã‚Œã‚‹ã€‚
- GPTsã¨Asistant APIã®é•ã„
	- https://x.com/gijigae/status/1724428173905989945?s=20
	- GPTsã¨Assistants APIã¯ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸChatGPTãŒä½œã‚Œã‚‹ç‚¹ã§ä¼¼ã¦ã„ã‚‹ã€‚ãŸã ã€ChatGPT Plusã¸ã®åŠ å…¥ã‚„ã‚¹ãƒ†ãƒ¼ãƒˆç®¡ç†ã‚’å«ã‚ã€é•ã„ã‚‚å¤šã„â†“ã€‚å¿™ã—ãã¦ä¸€ã¤ã—ã‹è©¦ã›ãªã„ã¨ã„ã†æ–¹ã«ã¯å¾Œè€…ã‚’ãŠå‹§ã‚ã—ãŸã„ã€‚ç‰¹ã«ã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸChatGPTã‚’ç”Ÿå¾’ã«å…¬é–‹ã™ã‚‹éš›ã€ChatGPT Plusã¸ã®åŠ å…¥ãŒä¸è¦ã¨ãªã‚‹ã®ã¯å¤§ãã„ã€‚
- ã€Œè¡¨è±¡ï¼ˆrepresentationï¼‰ã€æ¦‚å¿µã‚’åˆ†æã™ã‚‹RPPFãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
	- ç¥çµŒç§‘å­¦ãªã©ã§å¤šç”¨ã•ã‚Œã‚‹ãŒæ›–æ˜§ã§å•é¡Œå«ã¿ã®ã€Œè¡¨è±¡ï¼ˆrepresentationï¼‰ã€æ¦‚å¿µã‚’ã€20ï½30åã®å“²å­¦è€…ã¨ç¥çµŒç§‘å­¦è€…ã§åˆ†æã™ã‚‹ã€ŒRepresentation: Past, Present and Future (RPPF) projectã€
	- https://www.thetransmitter.org/representation/what-are-we-talking-about-clarifying-the-fuzzy-concept-of-representation-in-neuroscience-and-beyond/
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»è£œå®Œã«ç‰¹åŒ–ã—ãŸæ—¥æœ¬èªLLMã€ŒELYZA-japanese-CodeLlama-7bã€ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ˆå•†ç”¨åˆ©ç”¨å¯ï¼‰
	- https://note.com/elyza/n/n5bce23d7c9c8
	- https://zenn.dev/elyza/articles/fcbf103e0a05b1
- ã‚ãšã‹1åˆ†ã§10æ—¥é–“ã®å¤©æ°—ã‚’äºˆæ¸¬å¯èƒ½ãªAIã€ŒGraphCastã€ã‚’Google DeepMindãŒç™ºè¡¨ã€ã‚¹ãƒ‘ã‚³ãƒ³ã§æ•°æ™‚é–“ã‹ã‘ãŸäºˆæ¸¬ã‚ˆã‚Šé«˜ç²¾åº¦
	- https://gigazine.net/news/20231115-google-graphcast-global-weather-forecasting/
	- https://github.com/google-deepmind/graphcast
- RAG over Governments Document
	- https://github.com/deptofdefense/LLMs-at-DoD/blob/main/tutorials/Chatting%20with%20your%20Docs.ipynb
- GGUF ç‰ˆã® 5 bit é‡å­åŒ–ã•ã‚ŒãŸ Llama 2 ã‚’ WasmEdge ã§ã€‚7B ãŒ 24 token / sec ã§å‹•ä½œã—ã¾ã—ãŸâ†“
	- https://www.secondstate.io/articles/fast-llm-inference/
	- Mac ãƒ¦ãƒ¼ã‚¶ã¯è¦‹ãŸã‚‰ã¨ã‚Šã‚ãˆãšè©¦ã—ã¦ã€‚ã‚³ãƒãƒ³ãƒ‰ï¼”è¡Œå©ãã ã‘ãªã®ã§ï¼Rust x Wasm ã§ Llama 2 æ¨è«–ãŒãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ãã¾ã™
- ELYZA-japanese-CodeLlama-7b-instructã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆ
	- https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã¯ Copilot Studio ã‚’ç™ºè¡¨
	- Igniteã®ä¸­ã§æœ€å¾Œã«ç™ºè¡¨ã€GPTsã¿ãŸã„ãªã‚‚ã®ã«ãªã‚‹
	- 
- ãŠèŒ¶å¤§ã€ç¥å±±å…ˆç”Ÿã«ã‚ˆã‚‹ã€Googleã®æ°—è±¡äºˆæ¸¬ã®æ°—è±¡å­¦è€…ã‹ã‚‰ã®è§£é¡Œ
	- https://x.com/kohyama_met/status/1724986380546408878?s=20
	- ã€ŒAIæ°—è±¡äºˆå ±è«–æ–‡ã€ã®æ„Ÿæƒ³ã‚’æŠ•ç¨¿ã—ãŸã‚‰æ€ã„ã®ã»ã‹åéŸ¿ãŒå¤§ãã‹ã£ãŸã®ã§ã€æ°—è±¡å­¦è€…ã‹ã¤æƒ…å ±ç§‘å­¦ç§‘æ•™å“¡ã¨ã—ã¦ã€ã„ãã‚‰ã‹çœŸé¢ç›®ã«è§£èª¬ã—ã¾ã™ã€‚
	- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¾“æ¥å‹ãƒ¢ãƒ‡ãƒ«ã®ä¸å¾—æ‰‹ã«ã†ã¾ããƒãƒã£ã¦ã„ã‚‹
- Research Assistantã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒå…¬é–‹ã•ã‚Œã‚‹
	- https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant
	- With this template you can easily plug in an arbitrary retriever, allowing you to do research over a knowledge base of your choice.
- æ±å¤§ æ¾å°¾ç ”ã®PRMLï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨æ©Ÿæ¢°å­¦ç¿’ï¼‰è¼ªèª­ä¼šã‚¹ãƒ©ã‚¤ãƒ‰é›†
	- https://www.slideshare.net/matsuolab/
	- é»„è‰²ã„æœ¬ã¯ã‚„ã£ã±ã‚Šã€è–å…¸
- OpenCopilot
	- https://github.com/openchatai/OpenCopilot
- tldrawãŒæ´’è½ã«ãªã‚‰ãªã„ãã‚‰ã„å„ªã‚Œã¦ã„ã‚‹
	- https://makereal.tldraw.com/
	- ãƒ©ãƒ•ãªUIã®å›³è§£ã‚„èª¬æ˜ã‚’ã¤ãã‚‹ã ã‘ã§ã€GPT-4Vã§èªè­˜ã—ã¦è‰¯ã„æ„Ÿã˜ã«ä»•æ§˜ã‚’è§£é‡ˆã—ã¦å®Ÿéš›ã«å‹•ããƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œã£ã¦ãã‚Œã‚‹
- ç´—ã€…æ°ã€NTTæ­¦è”µé‡é€šç ”ã§é–‹å‚¬ã•ã‚ŒãŸR&Dãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§ã€AIåŒ–ã•ã‚Œã‚‹
	- https://x.com/03sasa03/status/1725479562094755951?s=20
-  OpenAI announces leadership transition
	- https://openai.com/blog/openai-announces-leadership-transition
	- ãˆã£ï¼
	- ã€Œå–ç· å½¹ä¼šã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ä¸€è²«ã—ã¦ç‡ç›´ã•ã‚’æ¬ ãã€å–ç· å½¹ä¼šã®è²¬ä»»é‚è¡Œã‚’å¦¨ã’ã¦ã„ã‚‹ã€
- OpenAIã‹ã‚‰è¿½ã„å‡ºã•ã‚ŒãŸç›´å¾Œã®ã€Sam Altomanã®ãƒ„ã‚¤ãƒ¼ãƒˆ
	- https://x.com/sama/status/1725742088317534446?s=20
	- i love you all.
- è¥¿æµ¦å…ˆç”Ÿã®è«–æ–‡ã«ã€ç­‘æ³¢å¤§ã®æ›è°·æ°ãŒã‹ã¿ã¤ãã‚‚ã€çµ±è¨ˆã®å°‚é–€åŒ–ã‹ã‚‰è¿”ã‚Šè¨ã¡ã«
	- https://x.com/behemuhemulove/status/1725749314000175387?s=20
	- ä¸»ãªå•é¡Œç‚¹ (1) ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è©•ä¾¡ãªã— (2) çŸ­æœŸäºˆæƒ³ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’ç¹‹ãåˆã‚ã›ã¦é•·æœŸã®ã‚·ãƒŠãƒªã‚ªã‚’ä½œã£ã¦ã„ã‚‹ æ˜æ—¥ã®å¤©æ°—ã‚’é«˜ç¢ºç‡ã§å½“ã¦ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã‚‚ã€ãã®äºˆæƒ³ã‚’ç¹‹ãåˆã‚ã›ã€ã€ã€
	- beæ°ã‚ˆã‚Šã€ï¼ˆ2ï¼‰ã¯ä¾‹ãˆã°1å¹´å¾Œã®äºˆæ¸¬ã™ã‚‹ã¨ã—ã¦1ãƒ¶æœˆãšã¤äºˆæ¸¬ã—ã¦ãã®ã‹ã€å¹´å˜ä½ã§äºˆæ¸¬ã—ã¦ãã‹ä½ã®é•ã„ã—ã‹ãªãã€çµ±è¨ˆå­¦ã‚„MLã§ã¯å…¨ãå•é¡Œãªã„ã¨æ€ã†ã®ã§ã€ã“ã®ç‚¹å©ã„ã¦ã‚‹æ–¹ãŒçµ±è¨ˆå­¦ã®è¦³ç‚¹ã‹ã‚‰ç„¡çŸ¥ã«ã¿ãˆã‚‹
-  create-llama ã«ã‚ˆã‚‹LlamaIndexã‚¢ãƒ—ãƒªã®ä½œæˆ by npakaã•ã‚“
	- https://note.com/npaka/n/neafa42455864?sub_rt=share_h
- ä½“è»¸ãŒç›´ç«‹ã—ãŸæ™‚ç‚¹ãŒäººé¡ãŒè‡ªå·±ã‚’èªè­˜ã—ãŸåˆ†å²ç‚¹ã‹ã‚‚ã—ã‚Œãªã„
	- https://x.com/daijapan/status/1725841037086892358?s=20
	- èªçŸ¥ç§‘å­¦è¬›åº§ã‚ˆã‚Šã€
- Building Research Assistant	
	- https://www.youtube.com/watch?v=DjuXACWYkkU
	- YouTube tutorial on building one from scratch. Covers LCEL, LangSmith, parallelization, retrievers
- Ilya Sutskeverã£ã¦èª°ãï¼Ÿ
	- https://x.com/mr_bay_area/status/1725808417376473167?s=20
	- ã€Œè‡ªç„¶è¨€èªå‡¦ç†æ¥­ç•ŒãŒæ·±å±¤å­¦ç¿’ä¸€è‰²ã«ãªã‚‹æµã‚Œã‚’æ±ºå®šã¥ã‘ãŸäººã€ã§ã™ã­ã€‚ãã‚Œãã‚‰ã„å½¼ãŒä½œã£ãŸseq2seqã¯è¡æ’ƒã ã£ãŸã—
- :smile:ã€:ikanai:
	
## 11/13

ä»Šé€±ã¯ã€OpenAI Dev Day(11/6)ãŒå…¨ã¦ã‚ã‚Šã€LLMå‘¨ã‚Šã®é¢¨æ™¯ãŒä¸€å¤‰ã—ãŸã€‚GPT-4 Turboã‚„Assistant APIã‚„ã€ä¾¡æ ¼ã®æ”¹å®šï¼ˆå®‰ããªã£ãŸï¼‰ã€æœ€å¾Œã«ç‹¬è‡ªã®GPTã‚’ã¤ãã‚Œã‚‹GPT Builderã¨ã€OpenAI ã¾ã‚ã‚Šã®OSSã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã‚’ç ´å£Šã™ã‚‹ãŒã”ã¨ãã®æ€’æ¶›ã®ãƒªãƒªãƒ¼ã‚¹ã€‚å¯¾å¿œã™ã‚‹OSSå´ã®LangChainã‚„llamaindexã‚‚æ–°æ©Ÿèƒ½ã®å–ã‚Šè¾¼ã¿ã‚„å¯¾æ¡ˆå®Ÿè£…ã§å¿™ã—ã„é€±ã ã£ãŸã€‚Assistant APIã£ã¦ã€**Code Interpreter**ã€**Retrieval**ã€**Function Calling**ã€€ãŒå‘¼ã³å‡ºã›ã€APIã‹ã‚‰ã‚‚ä½œã‚Œã‚‹ã‘ã©ã‚‚ã€playgroundã‹ã‚‰ã‚‚ä½œã£ã¦ç°¡å˜ã«è©¦ã›ã‚‹ã€‚Assistant APIã«å®Ÿè£…ã•ã‚ŒãŸæ©Ÿèƒ½(Assistants/Theads/Run )ã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚ç°¡å˜ã«ä½œã‚Œã‚‹ã€‚è©³ã—ãã¯Nakajimaã•ã‚“ã®GPTvsGPTãŒè‰¯ã„ä¾‹ã€‚ç„¡é™ã«ç’°å¢ƒå•é¡Œã«ã¤ã„ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒå£«ãŒè¨è«–ã™ã‚‹ã¨ã„ã†ãƒ‡ãƒ¢ã¯ã¡ã‚‡ã¨åœ°ç„çµµã€‚æ—©é€Ÿã€LangChainã‚‚ã€LlamaIndexã‚‚ã€Assistant APIã‚’ã¤ãã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚‹æ©Ÿèƒ½ã‚’å…¬é–‹ã€ã‚‚ã¨ã‚‚ã¨ã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨çµ„ã¿åˆã‚ã›ã¦ã¿ãŸã„ãªç™ºå±•ã‚‚ã€‚OpenAI ã®Retreiveæ©Ÿèƒ½ã¯ã€pdfã‚„docã‚„pptã‚„markdownç­‰å¤šå½©ãªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã‚“ã§ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦Chatã§ãã‚‹æ©Ÿèƒ½ã€‚ã¾ã•ã«ã€RAGã¤ã¶ã—ãªã‚“ã ã‘ã©ã‚‚ã€llamaindexã®äººJerry Liuã«ã‚ˆã‚‹ã¨ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã®é™ç•Œã‚’è¶…ãˆã‚‹ã¨æ™®é€šã®top-kå¼ã®å˜ç´”ãªRAGãŒå‹•ã„ã¦ã„ã‚‹ã®ã§ã¯ã¨ã„ã†ã“ã¨ã€‚è©¦ã—ã«ãƒŠã‚¦ã‚·ã‚«(Wikipediaã€57kãƒˆãƒ¼ã‚¯ãƒ³)ã‚’GPT-4ã§ã‚„ã£ã¦ã¿ãŸã‚‰ã€ç¢ºã‹ã«æ€§èƒ½ã‚ˆã‹ã£ãŸã€‚RAGã«ã¤ã„ã¦ã¯è‡ªã‚‰ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®æ–¹æ³•ãªã©ã®ï¼‰ç´°ã‹ã„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«èµ°ã‚‹ã‹ã€ãã‚Œã¨ã‚‚å…¥ã‚Šå£ã ã‘ç”¨æ„ã—ã¦ã‚ã¨ã¯ã€åˆ¥ã®OSSç­‰ã«ã¨ã„ã†æˆ¦ç•¥ã®ã©ã¡ã‚‰ã ã‚ã†ï¼ŸGPT-4ã‚‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸãŒã€$3M(ï¼•å„„å††å¼±)ã®[Submit]ãƒœã‚¿ãƒ³ã¯æŠ¼ã›ãªã„ã€‚ã€‚GPT-4ã‚’åŠç«¯ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã‚‚æ€§èƒ½ã¯å‘ä¸Šã—ãªã„ã¨ã„ã†ã®ã‚‚ã™ã”ã„ãªã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆæ”¯æ´ã‚‚ã€llamaindexã‹ã‚‰builder agentã€Langchainã‹ã‚‰ã‚‚ã€OpenGTPãŒç™ºè¡¨ã€‚OpenAIæœ¬å®¶ã‚‚GPTsã§ã€å¥½ã¿ã®GPTã‚’ä½œã£ã¦å…¬é–‹ã¨ã„ã†æ©Ÿèƒ½ãŒå…¬é–‹ã€Plusãƒ¦ãƒ¼ã‚¶ãƒ¼ãªã‚‰ä»–äººã®GPTã‚’ä½¿ã†ã“ã¨ã‚‚ã§ãã‚‹ã€‚ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«ã€ã©ã‚“ã©ã‚“ã€ç‹¬è‡ªã®GPTãŒå…¬é–‹ã•ã‚Œã¦ã€ã¾ã•ã«ç™¾èŠ±ç¹šä¹±ã€ã“ã‚Œã«åˆ©ç”¨æ–™ã‚’é‚„æµã™ã‚‹ä»•çµ„ã¿æ•´ãˆã°ã€ã¾ã•ã«ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ—ãƒ¬ãƒ¼ã‚¹çµŒæ¸ˆåœã«ä¸€ç›´ç·šã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®RAGã¨ã„ã†ã®ã‚‚å‡ºã¦ããŸã€‚PFNã®PLaMo-13B-Instructã®å…¬é–‹ã‚„ã€æ—¥æœ¬èªå‘ã‘ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æ”¹å®šã‚„ã€shi3zã•ã‚“ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³æ—¥æœ¬èªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ•´å‚™ãªã©ã€æ—¥æœ¬èªå¯¾å¿œã®æ”¹è‰¯ã‚‚ç€å®Ÿã«é€²ã‚“ã§ã„ã‚‹ã€‚ã€Œã‚¢ãƒŠãƒ­ã‚¸ã‚¢ AIã®æ¬¡ã«æ¥ã‚‹ã‚‚ã®ã€ã®ãƒ€ã‚¤ã‚½ãƒ³ã«ã‚ˆã‚‹ã¨ã€LLMã¯ã€ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚‹AIã®é™ç•Œã‚’è¶…ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ï¼‰ã‚¢ãƒŠãƒ­ã‚°ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«è¿‘ã„ã‚‚ã®ã‚‰ã—ã„ã€‚ãƒ€ã‚¤ã‚½ãƒ³ã®æœ¬ã‚’èª­ã¿ãªãŠã™ã¨ã€AGIã®å¯èƒ½æ€§ã«ã¤ã„ã¦ã‚‚ã€ãƒ‡ã‚¸ã‚¿ãƒ«ã§ã¯åˆ°é”ã§ããªã„ãŒã€ã‚¢ãƒŠãƒ­ã‚°ãªã‚‰ã°å¯èƒ½æ€§ã¯æ’é™¤ã§ããªã„ã¿ãŸã„ãªä¸»å¼µã ã£ãŸã€‚æœ€å¾Œã«ã€ChatGPTã®ç™»å ´ã¯ã€ãƒ‡ã‚¶ã‚¤ãƒŠã‚„ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã®è·ã‚’å¥ªã†ã ã‘ã§ãªãã€å˜ä¾¡ã‚‚ä¸‹ã’ãŸã€ç‰¹ã«é«˜åå…¥ã®å±¤ã‚’ã€ã¨ã„ã†FTã®è¨˜äº‹ãŒæ€–ã™ãã‚‹ã€‚

- ALMA-7B-Ja-V2
	- https://huggingface.co/webbigdata/ALMA-7B-Ja-V2
	- ç¿»è¨³ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®ALMA-jaã®V2æ¥ã¨ã‚‹ï¼!GPTQã‚‚ã‚ã‚‹
- TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT
	- Microsoftã‹ã‚‰ç™ºè¡¨ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚¿ã‚¹ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€Œãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã™ã‚‹ãƒ¢ãƒ‡ãƒ«Table-GPT
	- å¤šæ§˜ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚¿ã‚¹ã‚¯ã«ã¦GPT-3.5ã‚„ChatGPTã‚ˆã‚Šé«˜æ€§èƒ½ã€é«˜ã„æ±ç”¨æ€§ã‚’ç¤ºã™
	- https://arxiv.org/abs/2307.08674
- OpenAI dev day
	- GPT-4 Turbo ç™ºè¡¨ 
	- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·128k
	- JSON Mode 
	- ãƒŠãƒ¬ãƒƒã‚¸ã‚«ãƒƒãƒˆã‚ªãƒ• 2023/04
	- DALL E-3 / Text to Speech 
	- Whisper v3 
	- GPT-4 Fine-tuningå¯èƒ½ã«
	- GPT-3.5 Turbo ã¯ã‚‚ã† 16K ãŒãƒ‡ãƒ•ã‚©ãƒ¬ãƒ™ãƒ«ã§ã•ã‚‰ã«å®‰ããªã‚Šã€GPT-4 Turbo ã¯ä¾¡æ ¼ãŒå…¥åŠ› 1/3, å‡ºåŠ› 1/2 ã«ãªã£ãŸ
	- ã€Œå¾“æ¥ã®16å€ã¨ãªã‚‹300ãƒšãƒ¼ã‚¸ã‚’è¶…ãˆã‚‹é•·ã„æ–‡æ›¸ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã€2023å¹´4æœˆã¾ã§ã®æƒ…å ±ã‚’åæ˜ ã€
	- functionsã¨function_callãŒéæ¨å¥¨ã«ãªã£ã¦toolsã¨tool_choiceã«ãªã£ãŸã‚“ã 
- ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§ã€ŒChatGPTã€ã®ã‚«ã‚¹ã‚¿ãƒ ç‰ˆã‚’ä½œã‚Œã‚‹ã€ŒGPTsã€ã€æœ‰æ–™ä¼šå“¡ã«æä¾›ã¸
	- https://www.itmedia.co.jp/news/articles/2311/07/news074.html
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã®æŒ‡ç¤ºã§å¯¾è©±ã—ãªãŒã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ChatGPTã‚’æ§‹ç¯‰ã§ãã‚‹ã€‚ã€ŒWebæ¤œç´¢ã‚„ç”»åƒä½œæˆã€ãƒ‡ãƒ¼ã‚¿åˆ†æãªã©ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã€ã¨ã—ã¦ã„ã‚‹
- MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning
	- https://huggingface.co/papers/2311.02303
	- MFTcoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder fine-tuned model, CodeFuse-CodeLLama-34B, achieves an impressive pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4 performance (67\%, zero-shot).
- Assistants API ã®è§£èª¬ã¨å‹•ä½œç¢ºèªï¼ˆGoogle Colabï¼‰
	- https://note.com/schroneko/n/nd04c46242171
- llamaindexã‹ã‚‰ã€OpenAI dev dayã‚’ã†ã‘GPT builderã‚’æ¨¡æ“¬ã™ã‚‹Builder Agentã®ä¾‹ã‚’å…¬è¡¨
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_builder.ipynb
	- https://x.com/jerryjliu0/status/1721639447207583882?s=20
	- ä¾‹ï¼šã€Œãƒˆãƒ­ãƒ³ãƒˆã®ã“ã¨ã‚’ã‚ˆãã‚ã‹ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã€â†’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã§ãã‚‹ã€‚ã€‚
- LangChainã‹ã‚‰ã€OpenGPTã®ç™ºè¡¨ã€
	- https://github.com/langchain-ai/opengpts
	- builds upon LangChain, LangServe and LangSmith This gives you more control over the LLM you us
- OpenGTPã¯ã€ LangSmithã«é€£æºã™ã‚‹ã ã‘ã§åˆ©ç”¨ãƒ­ã‚°ãŒå–ã‚Œã‚‹ã®ã§ã€ã‚ã¨ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Toolsã‚’å……å®Ÿã•ã›ã‚Œã°ã€ãã‚Œãªã‚Šã®ã‚‚ã®ãŒæä¾›ã§ãã‚‹
	- https://x.com/mah_lab/status/1721684588874055764?s=20
- Levels of AGI: Operationalizing Progress on the Path to AGI
	- https://arxiv.org/pdf/2311.02462.pdf
	- DeepMindã‹ã‚‰ã€AGIã«ã„ãŸã‚‹Level0ã‹ã‚‰Level5ã¾ã§ã®æ®µéšã‚’ç¤ºã™ã€ãƒ¬ãƒ™ãƒ«åˆ†ã‘ã®Ontologyã‚’ææ¡ˆã¨ã„ã£ã¦ã„ã‚‹
	-  AGI by considering generality (either Narrow or General) in tandem with five levels of performance (Emerging, Competent, Expert, Virtuoso, and Superhuman).
-  OpenAI Python API Library v1.0 å…¥é–€ã€€by npakaã•ã‚“
	- https://note.com/npaka/n/n27b94df96179?sub_rt=share_sb
	- ã€ŒOpenAI Python API Libraryã€ã®ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ãŒä¸€æ–°ã•ã‚ŒãŸã€ã‚‰ã—ã„
- GPT-4ã®fine-tuningã§æœ‰åŠ¹ãªgainã‚’å¾—ã‚‹ã“ã¨ãŒ3.5-turboã‚ˆã‚Šé›£ã—ã„
	- ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚’é¸ã¶å½¢ã§Custom Models programã‚’æä¾›ã™ã‚‹æˆ¦ç•¥ã¸è»¢æ›ã‹ã€
	- GPT-4ãŒã™ã”ã™ãã‚‹ã®ã§ã€ä¸­é€”åŠç«¯ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‹ãˆã£ã¦æ€§èƒ½ã‚’åŠ£åŒ–ã•ã›ã‚‹ã€‚ã€‚ã€‚ã€‚
	- https://openai.com/blog/new-models-and-developer-products-announced-at-devday
- Assistants APIã‚’åˆ©ç”¨ã™ã‚Œã°ã€TOEICã‚„TOEFLã€è‹±æ¤œã€IELTSã«ç‰¹åŒ–ã—ãŸå®¶åº­æ•™å¸«ã‚‚ä¸€ç¬ã§ä½œã‚Œã‚‹
	- https://x.com/gijigae/status/1721737796724183504?s=20
	- ã„ã¾ã¾ã§ã€OpenAI Plus(3kå††/æœˆ)ã§å®Ÿç¾ã—ã¦ã„ãŸã‚‚ã®ãŒã€Assistans APIã§ã€æœˆ1,500å††ç¨‹åº¦ã®åŠé¡ã«ãªã‚‹ã¨ã„ã†ãŠè©±ã€ãªã‚‹ã»ã©
- OpenAI APIã®RetrievalãŸå¤šç¨®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œ
	- OpenAI API ã®ä»Šå›ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«å«ã¾ã‚Œã¦ã„ãŸ Knowledge Retrieval (ãƒ•ã‚¡ã‚¤ãƒ«å†…æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹æ©Ÿèƒ½) ã¯ PDF ã¯ã‚‚ã¡ã‚ã‚“ Word ã‚„ãƒ‘ãƒ¯ãƒã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚‚å¯¾å¿œã—ã¦ã‚‹ã‚ˆã†ã ã€‚ RAG é–¢é€£ã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ãƒ›ãƒ³ãƒˆè¦ã‚‰ãªã„å­ã«ãªã£ã¡ã‚ƒã£ãŸã­
- OpenAI Assistantsã§è©¦ã—ã«è‹±èªè«–æ–‡ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆä½œæˆä¾‹
	- ä»Šå›æ–°ãŸã«APIãŒç™ºè¡¨ã•ã‚ŒãŸRetrievalæ©Ÿèƒ½ã‚’ä½¿ã£ã¦PDFãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜ã‚’ã—ã¦ã¿ã¦ã¾ã™ã€‚
	- https://x.com/alexweberk/status/1721705504228192373?s=20
	- DPOã®è«–æ–‡26ãƒšãƒ¼ã‚¸åˆ†ãã‚‰ã„ã®è¦ç´„ã§$0.80ãã‚‰ã„
-  GPT-3.5-Turbo / GPT-4-Turbo 1106ã®JSONãƒ¢ãƒ¼ãƒ‰ã®ä½¿ã„æ–¹ by [shi3z](https://note.com/shi3zblog)ã•ã‚“
	- https://note.com/shi3zblog/n/nd72e0269dc3f?sub_rt=share_pb
- OpenAI DevDay ã§ç™ºè¡¨ã•ã‚ŒãŸæ–°ãƒ¢ãƒ‡ãƒ«ã¨æ–°é–‹ç™ºãƒ„ãƒ¼ãƒ« ã¾ã¨ã‚ by  [npaka](https://note.com/npaka)ã•ã‚“
	- https://note.com/npaka/n/n9cd206d96f85?sub_rt=share_sb
	- ã€ŒFunction Callingã€ã«ã€å˜ä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰è¤‡æ•°ã®Function (ã€Œè»Šã®çª“ã‚’é–‹ã‘ã¦ã‚¨ã‚¢ã‚³ãƒ³ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã€ãªã©) ã‚’å‘¼ã³å‡ºã™æ©Ÿèƒ½ãªã©ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚ç²¾åº¦ã‚‚å‘ä¸Šã—ã¦ã„ã¾ã™
	- 16Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ–°ã—ã„ã€ŒGPT-3.5 Turboã€ã‚‚ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã™ã€‚æŒ‡ç¤ºè¿½å¾“ã€ JSONãƒ¢ãƒ¼ãƒ‰ã€ä¸¦åˆ— Function Callingã‚’ã‚µãƒãƒ¼ãƒˆ
	- ã€ŒAssistant APIã€ã¯ã€ç‰¹å®šã®æŒ‡ç¤ºã‚’æŒã¡ã€è¿½åŠ ã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹å°‚ç”¨ã®AIã§ã™ã€‚
	- ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ã€å¿…è¦ã«å¿œã˜ã¦ã€**Code Interpreter**ã€**Retrieval**ã€**Function Calling**ã‚’å‘¼ã³å‡ºã›ã‚‹
- Google Colab ã§ OpenAI API ã® Retrieval ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/ndcacbefb2ef7
	- APIã‹ã‚‰Assistantã‚’ä½œã‚‹æ–¹æ³•ã€çµæœã¯playgroundã§ã‚‚ç¢ºèªã§ãã‚‹ã¨ã„ã†ã‹ã€playgroundã§assistantä½œæˆã®åˆ¥ã®ã‚„ã‚Šæ–¹
- Putting numbers into a better perspective and classifying them according to their level of complexity
	- https://thinkzone.wlonk.com/Numbers/NumberSets.htm?platform=hootsuite
- GLaMM: Pixel Grounding Large Multimodal Model
	- https://huggingface.co/papers/2311.03356
-  GPT-4Vã®APIã‚’ã‚µã‚¯ãƒƒã¨ä½¿ã£ã¦ã¿ã‚‹ï¼
	- https://note.com/peisuke/n/nef0616b8d7fc?sub_rt=share_sb
	- æ—©ç¨²ç”°å¤§å­¦ã®è¬›ç¾©ã®ãƒšãƒ¼ã‚¸ã‚’ä½¿ã‚ã›ã¦ã‚‚ã‚‰ã„ã¾ã™ã€‚åˆ¶ç´„æ¡ä»¶ä»˜ãæœ€é©åŒ–ã®å•é¡Œã‚’è§£ã‹ã™â†’è§£ã‘ã‚‹ã€‚
	- "ç”»åƒã®æ•°å¼ã®å¿œç”¨ä¾‹ã‚’ä¸€ã¤æŒ™ã’ã€ä½•ã‚‰ã‹ã®é©å½“ãªæ•°å€¤ã‚’è¨­å®šã—ã€ãã‚Œã‚’è§£ããŸã‚ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„"
- Google Colab ã§ OpenAI API ã® Text-to-Speech ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/nba4af88eb3cf?sub_rt=share_sb
	- 6ã¤ã®å†…è”µãƒœã‚¤ã‚¹ãŒä»˜å±ã—ã¦ãŠã‚Šã€æ¬¡ã®ç›®çš„ã§ä½¿ç”¨ã§ãã¾ã™ã€‚
		- æ›¸ã‹ã‚ŒãŸãƒ–ãƒ­ã‚°æŠ•ç¨¿ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
		- è¤‡æ•°è¨€èªã®éŸ³å£°ã‚’ç”Ÿæˆ
		- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡ºåŠ›
- Bayesian Optimization of Function Networks with Partial Evaluations
	- https://arxiv.org/abs/2311.02146
- Assistance APIã«ã¤ã„ã¦
	- ã“ã‚Œã¾ã§ãªã‚‰è‡ªåŠ› or LangChain ã§ã‚„ã£ã¦ããŸã“ã¨ãŒã€ãã‚Œãªã‚Šã« Assistants/Theads/Run ãªã©ã§ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¡ã¾ã£ãŸãœ
	- OpenAIã® [#AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ](https://twitter.com/hashtag/AI%E3%82%A2%E3%82%B7%E3%82%B9%E3%82%BF%E3%83%B3%E3%83%88?src=hashtag_click) ã¯é¢ç™½ã„ã‘ã©ã€ã¾ãŸãŠé‡‘ãŒé£›ã‚“ã§ã„ã
- Assistances APIã‚’ã¤ã‹ã£ã¦ã€GPTvsGPTã‚’ä½œã‚‹ä¾‹
	- https://x.com/yoheinakajima/status/1721769833212281231?s=20
	- https://github.com/yoheinakajima/GPTvsGPT
	- ä¾‹ã¨ã—ã¦ã€åœ°çƒæ¸©æš–åŒ–ãƒ†ãƒ¼ãƒã«å¯¾ã™ã‚‹ã€æµ·è³Švsäººé­šã®è«–äº‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼
- Langchainã‹ã‚‰ã€OpenAIã® assistance APIã®ã‚µãƒãƒ¼ãƒˆã‚’ç™ºè¡¨
	- https://github.com/langchain-ai/langchain/blob/master/cookbook/openai_v1_cookbook.ipynb
	- Spin up OpenAI assistants and run them as any other LangChain agent!
	- LangChainã®Agentã¨åŒã˜ã‚ˆã†ã«ã€OpenAIã®agentã‚’ä½¿ãˆã‚‹ã€ã‚‰ã—ã„
	- OpenAIAssistantRunnable.create_assistan
- Contrastive Error Attribution for Finetuned Language Models
	- https://arxiv.org/abs/2212.10722v2
	- æ–‡æ›¸ç”Ÿæˆã«ãŠã„ã¦ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¼•ãèµ·ã“ã™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’é«˜ç²¾åº¦ã§ç‰¹å®šã™ã‚‹æ‰‹æ³•ã®ææ¡ˆã€‚
- Tokyo Digital TwinãŒã€
	- https://info.tokyo-digitaltwin.metro.tokyo.lg.jp/
	- èª¿å¸ƒå¸‚ã®3æ¬¡å…ƒ [#ç‚¹ç¾¤](https://twitter.com/hashtag/%E7%82%B9%E7%BE%A4?src=hashtag_click) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ! ã•ã‚‰ã«ç‹¬è‡ªã®æ‰‹æ³•ã«ã¦å»ºç‰©ãƒ»æ¤ç‰©ãƒ»åœ°è¡¨é¢ã®è‡ªå‹•åˆ†é¡ã‚’è¡Œã„ã¾ã—ãŸ
- GPT-4ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ã€ï¼•å„„å††ã‹ã‹ã‚‹ï¼Ÿï¼Ÿï¼Ÿ
	- It costs $2-3 million to train a custom GPT-4 model with your own dataset.
	- https://x.com/tdinh_me/status/1721835213121265840?s=20
	- ã„ã‚„ã€ã“ã®ã€ŒSubmitã€ãƒœã‚¿ãƒ³ã¯æŠ¼ã›ãªã„ã€‚ã€‚ã€‚
- GPT4 Turbo ã¯Pyllms ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§GPT4ã‚’å‡Œé§•
	- https://github.com/kagisearch/pyllms
	- https://aider.chat/docs/benchmarks-1106.html
- CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding
	- https://huggingface.co/papers/2311.03354
- QGIS 3.34ã§3DTilesãŒè¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã®ã§ã€3Déƒ½å¸‚ãƒ¢ãƒ‡ãƒ«PLATEAUã®3DTilesã‚’QGISã§è¡¨ç¤ºã—ã¦ã¿ã¾ã—ãŸ
	- https://x.com/shi__works/status/1721808786393121197?s=20
	- https://north-road.com/2023/11/07/qgis-3d-tiles-thanks-to-cesium-ecosystem-grant/
- OpenAI Assistants API(Playground)ã‚’ä½¿ã£ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã‚Œã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’ä½œã‚‹
	- https://zenn.dev/karaage0703/articles/66949a39643557
	- ä»Šã¾ã§ã§ã‚‚ã€Custom Instructionsã¨Advanced Data Analysisï¼ˆCode Interpreterï¼‰ã§ã§ãã¦ã„ãŸã“ã¨ã‚’ã€æ‰‹è»½ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¦ä¾¿åˆ©ã«ãªã£ãŸã€‚APIçµŒç”±ã§ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã¨ã„ã†ã“ã¨ãªã®ã§ã€æœ¬è³ªçš„ãªå¤‰åŒ–ã¨ã„ã†ã‚ˆã‚Šã¯é †å½“ãªã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
- è‡ªåˆ†ã®ç™–ã«ã‚ã£ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’LLMã§ä½œã‚ã†ï¼ã€Calm2ã€‘
	- https://zenn.dev/saldra/articles/090c120b49e38c
	- LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯é‡è¦ãªã‚‚ã®ã¨ãªã‚Šã¤ã¤ã‚ã‚‹
	- ä»¥å‰ã¾ã§ã¯äººåŠ›ã§ä½œã‚‹å¿…è¦ãŒã‚ã£ãŸãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒåŠ¹ã7Bãƒ¢ãƒ‡ãƒ«ï¼ˆCalm2-chatï¼‰ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€LLMã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚‹ã“ã¨ãŒã§ãã‚‹
	- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¤ã¤ã€å‹•çš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿®æ­£ã—ã¦ã„ãæ‰‹æ³•ãŒç›¸å½“ã‚ˆã‹ã£ãŸ
- HuggingFace Diffusers v0.22.0ã®æ–°æ©Ÿèƒ½ by npakaã•ã‚“
	- https://note.com/npaka/n/n5aebfc60408a?sub_rt=share_sb
- OpenAI Assistants APIã«æ‹™è‘—ã€Œã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®çŸ¥çš„ç”Ÿç”£è¡“ã€ã‚’å…¥ã‚Œã¦è³ªå•ã€‚ã“ã‚Œã“ãã€Œæ›¸ç±ã‚’èª­ã‚€æ–¹æ³•ã®åŠ¹ç‡åŒ–ã€ã ãªæ„Ÿ
	- https://x.com/nishio/status/1721857526990586203?s=20
- OpenAIã® AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ã«å­çŒ«ã®çµµã‚’æã„ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸ
	- https://x.com/itnavi2022/status/1721945299713941944?s=20
- æ—¥æœ¬èªå¯¾å¿œ13Bãƒ¢ãƒ‡ãƒ«ã®PLaMo-13Bã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ
	- å¯¾è©±æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸæŒ‡ç¤ºå­¦ç¿’ï¼ˆinstruction tuningï¼‰æ¸ˆã¿å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«PLaMo-13B-Instructã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- https://tech.preferred.jp/ja/blog/llm-plamo-instruct/
- llamaindexã‚‚OpenAIã®Assistanceã«å¯¾å¿œ
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_assistant_agent.ipynb
	- OpenAIã®Retrievalã¨llamaindexã®Retrievalã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒå¯èƒ½ï¼ï¼ï¼
- OpenAIã®Retrieval APIã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒé•·ã„å ´åˆã€ç°¡æ˜“ãªtokp-k RAGã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ã‚‹æ¨¡æ§˜
	- The OpenAI retrieval API seems to be doing basic top-k RAG on limited context if there's context overflows.
	- https://x.com/jerryjliu0/status/1721987237771133219?s=20
- GPT-4 Turbo vs GPT-4 tests
	- GPT-4 Turbo has record accuracy (87% vs 52% of GPT-4 on PyLLMs benchmark), it is almost 5x faster with 48 vs 10 tokens/sec). 
	- And it is also 30% cheaper in practice (would be more, but it is 2x wordier in output compared to GPT-4)
	- https://x.com/vladquant/status/1721674365211738269?s=20
-  Google Colab ã§ OpenAI API ã® Function Calling ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/nc3713dba5df6?sub_rt=share_sb
	- ç¾¤é¦¬çœŒã®æ°—æ¸©ã‚’æ•™ãˆã¦ãã ã•ã„
-  Re-evaluating Retrosynthesis Algorithms with Syntheseus
	- https://arxiv.org/abs/2310.19796v1
	- é€†åˆæˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è«–æ–‡ã€‚
	- ç‹™ã„ã®ææ–™ã‹ã‚‰åŸæ–™ã‚’äºˆæ¸¬ã™ã‚‹é€†åˆæˆäºˆæ¸¬ã§ã¯å„è«–æ–‡ã§è©•ä¾¡æ–¹æ³•ãŒç•°ãªã£ã¦ã„ã¾ã—ãŸãŒã€Microsoftã•ã‚“ã‚‰ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ§‹ç¯‰ã€ã“ã‚Œã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒå¾“æ¥ã¨å¤‰ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸãã†ã§ã™ã€‚
-  Google Colab ã§ OpenAI API ã® Code Interpreter ã‚’è©¦ã™ by npakaã•ï½
	- https://note.com/npaka/n/nb90306341d41?sub_rt=share_sb
- GPT-3.5 Turbo ã®ä¾¡æ ¼ãŒ Fireworks ã‚„ Anyscale ãªã©ã® OSS LLM ãƒ‡ãƒ—ãƒ­ã‚¤ã‚µãƒ¼ãƒ“ã‚¹ã® 70B ã®ãƒ‡ãƒ—ãƒ­ã‚¤ä¾¡æ ¼ã¨å…¨ç„¶ç«¶äº‰ã§ãã‚‹ãƒ¬ãƒ™ãƒ«
	- ã©ã†ã‚‚ä»Šå›ã® OpenAI ã®ä¾¡æ ¼æ”¹å®šã§ã€GPT-3.5 Turbo ã®ä¾¡æ ¼ãŒ Fireworks ã‚„ Anyscale ãªã©ã® OSS LLM ãƒ‡ãƒ—ãƒ­ã‚¤ã‚µãƒ¼ãƒ“ã‚¹ã® 70B ã®ãƒ‡ãƒ—ãƒ­ã‚¤ä¾¡æ ¼ã¨å…¨ç„¶ç«¶äº‰ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã¾ã§æ›ã‹ã£ã¦ã„ã‚‹ã‚‰ã—ãã€OSS LLM ãŒæ™®åŠã—ãªã„ã®ã¯çµå±€ OpenAI ã® API ãŒã‚¯ã‚½å®‰ã™ãã‚‹ã‹ã‚‰ã§ã¯ï¼Ÿã¨ã„ã†æŒ‡æ‘˜
- OpenAI API ã® Assistant API ã®ã—ãã¿
	- https://note.com/npaka/n/n9fa7204e4af4?sub_rt=share_sb
- mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration
	- https://huggingface.co/papers/2311.04257
- llamaindexã‚ˆã‚Šã€parallel function callingã«ã‚ˆã‚‹åŠ¹ç‡åŒ–ã®ä¾‹
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_agent_parallel_function_calling.ipynb
- OpenAI API ã§æä¾›ã•ã‚Œã¦ã„ã‚‹ ãƒ¢ãƒ‡ãƒ« ã¾ã¨ã‚ by npakaã•ï½
	- https://note.com/npaka/n/n5d0a76b149f1?sub_rt=share_sb
- ã€ç”ŸæˆAIã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ã€
	- https://aiboom.net/archives/58414
	-  LLMãªã©ã®ç”ŸæˆAIã®èƒŒå¾Œã«ã‚ã‚‹æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯äººé–“ã¨ã¯å…¨ãç•°ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã‚’ç¤ºã™ä»®èª¬
	- AIãŒäººé–“ã®ã‚ˆã†ãªå‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ãªãŒã‚‰ã€ãã‚Œã‚’ç†è§£ã™ã‚‹èƒ½åŠ›ã¯å¿…ãšã—ã‚‚ä¼´ã‚ãªã„ã¨ã„ã†ä»®èª¬ã§ã™ï¼ˆä»®èª¬ã‚’ç«‹ã¦ã‚‹ã«è‡³ã£ãŸèƒŒæ™¯ã¯ã€å‰ç« ã‚’å‚ç…§ï¼‰ã€‚
- Streamlit+GPT4-Vision+TTSã§å‹•ç”»ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã‚’ã¤ãã£ãŸ
	- https://zenn.dev/olemi/articles/752d205987cb87
	-  å‹•ç”»ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‚’æŠ½å‡ºã—ã€Base64å½¢å¼ã«å¤‰æ›ã™ã‚‹
	- GPT4-Visionã«å‹•ç”»ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã•ã›ã‚‹
	- ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€TTS APIã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
	- Streamlitã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤ºãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹
-  Google Colab ã§ PLaMo-13B-Instruct ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n97a1ac080f76?sub_rt=share_sb
- æ—¥æœ¬èªã«å¯¾å¿œã—ãŸ Embedding Model ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã®ç²¾åº¦æ¯”è¼ƒï½œTatsuya Shirakawa
	- https://github.com/nouu-me/document_vector_search_benchmark
	- æ—¥æœ¬èªText Embeddingã§ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç²¾åº¦ã‚’ã„ã‚ã‚“ãªãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼ã—ã¦ã¿ã¾ã—ãŸã€‚e5è‰¯ã„ã§ã™ã­
-  Extracting List of  `Album`  (with Parallel Function Calling)
	- https://docs.llamaindex.ai/en/latest/examples/output_parsing/openai_pydantic_program.html#extracting-list-of-album-with-parallel-function-calling
- è¤‡æ•°ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è¨è«–ã•ã›ã‚‹ä¾‹
	- https://x.com/npaka123/status/1722761636937900541?s=20
- Zhenjie Yang et al., "A Survey of Large Language Models for Autonomous Driving"
	- LLMãŒå¾—æ„ã¨ã™ã‚‹ã€Œè¨ˆç”»ã€èªè­˜ã€è³ªå•å¿œç­”ã€ç”Ÿæˆã€ã®èƒ½åŠ›ãŒè‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã«åŠ¹æœçš„ã«ä½¿ãˆã‚‹ã¨ä¸»å¼µ
	- https://arxiv.org/abs/2311.01043
- Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach
	- https://arxiv.org/abs/2209.06995
	- è‰¯è³ªãªãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—å°‘é‡ã§é«˜ã„æ€§èƒ½ã‚’ç²å¾—ã™ã‚‹è©¦ã¿ã€‚
	- LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆç–‘ä¼¼ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã€åˆ†å¸ƒãŒä¸€æ§˜ã§ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„=å­¦ç¿’åŠ¹æœãŒé«˜ã„ã¨ã¿ãªã™ã€‚
	- ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ä¸Šã®è·é›¢ã‹ã‚‰å‘¨è¾ºã‚‚ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ã€ã‹ã¤æ¡ç”¨ãƒ‡ãƒ¼ã‚¿é–“ã®è·é›¢ã‚’ç©ºã‘ã‚‹ã€‚
	- 128ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ•ãƒ«å­¦ç¿’ã® 90% è¶…ã®ç²¾åº¦ã€‚
- A.R.I.A. (Aria) - Your AI Research Assistant
	- https://github.com/lifan0127/ai-research-assistant
-  OpenAI ã® Assistant Playuground ã® Function Calling ã‚’è©¦ã™
	- https://note.com/npaka/n/n6bf08e93840d?sub_rt=share_sb
- GPTs ä½œæˆç¬¬äºŒå¼¾ã¨ã—ã¦ arXiv Reader ã‚’ä½œã‚Šã¾ã—ãŸã€‚è«–æ–‡ã¯ PDF å…¥åŠ›ã‹ URL æ‰‹æ¸¡ã—ã‹é¸ã¹ã¾ã™ã€‚
	- https://chat.openai.com/g/g-qrOeOjLX6-arxiv-reader
- Tokenizerã®åˆ†å‰²ã‚’å¯è¦–åŒ–ã—ãªãŒã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ•°ãˆã¦ãã‚Œã‚‹ãƒšãƒ¼ã‚¸ãŒOpenAIã®ã‚µã‚¤ãƒˆã«ã‚ã‚‹
	- https://platform.openai.com/tokenizer
- GPTsã§ã€Kaggleã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ç¬¬6ç‰ˆã‚’èª­ã¿è¾¼ã¾ã›ã¦ã¿ã¦ã€è³ªå•ã—ã¦ã¿ã¾ã—ãŸã€‚
	- https://chat.openai.com/g/g-Z3a4iOzGR-kagglenotiyutoriarudi-6ban
- å¼Šç¤¾ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚’GPTsã§ä½œæˆã—ã¦ã¿ã¾ã—ãŸã€‚
	- https://chat.openai.com/g/g-uINwYG4Ja-trippy-kasutamasapoto
- LangChainã®# OpenAI Assistantã€jsç‰ˆ
	- https://js.langchain.com/docs/modules/agents/agent_types/openai_assistant
-  Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models
	- https://arxiv.org/abs/2305.17446
	- äº‹å‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã®è»¢ç§»å­¦ç¿’ãŒãƒ¢ãƒ‡ãƒ«å†…ã®å‰¯ç©ºé–“ã§è¡Œã‚ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ãŸç ”ç©¶ã€‚
	- é‡ã¿ã‚’Flatten ã—ã‚¨ãƒãƒƒã‚¯ã”ã¨ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ SVD ã«ã‹ã‘ã€ Fine Tuning ä¸­ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼å¤‰å‹•ã‚’èª¬æ˜ã™ã‚‹è»¸ã‚’ç™ºè¦‹ã€‚
	- ã“ã®è»¸ä¸Šã§å¤–ã‚Œå€¤ã«ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–ã—è‘—ã—ã„æ€§èƒ½åŠ£åŒ–ã‚’ç¢ºèª
- å±±å†…å¿—æœ—ã€å°ã•ãªå€«ç†å­¦å…¥é–€ã€
	- ã€Œäººé–“ã¯æ¬²æœ›ã‚’è‡ªåˆ†ã§ç”Ÿç”£ã§ããšã€ä»–ã®äººã‹ã‚‰ã“ã£ãã‚Šç›—ã‚“ã§ãã¾ã™ã€‚ã‚‚ã—ã‹ã™ã‚‹ã¨ã€äººé–“ã¯æ¬²æœ›ãŒæ¬ å¦‚ã—ã¦ã„ã¦ã€ãã‚Œã‚’éš ã™ãŸã‚ã«æ¬²æœ›ã¾ã¿ã‚Œã®å§¿ã‚’å–ã‚ŠãŸãŒã‚Šã¾ã™ã€‚ã‚„ã‚ŠãŸã„ã“ã¨ãŒè¦‹ã¤ã‹ã‚‰ãªã„äººã®æ–¹ãŒåœ§å€’çš„ã«å¤šã„ã®ã§ã™ã€‚ã€
- ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§ã®AIç ”ç©¶è€…Fei-Fei Liã•ã‚“ã®æ–°åˆŠâ€The Worlds I Seeâ€ã¯ã€æƒ³åƒã‚’è¶…ãˆã‚‹é¢ç™½ã•ã€‚å¼·ã•ã¨ã—ã¦ã®å¥½å¥‡å¿ƒã€‚
	- https://www.amazon.com/dp/1250897939?ref_=cm_sw_r_cp_ud_dp_QG23D73KJFT6GCP6GNVP
	- After 3+ years, today is the day that my book â€œThe Worlds I Seeâ€ gets to see the world itself. It is a science memoir of the intertwining histories of me becoming an #AI scientist, and the making of the modern AI itself. 
- æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãŒæ›´æ–°
	- æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã§ã‚ã‚‹ Stability-AI/lm-evaluation-harness ãŒã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚ŒãŸãŸã‚ã€Youri 7B ã‚·ãƒªãƒ¼ã‚ºã®ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã—ç›´ã—ã¾ã—ãŸã€‚ GPTQã«ã‚ˆã‚‹ 4bit é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚‚ç®—å‡ºã—ã¦ã„ã¾ã™ã€‚
	- https://rinnakk.github.io/research/benchmarks/lm/
- ç”ŸæˆAIã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦
	- ç”ŸæˆAIã¾ã‚ã‚ŠãŒã™ã”ã„æ¥½ã—ã„ã®ã¯ã€æŠ€è¡“ãã®ã‚‚ã®ã¯ã‚‚ã¡ã‚ã‚“ã€ç†è«–ã«è©³ã—ã„äººã€ã„ã¡æ—©ãå®Ÿè£…ã«è½ã¨ã™ã®ãŒå¾—æ„ãªäººã€ãã‚Œã„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã«è½ã¨ã™ã®ãŒå¾—æ„ãªäººã€é¢ç™½ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã«ä»•ç«‹ã¦ã‚‹äººã®å”åŠ›é–¢ä¿‚ãŒãƒãƒƒãƒãƒªå™›ã¿åˆã£ã¦ã‚‹ã¿ãŸã„ãªã¨ã“ã‚ãŒã™ã
	- https://x.com/uezochan/status/1722604877644497292?s=20
- GPT3.5ã‚’ç”¨ã„ã¦ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³æ—¥æœ¬èªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(16K)ã‚’ä½œã‚Šã¾ã—ãŸ
	- https://note.com/shi3zblog/n/nfc07c53d61a8?sub_rt=share_b
	- Wikipediaæ—¥æœ¬ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(izumi-lab/wikipedia-ja-20230720)ã¨GPT-3.5-Turboã§ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã—ãŸã€‚
-  Google Colab ã§ Japanese Wikipedia Conversation ã«ã‚ˆã‚‹ Llama 2 ã®LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã™
	- https://note.com/npaka/n/n723766f96cbc?sub_rt=share_sb
	- **<s> [INST]** æ—¥æœ¬ã®é¦–éƒ½ã¯ï¼Ÿ **[/INST]** æ±äº¬ã§ã™ã€‚**</s><s> [INST]** ãã®å ´æ‰€ã®è¦³å…‰åæ‰€ã‚’æ•™ãˆã¦ã€‚ **[/INST]** æ±äº¬ãƒ‰ãƒ¼ãƒ ã‚·ãƒ†ã‚£ã€ã‚µãƒ³ã‚·ãƒ£ã‚¤ãƒ³60ï¼ˆå…­ä¸‡åˆ†ä¸€ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚ **</s>**
- llamaindexã‹ã‚‰RAGã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_retrieval_benchmark.ipynb
	- OpenAIã®RAGãŒã€llamaindexã®5è¡Œã®ã‚³ãƒ¼ãƒ‰ã«åŠ£ã£ã¦ã„ã‚‹ã¨ã€ã€ã€
- LLM OS
	- https://x.com/karpathy/status/1723140519554105733?s=20
	- Specs:
		- LLM: OpenAI GPT-4 Turbo 256 core (batch size) processor @ 20Hz (tok/s)
		- RAM: 128Ktok
		- Filesystem: Ada002
- ChatGPTã¯ã€ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã‚„ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã®é›‡ç”¨ã‚’å¥ªã†ã¨ã¨ã‚‚ã«ã€å˜ä¾¡ã‚‚ä¸‹ã’ã¦ã„ã‚‹
	- ç±³å›½ã®æœ€æ–°ç ”ç©¶ã¯ã€ChatGPTã®ç«‹ã¡ä¸Šã’ã‹ã‚‰æ•°ã‚«æœˆã§ã€ä¸»è¦ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã‚„ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã®ä»•äº‹ã®æ•°ãŒå¤§å¹…ã«æ¸›å°‘ã—ã€åå…¥ã‚‚æ€¥æ¿€ã«æ¸›ã£ãŸã¨å ±ã˜ã¦ã„ã‚‹
	- https://www.ft.com/content/b2928076-5c52-43e9-8872-08fda2aa2fcf
	- ã€Œ6æ¡ç¨¼ãäººã¯30000ãƒ‰ãƒ«ã—ã‹ç¨¼ãŒãªã„äººã®3å€ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚’å—ã‘ã‚‹ã€
- Pattern Language for Generative AI book!
	- https://x.com/IntuitMachine/status/1722931733866143754?s=20
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯çµŒé¨“ã—ãŸè¨€èªã‚’ä¸€èˆ¬åŒ–ã™ã‚‹èƒ½åŠ›ãŒã‚ã‚‹ã‹ï¼ˆï¼‘ï¼æœˆï¼’ï¼•æ—¥ Nature ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ²è¼‰è«–æ–‡ï¼‰ - Lab BRAINS
	- https://lab-brains.as-1.co.jp/enjoy-learn/2023/11/55788/
- Jochen Wulf and Juerg Meierhofer, "Towards a Taxonomy of Large Language Model based Business Model Transformations"
	- https://arxiv.org/abs/2311.05288
	- LLMã‚’åˆ©ç”¨ã—ãŸãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦å®Ÿéš›ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚‚ã¨ã«èª¿æŸ»å ±å‘ŠãŒç™ºè¡¨ã•ã‚Œã¾ã—ãŸã€‚
		- ã€Œæ–°ã—ã„é¡§å®¢ãƒ¡ãƒªãƒƒãƒˆã®å‰µé€ ã€ã€ã€Œæ–°ã—ã„è²©å£²ãƒãƒ£ãƒãƒ«ã®é–‹æ‹“ã€ã€
		- ã€Œãƒ“ã‚¸ãƒã‚¹ãƒ—ãƒ­ã‚»ã‚¹è‡ªå‹•åŒ–ã®åŠ é€Ÿã€ã€ã€Œæƒ…å ±ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨ã®æ”¹å–„ã€
-  LlamaIndex ã® ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAG ã®ã—ãã¿ by npakaã•ã‚“
	- https://note.com/npaka/n/n53e8aabed0f2?sub_rt=share_sb
	- ã€ŒGPT-4V APIã€ã®å°å…¥ã«ã‚ˆã‚Šã€ã€ŒRAGã€ã®æ¦‚å¿µã‚’ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«æ‹¡å¼µã—ã€ã•ã‚‰ã«å¤§é‡ã®(ç”»åƒã‚’å«ã‚€) ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ä¾¡å€¤ã‚’å¼•ãå‡ºã™
	- SimpleDirectoryReaderã®ç”»åƒæ‹¡å¼µ
	- **MultiModalVectorIndex**ã®å°å…¥
- RAGã«ãŠã‘ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ç²¾åº¦å‘ä¸Šã«ã¤ã„ã¦(æ¦‚è¦ç·¨)
	- https://zenn.dev/sompojapan_dx/articles/eb755a18e893ce
	- æå®³ä¿é™ºã‚¸ãƒ£ãƒ‘ãƒ³æ ªå¼ä¼šç¤¾ DXæ¨é€²éƒ¨
	- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ‰‹ã‚’åŠ ãˆã‚‹
		- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å½¢/chunking**ã€**è¦ç´„ç”Ÿæˆ**ã€**è³ªå•æ–‡ã®æ‹¡å¼µ**ã€**Knowledge Graphã®æ´»ç”¨**
	- æ¤œç´¢ãƒ¢ãƒ‡ãƒ«ã«æ‰‹ã‚’åŠ ãˆã‚‹
		- **æ¤œç´¢ãƒ¢ãƒ‡ãƒ«ã®fine-tune**ã€**Re-rankingãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨**
-  PromptNER: Prompt Locating and Typing for Named Entity Recognition
	- https://arxiv.org/abs/2305.17104v1
- ã€Œã‚¢ãƒŠãƒ­ã‚¸ã‚¢ã€ã®ã‚¸ãƒ§ãƒ¼ã‚¸ãƒ»ãƒ€ã‚¤ã‚½ãƒ³ãŒLLMã«ã¤ã„ã¦èªã‚‹
	- https://www.hayakawabooks.com/n/n6b8cf31a9472
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã„ã‚ã‚†ã‚‹è¨€èªã®åœ°å›³ã¨ã‚‚è¨€ãˆã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ã„ã‚ã„ã‚ãªAIã¯ã€ãã®åœ°å›³ã‚’è¾¿ã£ã¦æœ‰ç”¨ãªç›®çš„åœ°ã¾ã§ãƒ‡ã‚¸ã‚¿ãƒ«æ–¹å¼ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚
	- ã“ã†ã—ãŸåœ°å›³ã¯ã¾ã å¸‚è²©ã®ç”»åƒå‡¦ç†ç”¨ãƒãƒƒãƒ—GPUã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸã ã‘ã®ã‚‚ã®ã§ã™ãŒã€ã„ãšã‚Œã“ã†ã—ãŸï¼ˆè¨€èªã°ã‹ã‚Šã‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚„ã‚ã‚Šã¨ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’é‡ã¿ã¥ã‘ã™ã‚‹ï¼‰å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã®ã‚¢ãƒŠãƒ­ã‚°ãƒãƒƒãƒ—ãŒåˆ©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã€å¾ã€…ã«æµ¸é€ã—ã¦ã„ãç¾è¡Œã®ã‚·ã‚¹ãƒ†ãƒ ã«ä»£ã‚ã£ã¦ã„ãã¨æ€ã„ã¾ã™ã€‚
- ã€Œã‚¢ãƒŠãƒ­ã‚¸ã‚¢ã€ã‚¸ãƒ§ãƒ¼ã‚¸ãƒ»ãƒ€ã‚¤ã‚½ãƒ³ã‚ˆã‚Š
	- é€£ç¶šä½“ä»®è¨­ã¯ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚‚ã€ã‚¢ãƒŠãƒ­ã‚°ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚‚ã©ã¡ã‚‰ã‚‚ç„¡é™ã®åŠ›ã‚’æŒã¤ãŒã€ãã‚Œãã‚ŒãŒã©ã‚Œã ã‘é€²åŒ–ã—ã¦ã‚‚ç™ºæ®ã™ã‚‹åŠ›ãŒç•°ãªã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹(P292)
	- ã‚¢ãƒŠãƒ­ã‚°ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ã¯è¤‡é›‘æ€§ã¯ã‚³ãƒ¼ãƒ‰ã§ãªãã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å®¿ã‚‹ã€‚
	- ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç¡¬ç›´åŒ–ã—ã¦ãƒã‚¤ã‚ºã‚’ã«å¯¾ã™ã‚‹è€æ€§ã‚’å¤±ã£ã¦ã—ã¾ã£ãŸã€ã‚¢ãƒŠãƒ­ã‚°ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã‚ã‚‹
	- ã‚¢ãƒŠãƒ­ã‚°ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ãƒã‚¤ã‚ºã‚’å—ã‘å…¥ã‚Œã‚‹ã°ã‹ã‚Šã‹ã€ï¼œç•¥ï¼æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã«ä¸€å®šã®èƒŒæ™¯ãƒã‚¤ã‚ºã‚’å¿…è¦ã¨ã•ãˆã—ã¦ã„ã‚‹ã€‚(P295)
- äººå·¥çŸ¥èƒ½ã®ä¸‰ã¤ã®æ³•å‰‡ã‹ã‚‰ã¿ã‚‹AIã®æ¬¡ã«ãã‚‹ã‚‚ã®ï¼ˆãƒ€ã‚¤ã‚½ãƒ³ï¼‰(P299)
	- ã€Œã‚¢ã‚·ãƒ¥ãƒ“ãƒ¼ã®å¿…è¦å¤šæ§˜æ€§ã®æ³•å‰‡ã€ã€å®ŸåŠ¹çš„ãªåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã¯å¯¾è±¡ã¨åŒã˜ç¨‹åº¦è¤‡é›‘ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„
	- ã€Œè¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´ã‚’è¦å®šã™ã‚‹ã®ã¯ã€ãã‚Œè‡ªèº«ã®æœ€ã‚‚å˜ç´”ãªå‹•ä½œã®è¨˜è¿°ã ã€ï¼ˆãƒã‚¤ãƒãƒ³ï¼‰ã€
	- ã€Œç†è§£å¯èƒ½ãªå˜ç´”ãªã‚·ã‚¹ãƒ†ãƒ ã¯ã€çŸ¥çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã«ã¯è¤‡é›‘ã•ãŒè¶³ã‚‰ãšã€çŸ¥çš„ãªæŒ¯ã‚‹èˆã„ãŒã§ãã‚‹ãã‚‰ã„è¤‡é›‘ãªã©ã‚“ãªã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚ã€ç†è§£ã™ã‚‹ã«ã¯è¤‡é›‘ã™ãã‚‹ã€
	- â†’è‡ªã‚‰æ€è€ƒã™ã‚‹äººå·¥çŸ¥èƒ½ã¯ã€äººé–“ã®çŸ¥æ€§ã‚’ç†è§£ã™ã‚‹ã¾ã§ã¯ã€ãƒã‚·ãƒ³ãŒè¶…äººçš„ãªçŸ¥èƒ½ã‚’æŒã¤ã“ã¨ã‚’å¿ƒé…ã™ã‚‹å¿…è¦ã¯ãªã„ã¨ã‚‚ã„ãˆã‚‹ãŒã€ç†è§£ã‚’ã›ãšã«ä½•ã‹ã‚’ä½œã£ã¦ã„ã‘ãªã„ã¨ã„ã†é“ç†ã‚‚ãªã„ã€‚

## 11/6

ä»Šé€±ã¯ã€Rinnaã®Youri 7Bã®ç™ºè¡¨(10/31)ã€Japanese Stable LM Beta 70Bã®ç™ºè¡¨(11/2)ã€åŒæ—¥CyberAgentLM2-7Bï¼ˆCALM2 -7Bï¼‰ã®å…¬é–‹(11/2)ç­‰ã€æ—¥æœ¬èªLLMã®ç™ºè¡¨ãƒ»å…¬é–‹ãŒç›¸æ¬¡ãã€‚ã‚ã£ã¨ã„ã†é–“ã«4bit é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚‚å…¬é–‹ã•ã‚Œã¦æ‰‹å…ƒã§è©¦ã›ã‚‹ã‚ˆã†ã«ã€‚ã€‚ã€‚70Bã‚‚ã³ã£ãã‚Šã™ã‚‹ãŒã€ç‰¹ã«Calm2ã¯3ä¸‡2000ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ—¥æœ¬èªã§ç´„5ä¸‡å­—ï¼‰ã«å¯¾å¿œã—ã¦ã„ã¦ã€RAGä¸è¦ã‹ã‚‚ã€‚Colabã§ã‚‚A100ãªã‚‰ã°å‹•ã‹ã›ã‚‹ã‚‰ã—ã„ã€‚ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã®LLMé–‹ç™ºå§‹å‹•ã‚„ã€NTTã®æ—¥æœ¬èªå¯¾å¿œè¨€èªãƒ¢ãƒ‡ãƒ«ã®tsuzumiã®ç™ºè¡¨ã€ç‰§é‡å…ˆç”ŸãŒã€MM-coreå°‚ä»»ï¼Ÿã«ãªã‚‹ã¨ã®è©±é¡Œã‚‚ã‚ã‚Šã€æ—¥æœ¬ã§ã‚‚LLMã®ã‚¤ãƒ³ãƒ•ãƒ©ãŒä»Šå¾Œãã‚ã£ã¦ãã‚‹ã®ã¯æ¥½ã—ã¿ã€‚æ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’SimCSEã£ã¦ã€LLMæœ¬(å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€)ã§ç´¹ä»‹ã•ã‚Œã¦ã„ãŸã‚„ã¤ã€‚èª¬æ˜å¯èƒ½AIã«ã‚ˆã‚‹ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå¤ªé™½é›»æ± é–‹ç™ºã£ã¦ã€AIã«èª¬æ˜ã•ã›ã¦äººé–“ãŒæ¬¡ã‚’è€ƒãˆã‚‹ã¨ã„ã†ã€AIã¨äººã¨ã®å”èª¿ã®æ–°ã—ã„æœªæ¥ã®å½¢ã€‚LLMè©•ä¾¡ã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã€å¾Œã§èª­ã‚‚ã†ã€‚ TinyLLaMaã€ã©ã“ã¾ã§å°ã•ãã§ãã‚‹ã‹ã€ã“ã†ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã„ã„ãªã‚ã€æœ¬å½“ã«1.1Bã§ã©ã“ã¾ã§ã„ã‘ã‚‹ï¼ŸLLMã‚’åˆ©ç”¨ã—ãŸFAQæ¤œç´¢ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã®å·¥å¤«ã¨ã‹ã€LangChainã®ã‚¢ãƒ—ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å…¬é–‹ã¨ã‹ã€å®Ÿç”¨é¢ã«è¿‘ã„é–‹ç™ºã‚‚é€²å±•ã‚ã‚Šã€‚npakaã•ã‚“ã®ã€LangChainã€LLamaIndexã®ç´¹ä»‹è¨˜äº‹ã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§æœ€æ–°ã®æƒ…å ±ãªã®ã§ãŠå¾—ã€‚ã¡ã‚‡ã†ã©æ—¥çµŒæ–°èã§ç´¹ä»‹ã•ã‚ŒãŸã€å²©æ³¢æ–°æ›¸ã®ã€è¨€èªå“²å­¦ãŒã¯ã˜ã¾ã‚‹ã€ã€ãƒ•ãƒ¬ãƒ¼ã‚²ã€ãƒ©ãƒƒã‚»ãƒ«ã€ãƒ´ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã€ã‚‚ã—å½¼ã‚‰ãŒä»Šç”Ÿãã¦ã„ãŸã‚‰LLMã‚’ã©ã†ç ”ç©¶ã—ãŸã®ã‹ã€‚Xã®Grok-1ã¯æ¬¡é€±ã«ç¶šãã ãªã€‚

- FP8-LM: Training FP8 Large Language Model
	- https://arxiv.org/abs/2310.18313
	- Microsoftã®ç ”ç©¶ãƒãƒ¼ãƒ ã«ã‚ˆã‚‹è«–æ–‡ã€‚
	-  FP8è‡ªå‹•æ··åˆç²¾åº¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€æ€§èƒ½ä½ä¸‹ã‚’æŠ‘ãˆã¤ã¤ ãƒ»BF16ã‚ˆã‚Šã‚‚64%é€Ÿã ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’42%å‰Šæ¸›ã— GPT-175Bã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ããŸ
- ControlLLM: Augment Language Models with Tools by Searching on Graphs
	- https://huggingface.co/papers/2310.17796
	- (1) a task decomposer that breaks down a complex task into clear subtasks with well-defined inputs and outputs; 
	- (2) a Thoughts-on-Graph (ToG) paradigm that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and
	-  (3) an execution engine with a rich toolbox that interprets the solution path and runs the tools efficiently on different computational devices.
- ãƒãƒ¼ãƒãƒ¼ãƒ‰å¤§å­¦ã¨BCGã®ç ”ç©¶ã«ã‚ˆã‚‹ã¨GPT-4ã®æ´»ç”¨ã§ä»•äº‹ã®ç²¾åº¦ã¯40%å‘ä¸Šã—ã€ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚‚25%æ—©ããªã£ãŸã¨ã®ã“ã¨ã€‚ã“ã®çµæœã‚’è¦‹ã¦ã‚‚AIã®ä½¿ã„æ–¹ã¯ç›Šã€…ã€çŸ¥çš„å·®åˆ¥åŒ–ã®é‡è¦ãªè¦ç´ ã¨ãªã‚‹ã€‚çŸ¥çš„ã•ã¯ã‚‚ã¯ã‚„AIã¨åˆ‡ã‚Šé›¢ã—ãŒå›°é›£ãªçŠ¶æ…‹ã€‚ã“ã†ã—ãŸå¤‰åŒ–ã«ã¤ã„ã¦ã„ããŸã‚ã«ã‚‚æœ€æ–°ã®AIã‚’ä½¿ã„ã“ãªã›ã‚‹åŠªåŠ›ã‚’ã—ã¦ã»ã—ã„ã€‚
	- https://x.com/gijigae/status/1718851299524096284?s=20
- ChatGPT ã®ã‚¢ãƒ—ãƒªç‰ˆã« Retrieval Augmented Generation (RAG)æ©Ÿèƒ½ãŒè¿½åŠ ï¼Ÿ
	- https://x.com/yi_ding/status/1719028284548382901?s=20
- ã‚·ãƒªã‚³ãƒ³ãƒãƒ¬ãƒ¼éŠ€è¡Œã®ç ´ç¶»ã‚’ã€ã‚·ãƒ³ãƒ—ãƒ«ã«è§£æã™ã‚‹notebookãŒå…¬é–‹ã€‚ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦Professor Ashwin Raoã«ã‚ˆã‚‹
	- https://colab.research.google.com/drive/15uxrAeCCL327kWH9N0X-ogKwf2zErjP5
- Microsoft ã†ã£ã‹ã‚Šgpt-3.5ãŒ20bç›¸å½“ã ã¨æ¼ã‚‰ã™ã€
	- CodeFusion: A Pre-trained Diffusion Model for Code Generation
	- https://arxiv.org/abs/2310.17680
	- Microsoft paper claims ChatGPT 3.5 has ~20 billion parameters
- Blokeãƒ‹ã‚­ãŒStability AI Japan ã®ãƒ¢ãƒ‡ãƒ«ã‚’4bité‡å­åŒ–
	- https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GPTQ
- rinnaã¯Llama 2ã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒYouri 7Bã€ã‚·ãƒªãƒ¼ã‚ºã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ 
	- https://rinna.co.jp/news/2023/10/20231031.html
	- â‘ Youri 7Bï¼šæ—¥è‹±40Bãƒˆãƒ¼ã‚¯ãƒ³ã§ç¶™ç¶šäº‹å‰å­¦ç¿’ 
	- â‘¡Youri 7B Instructionï¼šé«˜ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ 
	- â‘¢Youri 7B Chatï¼šè¤‡æ•°ã‚¿ãƒ¼ãƒ³ã®å¯¾è©±ã«å¼·ã„ 
	- GPTQ 4bit é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚‚å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
-  Google Colab ã§ Youri-7B ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/nccadcbcfe37e?sub_rt=share_sb
	- è¤‡æ•°ã‚¿ãƒ¼ãƒ³ã®å¯¾è©±ãƒ¢ãƒ‡ãƒ« (GPTQç‰ˆ)ã§ã‚ã‚‹ã€Œrinna/youri-7b-chat-gptqã€ã‚’ä½¿ã„ã¾ã™
- å¤šæ§˜ãªæ—¥æœ¬èªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’SimCSEã§æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã«fine-tuning
	- https://arxiv.org/abs/2310.19349
	- ã‹ãªã‚Šã„ã„æ„Ÿã˜ã®æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒã§ããŸã¨æ€ã†ã®ã§ã€ãœã²ãŠä½¿ã„ãã ã•ã„...ï¼ï¼
	- https://github.com/hppRC/simple-simcse-ja
- Youri 7B Instructionã®GPTQãƒ¢ãƒ‡ãƒ«ã¤ã‹ãˆã°ã€GPUãƒ¡ãƒ¢ãƒª8GBã§ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ã§LLMç¿»è¨³ãŒã§ããã†ãªæ°—é…
	- https://x.com/kis/status/1719284609761108462?s=20
- ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ å›½ç”£å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®é–‹ç™ºã‚’æœ¬æ ¼é–‹å§‹
	- https://www.softbank.jp/corp/news/press/sbkk/2023/20231031_01/
	- 2024å¹´å†…ã«3,500å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å›½ç”£LLMã®æ§‹ç¯‰ã‚’ç›®æŒ‡ã—ã¾ã™
-  Evaluating Large Language Models: A Comprehensive Survey
	- https://arxiv.org/abs/2310.19736
	- A comprehensive survey (100+ pages) on evaluating LLMs. 
	- â– ã€ŒçŸ¥è­˜ã¨èƒ½åŠ›ã€ã®è©•ä¾¡ 
		- â‘  ã‚¿ã‚¹ã‚¯ä¸­å¿ƒã®è©•ä¾¡ã‹ã‚‰èƒ½åŠ›ä¸­å¿ƒã®è©•ä¾¡ã¸ã¨ç§»è¡Œã—ã¦ã„ã‚‹ 
		- â‘¡ è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã¾ã™ã¾ã™æ‹¡å¼µã•ã‚Œã¦ã„ã‚‹
		- â‘¢ ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯é–“ã®åŒºåˆ¥ãŒã‚ã„ã¾ã„ 
		- â‘£ ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’ç·åˆçš„ã«è©•ä¾¡ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ 
	- â– ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰ã®è©•ä¾¡ 
		- â‘  äººé–“ã®ä¾¡å€¤è¦³ã¨ã®ä¸€è‡´ã‚’è©•ä¾¡ã™ã‚‹ç ”ç©¶ãŒå¢—ãˆã¦ã„ã‚‹ 
		- â‘¡ å€«ç†çš„ãªé¢ã‚‚å«ã‚ãŸãƒ¢ãƒ‡ãƒ«ã®é€²æ­©ã¨å¿œç”¨ãŒç›®æŒ‡ã•ã‚Œã¦ã„ã‚‹ 
	- â– å®‰å…¨æ€§ã®è©•ä¾¡ 
		- â‘  LLMã®ç™ºå±•ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯ã«å³æ ¼ãªè©•ä¾¡ãŒå¿…è¦ 
		- â‘¡ ä¾‹ãˆã°ãƒã‚¤ã‚¢ã‚¹ã®å¢—å¹…ã€èª¤æƒ…å ±ã®æ‹¡æ•£ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã®ä¾µå®³ãªã© 
		- â‘¢ ãƒªã‚¹ã‚¯è©•ä¾¡ã¨ã€å¯¾å‡¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ 
	- â– ç‰¹åŒ–å‹LLMã®è©•ä¾¡ 
		- â‘  ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸLLMã‚‚å­˜åœ¨ 
		- â‘¡ ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ã¯å°‚é–€çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ 
		- â‘¢ é«˜åº¦ãªçŸ¥è­˜ã‚„å°‚é–€çš„ãªæ¨è«–èƒ½åŠ›ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã•ã‚Œã¦ã„ã‚‹
- LanChainã‹ã‚‰ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã«ã‚¢ãƒ—ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãŒå…¬é–‹
	- https://blog.langchain.dev/langserve-hub/
	- LangChain Templates offers a collection of easily deployable reference architectures that anyone can use.
	- https://github.com/langchain-ai/langchain/tree/master/templates
- LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery
	- https://huggingface.co/papers/2310.18356
	- LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models.
- ggufç‰ˆã€japanese-stablelm-instruct-gamma-7bã€€å®Ÿç”¨ API ã‚µãƒ¼ãƒãƒ»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä¾‹
	- https://note.com/ai_meg/n/n0c449a877c6f?sub_rt=share_pb
	- ä¼šè©±ãƒ­ã‚°ã€requestãƒœãƒ‡ã‚£-ç°¡ç•¥åŒ–ã®ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã€‚llm()ã¸ã®ç”Ÿæˆæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ ãªã©ã€‚
- Youri 7Bã‚’FastChatã§ChatGPTäº’æ›APIã‚µãƒ¼ãƒã¨ã—ã¦å‹•ã‹ã—ã¦éŠã¶
	- https://qiita.com/takaaki_inada/items/fcb63da369b5bfd8a3cf?utm_campaign=post_article&utm_medium=twitter&utm_source=twitter_share
	- youri-7b-chatã‚’fastchatã§ChatGPTäº’æ›APIã§ãƒ›ã‚¹ãƒˆã—ã¦ChatVRMã§ã‚µã‚¯ãƒƒã¨éŠã¼ã†ã€‚prompt engineeringãŒåŠ¹ãã®ã§system promptè¨­å®šç”»é¢ã§èªå°¾ã‚„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã§ãã¾ã™
- Google Colab ã§ Japanese Stable LM Beta 7B ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n49387d8a8af4?sub_rt=share_sb
	- èªå½™æ‹¡å¼µæ¸ˆã¿æŒ‡ç¤ºãƒ¢ãƒ‡ãƒ«ã€Œstabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b ã€ã‚’ä½¿ã„ã¾ã™
- Generative AI for everyone	by Andrew Ngå…ˆç”Ÿ
	- https://www.deeplearning.ai/courses/generative-ai-for-everyone/
- Google Colabã«ã€API keyã‚’ç™»éŒ²ã§ãã‚‹æ–°æ©Ÿèƒ½ãŒå…¬é–‹
	- https://x.com/GoogleColab/status/1719798406195867814?s=20
- èª¬æ˜å¯èƒ½AIã«ã‚ˆã‚‹ãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå¤ªé™½é›»æ± é–‹ç™º
	-  Discovering Process Dynamics for Scalable Perovskite Solar Cell Manufacturing with Explainable AI
	- https://onlinelibrary.wiley.com/doi/10.1002/adma.202307160
	- æˆè†œéç¨‹ã®å‹•ç”»ã‚„ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰NNã«ã‚ˆã‚Šå¤‰æ›åŠ¹ç‡ã‚’äºˆæ¸¬ã€ãã‚Œã«åŸºã¥ãè§£é‡ˆã™ã‚‹æ‰‹æ³•ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ã‚»ã‚¹ã¨ç‰¹æ€§ã®æ–°ã—ã„æ´å¯Ÿã«ã¤ãªãŒã£ãŸãã†ã§ã™ã€‚
- Efficient LLM inference on CPUs! 
	- https://huggingface.co/papers/2311.00502
	- NeurIPS'23ã®è«–æ–‡
	- Compatible with GGML yet better performance up to 1.5x over llama.cpp!
	- https://github.com/intel/intel-extension-for-transformers
- The Computational Lens: from Quantum Physics to Neuroscience
	- è¨ˆç®—æ©Ÿçš„ãªè¦–ç‚¹ã‚’ç”¨ã„ã¦ã€é‡å­ç‰©ç†å­¦ã‹ã‚‰ç¥çµŒç§‘å­¦ã«è‡³ã‚‹ã¾ã§ã®åˆ†é‡ã‚’ç ”ç©¶ã—ãŸãƒãƒ¼ãƒãƒ¼ãƒ‰å¤§å­¦ã®åšå£«è«–æ–‡
	- https://arxiv.org/abs/2310.20539
- Japanese TinyLLaMa 1.1 B, llama.cpp ã§ wasm ã§ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚‚å‹•ã
	- https://github.com/lighttransport/japanese-normalizer-cpp
	- https://x.com/syoyo/status/1719646103891845438?s=20
-  LLMã‚’åˆ©ç”¨ã—ãŸFAQæ¤œç´¢ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã€œãã®ï¼’ã€œ
	- https://www.ai-shift.co.jp/techblog/3761
	- ã€Œ1.  FAQã®å›ç­”å†…å®¹ã‹ã‚‰è³ªå•å†…å®¹ã‚’æŠ½å‡ºã€ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ç”Ÿæˆæ™‚ã®promptã®å·¥å¤«ã«ã¤ã„ã¦å–ã‚Šçµ„ã‚“ã 
- calm2ã§è­°äº‹éŒ²ã‚’ã¾ã¨ã‚ã¦ã¿ã¾ã—ãŸã€‚AIæ™‚ä»£ã®çŸ¥çš„è²¡ç”£æ¨©æ¤œè¨ä¼šï¼ˆç¬¬ï¼‘å›ï¼‰
	- https://x.com/alfredplpl/status/1720005676829970472?s=20
	- https://www.kantei.go.jp/jp/singi/titeki2/ai_kentoukai/kaisai/index.html
	- ä¸»å¼µ1: AIã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚å«ã¾ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚ 
	- ä¸»å¼µ2: AIã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã€äººé–“ã«ã‚ˆã£ã¦å‰µä½œã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨åŒç­‰ã«ä¿è­·ã•ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚ 
	- ä¸»å¼µ3: è‘—ä½œæ¨©ã‚’ä¾µå®³ã™ã‚‹è¡Œç‚ºã«ã¯ã€AIã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚å«ã¾ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚ 
	- ä¸»å¼µ4: åç›Šé‚„å…ƒæ³•ã«ã¤ã„ã¦ã¯ã€AIã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚é©ç”¨ç¯„å›²ã«å«ã¾ã‚Œã‚‹ã¹ãã§ã‚ã‚‹ã€‚
-  Google Colab ã§ CALM2 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n443e3ea8d0b8?sub_rt=share_sb
	- ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã€Œcyberagent/calm2-7b-chatã€ã‚’ä½¿ã„ã¾ã™ã€‚
- CALM2-7B-chatã®Spaceã‚’ä½œã‚Šã¾ã—ãŸ
	- https://huggingface.co/spaces/hayas/CALM2-7B-chat
- llamaã¨llama2ã®é•ã„ by NTT è¥¿ç”°ã•ã‚“
	- https://speakerdeck.com/kyoun/llama-2-open-foundation-and-fine-tuned-chat-models
- æ—¥æœ¬èªDeBERTaV2ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼
	- å½¢æ…‹ç´ è§£æå™¨ã®äº‹å‰ã®å˜èªåˆ†å‰²ãªã—ã§ä½¿ãˆã‚‹base, smallãƒ¢ãƒ‡ãƒ«ã«ãªã£ã¦ã„ã¾ã™
	- https://huggingface.co/izumi-lab/deberta-v2-base-japanese
- ãªã‚“ã¨ã€japanese-stablelm-instruct-beta-70B-GGUF
	- TheBloke/japanese-stablelm-instruct-beta-70B-GGUF
	- ggufã®ãã›ã«40Gã‚‚ã‚ã‚‹ã‚ˆã€ã¾ã£ãŸã
- OpenChat3.5
	- https://huggingface.co/openchat/openchat_3.5
	- gpt-3.5ã«è¿«ã‚‹ï¼Ÿï¼Ÿ
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®KOSMOS-2ã‚’å–ã‚Šè¾¼ã‚“ã transformerã®æ›´æ–°ï¼ by huggingface
	- KOSMOSã®ã§ã‚‚ã¯ã“ã¡ã‚‰
		- https://huggingface.co/spaces/ydshieh/Kosmos-2
- Text generation web UIã‚’ã¤ã‹ã£ã¦ã€cyberagent_calm2-7b-chat
	- https://x.com/StelsRay/status/1720137767857029444?s=20
	- ãƒ¢ãƒ‡ãƒ«ã®Loadæ™‚ã«use_fastãŒONã˜ã‚ƒãªã„ã¨å‹•ã‹ãªã„ç‚¹ãŒç½ ã ã£ãŸï¼
-  LangChain ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ - Pythonç‰ˆã€€ by npakaã•ã‚“
	- https://note.com/npaka/n/n0fd7bd3ed27b?sub_rt=share_sb
	- 11/4ç‰ˆãªã®ã§ã€æ•´ç†ã•ã‚Œã¦ã„ã‚‹ã—ã€ã€Œ**LCEL**ã€(LangChain Expression Language)ãªã‚“ã‹ã‚ˆãåˆ†ã‹ã£ãŸ
- Idempotent Generative Network
	- https://assafshocher.github.io/IGN/
	- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„æ–°ã—ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒGoogleã¨UC Berkeleyã‹ã‚‰å‡ºãŸã‚ˆã†ã ã€‚ãƒã‚¤ã‚ºé™¤å»ã¨ã„ã†ã‚ˆã‚Šã‹åˆ†å¸ƒã‚’1stepã§å¤‰æ›ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã“ã¨ã‚’ä»®å®šã™ã‚‹ã‚‰ã—ã„
- CALM2ã®GPTQç‰ˆãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚VRAMãŒå°‘ãªã„æ–¹ã¯æ˜¯éãŠä½¿ã„ãã ã•ã„ã€‚
	- https://huggingface.co/mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k
-  CALM2ã§é•·ã„æ–‡ç« ã‚’ã¾ã‚‹ã”ã¨å–ã‚Šæ‰±ã†
	- https://note.com/alfredplpl/n/n5ed2ea2b78ec?sub_rt=share_sb
- ã€è²¬ä»»ã‚ã‚‹AI: ã€ŒAIå€«ç†ã€æˆ¦ç•¥ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã€
	- https://x.com/abenben/status/1720750416361877680?s=20
- ã€Calm2-7bã€‘ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€æ–°LLMãŒå„ªç§€ã™ããŸã®ã§ã€ChatGPTã¨æ¯”è¼ƒãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã¿ãŸ
	- https://weel.co.jp/media/cyberagentlm2-7b
-  Othello is Solved
	- https://arxiv.org/abs/2310.19387
	- PFNã‹ã‚‰ã€å¼±å•é¡Œã¨ã—ã¦è§£ã‘ãŸã¨ã„ã†è©±ã€åŒæ–¹æœ€å–„æ‰‹ã®çµæœã¯å¼•ãåˆ†ã‘
-  LlamaIndex v0.8 ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ - Pythonç‰ˆ
	- https://note.com/npaka/n/nd449d5190431?sub_rt=share_b
	- ã€ŒLlamaIndexã€ã¯ã€ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®çŸ¥è­˜ã‚’å¿…è¦ã¨ã™ã‚‹å°‚é–€çŸ¥è­˜ã‚’å¿…è¦ã¨ã™ã‚‹è³ªå•å¿œç­”ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ç°¡å˜ã«ä½œæˆã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚
- ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã®é¦–ç›¸ã¯ã€C++ã§æ•°ç‹¬ã‚½ãƒ«ãƒãƒ¼ã‚’å…¬é–‹ã—ã¦ã„ã‚‹	
	- https://t.co/rWig2ugILa
- CALM2-7Bã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã™ã‚‹(11/5è¿½è¨˜)
	- https://note.com/shi3zblog/n/n8b9ff5ea62bf?sub_rt=share_sb
- ä»Šã®é«˜æ ¡ã§ã¯ã€æƒ…å ±â… ã€ã¨ã„ã†ç§‘ç›®ãŒã§ãã¦ã€ITãƒ‘ã‚¹ãƒãƒ¼ãƒˆç›¸å½“ã®ã“ã¨ã‚’å­¦ã‚“ã§ã„ã‚‹â†’â€é«˜å’ç›¸å½“â€ã®ãƒ¬ãƒ™ãƒ«ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã¨ã„ã†è©±
	- https://togetter.com/li/2253207
- ï¼¢ï¼¸ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ã€€å®Ÿè·µè¡Œå‹•çµŒæ¸ˆå­¦2.0 äººã‚’å‹•ã‹ã™å¿ƒã®ãƒ„ãƒœ
	- https://www.amazon.co.jp/dp/4296115758?ref_=cm_sw_r_cp_ud_dp_BM2H3QZ9AHNCYW8F2ZY7
	- ä¼æ¥­çµŒå–¶ã®ç¾å ´ã§ã©ã®ã‚ˆã†ã«è¡Œå‹•å¤‰å®¹ã‚’ä¿ƒã›ã°ã‚ˆã„ã®ã‹ã¨ã„ã†çŸ¥è¦‹ãŒä½“ç³»çš„ã«æ•´ç†ã•ã‚Œã¦ãŠã‚Šã€æ³•å‰‡ã‚„ç†è«–ã‚’å¯„ã›é›†ã‚ãŸã“ã‚Œã¾ã§ã®äº‹ä¾‹é›†çš„ãªè¡Œå‹•çµŒæ¸ˆå­¦æœ¬ã¨ã¯ä¸€ç·šã‚’ç”»ã™è‰¯æ›¸ã§ã—ãŸã€‚
- Xã‹ã‚‰ã€Grokç™ºè¡¨, Elonâ€™s new LLM.
	- https://x.com/xai/status/1721027348970238035?s=20
	- 330å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Grok-0ï¼ˆLLaMA 2 (70B) ã®æ©Ÿèƒ½ã«è¿‘ã¥ãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒªã‚½ãƒ¼ã‚¹ã®åŠåˆ†ã—ã‹ä½¿ç”¨ã—ãªã„ï¼‰ã‚’å…ƒã«Grok-1ã‚’é–‹ç™ºã€‚
	- Grok-1 ã¯ GPT3.5ã‚„ Inflection-1ã‚’æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è¶…ãˆã‚‹ã€‚
- CALM2-7Bã®æ€§èƒ½ã‚’ä»–ã®æ—¥æœ¬èªLLMã¨æ¯”è¼ƒã—ã¦ã¿ãŸ
	- https://note.com/it_navi/n/n35e5fac2b3d3?sub_rt=share_pb
	- CALM2-7B-Chatã¯ã€ä¸€åº¦ã«**3ä¸‡2000ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ—¥æœ¬èªã§ç´„5ä¸‡å­—ï¼‰**ã®é•·æ–‡ã®å…¥å‡ºåŠ›ã«å¯¾å¿œ
	- **CALM2-7B-Chat**ã®å›ç­”ã‚’**ELYZA-japanese-Llama-2-7b-instruct**åŠã³**Youri-7B-chat**ã®å›ç­”ã¨æ¯”è¼ƒ
	- è«–ç†çš„æ€è€ƒåŠ›ã«ã¤ã„ã¦ã¯ã€**3ç¨®é¡ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¯äº”åæ­©ç™¾æ­©**ã§å¤§å·®ã‚ã‚Šã¾ã›ã‚“ã€‚ChatGPTï¼ˆGPT-3.5ï¼‰ã®æ€§èƒ½ã¨ã¯ã€ã¾ã ç›¸å½“å·®ãŒã‚ã‚‹ã‚ˆã†ã§ã™
- ã€è¨€èªå“²å­¦ãŒã¯ã˜ã¾ã‚‹ã€é‡çŸ¢èŒ‚æ¨¹è‘—
	- https://www.iwanami.co.jp/book/b633363.html
	- æ—¥çµŒã®æ›¸è©•(11/4æœåˆŠ)æ²è¼‰
	- è¨€è‘‰ã¨ã¯ä½•ã‹ã€‚ã“ã®å•ã„ã«ãƒ•ãƒ¬ãƒ¼ã‚²ã€ãƒ©ãƒƒã‚»ãƒ«ã€ã‚¦ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã¯ã©ã†æŒ‘ã‚“ã ã®ã‹ã€‚ã¨ã³ãã‚ŠãŸã®ã—ã„è¨€èªå“²å­¦ã®èª¬ãèªã‚Š
	- å˜èªå˜ç‹¬ã§æ„å‘³ã‚’æŒã¤ã®ã‹ã€æ–‡ç« ã®ä¸­ã®é–¢ä¿‚æ€§ã¨ã—ã¦æ„å‘³ã‚’æŒã¤ã®ã‹ã€LLMã¯ä½•ã‚’è¦‹ã¦ã„ã‚‹ï¼Ÿ
- ç‰§é‡å…ˆç”Ÿã€PFNé–‹ç™ºã®MN-coreé–‹ç™ºã«æ³¨åŠ›
	- https://jun-makino.sakura.ne.jp/articles/future_sc/note161.html
	- ç¥æˆ¸å¤§ã¨PFNã®ã‚¯ãƒ­ã‚¹ã‚¢ãƒã‚¤ãƒ³ãƒˆãƒ¡ãƒ³ãƒˆã ãã†ã ã€
	- ã€Œä»Šå¾Œã¯ç¤¾å“¡ã¨ã—ã¦ç›´æ¥MN-Core ã® é–‹ç™ºã«é–¢ã‚ã‚‹ã€ã€ã€Œæ™®åŠã¨ã„ã£ãŸã“ã¨ã‚’å«ã‚ã¦Mn-Core ã® é–‹ç™ºãŒæœ¬æ ¼åŒ–ã—ã¦ã„ã‚‹ã€

## 10/30

æ–°ã—ã„LLMãŒã©ã‚“ã©ã‚“ç™ºè¡¨ã•ã‚Œã‚‹ã€‚ã€ŒJapanese Stable LM 3B-4E1Tã€ã€ŒJapanese Stable LM Gamma 7Bã€ã€7Bã®LLMã®è¦‡è€…ã¯ã€Mistral 7Bã¨ã„ã†è©±é¡Œã‚‚ã‚ã£ãŸãŒã€ReActã‚’ã“ãªã›ã‚‹7bã¯ã€Zephyr-7b-betaã¨ã„ã†ã“ã¨ã‚‰ã—ã„ã€æ—¥æœ¬èªã¯ã©ã†ã‹ï¼ŸOSSã®LLMã§æ§‹é€ çš„ãªå‡ºåŠ›(Pydantic)ã‚’å‡ºã™ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæœ‰åŠ¹ã‚‰ã—ã„ã€‚text-to-SQLã‚‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæœ‰åŠ¹ã¨ã®ã“ã¨ã€‚å¿ƒã®ç†è«–(TOM)ã‚‚ã€å¿ƒç†å­¦ã®VoEç†è«–ã®å¿œç”¨ã¨ã‹ãŒã‚ã£ãŸã€‚LLM ã® ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ã„ã‚ã„ã‚ç´¹ä»‹ã•ã‚Œã‚‹ãŒã€è‡ªå‹•è©•ä¾¡ã®çµæœãŒãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã§å¯è¦–åŒ–ã•ã‚Œã‚‹MT-BenchãŒè‰¯ã„ã‹ã‚‚ã€‚æ—¢å­˜ã®æ¦‚å¿µã‚’çµ„ã¿åˆã‚ã›ã‚‹systematic compositionalityã®èƒ½åŠ›ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒæŒã¤ã“ã¨ãŒã§ãã‚‹ã£ã¦ã®ã¯ã€ã“ã‚Œã¯ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ç†è«–ã«ã‚ˆã‚‹èªçŸ¥ã®ä»•çµ„ã¿ã®è§£æ˜ãŒä¸€æ­©ç¾å®Ÿã«è¿‘ã¥ã„ãŸã®ã‹ã€‚Prompt ã«ã‚ˆã‚‹LLMã¸ã®æŒ‡ç¤ºã‚’è¶…ãˆã‚‹ã¨ã„ã†ã€LLM programã¯ã¯ã€åˆ†å‰²çµ±æ²»ã¨ã„ã†ã‹ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ã„ã†ã‹ãã†ã„ã†æ„Ÿã˜ã€‚Microsoftã®Agent Frameworkã£ã¦å‰ã‹ã‚‰ã‚ã£ãŸã‚ˆã†ãªæ°—ã‚‚ã™ã‚‹ãŒã€ãªãœæ³¨ç›®ï¼ŸHintonå…ˆç”Ÿã¨Lecumå…ˆç”Ÿã®è­°è«–ãŒLLMã®æ¬¡ã‚’è¦‹æ®ãˆãŸè­°è«–ã§é¢ç™½ã„ã€‚é™ç•Œã¯ã€ã²ã‚‡ã‚“ãªã“ã¨ã‹ã‚‰è¶…ãˆã‚‰ã‚Œã¦ã‚†ãã¨ã„ã†æ­´å²ã‚‚ã‚ã‚‹ã‚ˆãªã€‚FastChatã§æ§˜ã€…ãªLLMã‚’è©¦ã›ã¦è©•ä¾¡ã®å¹…ãŒåºƒãŒã‚‹ã€M-Benchã‚‚FastChatåˆ©ç”¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã®ã‹ã€‚

- 7bã®ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒcolabã§å‹•ãï¼ŸVRAM 32Gç¨‹åº¦ã§è¡Œã‘ã‚‹ã¨
	- https://x.com/Sakkusakumura/status/1716158933319246289?s=20
- Character-LLM: A Trainable Agent for Role-Playing
	- https://aiboom.net/archives/57223
	- ç‰¹å®šã®äººç‰©ã€ä¾‹ãˆã°ãƒ™ãƒ¼ãƒˆãƒ¼ãƒ´ã‚§ãƒ³ã‚„ã‚¯ãƒ¬ã‚ªãƒ‘ãƒˆãƒ©ãªã©ã®è¡Œå‹•ã‚„æ„Ÿæƒ…ã‚’æ¨¡å€£ã•ã›ã‚‹ã‚ˆã†è¨“ç·´ã™ã‚‹æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Character-LLMï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼LLMï¼‰ã€
	- è¨“ç·´ã•ã‚ŒãŸLLMã¯ã€ç‰¹å®šã®äººç‰©ã¨ã—ã¦ã®è¡Œå‹•ã‚„æ„Ÿæƒ…ã‚’åŠ¹æœçš„ã«æ¨¡å€£ã§ãã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚
-  Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts
	- https://cxh0519.github.io/projects/Progressive3D/?ref=aiartweekly
	- Progressive3D brings region specific object manipulation through text with a DALL-E 3 like level of prompt understanding to the table.
	- ï¼“Dãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€æ§˜ã€…ãªåŠ å·¥ã‚’è¨€èªã§è¡Œã†
- Courtland Leer et al., "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models"
	- https://arxiv.org/abs/2310.06983
	- ã€Œå¿ƒã®ç†è«–ï¼ˆTheory of Mindï¼‰ã€ã‚’ãƒ¡ã‚¿èªçŸ¥èƒ½åŠ›ã‚’ã¤ã‹ã£ã¦å‘ä¸Šã§ãã‚‹ã€‚
	- å¿ƒç†å­¦ã«ãŠã‘ã‚‹ã€ŒViolation of Expectationï¼ˆæœŸå¾…é•åï¼‰ï¼šVoEã€ç†è«–ã‚’é©ç”¨
- llamaindexãŒã¤ã‹ã†ã€ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤ºãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹I/FãŒå…¬é–‹
	-  Accessing/Customizing Prompts within Higher-Level Modules
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompt_mixin.ipynb
- LangChainã‹ã‚‰ã€ã‚¢ãƒ‰ãƒãƒ³ã‚¹ãªRAGã§ã‚‚ã‚ã‚‹ã€"Query Transformation"
	- https://blog.langchain.dev/query-transformations/
	- è³ªå•ã®ã»ã†ã‚’å¤‰æ›ã™ã‚‹ã¨ãªï¼Ÿ
- llamaindexã§ã€HuggingFaceã®LLMã‚’æ´»ç”¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ‹¡å¼µã•ã‚ŒãŸ(ä¼šè©±ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€ãªã©ï¼‰
	- you can now plug any `conversational`, `text_generation`, `feature_extraction` endpoints 
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/huggingface.ipynb
- Finetuning LLaMa + Text-to-SQL
	- https://github.com/run-llama/modal_finetune_sql
	- how to fine-tune Llama2 for better text-to-SQL + easily plug into your LLM app, ordered from easy to hard:
	- text-to-SQLã§æœ€ã‚‚æ€§èƒ½ãŒè‰¯ã„ã®ã¯ã€GPT-4/3.5ã§ã‚‚ã€llamaã§ã‚‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚Œã°ã©ã†ã«ã‹ãªã‚‹ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹æ³•ã®æ§˜ã€…ã‚’ç´¹ä»‹ã€
- State of Open Source AI Book - 2023 Edition
	- https://book.premai.io/state-of-open-source-ai/
	- å½“ç„¶æœ¬è‡ªèº«ã‚‚OpenSoruce
	- https://github.com/premAI-io/state-of-open-source-ai
-  ComfyUI-LCMã«ã‚ˆã‚‹Vid2Vidã®é«˜é€Ÿå¤‰æ›ã‚’è©¦ã™(Latent Consistency Models)
	- https://note.com/bakushu/n/nec4cee4f4f37
	- Latent Consistency Modelsï¼ˆLCMï¼‰ã¯ã€æœ€å°é™ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§è¿…é€Ÿã«æ¨è«–ã§ãã‚‹æ–°ãŸãªç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
	- Google Colabã®æ¨™æº–GPUï¼ˆVRAM 16GBï¼‰ã§è©¦ã—ãŸã¨ã“ã‚ã€512x512ã‚µã‚¤ã‚ºã®120ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»å¤‰æ›ã§1åˆ†å¼±ã€‚1024x1024ã‚µã‚¤ã‚ºã®120ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»å¤‰æ›ã ã¨12-13åˆ†ã»ã©ã§ã—ãŸã€‚
-  AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
	- https://arxiv.org/abs/2308.08155
	- https://microsoft.github.io/autogen/
	- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆè¬¹è£½ã®Agentãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€å‰ã‹ã‚‰ã‚ã£ãŸã‚ˆã†ãªæ°—ã‚‚ã™ã‚‹ãŒã€‚
- Top AI Shops Fail Transparency Test
	- https://spectrum.ieee.org/ai-ethics#toggle-gdpr
	- Stanford transparency index rates Meta, OpenAI, and others on 100 indicators
	- The highest total score goes to Metaâ€™s Llama 2, with 54 out of 100.
-  llm-jpã‚’Colabã§è©¦ã™
	- https://note.com/alexweberk/n/n6b26b324904c?sub_rt=share_pw
	- ã€Œjaster ã‚’å«ã‚€ã‚‚ã®ã¯å›ç­”ãŒãã£ã‘ãªã„ã€ã‚‰ã—ã„ã®ã§ã€ãã‚Œã‚’é™¤ã„ãŸãƒ†ã‚¹ãƒˆ
	- æµçŸ³æ—¥æœ¬èªç‰¹åŒ–ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã‚ã£ã¦æ—¥æœ¬èªã¯è‡ªç„¶ãªå½¢ã§ç”Ÿæˆã§ãã¾ã—ãŸã€‚æ—¥æœ¬ã«é–¢ã™ã‚‹åŸºæœ¬çš„ãªçŸ¥è­˜ã‚‚å‚™ãˆã¦ã„ã‚‹ã®ã¯å¬‰ã—ã„
-  LLM ã® ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ã¾ã¨ã‚ by npakaã•ã‚“
	- https://note.com/npaka/n/ndec10f78fe2f
	- äººé–“ã‚’è©•ä¾¡è€…ã¨ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ GPT-4ã‚’è©•ä¾¡è€…ã¨ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€åŸ‹ã‚è¾¼ã¿ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	- ç¾çŠ¶ã§è‡ªå‹•è©•ä¾¡å¯èƒ½ãªæœ€è‰¯ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯GPT-4ã‚’è©•ä¾¡è€…ã¨ã™ã‚‹æ–¹æ³•ã€‚ãŸã ã—ã‚³ã‚¹ãƒˆãªã©èª²é¡ŒãŒã‚ã‚‹
- MiniGPT-V
	- https://note.com/ai_meg/n/n748acc8e824b
	- MiniGPT-4ã®APIã‚’å®Ÿè£…ã™ã‚‹ã€‚ã€€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’è‡ªç”±ã«æ“ä½œã™ã‚‹ã€‚
-  Google Colab ã§ Japanese Stable LM Gamma 7B ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n4f2d6e6c11f7?sub_rt=share_b
- æ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒJapanese Stable LM 3B-4E1Tã€ã€ŒJapanese Stable LM Gamma 7Bã€
	- https://ja.stability.ai/blog/japanese-stable-lm-3b-4e1tjapanese-stable-lm-gamma-7b
	- ç´„30å„„ã¨70å„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã®æ€§èƒ½è©•ä¾¡ã§ãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹
	- 3Bã¨7Bã®ã‚µã‚¤ã‚ºã§ãã‚Œãã‚Œåœ§å€’çš„æ€§èƒ½ã‚’èª‡ã‚‹è‹±èªLLMã€ŒStable LM 3B-4E1Tã€ã€ŒMistral-7B-v0.1ã€ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã‚µã‚¯ãƒƒã¨ã‚ã¡ã‚ƒãƒ„ãƒ¨æ—¥æœ¬èªLLM
-  Japanese research is no longer world class â€” hereâ€™s why
	- https://www.nature.com/articles/d41586-023-03290-1?error=cookies_not_supported&code=dd59d16e-8d54-49a4-95a3-8fcded36917f&utm_medium=Social&utm_campaign=nature&utm_source=Twitter#Echobox=1698226936
	- natureè¨˜äº‹ã‚ˆã‚Š
	- **è³‡é‡‘ä¸è¶³ã¨æ™‚é–“ä¸è¶³**ã€**è‹¥æ‰‹ç ”ç©¶è€…ã®ä¸æº€ã¨æ¸›å°‘**ã€€ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹ã€‚
-  Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
	- https://arxiv.org/abs/2310.15123
	- Promptã‚’è¶…ãˆãŸï¼ŸLLMè‡ªèº«ã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸€éƒ¨ã«åŸ‹ã‚è¾¼ã‚“ã§ä½¿ã†ã‚ˆã†ãªã€LLM programã¨å‘¼ã°ã‚Œã‚‹ã‚ˆã†ãªæ‰‹æ³•
- Large Language Model Programs
	- https://arxiv.org/pdf/2305.05364.pdf
	- LLMã‚’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ã‚’LLM Programã¨ã¨å‘¼ã¶ã‚‰ã—ã„ã€åˆ†å‰²çµ±æ²»ãªã‚“ã‹ãã†ãªã‚“ã ã‘ã©ã€ãƒ¡ã‚¿ãªLLMã¿ãŸã„ãªæ„Ÿã˜
-  LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions
	- https://arxiv.org/abs/2310.14029v1
	- çµæ™¶æ§‹é€ ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ã€ãã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ã£ã¦ç‰©æ€§äºˆæ¸¬ã‚’è¡Œã†ã¨å¾“æ¥ã®SOTAã§ã‚ã‚‹GNNãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé«˜ç²¾åº¦ãªäºˆæ¸¬
-  LangChain ã® Step-back Prompting ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n55f276ad2988?sub_rt=share_sb
	- (1) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã«åŸºã¥ã„ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒƒã‚¯è³ªå•ã‚’ç”Ÿæˆ  
	- (2) å…ƒã®è³ªå•ã¨ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒƒã‚¯è³ªå•ã®ä¸¡æ–¹ã‚’æƒ…å ±åé›†  
	- (3) å–å¾—ã—ãŸä¸¡æ–¹ã®æƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆ
- mmnga/japanese-stablelm-instruct-gamma-7b-gguf
	- stabilityAIã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹japanese-stablelm-instruct-gamma-7bã®gguf
	- Mistral-7bã®æ—¥æœ¬èªç‰ˆã§ã€AIã®ã¹ã‚Šã™ã¨ã•ã‚“ã‹ã‚‰æä¾›ã•ã‚ŒãŸé«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ã„ã‚‹
- ãƒ•ã‚£ãƒ¼ãƒ«ã‚ºè³å—è³è€…ã®ãƒ†ãƒ¬ãƒ³ã‚¹ãƒ»ã‚¿ã‚ªã•ã‚“ãŒã€è¨¼æ˜æ”¯æ´ç³»Leanã‚’ä½¿ã†ã“ã¨ã§è‡ªåˆ†ã®è«–æ–‡ã®ä¸­ã®ãƒã‚°ï¼ˆãƒŸã‚¹ï¼‰ã«æ°—ã¥ã„ãŸã¨ã„ã†è©±
	- https://mathstodon.xyz/@tao/111287749336059662
	- å®šç†è¨¼æ˜ç³»ãŒå®Ÿæ•°å­¦è€…ã®ãŸã‚ã«ãªã£ã¦ã„ã‚‹ã®ã‹ã€‚ã€‚ã€‚
-  KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval
	- https://huggingface.co/papers/2310.15511
	-  (e.g., 'a list of ice cream shops in San Diego')
-  LLMã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã¾ã¨ã‚
	- https://qiita.com/fuyu_quant/items/157086987bd1b4e52e80
-  Zephyr: Direct Distillation of LM Alignment
	- https://arxiv.org/abs/2310.16944
	- ãªã‚“ã‹ã™ã”ã„æ€§èƒ½ã‚‰ã—ã„ã€‚
-  Human-like systematic generalization through a meta-learning neural network
	- https://www.nature.com/articles/s41586-023-06668-3
	- æ—¢å­˜ã®æ¦‚å¿µã‚’çµ„ã¿åˆã‚ã›ã‚‹systematic compositionalityã®èƒ½åŠ›ã‚’ã€ãƒ¡ã‚¿å­¦ç¿’ã‚’æ–½ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§å®Ÿç¾ã€‚35å¹´å‰ã®Fodorï¼†Pylyshynã®ã€Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯compositionalityã‚’æŒã¦ãªã„ã€ã¨ã®ä¸»å¼µã¸ã®å¿œç­”ã¨ã—ã¦æ›¸ã„ã¦ã„ã‚‹
-  MT-Bench ã®ä½¿ã„æ–¹ by npakaã•ã‚“
	- https://note.com/npaka/n/na28f31e96599?sub_rt=share_b
	- ã€Œ[**MT-Bench**](https://chat.lmsys.org/?leaderboard)ã€ã¯ã€80ã®é«˜å“è³ªã§ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®è³ªå•ã‚’å«ã‚€ã€æ…é‡ã«ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸLLMã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã™ã€‚
	- ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã§ã§ã‚‹ã®ãŒã‚ˆã„ã€‚
-  7Bã®LLMã®è¦‡è€…ã¯ã€Mistral 7B ï¼Ÿï¼Ÿ
	- https://www.promptingguide.ai/models/mistral-7b
- Getting started  with Llama by Meta
	- Metaè¬¹è£½ã®Llmaã‚¬ã‚¤ãƒ‰
	- https://ai.meta.com/llama/get-started/
	- Yann LeCunå…ˆç”Ÿã®ãŠã™ã™ã‚ã§ã‚‚ã‚ã‚‹ã€‚
- bakLLaVA vision AI can read xrays with only 6Gb of RAM
	- https://github.com/SkunkworksAI/BakLLaVA
	- OSSã®LLMã§ãŒã‚“ç”»åƒæ¤œè¨ºãŒã§ãã‚‹ï¼Ÿ
- Zephyr-7b-betaã£ã¦ç„¡æ•µã‹ã‚‚
	- https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing
	- found itâ€™s the only 7B LLM that can handle ReAct agent tasks over data
	- ã¤ã¾ã‚Šã€dataã«å¯¾ã—ã¦ã€ReActã™ã‚‹Agentã‚’å®Ÿè£…ã§ãã‚‹å”¯ä¸€ã®7B LLMã¨ã„ã†ã“ã¨ã‚‰ã—ã„
	- Jelly Liuã•ã‚“(llamaindexä½œè€…)ã‚‚æ¿€è³
	- https://x.com/jerryjliu0/status/1718054817640390840?s=20
-  Evaluating RAG pipelines with Ragas + LangSmith
	- https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/
	- RAGã®æ€§èƒ½è©•ä¾¡ã‚’Ragasã¨LangSmithã§è¡Œã†æ–¹æ³•ã‚’ç´¹ä»‹ã—ãŸè¨˜äº‹
	- Ragasã¯LLMã«ã‚ˆã‚‹RAGã®è‡ªå‹•è©•ä¾¡ã‚’æ”¯æ´ã™ã‚‹OSSã€è©¦ã—ãŸã‘ã©ãŠé‡‘ã‹ã‹ã‚‹ã‚“ã ã‚ˆãªã€‚
- llama2 7bã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€å‡ºåŠ›ã‚’ç‰¹å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã™ã‚‹ã“ã¨ãŒã§ãã‚‹
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/finetuning/gradient/gradient_structured.ipynb
	- structured Pydantic objectsã‚’å‡ºåŠ›ã™ã‚‹
- å¸äººã®çµ±åˆå ±å‘Šæ›¸2023ã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ç‰¹è¨±æƒ…å ±åˆ†æã€‚ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å¤‰åŒ–ã«ã¤ã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹å…¨ä½“ä¿¯ç°ã¨ç‰¹è¨±ä¾¡å€¤è©•ä¾¡ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å¯è¦–åŒ–
	- https://ssl4.eir-parts.net/doc/3401/ir_material_for_fiscal_ym1/141477/00.pdf
- Hintonå…ˆç”Ÿã®ã€æ–°ã—ã„LLMã®é–‹ç™ºï¼ˆãŸã¶ã‚“OpenAI)ã«å¯¾ã™ã‚‹å±æƒ§ã«å¯¾ã—ã¦ã€Lecumå…ˆç”Ÿã¯ã€ã©ã†ã›ä»Šã®Auto-Regressive LLMã®å»¶é•·ç·šä¸Šã®é–‹ç™ºãªã®ã§ã€é™ç•Œã¯è‡ªæ˜ã„ã€‚çœŸã«å¿…è¦ãªAIã¯ã€ã€ã¨åè«–ã€‚
	- https://x.com/ylecun/status/1718263303485501784?s=20
	- Objective-Driven AI architecturesãŒå¿…è¦ã¨ã®ã“ã¨
- Advanced Prompt Engineering for RAG by llamaindex
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/prompts/prompts_rag.ipynb
	- åŸºæœ¬çš„ãªRAGã‹ã‚‰ã€few-shotè¿½åŠ ã—ãŸã‚Šã€contextå¤‰æ›ã—ãŸã‚Šã¨ã„ã†è©±é¡Œ
- Stability AI ã® Japanese MT-Bench ã‚’è©¦ã—ãŸ
	- https://x.com/npaka123/status/1718403656725483961?s=20
- Demystifying Advanced RAG Pipelines
	- https://github.com/pchunduri6/rag-demystified
-  Chatting With Your Data Ultimate Guide
	- https://medium.com/aimonks/chatting-with-your-data-ultimate-guide-a4e909591436
-  MT-Bench ã«ã‚ˆã‚‹æ—¥æœ¬èªLLMã®è©•ä¾¡ by npakaã•ã‚“
	- https://note.com/npaka/n/n0530f6f9123f?sub_rt=share_sb
	- ã€ŒStability AIã€ãŒæä¾›ã™ã‚‹**ã€ŒJapanese MT-Benchã€ã®è³ªå•ãƒ•ã‚¡ã‚¤ãƒ«**ã¨**å‚ç…§å›ç­”ãƒ•ã‚¡ã‚¤ãƒ«**ã‚’ä½¿ã†
	- è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€FastChatãŒå¯¾å¿œã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## 10/23

ä»Šé€±ã¯ã€NIIã‹ã‚‰llm-jp-13b-v1.0ãŒå…¬é–‹ã•ã‚ŒãŸã®ãŒè©±é¡Œã§ã—ãŸã€ã•ã£ããcolabã§ä½¿ã£ãŸä¾‹ãŒå…¬é–‹ã•ã‚ŒãŸã‚Šã€4bité‡å­åŒ–ç‰ˆãŒhuggingfaceã§å…¬é–‹ã•ã‚ŒãŸã‚Šã¨ã€ç››ã‚Šä¸ŠãŒã£ã¦ã¾ã™ã€‚é–¢ä¿‚è€…ã®åŠªåŠ›ã¨ABCIã®æ´»èºã«é ­ãŒä¸‹ãŒã‚Šã¾ã™ã€‚LLMæ´»ç”¨ã‚¢ãƒ—ãƒªã®æ€§èƒ½ã‚’è€ƒãˆã‚‹ã¨ãã«ã€RAGã§ã‚‚ãã†ãªã‚“ã ã‘ã©ã€LLMã¨embeddingã®çµ„ã¿åˆã‚ã›ã‚’ã¡ã‚ƒã‚“ã¨è©•ä¾¡ã™ã‚‹ã£ã¦ã®ãŒæœ€åˆã«ã‚ã‚‹ã¹ããªã®ã‹ã‚‚ã€‚ä½¿ã£ãŸã“ã¨ãªã„ã‘ã©ã‚‚Replicateã¯ãã“ã‚“ã¨ã“ã‚ã†ã¾ãã¤ã„ãŸã‚µãƒ¼ãƒ“ã‚¹å±•é–‹ã¨ã„ãˆã‚‹ã€‚LLMã‚’ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§æ´»ç”¨ã§ãã‚‹ã¨ã„ã†è«–æ–‡ãŒè©±é¡Œã«ã€‚OpenAIã€é™ã‚ŠãªãAGIã«è¿‘ã„ã¨ã†ã‚ã•ã®Arrakisã®é–‹ç™ºæ–­å¿µï¼Ÿæ˜ ç”»Duneï¼’(Arrakisã¨ã„ã†æ˜ŸãŒéƒ¨éšŠï¼‰ã®å…¬é–‹ã‚‚æ˜¥ã«ãšã‚Œè¾¼ã‚“ã ã‹ã‚‰ã€ä¼¼ãŸã‚ˆã†ãªé‹å‘½ã‚’ãŸã©ã‚‹ã®ã‹ï¼Ÿãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼ã®ãƒ¬ãƒãƒ¼ãƒˆã€ç”ŸæˆAIã«ã‚ˆã‚Šã€AIã®ä½œæ–‡åŠ›ãŒäººé–“ã®ä¸Šä½25%ã‚’è¶…ãˆã‚‹æ™‚æœŸã®äºˆæ¸¬ãŒ25å¹´å‰å€’ã—ã¨ã„ã†ã®ã¯é©šã„ãŸã€ã¤ã¾ã‚Šæˆ‘ã€…ã¯25å¹´å…ˆã®æŠ€è¡“ã‚’ä»Šè¦‹ã¦ã„ã‚‹ã“ã¨ã«ãªã‚‹ã€ãã‚Šã‚ƒï¼ˆå¤šãã®äººã«ã¯LLMã®å‡„ã•ãŒï¼‰åˆ†ã‚‰ã‚“ã‚ãªã€‚ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§ã®ã€Œç§‘å­¦è«–æ–‡ã®æŸ»èª­ã€ã«ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãŒæœ‰ç”¨ã§ã‚ã‚‹ã¨ã„ã†è«–æ–‡ã€ã“ã‚Œã¯æœ—å ±ã ï¼ˆèª°å¾—ï¼Ÿï¼‰ã€‚ã€Œkaggle LLMã‚³ãƒ³ãƒšã€€ä¸Šä½è§£æ³•ã®ã¾ã¨ã‚ã€ã¯ã“ã‚Œã¯LLMãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚·ãƒ§ãƒŠãƒ¼ã«ã¯å¿…èª­ã ã€‚ã¡ã‚ƒã‚“ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«ä¸ãˆã‚‹ã“ã¨ãŒé‡è¦ã€‚ã‚ãŸã‚Šã¾ãˆã ã‘ã©ã€ãã‚Œã‚’è¡Œã†ã®ã¯é›£ã—ã€‚ã€Œä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã€ã«å¯¾ã™ã‚‹OpenAIå…±åŒè¨­ç«‹è€…ã®Ilya Sutskeveræ°ã®å¯¾è«‡ã€å¤§è¦æ¨¡æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯è¨€è‘‰ã‚’ç”Ÿæˆã™ã‚‹ä½•ç­‰ã‹ã®è¡¨ç¾ï¼ˆã¤ã¾ã‚Šä¸–ç•Œãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’å­¦ç¿’ã—ã€ã“ã‚Œã‹ã‚‰æ¼ã‚Œå‡ºã‚‹ã‚‚ã®ãŒãƒ†ã‚­ã‚¹ãƒˆã§ã‚ã‚‹ã¨è¨€ã£ã¦ã„ã‚‹ï¼ˆãƒŠã‚¦ã‚·ã‚«ã®ã€Œå¢“æ‰€ã®ä¸»ã€ã¿ãŸã„ãªã‚‚ã®ã‹ï¼‰ã€‚LLMã®å› æœæ¨è«–èƒ½åŠ›ã®ãƒ™ãƒ³ãƒãƒ¼ãƒãƒ¼ã‚¯ã€fine-tuningã™ã‚‹ã¨æ€§èƒ½ã¯ã‚ãŒã‚‹ãŒã€å°‘ã—è¡¨ç¾ã‚’å¤‰ãˆã‚‹ã¨æ€§èƒ½ãŒçˆ†ä¸‹ãŒã‚Šã£ã¦ï¼ã€ãã‚ŒãŒLLMãªã®ã‚ˆï¼æœ€æ–°ã®è¨€èªç†è«–ã§ã‚ã‚‹ã€Œã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚²ãƒ¼ãƒ ã€ã§äººé–“ã®è¨€èªèƒ½åŠ›ãŒèº«ã«ã¤ã„ãŸã¨ã™ã‚‹ã¨ã€LLMãŒç¤ºã™ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆèƒ½åŠ›ã¯ä½•ï¼Ÿï¼ŸIlya Sutskeveræ°ã®å¯¾è«‡ã®è©±ã¨çœŸã£å‘ã‹ã‚‰å¯¾ç«‹ã™ã‚‹æ„Ÿã˜ã€‚ã€Œè¨€èªã‚²ãƒ¼ãƒ ã€ã¨ã„ãˆã°ãƒ´ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã€ã‚¦ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ç ”ç©¶ã‚’å°‚é–€ã¨ã™ã‚‹å¤§è°·å…ˆç”Ÿã®å¯¾è©±è¨˜äº‹ã«ã‚ˆã‚‹ã¨ã€‚LLMã¨è¨€èªã‚²ãƒ¼ãƒ ã£ã¦ä¼¼ãŸã¨ã“ã‚ãŒã‚ã‚‹ãã†ã ã€‚ãªã‚“ã‹ã€æ¥½ã—ããªã£ã¦ããŸã€‚

- Ilya Sutskeveræ°LLMã¨ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦èªã‚‹ with Jensen Huang, CEO of Nvidia:
	- https://twitter.com/i/status/1713368556618887670
	- OpenAIã®å…±åŒè¨­ç«‹è€…ã§ã‚ã‚‹Ilya Sutskeveræ°ã¨Nvidiaã®ensen Huangç¤¾é•·ã¨ã®å¯¾è«‡ã‚ˆã‚Š
	- ï¼ˆå·¨å¤§ãªï¼‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒå­¦ã‚“ã§ã„ã‚‹ã®ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€Œä½•ã‹ã€ã«å¯¾ã™ã‚‹è¡¨ç¾ã‚’å­¦ã‚“ã§ã„ã‚‹ã€‚ãã®ã€Œä½•ã‹ã€ã¨ã¯ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãã‚ŒãŒå°„å½±ã•ã‚ŒãŸã‚‚ã®ãŒç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãªã®ã§ã‚ã‚‹ã€‚
- Jonas Belouadi et al., "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ"
	- https://arxiv.org/abs/2310.00367
	- LLMã‚’æ´»ç”¨ã—äººé–“ã®ã‚ˆã†ã«ç§‘å­¦çš„ãªå›³ã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€AutomaTikZã€
	- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç§‘å­¦çš„ãªãƒ™ã‚¯ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ 
	- LLaMAã‚’DaTikZãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¾®èª¿æ•´
-  Can Large Language Models Infer Causation from Correlation?
	- https://arxiv.org/abs/2306.05836
	- https://ai-scholar.tech/articles/large-language-models/llm_causal_inference_skill
	-  LLMã«å› æœæ¨è«–èƒ½åŠ›ã¯ã‚ã‚‹ã‹ï¼Ÿ
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å› æœæ¨è«–èƒ½åŠ›ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆ  
	- 17ã®æ—¢å­˜ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡  
	- ç¾çŠ¶ã®ãƒ¢ãƒ‡ãƒ«ã¯å› æœæ¨è«–èƒ½åŠ›ãŒä½ã„ã“ã¨ãŒã‚ã‹ã£ãŸ
	- fine-tuningã«ã‚ˆã‚Šæ€§èƒ½å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã‚‹ä¸€æ–¹ã§ï¼Œå°‘ã—è¡¨ç¾ã‚’å¤‰ãˆãŸã ã‘ã§æ€§èƒ½ãŒä¸‹ãŒã‚‹ç¾è±¡ã‚‚è¦‹ã‚‰ã‚Œã‚‹
- Yijun Tian et al., "Graph Neural Prompting with Large Language Models"
	- https://arxiv.org/abs/2309.15427
	- LLMã«ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆçŸ¥è­˜ã‚°ãƒ©ãƒ•ï¼‰ã‚’é€£æºã•ã›ã‚‹ã“ã¨ã§ã€ã‚¿ã‚¹ã‚¯é‚è¡Œèƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Graph Neural Promptingï¼ˆGNPï¼‰ã€
	- GNPã¯ã€LLMã«æœ‰ç›ŠãªçŸ¥è­˜ã‚’åŠ¹æœçš„ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«å‹•ã„ã¦ã‚‹ã‹ã‚’è¦–è¦šçš„ã«èª¬æ˜ã™ã‚‹ã‚¤ãƒ³ãƒ•ã‚©ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãŒç´ æ™´ã‚‰ã—ã„ã¨
	- https://ig.ft.com/generative-ai/
	- Fanatical Timesã®ã‚¤ãƒ³ãƒ•ã‚©ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯
- Large Language Models for Software Engineering: Survey and Open Problems
	- https://arxiv.org/abs/2310.03533
	- LLMã‚’ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°(SE)ã«ã©ã†ã‚„ã£ã¦é©ç”¨ã™ã‚‹ã‹ï¼Ÿ
	- è¦æ±‚ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°/ãƒ‡ã‚¶ã‚¤ãƒ³ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒ†ã‚¹ãƒˆã€é‹ç”¨/ãƒ‡ãƒ—ãƒ­ã‚¤ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã€‚ã¾ãŸãƒªã‚µãƒ¼ãƒé ˜åŸŸã§ã®æ´»ç”¨ãªã©ã‚‚
	- ä¼çµ±çš„ãªSEã¨LLMã‚’èåˆã—ãŸã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«ã‚ˆã‚Šä¿¡é ¼ã‚ã‚‹åŠ¹ç‡çš„ãªLLMãƒ™ãƒ¼ã‚¹ã®SEãŒå®Ÿç¾ã§ããŸ
- JapaneseEmbeddingEvalã€€æ—¥æœ¬èªã«ãŠã‘ã‚‹embeddingã®è©•ä¾¡
	-  https://github.com/oshizo/JapaneseEmbeddingEval
	- multilingual-e5ã£ã¦ã„ã„ç·šã„ã£ã¦ã‚‹ã®ã‹ã€‚ã€‚
- PaLI-3 Vision Language Models: Smaller, Faster, Stronger
	- https://huggingface.co/papers/2310.09199
	- Googleã«ã‚ˆã‚‹ã€é«˜æ€§èƒ½ã§å°ã•ã„vision language model (VLM)
- ãƒãƒƒã‚­ãƒ³ã‚¼ãƒ¼ã‹ã‚‰ç™ºè¡¨ã•ã‚ŒãŸAIå‹•å‘ã«é–¢ã™ã‚‹ãƒ¬ãƒãƒ¼ãƒˆãŒãªã‹ãªã‹è¡æ’ƒçš„
	- https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-AI-the-next-productivity-frontier#introduction
	- ç”ŸæˆAIï¼ˆã¨ã„ã†ã‹ChatGPTã«ä»£è¡¨ã•ã‚Œã‚‹LLM)ã®ç™»å ´ã«ã‚ˆã‚Šã€AIã®ä½œæ–‡åŠ›ãŒäººé–“ã®ä¸Šä½25%ã‚’è¶…ãˆã‚‹æ™‚æœŸã®äºˆæ¸¬ãŒ25å¹´å‰å€’ã—ã«ãªã£ãŸ
		- 2017å¹´ã®äºˆæ¸¬ï¼š2050å¹´ ãƒ»2023å¹´ã®äºˆæ¸¬ï¼š2024ã€œ2025å¹´ 
- Xinyun Chen et al, "Teaching Large Language Models to Self-Debug"
	- https://arxiv.org/abs/2304.05128
	- GPT-4ãªã©LLMã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆèƒ½åŠ›ã«ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ã€SELF-DEBUGGINGï¼ˆã‚»ãƒ«ãƒ•ãƒ‡ãƒãƒƒã‚®ãƒ³ã‚°ï¼‰ã€
	- LLMã«è‡ªå·±ãƒ‡ãƒãƒƒã‚°ã®èƒ½åŠ›ã‚’æ•™ãˆã‚‹ã“ã¨ã§ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹
- ChatGPTã‚’ç”¨ã„ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å­¦ã¶æ–¹æ³•ã«ã¤ã„ã¦ï¼ˆæ…¶å¿œç¾©å¡¾å¤§å­¦ï¼‰
	- https://speakerdeck.com/keio_smilab/keio-univ-intro-to-ml-02-coding
	- ãªã‚“ã¨ã€å­¦ç”Ÿå‘ã‘ã«ã€ChatGPTã‚’ç”¨ã„ã¦Pythonãªã©ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å­¦ã¶ã¨ã„ã†æˆæ¥­ãŒã€ã€
	- ChatGPTãƒã‚¤ãƒ†ã‚£ãƒ–ãªå­¦ç”Ÿã¯ã€ChatGPTã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å­¦ã¶ã®ã‹ã€‚ã€‚
- Andrew Ngå…ˆç”Ÿã‹ã‚‰ã€deeplearning.aiã®ã€Œç”ŸæˆAIã€ã®è¬›ç¾©ã®å®£ä¼
	- https://www.deeplearning.ai/courses/generative-ai-for-everyone/
- OpenAIã€æ¬¡ä¸–ä»£LLMã§ã‚ã‚‹ã€Arrakisã®é–‹ç™ºã‚’æ–­å¿µï¼Ÿ
	-  OpenAI Dropped Work on New â€˜Arrakisâ€™ AI Model in Rare Setback
	- é™ã‚ŠãªãAGIã«è¿‘ã„ã¨ã†ã‚ã•ã•ã‚Œã‚‹æ¬¡ä¸–ä»£ã®LLMã€
	- ã©ã†ã†ã‚‚é–‹ç™ºä¸­ï¼ˆå­¦ç¿’ä¸­ï¼‰ã®æ€§èƒ½è©•ä¾¡ã§æ€ã£ãŸã»ã©æ€§èƒ½ãŒå‡ºãªã‹ã£ãŸãŸã‚ã€‚
	- https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback
- llamaindexã®Liuã•ã‚“ã‚ˆã‚Šã€â€œEvaluation Driven Developmentâ€ (EDD)ã®ææ¡ˆ
	- https://x.com/jerryjliu0/status/1713936561480610104?s=20
	- ã¾ãšã¯ã€LLMï¼‹Embeddingã®çµ„ã¿åˆã‚ã›ã‚’ã¡ã‚ƒã‚“ã¨è©•ä¾¡ã™ã‚‹ã¨ã“ã‚ã‹ã‚‰å§‹ã‚ã‚ˆã†ã¿ãŸã„ãªã€‚
- Replicateã‚’åˆ©ç”¨ã™ã‚‹ã¨ã€ä»»æ„ã®LLMã¨embeddingã®çµ„ã¿åˆã‚ã›ã‚’ç°¡å˜ã«è©•ä¾¡ã§ãã‚‹
	- https://replicate.com/explore
	- ã¤ã¾ã‚Šhuggingfaceã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‹•ã‹ã™æ‰‹é–“ã‚’ã€å°‘ã—çœãã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã€
	- ãƒŠã‚¤ã‚¹ã ãªã€‚
- NIIã‹ã‚‰ã€LLM-jp-13B ãŒå…¬é–‹ã•ã‚Œã‚‹
	- LLM-jp ï¼ˆLLM å‹‰å¼·ä¼šï¼‰ã¯ã€æ—¥æœ¬èªã¨è‹±èªã‚’ä¸­å¿ƒã«äº‹å‰å­¦ç¿’ã—ãŸ130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§å…¬é–‹
	- https://llm-jp.nii.ac.jp/release/
	- ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚„è¨“ç·´ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ç”¨ã„ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚‚å…¬é–‹
- ãƒ‡ãƒ¼ã‚¿ã§ã§ãã‚‹ã“ã¨ã®ãƒ¬ãƒ™ãƒ«æ„Ÿã‚’ç†è§£ã™ã‚‹ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«åºã®äººã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚ˆã‚Šï¼‰
	- https://speakerdeck.com/hik0107/data-design-and-government?slide=10
	- ç¾çŠ¶ã®æŠŠæ¡(lv.1)ã€åˆ†è§£ã¨å·®ç•°ã®æŠŠæ¡(Lv.2)ã€åŸå› ã®æŠŠæ¡(Lv.3)ã€å¯¾ç­–ã®æŠŠæ¡(Lv4)
-  Google Colab ã§ LLM-jp-13B ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n60b0abf54ed5?sub_rt=share_sb
	- T4 ãƒã‚¤ãƒ¡ãƒ¢ãƒªã§å‹•ä½œç¢ºèª
	- æ—©é€Ÿè©¦ã•ã‚Œã¦ã„ã‚‹
- BEYOND MEMORIZATION: VIOLATING PRIVACY VIA INFERENCE WITH LARGE LANGUAGE MODELS
	- https://arxiv.org/pdf/2310.07298v1.pdf
	- Redditã®åŒ¿åãƒã‚¹ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€GPT-4ã¯ãã®äººã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåå…¥ã€æ€§åˆ¥ã€ä½æ‰€ï¼‰ã‚’85%ã®æ­£ç¢ºã•ã§ã€ã‹ã¤äººé–“ã®1%ã®ã‚³ã‚¹ãƒˆã§å½“ã¦ãŸã€‚ã€‚
	- A paper that really illustrates both the unexpected power, and unexpected risks, that come from LLMs.
-  InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
	- https://arxiv.org/abs/2310.07713
	- pre-train LLMs with Retrieval Augmentation
-  An Emulator for Fine-Tuning Large Language Models using Small Language Models
	- https://huggingface.co/papers/2310.12962
	- Emulator for Fine-Tuning(EFT)ã¯ã€å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å°è¦æ¨¡ãªå¾®èª¿æ•´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹ã“ã¨ã§ã€å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸçµæœã‚’ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã¨ã„ã†ã€ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«ãªã£ãŸ
-  Can large language models provide useful feedback on research papers? A large-scale empirical analysis
	- https://arxiv.org/abs/2310.01783
	- ã€Œç§‘å­¦è«–æ–‡ã®æŸ»èª­ã€ã«ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãŒæœ‰ç”¨ãªå¯èƒ½æ€§ãŒã‚ã‚‹
	- ç±³ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§ã‚‰ãŒæ¤œè¨¼ã€€å‚åŠ è€…ã®80ï¼…ä»¥ä¸Šã€ŒAIæŸ»èª­ã¯æœ‰ç›Šã€
	- https://www.itmedia.co.jp/news/articles/2310/19/news072.html
	- Natureç³»åˆ—ã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®çµæœã€GPT-4ãŒæä¾›ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆã®57.55ï¼…ã¯ã€å…¨ä½“ã®æŸ»èª­è€…ã®ä¸­ã§å°‘ãªãã¨ã‚‚1äººã®äººé–“ã®æŸ»èª­è€…ãŒè¨˜è¼‰ã—ã¦ã„ãŸ
- A quantized version of the mistral that is instruction following over 32k tokens.
	- https://huggingface.co/TheBloke/MistralLite-7B-AWQ
	- mistralã£ã¦æ€§èƒ½ãŒã‚ˆã„ã¨å…ˆé€±è©•åˆ¤ã«ãªã£ã¦ãŸã‚„ã¤ã®ã€4bité‡å­åŒ–ç‰ˆãŒå…¬é–‹ï¼Ÿ
- llm-jp-13b-v1.0ã‚‚æ—©é€ŸGPTQç‰ˆãŒå…¬é–‹ã•ã‚Œã‚‹
	- https://huggingface.co/mmnga/llm-jp-13b-v1.0-4bit-g128-GPTQ-calib-ja-1k
	- llm-jp-13b-v1.0ã‚’ã€ æ—¥æœ¬èªã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã§ç”Ÿæˆã—ãŸGPTQãƒ¢ãƒ‡ãƒ«
-  è¨€èªã¯ã“ã†ã—ã¦ç”Ÿã¾ã‚Œã‚‹â€•ã€Œå³èˆˆã™ã‚‹è„³ã€ã¨ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚²ãƒ¼ãƒ â€•
	- https://www.shinchosha.co.jp/book/507311/
	- è¨€èªã®ç”Ÿå¾—æ€§ã‚’å¦å®šã—ã€æ–‡åŒ–é€²åŒ–ã‚„èªç”¨è«–çš„ãªè¦³ç‚¹ã‹ã‚‰è¨€èªç²å¾—ã‚’è«–ã˜ã¦ã„ã¾ã™
	- æ­´å²ï¼šãƒãƒ¼ãƒ ãƒ»ãƒãƒ§ãƒ ã‚¹ã‚­ãƒ¼ã¯ã€Œæ™®éæ–‡æ³•ã€ã¨ã„ã†æ¦‚å¿µã‚’å°å…¥ã—ã€ã€Œäººé–“ã®éºä¼çš„é’å†™çœŸã«ã¯ã€è¨€èªã‚’æ”¯é…ã™ã‚‹æŠ½è±¡çš„ãªæ•°å­¦çš„åŸç†ãŒå†…åŒ…ã€ã—ã¦ã„ã‚‹ã¨ã„ã£ãŸ
	- æ­´å²ï¼šå¿ƒç†å­¦è€…ã‚¹ãƒ†ã‚£ãƒ¼ãƒ–ãƒ³ãƒ»ãƒ”ãƒ³ã‚«ãƒ¼ãŒã•ã‚‰ã«ã€è¨€èªã‚’ç”Ÿã¿ã ã™æœ¬èƒ½ã€ï¼ˆNHKãƒ–ãƒƒã‚¯ã‚¹ï¼‰ã¸ã¨ç™ºå±•ã•ã›ã‚‹
	- ä¸»å¼µï¼šã€Œã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚²ãƒ¼ãƒ ã€ã€‚è¨€èªã¯éºä¼çš„ã«æ±ºå®šã•ã‚ŒãŸã‚‚ã®ãªã©ã§ã¯ãªãã€èº«æŒ¯ã‚Šæ‰‹æŒ¯ã‚Šã€ç™ºå£°ã€ã‚ã‚‹ã„ã¯ãã®ä¸¡æ–¹ã§è‡ªåˆ†ã®æ„æ€ã‚’åŒæ–¹å‘çš„ã«ä¼ãˆåˆã†ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚²ãƒ¼ãƒ ã®ã‚ˆã†ãªã‚‚ã®ãŒèµ·æºãªã®ã§ã¯ãªã„ã‹ã¨ã„ã†æ–¬æ–°ãªã‚¢ã‚¤ãƒ‡ã‚¢ã ã€‚ãã“ã«ã¯æ™®éæ–‡æ³•ãŒå…¥ã‚Šè¾¼ã‚€ä½™åœ°ãªã©ãªã„ã€‚
- LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€Œè¨€èªã‚²ãƒ¼ãƒ ã€çš„ã‹  æ±äº¬å¥³å­å¤§å­¦ç¾ä»£æ•™é¤Šå­¦éƒ¨å‡†æ•™æˆãƒ»å¤§è°·å¼˜æ°ã«èãï¼ˆï¼‘ï¼‰
	- [ITæ‰¹è©•ã®è¨˜äº‹](https://it-hihyou.com/recommended/llm%EF%BC%88%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%89%E3%81%AF%E3%80%8C%E8%A8%80%E8%AA%9E%E3%82%B2%E3%83%BC%E3%83%A0%E3%80%8D%E7%9A%84%E3%81%8B-%E2%80%95/)ã‚ˆã‚Š
	- LLMã£ã¦ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å­¦ã³ã€ãã®èƒŒå¾Œã«ã¯äººé–“ãŒã‚ã‚‹ã‹ã‚‰ã€ãƒ´ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã®ã„ã†ã€Œè¨€èªã‚²ãƒ¼ãƒ ã€ã«ä¼¼ã¦ã„ã‚‹ã€ã‚‰ã—ã„ã€‚è¨˜å·æ¥åœ°ã—ã¦ãªã„ã¨ã„ã†æ‰¹åˆ¤ã«ã‚‚ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èƒŒå¾Œã®äººé–“ã®ã‚ãŸã‚Šã§æ¥åœ°ã—ã¦ã„ã‚‹ã®ã‹ã‚‚ã¨ã‚‚ã„ã†ã€‚
- å¤šæ§˜ä½“ä¸Šã®æœ€é©åŒ–ç†è«–
	- https://www.amazon.co.jp/exec/obidos/ASIN/4274231186?&linkCode=sl1&tag=mathlang09-22&linkId=bd145734052442298eb01413d823ca91&language=ja_JP&ref_=as_li_ss_tl
	- å¤šæ§˜ä½“ä¸Šã®æœ€é©åŒ–ç†è«–ã«ã¤ã„ã¦ã€åŸºç¤ã¨ãªã‚‹æ•°ç†ã‹ã‚‰å¿œç”¨ä¾‹ã¾ã§ã‚’è§£èª¬
-  Introducing CliffordLayers: Neural Network layers inspired by Clifford / Geometric Algebras.
	- https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/articles/introducing-cliffordlayers-neural-network-layers-inspired-by-clifford-geometric-algebras/
	- MSç ”ç©¶æ‰€ã‹ã‚‰ã€ã‚¯ãƒªãƒ•ã‚©ãƒ¼ãƒ‰ä»£æ•°ã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸæ–°ã—ã„NNãƒ¬ã‚¤ãƒ¤ã®ç™ºæ˜
-  OpenAgents: An Open Platform for Language Agents in the Wild
	- https://arxiv.org/abs/2310.10634v1
- llamaindexã‚ˆã‚Šã€Unifying Text-to-SQL and RAG with our SQLRetrieve
	- https://docs.llamaindex.ai/en/latest/examples/index_structs/struct_indices/SQLIndexDemo.html
	- SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å¯¾ã—ã¦ã€RAGã‚’è¡Œã†Retriverã«ã¤ã„ã¦ã€å‹•ã„ãŸãã€å½¹ã«ç«‹ã¤ãã€‚
- kaggle LLMã‚³ãƒ³ãƒšã€€ä¸Šä½è§£æ³•ã¾ã¨ã‚
	- https://zenn.dev/yume_neko/articles/7347ba6b081e93
	- ç§‘å­¦åˆ†é‡ã®5æŠå•é¡Œã‚’è§£ãLLMã®ç²¾åº¦ã‚’è¦å‰‡ã‚³ãƒ³ãƒšã®ã¹ã‚¹ãƒ—ãƒ©
	- ä»Šå›ã®ã‚³ãƒ³ãƒšã§ä¸Šä½ã«è¡Œãã«ã¯RetrievalãŒæœ€ã‚‚ã‚­ãƒ¼ã ã£ãŸã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ã‚„ã¯ã‚Šæ­£è§£æƒ…å ±ã‚’ç›´æ¥å‚ç…§ã§ãã‚‹ã®ã§ã€contextã‚’ã‚ˆã‚Šè‰¯ãã™ã‚‹ã“ã¨ãŒé‡è¦ã ã£ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚
- llama2ã®pretrainingã‚’è©¦ã™
	- https://zenn.dev/if001/articles/6c507e15cd958b
	- å°ã•ã„ã‚µã‚¤ã‚ºã®llama2ã‚’æ—¥æœ¬èªã§pre_trainingã—ã¦ã¿ã¾ã™
	- pre_trainingã‹ã‚‰huggingfaceã¸ã®uploadã¾ã§ã‚’è¡Œã£ã¦ã¿ã¾ã—ãŸã€‚
	- å°ã•ã„ã‚µã‚¤ã‚ºã§ã‚ã‚Œã°google colabã§å­¦ç¿’ã§ãã‚‹
- llm-jp-13b-v1.0-gguf
	- https://huggingface.co/mmnga/llm-jp-13b-v1.0-gguf
	- llm-jpã•ã‚“ãŒå…¬é–‹ã—ã¦ã„ã‚‹llm-jp-13b-v1.0ã®ggufãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ç‰ˆ
	- ãƒ–ãƒ©ãƒ³ãƒã‚‰ã—ã„ã€LLama.cppãŒã€ãªã‚“ã‹ã®å¤‰æ›´ã‚’è¡Œã†ã¨ggufãŒå‹•ã‹ãªããªã‚‹ã‚‰ã—ã„ã€æ€–ã£

## 10/16

RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½å‘ä¸Šã¯ä¾ç„¶ã‚‚ã‚Šã‚ãŒã£ã¦ã„ã‚‹ã€‚Stanfordã®DSpyã€ã©ã†ã‚‚LLMã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ©ç”¨ã‚’åˆ¥ã®æ¬¡å…ƒã«å¼•ãä¸Šã’ã‚‹ç”»æœŸçš„ãªé–‹ç™ºã®ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒè¿½ã„ã¤ã‘ãªã„ã€‚RAGã¨Finetuningã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸ŠãŒã„ã¾ã¾ã§æŠœã‘ã¦ã„ãŸã¨ã¯ã€‚LLMã®å¿ƒã®ç†è«–(ToM)ã«ã¤ã„ã¦ã®è«–æ–‡ã§ã¯ã€ä»–äººã®å¿ƒã®çŠ¶æ…‹ã®æ¨å®šã¨ã„ã†ã®ãŒè‚ãªã®ã‹ã€‚zephyr-7b-alphaã¨ã‹ã€Japanese StableLM Instruct Alpha v2 ã¨ã‹ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½¿ã„ã‚‚ã®ã«ãªã‚‹LLMã‚‚ã©ã‚“ã©ã‚“å‡ºã¦ããŸã€‚ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰AIã®ã€State of AI Report 2023ã€ Kaggleã®AI Report 2023ã€ãã‚Œãã‚Œã®ç«‹å ´ã§æœ€æ–°ã®AIã‚’å–ã‚Šå·»ãæ§˜ã€…ãªè¦–ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã‚Œã¦ã„ã‚‹ã€‚ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ï¼ˆé¡æ¨ï¼‰ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€Œã‚¢ãƒŠãƒ­ã‚¸ã‚«ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã€ã¯ã€äººé–“ã®æ‰‹é–“ã‚’çœã‘ã‚‹ã‹ï¼Ÿçµ„ã¿è¾¼ã¿(embeding)ã®é•ã„ã«ã‚ˆã‚‹RAGæ€§èƒ½ã®é•ã„ã®æ¤œè¨¼ã‹ã‚‰ã€ã‚„ã£ã±e5(intfloat/multilingual-e5-large)ãŒå½“é¢æœ€å¼·ãªã®ã‹ï¼ŸPFNã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã®æˆæœãªã©ãŒã„ãã¤ã‹å…¬é–‹ã€‚ãã‚Œã«ã—ã¦ã‚‚PFNã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã¤ã‚ˆã¤ã‚ˆã ã‚ã†ã€ã¡ã‚‡ã£ã¨ã†ã‚‰ã‚„ã¾ã—ã„ã€‚DeepMindã®Yasunagaã•ã‚“ã‚„ã‚¨ã‚¸ãƒ³ãƒãƒ©å¤§å­¦ã®Matsubaraã•ã‚“ãªã©ã®æ—¥æœ¬äººã®æ´»èºã‚‚ã¡ã‚‰ã»ã‚‰ã€‚


- Large Language Models (in 2023)
	- https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0
	- OpenAIã®Hyung Won Chungã•ã‚“ã«ã‚ˆã‚‹LLMã®ç¾çŠ¶ã‚’ã¾ã¨ã‚ãŸã‚¹ãƒ©ã‚¤ãƒ‰
	- The biggest progress in the past 10 years (or even more) can be summarized as
		- Create weaker inductive biases and scale up
		- Do not teach machines how we think we think. Let it learn in a machineâ€™s way
- Masking PII Data in RAG Pipeline
	- https://betterprogramming.pub/masking-pii-data-in-rag-pipeline-326d2d330336
	- PII(Personal Identification Information)ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ã€RAGã«ãŠã„ã¦è¡Œã†æ–¹æ³•
	-  LlamaIndexã® NERPIINodePostprocessorã‚’æ´»ç”¨ã™ã‚‹ã®ãŒã¿ã
- Jerryã‚ˆã‚Šã€RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½å‘ä¸Šã«é–¢ã™ã‚‹ã€æ§˜ã€…ãªæ‰‹æ³•ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯é›†
	- Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex
		- https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5
	- Building Performant RAG Applications for Production
		- https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/production_rag.html
	- Multi-Document Agents
		- https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html
	- Finetuning
		- https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/finetuning.html
- Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models
	- https://arxiv.org/abs/2310.04406
	- Substantially improving over the existing prompting methods such as Reflexion, e.g., 68.1% -> 86.9% on HumanEval with GPT-3.5
- Ida Momennejad et al., "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval"
	- https://arxiv.org/abs/2309.15129
	- äººé–“ã®æ¸¬å®šæ³•ã¨ä¼¼ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§LLMã®èªçŸ¥æ©Ÿèƒ½ã‚’èª¿æŸ»ã—ãŸè«–æ–‡
	- LLMã®ã€ŒèªçŸ¥ãƒãƒƒãƒ—ã€ã¨ã€Œè¨ˆç”»èƒ½åŠ›ã€ã‚’è©•ä¾¡ã€‚
		- èªçŸ¥ãƒãƒƒãƒ—ï¼šå¤–éƒ¨ç’°å¢ƒã‚’å†…éƒ¨ã«è¡¨ç¾ã™ã‚‹æ©Ÿèƒ½ 
		- è¨ˆç”»èƒ½åŠ›ï¼šç›®æ¨™ã«å‘ã‹ã£ã¦è¨ˆç”»ã‚’ç«‹ã¦ã¦é‚è¡Œã™ã‚‹èƒ½åŠ›
	- GPT-3.5ã€GPT-4ã€Bardã€LLaMA-13Bãªã©
	- çµæœ
		- â‘  èªçŸ¥ãƒãƒƒãƒ—ã®ç†è§£ã‚„è¨ˆç”»èƒ½åŠ›ã¯ã€Œç®±ã‹ã‚‰å‡ºã—ã¦ã™ãã«ã€ã¯æŒã£ã¦ã„ãªã„ 
		- â‘¡ èªçŸ¥ãƒãƒƒãƒ—ã®æ¬ å¦‚ãŒç†ç”±ã§è¨ˆç”»ã‚¿ã‚¹ã‚¯ã«å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ 
		- â‘¢ æ–°ã—ã„è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼ˆCogEvalï¼‰ã¯æœ‰æœ›ã§ã‚ã‚‹ 
		- â‘£ LLMã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å·¥å¤«ã®ä½™åœ°ãŒã‚ã‚‹ 
		- â‘¤ LLMã®èªçŸ¥æ©Ÿèƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã«ã¯ã€ãƒ¡ãƒ¢ãƒªï¼ˆè¨˜æ†¶å®¹é‡ï¼‰ã®æ‹¡å¼µãªã©ãŒæœ‰åŠ¹
- FireAct: Toward Language Agent Fine-tuning
	- https://fireact-agent.github.io/
	- LLM Agentã¨Fintuningã®åˆã‚ã›æŠ€ã«ã¤ã„ã¦ã®æ¤œè¨¼ã€ReActã®æ€§èƒ½ã‚’fine-tuningã§å‘ä¸Šã§ããŸ
	- FireAct is a novel way to finetune LMs w/ agent trajectories of a mix of tasks & prompting methods.
	- Fine-tuning >> Prompting:
		- Notably, small LMs benefit most --- Llama2-7B improves 77% after fine-tuning!
- Pei Zhou et al., "How FaR Are Large Language Models From Agents with Theory-of-Mind?"
	- https://arxiv.org/abs/2310.03051
	- LLMã®ã€Œå¿ƒã®ç†è«–(ToM:Theory of Mind)ã€ã«ãŠã‘ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Thinking for Doing (T4D)ã€
		- â‘  ä»–è€…ã®å¿ƒã®çŠ¶æ…‹ï¼ˆä¿¡å¿µã€é¡˜æœ›ã€æ„å›³ãªã©ï¼‰ã«ã¤ã„ã¦ã©ã‚Œã ã‘åŠ¹æœçš„ã«æ¨è«–ã§ãã‚‹ã‹
		- â‘¡ æ¨è«–ã—ãŸä¸Šã§ã„ã‹ã«è¡Œå‹•ã«ç§»ã›ã‚‹ã‹
	- å¾“æ¥ã®å¿ƒç†å­¦çš„ãƒ†ã‚¹ãƒˆã§ã¯LLMã®ToMèƒ½åŠ›ã®è©•ä¾¡ã¯ååˆ†ã«ã¯å‡ºæ¥ãªã„ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- ã€ŒForesee and Reflect (FaR)ã€ã¨ã„ã†æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
		- â‘  å°†æ¥ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’äºˆæ¸¬ï¼ˆForeseeï¼‰ 
		- â‘¡ ãã‚Œã«å¯¾ã™ã‚‹è¡Œå‹•ã‚’è€ƒæ…®ï¼ˆReflectï¼‰
	- ã€ŒFaRã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨è©•ä¾¡ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã€ŒThinking for Doing (T4D)ã€ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã£ã¦ã€åŠ¹ç‡çš„ã«LLMã®ToMèƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã‚‹
- 7 Query Strategies for Navigating Knowledge Graphs With NebulaGraph and LlamaIndex
	- https://www.nebula-graph.io/posts/Knowledge-Graph-and-LlamaIndex
	- NebulaGraph ã‚’ä½¿ã£ã¦ã‚°ãƒ©ãƒ•æ§‹é€ ã«å¯¾ã™ã‚‹ã€Q&Aã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ via Llamaindex
- Stanfordã®DSpyã‚’ç”¨ã„ã‚‹ã“ã¨ã«ã‚ˆã‚‹ã€Q&Aã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç°¡å˜ã«ãªã‚‹ï¼Ÿ
	- https://x.com/lateinteraction/status/1712135660797317577?s=20
- Kaggleã®AI Report 2023
	- https://www.kaggle.com/AI-Report-2023
	- ã“ã‚Œã¯AIã®ç¾çŠ¶ã«é–¢ã™ã‚‹ã‚¨ãƒƒã‚»ã‚¤ã‚³ãƒ³ãƒšã®çµæœã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã€æœ€æ–°ã®AIã‚’å–ã‚Šå·»ãæ§˜ã€…ãªè¦–ç‚¹ã‹ã‚‰ã®è¦‹æ–¹ãŒã‚ã‹ã‚‹ã€‚
- HuggingFaceã«ãŠã‘ã‚‹LLMè©•ä¾¡ã§ã€zephyr-7b-alphaãŒChatLlama 70Bã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ã ã—ãŸã‚‰ã—ã„ã®ã§llamaindexã§ç¢ºã‹ã‚ã¦ã¿ãŸ
	- https://colab.research.google.com/drive/16Ygf2IyGNkb725ZqtRmFQjwWBuzFX_kl?usp=sharing#scrollTo=lMNaHDzPM68f
	- We found that it is the ONLY open 7B model atm that does well on advanced RAG/agentic task
- DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation
	- https://huggingface.co/papers/2309.16653
	- ã“ã“ã§ï¼“Dãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’è©¦ã›ã‚‹ã€ãªã‚“ã‹ã™ã”ã„ãã€‚
		- https://huggingface.co/spaces/jiawei011/dreamgaussian
-  Multimodality and Large Multimodal Models (LMMs)
	- https://huyenchip.com/2023/10/10/multimodal.html
	- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤ã€‚é‡è¦è«–æ–‡ã¨ã—ã¦CLIPã¨Flamingoã‚’è§£èª¬ã—ãŸä¸Šã§ã€ä»Šå¾Œã®æ–¹å‘æ€§ã¨ã—ã¦ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®è¿½åŠ ã€å‡ºåŠ›ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŒ–ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ•´å‚™ãªã©ã‚’æŒ™ã’ã¦ã„ã‚‹
- LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression
	- https://arxiv.org/abs/2310.06839
	- Gains a performance boost of up to 17.1% on NaturalQuestions over the original prompt with ~4x fewer tokens
- MatGPT: A Vane of Materials Informatics from Past, Present, to Future
	- https://onlinelibrary.wiley.com/doi/10.1002/adma.202306733?af=R
	- **GPT AI**ã®å‡ºç¾ã¯ã€ç§‘å­¦ç ”ç©¶åˆ†é‡ãŒã€Œãƒ‡ãƒ¼ã‚¿ã€ã‚’åŸºæœ¬è¦ç´ ã¨ã—ã€ã€Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  + è¨ˆç®—èƒ½åŠ›ã€ã‚’æ ¸å¿ƒç”Ÿç”£åŠ›ã¨ã™ã‚‹çŸ¥èƒ½æ–‡æ˜æ™‚ä»£ã«å…¥ã£ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
	- äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€æŒ‡å‘æ€§è¨­è¨ˆãƒ¢ãƒ‡ãƒ«ã€å”èª¿å­¦ç¿’ã€å®Ÿé¨“ãƒ­ãƒœãƒƒãƒˆãªã©
- PFNã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç™ºè¡¨ï¼š éºä¼â¼¦ã«é–¢ã™ã‚‹ã‚°ãƒ©ãƒ•ã‚’åˆ©â½¤ã—ãŸãƒ¢ãƒ‡ãƒ«ã®é–‹ç™º
	- https://tech.preferred.jp/ja/blog/model-learning-using-gene-graph/
	- RNAã‹ã‚‰Proteinã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã¯ã€å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒé™ã‚‰ã‚Œã€ã‹ã¤ä½¿ç”¨ã§ãã‚‹ç‰¹å¾´é‡ãŒå°‘ãªã„çŠ¶æ³ã«ãŠã„ã¦ã¯ã€äºˆæ¸¬å¯¾è±¡ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®åˆ¶å¾¡ã«é–¢ä¸ã™ã‚‹ç‰¹å®šã®ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æ€§èƒ½ã®æ”¹å–„ãŒèªã‚ã‚‰ã‚Œã¾ã—ãŸã€‚
- ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒOpenCaml2ã‚’é–‹ç™ºä¸­ã‚‰ã—ã„
	- https://aws.amazon.com/jp/blogs/news/open-calm-and-openai-chatgpt-accuracy-on-jaqket-experiment-in-amazon-sagemaker/
- RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation
	- https://arxiv.org/abs/2310.04408
	- LLMã§ã®RAGã®æ€§èƒ½å‘ä¸Šã®ãŸã‚ã«ã€2ã¤ã®åœ§ç¸®å™¨(é‡è¦éƒ¨åˆ†æŠ½å‡ºãƒ»è¤‡æ•°æ–‡æ›¸è¦ç´„)ã‚’ä½¿ã†RECOMPæ³•ã®ææ¡ˆã€‚å„åœ§ç¸®å™¨ã¯å­¦ç¿’ã•ã›ã‚‹å¿…è¦æœ‰
- æ©Ÿæ¢°å­¦ç¿’æ³¢å‹•é–¢æ•°ï¼Ÿï¼Ÿ
	- https://www.nature.com/articles/s41524-023-01130-4
	- å¾“æ¥ã¯1ç¨®é¡ã®æ§‹é€ ã—ã‹è¨“ç·´ã«ä½¿ãˆã¾ã›ã‚“ã§ã—ãŸãŒã€ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’å¯¾ç§°æ€§ã«åŸºã¥ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã§æ§˜ã€…ãªæ§‹é€ ã‚’è¨“ç·´ã§ãã€è»¢ä½ãŒã‚ã‚‹ç´„5000åŸå­ã‚»ãƒ«ã®é›»å­çŠ¶æ…‹äºˆæ¸¬ã‚’å®Ÿç¾ã—ãŸ
- Google Colab ã§ Japanese StableLM Instruct Alpha v2 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n0e463dbbce11?sub_rt=share_sb
	- ã€ŒStability AI Japanã€ãŒé–‹ç™ºã—ãŸ7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªLLM
	- å•†ç”¨åˆ©ç”¨ã‚’åˆ¶é™ã—ãªã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€åŒç­‰ãƒ¬ãƒ™ãƒ«ã®æ€§èƒ½ã‚’æŒã¤**å•†ç”¨åˆ©ç”¨ãŒå¯èƒ½**
	- Colabç„¡æ–™æ (T4)ã§å‹•ä½œã™ã‚‹æ¨¡æ§˜
- StanfordAIã«ã‚ˆã‚‹ã€ State of AI Report 2023
	- https://www.stateof.ai/2023-report-launch
	- OpenAIã®**GPT-4**ã¯ã€ã™ã¹ã¦ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„äººé–“å‘ã‘ã®è©¦é¨“ã«ãŠã„ã¦ä»–ã®LLMã‚’å‡Œé§•ã—ã¦ã„ã‚‹ã€‚
	- Meta AIã¯ã‚ªãƒ¼ãƒ—ãƒ³ï¼ˆãªï¼‰AIã®ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã¨ã—ã¦ç™»å ´ã—ã€LLaMaãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æœ€ã‚‚å¼·åŠ›ãªå…¬é–‹ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªOpenAIä»£æ›¿å“ã¨ãªã£ã¦ã„ã‚‹
	- LLMã‚„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ã€ç‰¹ã«ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¨ãƒ³ã‚¹åˆ†é‡ã§å®Ÿç”¨çš„ãªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’ã‚‚ãŸã‚‰ã—ã¦ãŠ
	- ç”ŸæˆAIãŒã€ä½è¿·ã—ã¦ã„ã‚‹ã€ãƒ†ãƒƒã‚¯ç•Œéšˆã®VCã‚’æ•‘ã†ã€‚
	- å®‰å…¨æ€§ã¯AIç ”ç©¶ç•Œã§ä¸­å¿ƒçš„ãªãƒ†ãƒ¼ãƒã¨ãªã‚Šã€ä¸–ç•Œä¸­ã®æ”¿åºœã‚„è¦åˆ¶æ©Ÿé–¢ãŒå¯¾ç­–ã‚’è¬›ã˜å§‹ã‚ãŸã€‚
	- æ¨™æº–çš„ãªLLMã¯é ‘å¥æ€§ã«å•é¡ŒãŒã‚ã‚Šã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ãŒå›°é›£ã«ãªã£ã¦ã„ã‚‹
- Michihiro Yasunaga et al., "Large Language Models as Analogical Reasoners"
	- https://arxiv.org/abs/2310.01714
	- äººé–“ã®ã€Œéå»ã®é¡ä¼¼äº‹ä¾‹ã€ã¨ã€Œè‡ªã‚‰ã®çŸ¥è¦‹ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å€£ã£ãŸã€LLMã®å„ªã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
	- LLMã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹CoTã¯æœ‰ç”¨ã§ã™ãŒã€æ‰‹é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ æ‰‹å‹•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæ¥­ã‚’å°‘ã—ã§ã‚‚è»½æ¸›ã™ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ 
	- ãã“ã§ç ”ç©¶è€…ã‚‰ã¯ã€äººé–“ã®ã‚ˆã†ã«è‡ªå‹•çš„ã«çŸ¥è­˜ã‚’ç”Ÿæˆã™ã‚‹ã€Œã‚¢ãƒŠãƒ­ã‚¸ã‚«ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã€ã‚’ç™ºæ˜ã—ã¾ã—ãŸã€‚
- Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems
	- https://arxiv.org/pdf/2310.07680.pdf
	- ã‚¨ã‚¸ãƒ³ãƒãƒ©å¤§å­¦ã®æ¾åŸã•ã‚“ã®è«–æ–‡
	- infinite-dimensional Hamiltonian system behind Bayesian inference.
	- ãƒ™ã‚¤ã‚ºæ¨è«–ã®è£ã«ã€ç„¡é™æ¬¡å…ƒã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚‹ã¨ã„ã†ã€ã€
- Zijun Liu et al., "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization"
	- https://arxiv.org/abs/2310.02170
	- è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å”åŠ›ã—ã¦ä»•äº‹ã‚’é–‹å§‹ã•ã›ã€ã‚¿ã‚¹ã‚¯ã®é€²è¡Œã«å¿œã˜ã¦é‡è¦ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å–æ¨é¸æŠã™ã‚‹ã€Dynamic LLM-Agent Networkï¼ˆDyLANï¼‰ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
	- ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦å‹•çš„ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠã™ã‚‹æ–¹å¼ã‚’è€ƒãˆã¾ã—ãŸã€‚
- LangChain ã‚’ä½¿ã£ãŸ RAG ã«ãŠã‘ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
	- https://note.com/alexweberk/n/ncccfdab3f4bb
	- Wikipedia è¨˜äº‹ã‚’ LangChain ã® CharacterTextSplitter ã‚’ä½¿ã£ã¦ã€ï¼”ç¨®é¡ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€RAG ã«ã‚ˆã‚‹è³ªå•å¿œç­”ã‚’è©¦è¡Œ
	- `intfloat/multilingual-e5-large` >= `pkshatech/GLuCoSE-base-jap` > `cl-nagoya/sup-simcse-ja-large` >= `openai/text-embedding-ada-002` ã¨ã„ã†ã‚ˆã†ãªæ„Ÿè§¦
	- 4ã¤ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸRAGã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸ: 
		- intfloat/multilingual-e5-large 
		- cl-nagoya/sup-simcse-ja-large 
		- pkshatech/GLuCoSE-base-ja 
		- openai/text-embedding-ada-002
- OpenAI gpt-3.5-turbo ã¨ gpt-3.5-turbo-instruct ãƒ¢ãƒ‡ãƒ«ã®é•ã„ã«ã¤ã„ã¦
	- https://corp.langcore.org/media/chatgpt-instruct
	- gpt-3.5-turbo ãƒ¢ãƒ‡ãƒ«ã¯ä¼šè©±ã«ç§€ã§ã¦ã„ã‚‹ã®ã§å¯¾è©±ã‚’ã•ã›ã‚‹ã®ã§ã‚ã‚Œã°ã“ã¡ã‚‰ã‚’ä½¿ã†æ–¹ãŒã‚ˆã„ã§ã™ã€‚
	- ä¼šè©±ä»¥å¤–ã®ã‚¿ã‚¹ã‚¯ã®å ´åˆã ã¨**ä¸€å•ä¸€ç­”ã®ã‚ˆã†ãªå˜ç´”ãªèª²é¡Œã‚’è§£ãã‚±ãƒ¼ã‚¹ã§ã¯ gpt-3.5-turbo-instruct ã®æ–¹ãŒæœŸå¾…ã™ã‚‹å‡ºåŠ›ã«ãªã‚‹å¯èƒ½æ€§**ãŒã‚ã‚Šã¾ã™ã€‚
- Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction
	- https://arxiv.org/abs/2310.05627
	- IJCAIã§LLM(chatGPT)ä½¿ã£ãŸæ ªä¾¡ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬ã®è«–æ–‡
	- LLMã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã¨æ ªå¼ã®ç‰¹å¾´ã‚’åŒã˜semantic spaceã§é…ç½®ã•ã›ã‚‹å¼·åŒ–å­¦ç¿’ã®æ çµ„ã¿ã‚’å°å…¥ã—ã¦ã„ã‚‹ã€‚
- Zhiyu Chen et al., "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting"
	- https://arxiv.org/abs/2310.07146
	- GPT-4ã‚’ã‚»ãƒ©ãƒ”ã‚¹ãƒˆã¨ã—ã¦å®Ÿè¡Œã—ã€äººã€…ã®ã€ŒèªçŸ¥ã®æ­ªã¿ã€ã‚’è¨ºæ–­ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Diagnosis of Thought (DoT)ã€
	- â‘  DoTã¯ã€ã€ŒèªçŸ¥ã®æ­ªã¿ã€è©•ä¾¡ã¨åˆ†é¡ã§é«˜æ€§èƒ½ã‚’ç¤ºã—ãŸ 
	- â‘¡ GPT-4ã¯ã€ã€ŒèªçŸ¥ã®æ­ªã¿ã€åˆ†é¡ã§ç‰¹ã«é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸ 
	- â‘¢ å°‚é–€å®¶ã«ã‚ˆã£ã¦GPT-4ã«ã‚ˆã‚‹æœ¬è¨ºæ–­æ–¹æ³•ã¯ã€ŒåŒ…æ‹¬çš„ã§ã‚ã‚‹ã€ã¨è©•ä¾¡ã•ã‚ŒãŸï¼ˆ84.5%ï¼‰
-  Large Language Models can Learn Rules
	- LLMãŒãƒ«ãƒ¼ãƒ«ã‚’å­¦ç¿’ã§ãã‚‹ï¼Ÿ
	- https://arxiv.org/abs/2310.07064
	- LLMs can learn (sometimes uncommon) rules with 2 stages: (1) induction: generate and verify rules from exemplars; (2) deduction: utilize the rule library for new problems. 11-27% gain on reasoning tasks that require rule learning.

## 10/10

function callã‚’å«ã‚€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’OpenAIãŒå°å…¥ã•ã‚ŒãŸã‚Šã€LLMã®RAGã«å¯¾ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã®è€ƒå¯ŸãŒã‚ã£ãŸã‚Šã¨ã€æ€§èƒ½é¢ã§ã®è©•ä¾¡ã‚’å«ã‚RAGé–¢ä¿‚ã¯æˆç†Ÿã—ã¦ããŸæ„Ÿã˜ã€‚LLMãŒã©ã‚Œã ã‘è«–ç†çš„ã‹ã¨ã„ã†æ¤œè¨¼ã‚‚ã€Œé€†è»¢ã®å‘ªã„ã€ã‚’ä¾‹ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ãŒã€LLMãŒã€Œç‰©äº‹ãŒã©ã®ã‚ˆã†ã«ä½ç½®ã¥ã‘ã‚‰ã‚Œã€æ™‚é–“ãŒã©ã®ã‚ˆã†ã«é€²è¡Œã™ã‚‹ã‹ã‚’ç†è§£ã€ã—ã¦ã„ã‚‹ã¨ã„ã†å®Ÿé¨“ã¯ã“ã‚Œã‹ã‚‰ã®LLMã‚’ç”¨ã„ãŸè¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã®é–‹ç™ºã«å¼¾ã¿ã§ã‚‚ã‚ã‚‹ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã§ã¯ã€æ—©é€ŸGPT-4Vã«å¯¾æŠ—ã™ã‚‹OSSã§ã‚ã‚‹LLaVAãŒç™»å ´ã—ãŸã€LLaVA-1.5ã¯ã™ã§ã«è©¦ã›ã‚‹æ¨¡æ§˜ã€‚ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ãŒAIã«é–¢ã™ã‚‹å¤šè§’çš„ãªãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒãƒ¼ãƒˆã™ã°ã‚‰ã—ã„ã€‚OpenAIã¯ã€GPT-4Vã«å¼•ãç¶šãã€DALLÂ·E 3ã«å¯¾ã™ã‚‹å“è³ªã‚«ãƒ¼ãƒ‰(System Card)ã‚’å…¬é–‹ã€å®‰å…¨ãªç”»åƒç”Ÿæˆã‚’ã‚¢ãƒ”ãƒ¼ãƒ«ã€‚MSã®DeepSpeedãƒãƒ¼ãƒ ã®ç§‘å­¦çš„åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã£ã¦ã‚‚é ‘å¼µã£ã¦ã‚‹ãªã€æ°—è±¡äºˆæƒ³ã«ä½¿ãˆãã†ã€‚Microsoftã®H100 GPUå¯¾æŠ—ãƒãƒƒãƒ—ã®ATHENAã€æœ¬å½“ã«ã‚„ã‚‹æ°—ãŒã‚ã‚‹ã®ã‹ï¼Ÿ


- ã€é€†è»¢ã®å‘ªã„ã€:ã€ŒAã¯Bã§ã‚ã‚‹ã€ã¨å­¦ç¿’ã—ãŸLLMã¯ã€ã€ŒBã¯Aã§ã‚ã‚‹ã€ã¨å­¦ç¿’ã—ã¥ã‚‰ããªã‚‹ã€‚
	- https://arxiv.org/abs/2309.12288
	- LLMãŒã©ã‚Œã ã‘è«–ç†çš„ã‹ï¼Ÿã¨ã„ã†å•ã„ã«å¯¾ã—ã¦ã€LLMã®è‹¦æ‰‹ãªç‚¹ã‚’æŒ™ã’ã‚‹
	- ã€é€†è»¢ã®å‘ªã„ã€LLMã¯ã€çŸ¥è­˜ã‚’æ§‹é€ åŒ–ã—ã€â€å¸°çµã‚’ä¸»èªã«ã—ã¦åŒã˜ã“ã¨ã‚’è¨€ã†â€ã®ãŒè‡ªå‹•çš„ã«ã¯ã§ããªã„
	- LLMã®ã€Œé€†è»¢ã®å‘ªã„ã€ã‚’èªè­˜ã—ãŸä¸Šã§ã™ã¹ãã“ã¨ã®è€ƒå¯Ÿ
-  Knowledge Graph Construction w/ WikiData Filtering  by llamaindex
	- https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/knowledge_graph2.html
	- REBELã‚’ç”¨ã„ã¦ã€æ–‡ç« ã‚ã‹ã‚‰çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã™ã‚‹æ–¹æ³•ã«ãŠã„ã¦ã€Wikipediaã‚’ãƒ•ã‚£ãƒ«ã‚¿ã¨ã—ã¦ç”¨ã„ã‚‹ã“ã¨ã§ã€æ˜¥å¸‚ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŠ‘ãˆã‚Œã‚‹
- Ronen Eldan et al., "Who's Harry Potter? Approximate Unlearning in LLMs"
	- https://arxiv.org/abs/2310.02238
	- LLMã®è¨˜æ†¶ã®ä¸€éƒ¨ã‚’æ„å›³çš„ã«å¿˜å´ã•ã›ã‚‹
	- ç´„1GPUæ™‚é–“ã®å¾®èª¿æ•´ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯Harry Potteré–¢é€£ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã¾ãŸã¯å›æƒ³ã™ã‚‹èƒ½åŠ›ã‚’åŠ¹æœçš„ã«æ¶ˆå»
-  Fine-tuning with Retrieval Augmentation  by llamaindex
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_retrieval_aug.html
	- https://arxiv.org/abs/2310.01352
	- gpt-4ã¨DatasetGeneratorã‚’ã¤ã‹ã£ã¦ã€æ­£è§£qaãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
	- gpt-3.5-turboã‚’æ­£è§£qaãƒ‡ãƒ¼ã‚¿ã‚’ã¤ã‹ã¦ã€RAGã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
	- çµæœcorrectnesã¯ã€ç´ ã®LLMï¼3.2ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï¼3.65ã€
- éä¾µè¥²ã®è„³æ´»å‹•ã‚»ãƒ³ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
	- Decoding speech from non-invasive recordings of brain activity
	- https://huggingface.co/papers/2208.12266
	- contrastive learningã¨ã„ã†ã®ã‚’ã¤ã‹ã£ã¦ã€è„³æ³¢ã‹ã‚‰ã‚¹ãƒ”ãƒ¼ãƒã‚’æ¨å®š
- OpenAIãŒã€function calling fine-tuningæ©Ÿèƒ½ã‚’æ–°ãŸã«è¿½åŠ ã€€by llamaindex
	-  Fine Tuning with Function Calling
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/openai_fine_tuning_functions.html
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/finetuning/openai_fine_tuning_functions.ipynb
	- æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã‚’LLMã‹ã‚‰å¾—ãŸã„ã¨ãã«ã€functio/n callã‚’ã¤ã‹ã†ã‚‰ã—ã„ãŒã€ã“ã®æ©Ÿèƒ½ã‚’fine-tuneã™ã‚‹ã“ã¨ãŒã§ãã‚‹
- LLMã¯ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’ã‚‚ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
	-  Language Models Represent Space and Time
	- https://arxiv.org/abs/2310.02207
	- LLMã¯ã‚·ãƒ³ãƒ—ãƒ«ã«çµ±è¨ˆï¼ˆç¢ºç‡ï¼‰ã‹ã‚‰æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã®ã§ã¯ãªãã€ã€Œç‰©äº‹ãŒã©ã®ã‚ˆã†ã«ä½ç½®ã¥ã‘ã‚‰ã‚Œã€æ™‚é–“ãŒã©ã®ã‚ˆã†ã«é€²è¡Œã™ã‚‹ã‹ã‚’ç†è§£ã€ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã¾ã—ãŸã€‚ ã¤ã¾ã‚Šã€LLMãŒ"ä¸–ç•Œãƒ¢ãƒ‡ãƒ«"ã‚’å½¢æˆã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã¨ã„ã†å ±å‘Š
	- ä¸–ç•Œã€ç±³å›½ã€NYCã®åœ°åã€æ­´å²çš„äººç‰©ã€èŠ¸è¡“ä½œå“ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ãªã©ã‚’å«ã‚€6ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„
	- ç©ºé–“ã¨æ™‚é–“ã®ç†è§£åº¦ã¯ã€LLMã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹éšå±¤ã‚’åŠåˆ†ã¾ã§é€²ã‚“ã ã¨ã“ã‚ã§å“è³ªãŒå‘ä¸Šã—ã€ãã®ã‚ã¨é™ç•Œç‚¹ã«é”ã™ã‚‹
	- LLMãŒã€Œä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã€ã‚’å½¢æˆã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã®ã§ã‚ã‚Œã°ã€LLMãŒã‚ˆã‚Šé«˜åº¦ãªèªçŸ¥ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã§ãã‚‹ã“ã¨ã«ç¹‹ãŒã‚Šã¾ã™ã€‚ ä¾‹ãˆã°è‡ªå‹•é‹è»¢è»Šã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«LLMã‚’æ´»ç”¨ã™ã‚‹ã®ã¯å„ªã‚ŒãŸæˆ¦ç•¥ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- huggingface/transformers v4.34ã®æ›´æ–°ã¯ã‹ãªã‚Šagressive
	- https://github.com/huggingface/transformers/releases/tag/v4.34.0
	- tokenizerã®æŒ™å‹•ã‚’ç´°ã‹ãåˆ¶å¾¡ã—ã¦ã„ãŸäººãŸã¡ã«ã¨ã£ã¦ã¯ã†ã‚Œã—ã„ã‹ã‚‚
- ModuLoRA is the first method to finetune 3-bit LLMs
	- 3-bitã‚„2-bitã«é‡å­åŒ–ã—ãŸLLMã®è©±é¡Œã®è£ã«ã‚ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ModulLoRAãŒå…¬é–‹
	- https://browse.arxiv.org/pdf/2309.16119.pdf
- RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS
	- https://arxiv.org/abs/2310.03025
	- NVIDIAã‚ˆã‚ŠRAGã¨Context Window (CW)ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒè«–æ–‡ã€‚4K CWã®LLMï¼‹RAGã¯ã€16K CWã®LLMã¨åŒç­‰ã€32K CWã®LLaMA2-70Bï¼‹RAGã¯é•·ã„Contextã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦GPT-3.5-turbo-16kã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã¨äº‹ã‚’å®Ÿè¨¼åˆ†æ 
- llama.cpp å˜ä½“ã§ LoRA ä½œã‚Œã‚‹æ©Ÿèƒ½ãŒè¿½åŠ 
	- https://github.com/ggerganov/llama.cpp/pull/2632
- Why you should build RAG from scratch - with Jerry Liu from LlamaIndex
	- LlamaIndexã®ä¸­ã®äººã«èãå›ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã€RAGã€ReActã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç­‰ã€…ã«ã¤ã„ã¦JerryãŒã©ã†è€ƒãˆã¦ã‚‹ã‹è´ã‘ã‚‹ã€‚RAGã¯ãƒãƒƒã‚¯ã ã¨è¨€ã„åˆ‡ã£ã¦ã¦é¢ç™½ã„ã€‚
	- https://www.latent.space/p/llamaindex?utm_campaign=post&utm_medium=web
-  Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study
	- https://arxiv.org/abs/2307.08072
	- é‡å­åŒ–ã•ã‚ŒãŸLLMã«ã¤ã„ã¦ã€ä¸€èˆ¬çš„ã«LLMã§ç™ºç¾ã™ã‚‹ã¨ã•ã‚Œã¦ã„ã‚‹in-context learningã€chain-of-thought, instruction-followingã¨ã„ã£ãŸèƒ½åŠ›ãŒã©ã®ç¨‹åº¦ä¿ã¦ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã—ãŸç ”ç©¶ã€‚çµæœã¨ã—ã¦4-bitã¾ã§ã®é‡å­åŒ–ã§ã‚ã‚Œã°æ€§èƒ½ã®åŠ£åŒ–ãŒè¦‹ã‚‰ã‚Œãªã„ã“ã¨ã‚’ç¢ºèª
- OpenAIã®Super aligment
	- https://openai.com/blog/introducing-superalignment
	- â€œSuperintelligence will be the most impactful technology humanity has ever invented.â€
	- Superintelligence "could lead to ... human extinction. ... We believe [superintelligence] could arrive this decade."
- æ—©é€ŸGPT-4Vã«å¯¾æŠ—ã™ã‚‹OSSã§ã‚ã‚‹LLaVAãŒç™»å ´
	-  LLaVA: Large Language and Vision Assistant
	- https://llava-vl.github.io/
	- Haotian Liu et al., "Improved Baselines with Visual Instruction Tuning"
	- https://arxiv.org/abs/2310.03744
	- ãŠè©¦ã—ã§ãã‚‹ã€https://llava.hliu.cc/
- How to build ChatGPT for your company data? by ABACUS AI
	- llama2ã‚’ä½¿ã†ã®ãŒè‰¯ã„ã¿ãŸã„ã€€
	- https://x.com/Saboo_Shubham_/status/1710505571072278932?s=20
- æ­£å‰‡åŒ–é …ä»˜ãç·šå½¢å›å¸°ã¯çœŸã®åå›å¸°ä¿‚æ•°ã‚’æ¨å®šã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ
	- https://bob3.hatenablog.com/entry/2023/10/06/224133
	- æ­£å‰‡åŒ–é …ä»˜ãç·šå½¢å›å¸°ï¼ˆRidgeã€LASSOã€Elastic netï¼‰ã§çœŸã®åå›å¸°ä¿‚æ•°ã‚’æ¨å®šã§ãã‚‹ã®ã‹ï¼Ÿã‚’å®Ÿé¨“ã—ã¦ã¿ã¾ã—ãŸã€‚
- RAGã«ãŠã‘ã‚‹chankã‚µã‚¤ã‚ºã«ã¤ã„ã¦
	- https://docs.google.com/presentation/d/18Z7H3WSncPzLOTHKZAj36w0E7HSGY78VkDooSzvvySE/edit#slide=id.g286c47b4bb8_1_0
	- More chunks â‰  better (lost in the middle problems / context overflows)
	- Reranking retrieved chunks doesnâ€™t necessarily improve results, in fact can worsen them.
- Science Behind Why LLMs Can Easily Be Tricked And Are Predictably Gullible
	- https://x.com/bindureddy/status/1710504584496779675?s=20
	- while large language models exhibit impressive linguistic abilities, their lack of true understanding, combined with the intricacies of data-driven learning, makes them susceptible to errors and easy to fool.
- æ–°ã—ã„OSSã®embeddingãƒ¢ãƒ‡ãƒ«gte-tinyãŒç™»å ´ã€OpenAIã®text-embedding-ada-002ãªã¿ã®èƒ½åŠ›ã‚’ã‚‚ã¡ã¤ã¤ã€å°ã•ãã¦è»½ã„
	- https://huggingface.co/TaylorAI/gte-tiny/tree/main
- OpenAI, "DALLÂ·E 3 System Card"
	- https://openai.com/research/dall-e-3-system-card
	- DALLÂ·E 3ã§ã®å®‰å…¨å¯¾ç­–
	- OpenAIã¯ã€DALLÂ·E 3ã®è«–æ–‡ã‚’é€šã—ã¦ã€Œç”»åƒç”ŸæˆAIã®å®‰å…¨æ€§ã¯å‰é€²ã—ãŸã€ã“ã¨ã‚’å ±å‘Š
- Artificial Intelligence Index Report 2023
	- https://arxiv.org/abs/2310.03715
	- ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ãŒAIã«é–¢ã™ã‚‹æŠ€è¡“ãƒ»æ³•å¾‹ãƒ»çµŒæ¸ˆãƒ»ç’°å¢ƒãƒ»ä¸–è«–ãªã©ã®å¤šè§’çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦ã¾ã¨ã‚ãŸå ±å‘Šæ›¸ã€ŒAI index Report 2023ã€ã‚’arxivã«å…¬é–‹
- MSã®DeepSpeedãƒãƒ¼ãƒ ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ç§‘å­¦å¿œç”¨ã‚’ç›®æŒ‡ã—ãŸDeepSpeed4Scienceãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
	- https://deepspeed4science.ai/
	- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/japanese/README.md
	- ç§‘å­¦çš„åŸºç›¤ãƒ¢ãƒ‡ãƒ«(SFM)ã¨ã‚ˆã¶ã‚‰ã—ã„
	- ClimaXã¯ã€ã•ã¾ã–ã¾ãªæ°—è±¡ãŠã‚ˆã³æ°—å€™ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸæœ€åˆã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã™
	- åˆ†å­å‹•åŠ›å­¦ã¨æ©Ÿæ¢°å­¦ç¿’å‹åŠ›å ´
	- å¤©æ°— from Microsoft Start
- Google Colabã«ã¤ã„ã«AIæ©Ÿèƒ½ãŒæ¥ã¦ã‚‹ï¼Ÿ
	- Proã«ã—ã‹æ¥ã¦ãªã„ã‚‚ã‚ˆã†ã€‚
- Best Practices for LLM Evaluation of RAG Applications A Case Study on the Databricks Documentation Bot
	- https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG
	- RAGï¼ˆRetrieval Augmented Genenerationï¼‰ã®è©•ä¾¡ã€ç‰¹ã«"LLMã‚’ä½¿ã£ãŸæ™‚ä»£è©•ä¾¡ã®è¦³ç‚¹"ã‹ã‚‰ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- æ§˜ã€…ãªLLMãŒä½•ãŒã§ãã‚‹ã‹ã®æ¯”è¼ƒè¡¨ by llamaindex
	- https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking
	- ã¡ã‚‡ã£ã¨ã€llama2-7b-4bitãŒæ‚²ã—ã„çµæœã«ã€‚ã€‚
		- OpenAI models (gpt-3.5-turbo, gpt-3.5-turbo-instruct, gpt-4)
		-  Anthropic models (claude-2, Claude-instant-2)
		- llama2-chat-7b 4bit
		- Mistral-7b
- Microsoftã€Nvidia GPUä¾å­˜ã¸ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«ã¤ãªãŒã‚‹AIãƒãƒƒãƒ—ã‚’æ¥æœˆãƒ‡ãƒ“ãƒ¥ãƒ¼ã¸
	- https://texal.jp/2023/10/08/microsoft-is-developing-its-own-ai-chip-and-working-with-amd-to-stop-nvidias-monopoly/
	- ã€Œ**Athena**ã€ï¼‘ï¼‘æœˆã®é–‹ç™ºè€…ä¼šè­°ã§ç™ºè¡¨äºˆå®šï¼Ÿ
	- NVIDIAã®H100 GPUã¨åŒç­‰ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹


## 10/2

ä»Šé€±ã¯ã€ã„ã‚„ã€ä»Šé€±ã‚‚ã„ã‚ã„ã‚ã‚ã‚Šã™ãã¦ã€æ¶ˆåŒ–ã—ãã‚Œãªã„ã€‚GPT-4V(ision) ãƒ‡ãƒ“ãƒ¥ãƒ¼ã€ç”»åƒç†è§£ã¨ã‹ã€ã¤ã„ã«LLMãŒçœ¼ã‚’æŒã£ãŸï¼ˆã‚«ãƒ³ãƒ–ãƒªã‚¢ç´€ï¼‰ã€ã“ã‚Œã£ã¦AGIã®å‰è§¦ã‚Œï¼Ÿã€€GPT-4VåˆæœŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã¯ã€éœãŒé–¢ãƒ‘ãƒ¯ãƒã‚’å…¥åŠ›ã—ãŸã‚Šã€å¤©ä¸€ã®ãƒãƒ¼ã‚¯ã‚‚ã€Œä¾µå…¥ç¦æ­¢ã€æ¨™è­˜ã¨èª¤èªè­˜ã¯ã—ãªã„ã€ã‚µã‚¤ã‚¼ãƒªã‚¢ã®ã€Œé–“é•ãˆæ¢ã—ã€ã¯è‹¦æ‰‹ã€ã¨ã„ã†å ±å‘Šã‚‚ã€‚ChatGPTã«ã‚‚visionã‚„éŸ³å£°å¯¾è©±æ©Ÿèƒ½ãŒæ¥é€±ã‹ã‚‰ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹(Plusä»¥ä¸Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰ã€‚Amazonã¯ã€ç”ŸæˆAIã®Anthropicã«5900å„„å††å‡ºè³‡ã€‚QuoraãŒæä¾›ã™ã‚‹ [poe.com](http://poe.com) ã§è©¦ç”¨ã§ãã‚‹ã€‚Googleã®GPT-4è¶…ãˆã®Geminiã¯æ°´æ›œ(10/4)ã«ç™ºè¡¨ã•ã‚Œã‚‹ã€‚LLMã§LLMã‚’è©•ä¾¡ã™ã‚‹LLM-as-a-judge ãŒã¯ã‚„ã‚Šã€‚ä¸€æ–¹OpenAIã®æ¬¡ä¸–ä»£LLMã§ã‚ã‚‹Arrakisã¯AGIã ã¨ã„ã†ã†ã‚ã•ã‚‚ï¼ˆã‚³ãƒ¼ãƒ‰ãƒãƒ¼ãƒ ã¯ã€Œç ‚ã®æƒ‘æ˜Ÿã€ã‹ã‚‰ãã¦ã„ã‚‹ï¼Ÿï¼Ÿï¼‰ã€‚ç‰¹è¨±æ¤œç´¢ã ã‘ã‹ã‚‰ã‚¦ã‚¤ãƒ«ã‚¹è–¬ã‚’ç™ºè¦‹ã—ãŸã‚Šã€éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®‰å®šãªæº–çµæ™¶ã®åŒ–å­¦çµ„æˆã‚’ã‚ãã‚‰ã‹ã«ã—ãŸã‚Šã¨ã€ãƒã‚¤ã‚ªãƒ»ææ–™ç³»ã§LLMã¯å¤§æ´»èºã€‚PFN ã® PLaMo-13B ã€4 bit é‡å­åŒ–ã™ã‚‹ã¨Colab ç„¡æ–™ç‰ˆã§å‹•ããã€‚æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šåå¾®åˆ†æ–¹ç¨‹å¼ã‚’è§£ãè©±ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€†è¨­è¨ˆã§ãã‚‹ãªã‚‰ã°ç”»æœŸçš„ã™ãã‚‹ã€‚ Gaussian Splatã‚’ã¤ã‹ã£ãŸä¸‰æ¬¡å…ƒç”Ÿæˆã®è«–æ–‡ã¨GitHubå…¬é–‹ãŒåŒæ™‚ã«ï¼’ã‹æ‰€ã§ï¼ã€‚LINEã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã«ã‚ˆã‚‹é‡å­åŒ–ã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ã®åŠ¹æœæ¸¬å®šã€ã“ã“ã¾ã§ï¼–é€±é–“ã§ã§ãã‚‹ã®ã‹ã€‚ChatGPTã®æ¤œç´¢ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¾©æ´»ã€ã©ã†ã‚‚æœ¬æ¥ãƒšã‚¤ã‚¦ã‚©ãƒ¼ãƒ«ã§å®ˆã‚‰ã‚Œã¦ã„ã‚‹è¨˜äº‹ã§ã‚ã£ã¦ã‚‚å…¨æ–‡ãŒè¡¨ç¤ºã•ã‚Œã¦ã—ã¾ã†ã¨ã„ã†å ±å‘Šã§åœæ­¢ã—ã¦ã‚‚ã®ã«å¯¾ç­–ãŒæ‰“ãŸã‚ŒãŸæ¨¡æ§˜ã€‚RAGé–¢ä¿‚ã®é€²æ—ã‚‚ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚„MergeRetrieverãªã©é€²å±•ãŒã‚ã‚‹ã€‚

- Agents: LLMã‚’ã¤ã‹ã£ãŸæ–°ã—ã„agentãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ„ãƒ¼ãƒ«è»
	- https://github.com/aiwaves-cn/agents
	- **Agents** is an open-source library/framework for building autonomous language agents. The library is carefully engineered to support important features including **long-short term memory**, **tool usage**, **web navigation**, **multi-agent communication**, and brand new features including **human-agent interaction** and **symbolic control**.
- llamaindexã‹ã‚‰neo4jã‚’ä½¿ã£ãŸã‚°ãƒ©ãƒ•agent
	- https://llamahub.ai/l/tools-neo4j_db
	- The `Neo4jQueryToolSpec` class provides a way to query a Neo4j graph database based on a provided schema definition.
-  LLM Fine-Tuning (æ±å¤§æ¾å°¾ç ”LLMè¬›åº§ Day5è³‡æ–™)
	- https://speakerdeck.com/schulta/llm-fine-tuning-dong-da-song-wei-yan-llmjiang-zuo-day5zi-liao
- OSSã®LLMã¯ã GAFAMã®LLMã«å‹ã¡ç›®ãŒã„ãªã„ã‹ã‚ã‚‹ã‹ï¼Ÿ
	- https://x.com/bindureddy/status/1706092114063639035?s=20
	- OSSã®LLMã¯ã€AIã®æ°‘ä¸»åŒ–ã¨é€æ˜æ€§ã®ãŸã‚ã«ã¯å¿…è¦ã¨ã„ã†è©±
-  LLMã‚’ç”¨ã„ãŸLLMã®è‡ªå‹•è©•ä¾¡ã«ã¤ã„ã¦ ã€œå¯èƒ½æ€§ã¨æ³¨æ„ç‚¹
	- https://engineers.ntt.com/entry/2023/09/25/091245
	- LLM-as-a-judge ã§ã¯ã€**äººæ‰‹è©•ä¾¡ã«åŒ¹æ•µã™ã‚‹ã‚¯ã‚ªãƒªãƒ†ã‚£ã®è©•ä¾¡ã‚’ã€ãŠé‡‘ã‚„æ™‚é–“ã€åŠ´åŠ›ã‚’ã‹ã‘ãšã«æ©Ÿæ¢°çš„ã«è¡Œãˆã‚‹**ã“ã¨ãŒæœŸå¾…ã§ãã¾ã™ã€‚
-  Community-developed checklists for publishing images and image analyses(Nature)
	- https://www.nature.com/articles/s41592-023-01987-9
	- ç”»åƒã‚„ç”»åƒè§£æçµæœã‚’å ±å‘Šã™ã‚‹éš›ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«é–¢ã™ã‚‹Nature MethodsèªŒã®è¨˜äº‹
	- ç”»åƒã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚„æ³¨é‡ˆã€è‰²ã®é¸æŠã€ãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨å¯èƒ½æ€§ã€ç”»åƒè§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å ±å‘Šã«é–¢ã™ã‚‹é‡è¦ãªæ¨å¥¨äº‹é …ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚
- OpenAIã‹ã‚‰GPT-4V(ision) ãŒç™ºè¡¨ã€ã¤ã„ã§ã«å“è³ªã‚«ãƒ¼ãƒ‰System Cardã‚‚å…¬é–‹
	- https://cdn.openai.com/papers/GPTV_System_Card.pdf
	- GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities
	- è¤‡é›‘ãªæ¨™è­˜ã‚’èª­ã¿å–ã‚‹ã€https://x.com/petergyang/status/1707169696049668472?s=20
	- ã‚µã‚¤ã‚¼ãƒªã‚¢ã®ã€Œé–“é•ãˆã•ãŒã—ã€ã®æ­£ç­”ç‡ã¯ï¼‘å‰²ã€https://x.com/cumulo_autumn/status/1707574932153282728?s=20
	- GPT-4V vs. éœãŒé–¢ã€€https://x.com/horromary/status/1707373718534824305?s=20
- å¤–éƒ¨çŸ¥è­˜ã«ã‚ˆã‚Šãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ 
	- https://www.jstage.jst.go.jp/article/jjske/22/2/22_TJSKE-D-22-00053/_article/-char/ja/
	- æ§˜ã€…ãªæ¦‚å¿µã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢å¿ƒã‚’æ¨å®šã—ï¼ŒçŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã™ã‚‹æ‰‹æ³•ã‚’ç”¨ã„ã¦ï¼Œé›‘è«‡ã«ãŠã‘ã‚‹å…±æ„Ÿæ€§ã‚„æƒ…å ±æä¾›ã‚’ç›®æŒ‡ã™
- ChatGPT(Pllusãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¥ä¸Šï¼‰ã«ã€æ¥é€±ã‹ã‚‰æ–°æ©Ÿèƒ½ã‚’roll-outã™ã‚‹ã¨ã®ç™ºè¡¨
	- Voice Capabilities:
	- Image Interaction
	- New Text-to-Speech Model:
	- Collaboration with Spotify
-  Amazonã€ç”ŸæˆAIæ–°èˆˆã«5900å„„å††å‡ºè³‡ã€€Microsoftã«å¯¾æŠ—
	-  Claude-2-100kã¯ã€Anthropicã®æœ€ã‚‚å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã§ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒ10ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç´„75,000èªï¼‰
	- ã°ã£ã¡ã‚Šæ—¥æœ¬èªã«ã‚‚å¯¾å¿œã—QuoraãŒæä¾›ã™ã‚‹ [poe.com](http://poe.com)  ã§å®Ÿéš›ã«ä½¿ã£ã¦ã¿ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- llamaindexã®Auto Merging Retriever
	- https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html
	- æœ¨æ§‹é€ ã§æ•´ç†ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¯¾ã—ã¦é¡ä¼¼ã™ã‚‹æã‹ã‚‰é †ã«ãƒãƒ¼ã‚¸ã—ã¦è¦‹ã›ã‚‹ã‚‰ã—ã„ã€‚
	- RAGã‚’è©•ä¾¡ã™ã‚‹æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’GPT4ã§ç”Ÿæˆã™ã‚‹ã€DatasetGeneratorã‚‚ã¤ã„ã§ã«ç´¹ä»‹ã€‚ã„ã‚ã‚†ã‚‹ã€ LLM-as-a-judge ã®ä¸€ç¨®ã‚’lllamaindexãŒnativeã‚µãƒãƒ¼ãƒˆã—ãŸ
- ç‰¹è¨±ã‹ã‚‰åˆ†å­ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
	-  Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures
	- https://arxiv.org/abs/2309.08765v1
	- ChatGPTã‚’ä½¿ã£ã¦ç‰¹è¨±ã‹ã‚‰10ä¸‡ä»¶ã®åˆ†å­ã¨é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é«˜ç²¾åº¦ã«æŠ½å‡ºã€ã“ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¦ã‚¤ãƒ«ã‚¹è–¬ã‚’é€†æ¢ç´¢ã™ã‚‹ã¨ãã‚Œã£ã½ã„åˆ†å­ã‚’æŠ½å‡ºã§ããŸ
	- ç‰¹è¨±åˆ†æã ã‘ã‹ã‚‰ã€ã€ã€
- ChatGPT-4Vå…¬é–‹ã€iOSã‚„Androidç‰ˆã«ã‚‚æ­è¼‰ã€æ§˜ã€…ãªè©•ä¾¡ãŒå ±å‘Šã•ã‚Œã‚‹
	- ãƒ‡ãƒ¢ã®ç”»åƒã¨è¨€èªã‚’äº¤ãˆãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã¯æœªæ¥æ„Ÿã‚ã‚‹ã€‚æ§‹é€ åŒ–æ–‡æ›¸ã‚’ç”»åƒã§è¦‹ã›ã¦ã‚‚ã‚ã‚‹ç¨‹åº¦ç†è§£ã§ãã‚‹æ¨¡æ§˜ã€‚
	- äººã®è¦‹ãŸç›®ã«å¯¾ã™ã‚‹è¨€åŠãªã©æ–°ãŸãªãƒªã‚¹ã‚¯ã‚‚è©•ä¾¡ãƒ»å¯¾ç­–æ¸ˆã¿ã¨ã®ã“ã¨
	- è‹±èªã®ã»ã†ãŒOCRç²¾åº¦ãŒè‰¯ã„ã—è‰²ã€…è©¦ã—ã¦ã‚‹ã‘ã©ã€ã‚·ãƒ³ãƒ—ãƒ«ãªå›³è¡¨ã®Reasoningã¯ã‹ãªã‚Šã§ãã‚‹ã€‚å›³è¡¨ã«å«ã¾ã‚Œãªã„èƒŒæ™¯æƒ…å ±ã‚‚ã€GPTå†…éƒ¨ã®çŸ¥è­˜ã§è£œãˆã‚‹ã®ãŒå¼·åŠ›ã€‚
- Calibrating LLM-Based Evaluator
	- https://huggingface.co/papers/2309.13308
	-  LLMãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡å™¨ã®æ ¡æ­£: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’è‡ªç„¶è¨€èªç”Ÿæˆã®å“è³ªè©•ä¾¡ã«åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ï¼Œäººé–“ã®è©•ä¾¡ã¨ã®ä¸€è‡´åº¦ã‚’é«˜ã‚ã‚‹ãŸã‚ã®æ ¡æ­£æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ï¼
- Sam Altmanæ°ã€ã€Œç¤¾å†…å†…éƒ¨çš„ã«ã¯ã€AGIã¯å®Œæˆã—ãŸã€ã¨tweetã€‚
	- am Altman says "agi has been achieved internally" at OpenAI.
	- å™‚ã§ã¯OpenAIã¯Arrakisã¨ã„ã†é™ã‚ŠãªãAGIã«è¿‘ã„any-to-any modelã‚’é–‹ç™ºã—ã¦ãŠã‚Šã€ã‚µãƒ ã‚¢ãƒ«ãƒˆãƒãƒ³ã‚‰ã—ãã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒAGIã®é–‹ç™ºã«æˆåŠŸã—ãŸ(è¿½è¨˜: ã¾ãè½ã¡ç€ã“ã†ã‚„) ã¿ãŸã„ãªã“ã¨ã‚’è¨€ã£ãŸã¨ã„ã†å ±å‘Šã‚‚ã‚ã‚‹ã€‚
	- ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã§äºˆå®šã•ã‚Œã¦ã„ã‚‹é–‹ç™ºè€…ä¼šè­°ï¼ˆ11/6ï¼‰ã«ä½•ã‹ã—ã‚‰ã®ç™ºè¡¨ãŒã‚ã‚‹ã€‚
- ã€ç¶šã€‘Flash Attentionã‚’ä½¿ã£ã¦LLMã®æ¨è«–ã‚’é«˜é€Ÿãƒ»è»½é‡åŒ–ã§ãã‚‹ã‹ï¼Ÿ
	- https://qiita.com/jovyan/items/5716cd83e246df4a158e
	- æœ€è¿‘å…¬é–‹ã•ã‚ŒãŸhuggingfaceã‹ã‚‰ç›´æ¥å…¬å¼å®Ÿè£…ã®Flash Attention2ã‚’ä½¿ãˆã‚‹æ©Ÿèƒ½ï¼ˆfrom_pretrainedã§use_flash_attention_2=Trueã‚’æŒ‡å®šï¼‰ã«ã¤ã„ã¦ã‚‚å®Ÿé¨“
- ã€LogiCoTã€GPT-4ãªã©ã®LLMã«ã€Œè‡ªã‚‰ã®è«–ç†çš„ãªæ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã€ã•ã›ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
	- "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic"
	- å‰æï¼ˆPremiseï¼‰ã€è€ƒãˆï¼ˆThoughtï¼‰ã€æ¤œè¨¼ï¼ˆVerificationï¼‰ã«ã¤ã„ã¦æ˜ç¢ºã«æŒ‡ç¤ºã™ã‚‹
- çµ±èªçš„è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ JCoLA ãŒ https://huggingface.co/datasets/shunk031/JGLUEã«è¿½åŠ 
	- JGLUE ã®å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒãã‚ã£ãŸã‚‰ã—ã„
- ChatGPT ã®æ¤œç´¢ãƒ—ãƒ©ã‚°ã‚¤ãƒ³(Plusç”¨ï¼Ÿï¼‰ãŒå¾©æ´»
-  Pair Programming with a Large Language Model
	- https://www.deeplearning.ai/short-courses/pair-programming-llm/
	- DeepLearningAIã‚ˆã‚Šã€ã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ã‚¹ãŒå…¬é–‹ã€‚LLMã¨ãƒšã‚¢ãƒ—ãƒ­ã¨ã¯
- llamaindexã®TimescaleDBã¨ã®é€£æº
	- https://medium.com/llamaindex-blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0
- å¤§å­¦ã«ãŠã‘ã‚‹æ•°ç†ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AI æ•™è‚² ã®ä¸­ã§ã®çµ±è¨ˆç§‘å­¦ã®æ•™è‚²ã«ã¤ã„ã¦ï¼ˆæ—¥æœ¬å­¦è¡“ä¼šè­°ï¼‰
	- https://www.scj.go.jp/ja/info/kohyo/pdf/kohyo-25-k230926-24.pdf
	- (1) æ•°ç†ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AI åˆ†é‡ã®ç†è«–çš„åŸºç¤ã¨ã—ã¦ã®çµ±è¨ˆç§‘å­¦ã®ä½ç½®ä»˜ã‘
	- (2) æ•°ç†ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»AI åˆ†é‡ã®å†æ•™è‚²(ãƒªã‚¹ã‚­ãƒªãƒ³ã‚°)ã®æ¨é€²
	- (3) å­¦å£«èª²ç¨‹åŠã³å¤§å­¦é™¢æ•™è‚²ãŒå¿…è¦ã¨ã™ã‚‹çµ±è¨ˆæ•™å“¡ã®è‚²æˆ
	- (4) åˆç­‰ãƒ»ä¸­ç­‰æ•™è‚²ã«ãŠã‘ã‚‹æ•™æã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã€ãƒ‡ã‚¸ã‚¿ãƒ«ç’°å¢ƒã®æ•´å‚™ã¨çµ±è¨ˆæ•™è‚²ã® ã•ã‚‰ãªã‚‹å……å®Ÿ
	- ãã£ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãŒä¸»äººå…¬ã®ã‚¢ãƒ‹ãƒ¡ãŒå¿…è¦ã ã¨æ€ã†ãã€‚
- RAGã‚’OSSã ã‘ã§æ§‹ç¯‰ã™ã‚‹æ–¹æ³•(llamaindex)
	-  Building RAG from Scratch (Open-source only!)
	- https://gpt-index.readthedocs.io/en/latest/examples/low_level/oss_ingestion_retrieval.html
	- Sentence Transformers as the embedding model
	- Postgres as the vector store (we support many other vector stores too!)
	- Llama 2 as the LLM (through llama.cpp)
- Google Colab ã§ Preferred Networks ã® PLaMo-13B ã‚’è©¦ã™by npaka
	- https://note.com/npaka/n/n19ff9dd4a537?sub_rt=share_sb
-  æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒç™ºè¦‹ã—ãŸåˆã‚ã¦ã®æº–çµæ™¶(çµ±è¨ˆæ•°ç†ç ”ç©¶æ‰€ï¼‰
	- https://www.ism.ac.jp/ura/press/ISM2023-05.html
	- ã“ã‚Œã¾ã§ã«åˆæˆã•ã‚Œã¦ããŸæº–çµæ™¶ã‚„é–¢é€£ç‰©è³ªã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª­ã¿è§£ãã€ç†±çš„ã«å®‰å®šãªæº–çµæ™¶ã‚’å½¢æˆã™ã‚‹åŒ–å­¦çµ„æˆã‚’äºˆæ¸¬ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã‚’é–‹ç™º
- PFN ã® PLaMo-13B ã‚’ 4 bit é‡å­åŒ–ã™ã‚‹ã¨Colab ç„¡æ–™ç‰ˆã® T4 15GB ã§ã‚‚æ¨è«–ã§ãã‚‹ã‚‰ã—ã„
	- https://colab.research.google.com/drive/1vgHInjIL5dJYoaIXL-s6ickbp3cwIQti?usp=sharing
- DreamGaussianãŒ ç„¡æ–™Colabã§è©¦ã›ã‚‹ã€‚5åˆ†ã»ã©ã§å®Œæˆ
	- https://github.com/camenduru/dreamgaussian-colab
-  Mastering Customer Segmentation with LLM
	- https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41
	- ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’LLMã®embeddingã§æ•°å€¤åŒ–ã—ã€k-meansã‚„t-SNEã§ã‚¯ãƒ©ã‚¹ã‚¿ã®ç‰¹å¾´ã‚’æ¢ã‚‹æµã‚Œã®è‰¯ã„è§£èª¬è¨˜äº‹
- ãƒ‡ã‚¸ã‚¿ãƒ«åºã®ITã‚³ãƒ³ã‚µãƒ«/PM/é€±5æ—¥/ä¸€éƒ¨ãƒªãƒ¢ãƒ¼ãƒˆ/ãƒ‡ã‚¸ã‚¿ãƒ«åºITæ”¯æ´ã®æ±‚äººãŒè©±é¡Œã«
	- å˜ä¾¡ã¯ã€1,54ä¸‡å††/ä¸‡
	- ä½“èª¿ãŒå®‰å®šã—ã¦ãŠã‚Šç—…æ¬ ãŒå°‘ãªã„æ–¹
- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šåå¾®åˆ†æ–¹ç¨‹å¼ã‚’è§£ãè«–æ–‡
	-  Neural Operators for Accelerating Scientific Simulations and Design
	- https://arxiv.org/abs/2309.15325v1
	- å…¥å‡ºåŠ›ã®ãƒãƒƒãƒ”ãƒ³ã‚°æ¼”ç®—å­ã‚’å­¦ç¿’ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¼”ç®—å­ã€‚æ•°å€¤è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã ã‘ã§ãªãã€å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’ã‚„é€†è¨­è¨ˆã¾ã§ã§ãã‚‹ãã†ã§ã™ã€‚
- ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning
	- https://huggingface.co/papers/2309.16650
	- ï¼“Dã®çŠ¶æ³ã‚’æ¦‚å¿µãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ç†è§£ã™ã‚‹ãŸã‚ã®èªå½™ã‚’æä¾›ã€ã“ã‚Œã¯ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®ä¸–ç•Œã‹ã€‚ã€‚
- Gaussian Splatï¼‹ä¸‰æ¬¡å…ƒç”Ÿæˆã®è«–æ–‡ãŒä¸€ã¤ã©ã“ã‚ã‹äºŒã¤åŒæ™‚ã«å‡ºã¦ã„ã‚‹ã®ãŒæˆ¦å›½æ™‚ä»£ã£ã½ã„ã¨ã“ã‚
	- https://gsgen3d.github.io/
	- https://dreamgaussian.github.io/
	- Gaussian Splatting ã¯ã€3D ã‚·ãƒ¼ãƒ³ã‚’ã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³é–¢æ•°ã§è¡¨ã•ã‚ŒãŸç‚¹ç¾¤ã®é›†åˆã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚ã“ã®ç‚¹ç¾¤ã®é›†åˆã‚’ã€ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã«ã€å…‰ç·šã«æ²¿ã£ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒ¼ãƒ³ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
- lama_indexã® AutoMergingRetrieverã‚’å›³è§£ã—ãŸçµµãŒç´ æ™´ã‚‰ã—ã„
	- https://x.com/clusteredbytes/status/1707864519433736305?s=20
- OpenAPIã®æ–°ã—ã„instructãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãªã«ã‹æ©Ÿèƒ½ãŒè½ã¡ãŸæ¨¡æ§˜
	- OpenAI is removing the ability to evaluate P(completion | prompt) for user-provided completions to the `gpt-3.5-turbo-instruct` model.
- Googleã€æ–°LLMã€€Geminiã‚’ 10æœˆ4æ—¥ã«ç™ºè¡¨ã‹ã€
	- Gemini might be coming out on Wednesday
	- "plus few more surprizes"ã¨invitationã«æ›¸ã„ã¦ã‚ã‚‹ã‚‰ã—ã„
-  7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex
	- https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416
- ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã€‘é‡å­åŒ–ã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ã®åŠ¹æœæ¸¬å®š
	- https://engineering.linecorp.com/ja/blog/quantization-lightweighting-llms
	- LINEã®æŠ€è¡“è· å°±æ¥­å‹ã‚³ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚·ãƒƒãƒ—ç”Ÿã®ç™ºè¡¨
	- 6é€±é–“ç¨‹åº¦ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³æœŸé–“ã‚‰ã—ã„
	- FP8ã«ã‚ˆã‚‹å½±éŸ¿ã¾ã¨ã‚
		-  å¤§ããªãƒ¢ãƒ‡ãƒ«ã§æœ€å¤§1.2å€ã®æ¨è«–é«˜é€ŸåŒ–
	- GPTQã«ã‚ˆã‚‹é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœæ¸¬å®š
- Streamlitã¨Github Codespacesã§ãƒ–ãƒ©ã‚¦ã‚¶ã®ã¿ã§ChatGPT APIé–‹ç™ºã‚’ã™ã‚‹
	- https://corp.langcore.org/media/codespaces

## 9/25

ç›¸ã‚‚åˆã‚ã‚‰ãšã€RAG(Retrieval Augmented Generation)é–¢ä¿‚ãŒå¤šã„ã®ã¯ã”å®¹èµ¦ã€‚ä¸Šä½ã®LLM(GPT-4ã¨ã‹ï¼‰ã‚’ã¤ã‹ã£ã¦æ­£è§£ã‚’ã¤ãã£ã¦ã€RAGã‚’è©•ä¾¡ã™ã‚‹ä»•çµ„ã¿ã¨ã‹ã€ã“ã®è©•ä¾¡ã®ä»•çµ„ã¿ã‚’ã¤ã‹ã£ã¦åˆ¥ã®LLï¼­(gpt-3.5-turboã¨ã‹)ã‚’RAGå‘ã‘ã«fine-tuningã™ã‚‹ãªã‚“ã¦ã®ãŒã€e2e(end-to-end)ã®æ‰‹æ³•ã¨ã—ã¦å½“ãŸã‚Šå‰ã«ãªã‚Šã¤ã¤ã‚ã‚‹ã€‚ã€ŒçŸ¥è­˜ã¯æ¨¹æœ¨ã®ã‚ˆã†ãªã‚‚ã®ã€ã¨ã®ãŸã¾ã†ã‚¹ã‚¯ã‚¨ãƒ‹ã®ä¸‰å®…ã•ã‚“ã®è©±ã¯ã„ã¤ã‚‚é¢ç™½ã„ã€‚SOPã‚’ã¤ã‹ã£ãŸAgentsã¨ã„ã†ã®ã¯agentã®å¯åˆ¶å¾¡æ€§ã¨ã„ã†æ„å‘³ã§é¢ç™½ã„ã€‚Transformers.jsã‚’ã¤ã‹ã£ãŸWeb LLMã®æ–°æ‰‹ãŒç™»å ´ã€‚Xwin-LM-70BãŒGPT-4è¶…ãˆã‹ï¼Ÿã¨ã„ã†ã®ãŒã‚‚ã£ã±ã‚‰ã®è©±é¡Œã€‚LLMãŒå‰µé€ æ€§ã‚’æŒã¤ã‹ï¼Ÿã®è«–æ–‡ã§ã®å‰µé€ æ€§ã®ï¼“ã¤ã®åŸºæº–ï¼ˆä¾¡å€¤ã€æ–°è¦æ€§ã€é©šãï¼‰ã£ã¦ã€ç‰¹è¨±ææ¡ˆã¨åŒã˜ã ã‚ˆã­ã€LLMãŒç‰¹è¨±ææ¡ˆã§ãã‚‹ã‹ï¼Ÿã«ç½®ãæ›ãˆã¦ã‚‚åŒã˜ã€‚instructorã¨ã„ã†openai function callingã«pydanticã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ã£ã¦ã¿ãŸã„ã€‚RAGã§ã‚‚ãƒ¡ã‚¿æƒ…å ±æŠ½å‡ºã«pydanticä½¿ã£ãŸã‚Šã¨ã‹ã€ã“ã®è¾ºã‚Šã‚‚å®šç•ªåŒ–ã‹ã€‚ChatGPTã®çŸ¥è­˜ãŒã€2022å¹´1æœˆã¾ã§ã®çŸ¥è­˜ã¾ã§ã‚¢ãƒ—ãƒ‡ã•ã‚ŒãŸã€‚LLMã®åˆ©ç”¨ã‚µãƒ¼ãƒ™ã‚¤ã€ã€Œï¼•ä½ï¼šãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥ç«‹æ¡ˆã€ã£ã¦ã®ã¯ç¬‘ã£ãŸã­ã€‚gpt-3.5-turbo-instructã¨ã„ã†ã®ãŒå‡ºã¦ã‚‹ã®ã­ã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§ã€è¨€èªç”Ÿæˆã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ£ãƒƒãƒˆç”¨ã§ã¯ãªã„ï¼‰ã€ã“ã‚Œã¯fine-tuningç”¨ãªã®ã‹ï¼Ÿï¼Ÿã€LLMå‘ã‘AIåŠå°ä½“ã€ŒSN40Lã€ã£ã¦ã®ã‚‚æœŸå¾…ã€‚

- ã¡ã‚‡ã£ã¨ã—ãŸæ°—é…ã‚Šã§çš†ã‚’å¹¸ã›ã«ã™ã‚‹ GitHub ã®ä½¿ã„æ–¹
	- https://qiita.com/squid-cat/items/7166317e60d3ff96ccb7
	- PR ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ã•ã‚Œãªã„ç’°å¢ƒã‚’ä½œã‚‰ãªã„
- ç±³å›½ã®AIä¼æ¥­å…¬è´ä¼šã‚ˆã‚Šã€Nvidiaã®è¨¼è¨€ãŒç´ æ™´ã‚‰ã—ã„
	- https://x.com/Yampeleg/status/1703774531771363738?s=20
	- OpenAI: AI will kill us. 
	- Anthropic: AI will kill us. 
	- InflectionAI: AI will kill us. 
	- Nvidia: Fortunately uncontrollable Artificial General Intelligence is Science Fiction not reality.
- çŸ¥è­˜ã¨æŠ€è¡“ã®ç¶™æ‰¿ã¨ã—ã¦ã®AI by ã‚¹ã‚¯ã‚¨ãƒ‹ä¸‰å®…ã•ã‚“
	- https://togetter.com/li/2226417
	- ãã®åˆ†é‡ã®å°‚é–€å®¶ãŒæŒã¤ãã†ã„ã£ãŸçŸ¥è­˜ä½“ç³»ãŒã€ãã®æ•™æˆãªã‚Šå°‚é–€å®¶ã®ä¾¡å€¤ãªã‚ã‘ã§ã‚ã‚‹ãŒã€å®Ÿéš›ã®ã¨ã“ã‚ã€è¿‘ãã«ã„ã¦è©±ã—ã‹ã‘ãªã‘ã‚Œã°ã€è‡ªåˆ†ã«ã¨ã£ã¦ä¾¡å€¤ã‚ã‚‹ã‚‚ã®ã‚’å¼•ãå‡ºã›ãªã„ã€‚ã ã‹ã‚‰ã“ãã€ç ”ç©¶å®¤ãŒã‚ã‚Šå­¦ç”ŸãŒã‚ã‚‹ã€‚ã—ã‹ã—ã€ãã†ã„ã£ãŸçŸ¥ã®ä½“ç³»ã¯ã€ä¸‡äººã«é–‹ã‹ã‚Œã‚‹ã¹ãã 
	- AIã«ã‚ˆã£ã¦æ—¥ã€…ç©ã¿é‡ãªã‚‹è«–æ–‡ã‚„ç™ºè¡¨è³‡æ–™ã€è¬›æ¼”éŒ²ã‚’å¸åã—ã€çŸ¥ã®ç³»çµ±æ¨¹ã‚’ä½œã‚‰ã›ã‚‹ã€‚æˆ‘ã€…ã¯ãã‚ŒãŒå·¨å¤§ãªæ¨¹æœ¨ã¨ãªã£ã¦ã„ãã®ã‚’è¦‹ãªãŒã‚‰ã€æ¬ ã‘ã¦ã„ã‚‹ãƒ”ãƒ¼ã‚¹ã‚„æ¥ã‚‹ã¹ãæè‘‰ã‚’æº–å‚™ã™ã‚‹
- Intel/Llama-2-70b-chat-hf-onnx-int4
	- https://huggingface.co/Intel/Llama-2-70b-chat-hf-onnx-int4
	- high-quality, INT4, ONNX models for all LLama2 variants (base vs. chat, 7B to 70B).
- Best Practices for LLM Evaluation of RAG Applications by DataBricks
	- https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG
	- Human and GPT-4 judges can reach above 80% agreement on the correctness and readability score. And if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can reach above 95%.
-  Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?
	- https://arxiv.org/abs/2309.08963
	- structure-aware fine-tuning method, applied to Llama-7B, which significantly outperform other model like GPT-3.5/4 and Vicuna-13B.
- Azure Cognitive Search ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰+ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¯ã€ç´”ç²‹ãªãƒ™ã‚¯ã‚¿ãƒ¼ã‚µãƒ¼ãƒã‚ˆã‚Šã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è‰¯ã‹ã£ãŸãã†ã§ï¼
	- https://techcommunity.microsoft.com/t5/azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167
- "Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality"
	- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321
	- â‘  GPT-4ã‚ã‚Šã®é›†å›£ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å„ªã‚Œã¦ã„ãŸ ãƒ»ã‚¿ã‚¹ã‚¯ã®å®Œäº†æ•°ãŒå¹³å‡ã§12.2%å¤šã„ ãƒ»ã‚¿ã‚¹ã‚¯ã®å®Œäº†é€Ÿåº¦ãŒå¹³å‡ã§25.1%æ—©ã„ ãƒ»ã‚¿ã‚¹ã‚¯ã®å“è³ªãŒå¹³å‡ã§40%é«˜ã„ 
	- â‘¡ ã‚‚ã¨ã‚‚ã¨æˆç¸¾ã®ã‚ˆããªã„äººãŒç›®è¦šã¾ã—ãå‘ä¸Šã—ãŸ
- GPT-3.5-turbo ã‚’ Fine-tuning ã—ã¦ GPT-4 ç›¸å½“ã®æ€§èƒ½ã‚’ç²å¾—ã™ã‚‹
	- https://tech.drobe.co.jp/entry/2023/09/19/140000
	- Lambda ã§ GPT-4 ã‚’å©ãã¤ã¤ã€å…¥åŠ›ã¨å‡ºåŠ›ã®ãƒšã‚¢ã‚’ json å½¢å¼ã§ Cloudwatch ã«è½ã¨ã—ã¾ã™ã€‚
	- ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã“ã“ã‚’å‚è€ƒã« Fine-tuning ã®ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨ validation ã‚’è¡Œã„ã¾ã™ã€‚
	- Fine-tuning ã®å®Ÿæ–½ã¯ç°¡å˜ã§ã™ã€‚OpenAI ã® API ã‚’åˆ©ç”¨ã—ã¦ä»¥ä¸‹ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
		- 1.  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
		- 2.  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¤ã¤ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹
	- Fine-tuning ã™ã‚‹ã¨çµæœãŒ GPT-4 ã«è¿‘ã¥ãäº‹ãŒè¦³æ¸¬ã§ããŸ
- Let's Verify Step by Step
	- https://arxiv.org/abs/2305.20050
	- LLMãŒè¤‡é›‘ãªå•é¡Œã‚’æ¨è«–ã§ãã‚‹ã®ã¯ã€å­¦ç¿’ä¸­ã«æ¨è«–æ–¹æ³•ï¼ˆè§£ãæ–¹ï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ãã®è§£ãæ–¹ã‚’å­¦ã‚“ã§ã„ã‚‹ã‹ã‚‰ã¨ã„ãˆã‚‹
- è‡ªå¾‹è¨€èªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ Agents ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n089614881df8
	- ã€Œ**Agents**ã€ã¯ã€**è‡ªå¾‹è¨€èªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
	- ã€Œ**SOP**ã€(Standard Operation Process) ã‚’é€šã˜ã¦è¨€èªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãã‚ç´°ã‹ã„åˆ¶å¾¡ã¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã§ãã‚‹ã“ã¨ã§ã™ã€‚ã€ŒSOPã€ã¯**ã‚¿ã‚¹ã‚¯å…¨ä½“ã®ã‚µãƒ–ã‚´ãƒ¼ãƒ« / ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®šç¾©**ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€èªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãã‚ç´°ã‹ã„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
-  Benchmarking `gpt-3.5-turbo-instruct` on agents doing question-answering over tabular data
	- https://github.com/langchain-ai/langchain-benchmarks/blob/main/csv-qa/pandas_agent_instruct.py
	- It performed roughly the same as gpt-3.5-turbo (the chat model) with roughly ~67% accuracy
	- It errored twice due to misformatted output - without function prompting for output format becomes much more important
- StableDiffusionã§ç”Ÿæˆã—ãŸç”»åƒã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’"AIã§"ä½œæˆã—ã€Unityä¸Šã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’å‹•ã‹ã™ã¾ã§ã€CSM AIã®ä½¿ã„æ–¹ã€‘
	- https://note.com/okp_/n/n89b96384e0cb?sub_rt=share_b
- llamaindexã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€â€œbuilding RAG from scratchâ€ -
	- https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/low_level/root.html
- SambaNovaã€æœ€å¤§5å…†å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œå¯èƒ½ãªLLMå‘ã‘AIåŠå°ä½“ã€ŒSN40Lã€ã‚’ç™ºè¡¨
	- https://news.mynavi.jp/techplus/article/20230920-2775419/
	- Ceruleanã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚NVIDIA H100ã®24å°åˆ†ã®æ€§èƒ½ã§ã€GPUã«æ­è¼‰ã•ã‚Œã¦ã‚‹æ§˜ãªé«˜é€Ÿãƒ¡ãƒ¢ãƒªãŒä¸è¦ã§ãƒ¡ãƒ¢ãƒªå¤§å®¹é‡åŒ–ãŒå¯èƒ½ï¼DDRãŒä½¿ãˆã‚‹
- sam altmanæ°ã€DALE 3ã®ãƒ‡ãƒ¢ç”»åƒã‚’è‡ªæ…¢ã™ã‚‹
	- https://x.com/sama/status/1704561613070893428?s=20
- OpenAIæœ¬å®¶ã§ã€Fine-tuningç”¨ã®web pageãŒå…¬é–‹ã•ã‚ŒãŸ
	- https://x.com/OfficialLoganK/status/1704181284036300970?s=20
	- èª°ã§ã‚‚ç°¡å˜ã«ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ãŒã§ã
- JSONã®å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ« jsoncrack
	- https://jsoncrack.com/
- GPT-4ãªã©ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§åŒ–å­¦ç ”ç©¶ã‚’è¡Œã†ã«ã‚ãŸã£ã¦ã®ï½¤ç¾çŠ¶ãƒ»èª²é¡Œãƒ»å±•æœ›ã‚’æ•´ç†ã—ãŸè«–æ–‡
	- Prompt engineering of GPT-4 for chemical research: what can/cannot be done?
	- https://www.tandfonline.com/doi/full/10.1080/27660400.2023.2260300
	- GPT-4ã¯ã€åŒ–å­¦ç ”ç©¶ã«ãŠã‘ã‚‹è¨€èªå‡¦ç†ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®çµ„ã¿è¾¼ã¿ã«æœ‰åŠ¹ãªãƒ„ãƒ¼ãƒ«ã¨ãªã‚Šå¾—ã¾ã™ã€‚
	- ä»¥ä¸‹ãŒå¿…è¦
		- åˆ†å­æ§‹é€ ã‚„å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
		- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºæœ€æ–°ã®åŒ–å­¦æƒ…å ±ã‚’å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«
		- æ¨è«–ã‚„è¨ˆç”»èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®é©æ–°
- llamaindexã«ã¦ã€RAGã«ãŠã„ã¦ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã¤ã‹ã£ãŸQueryã‚’ä½¿ã†æ–¹æ³•ã€
		- RAGStringQueryEngineã¨ã„ã†ã®ã§ã€ä»»æ„ã®promptã‚’æŠ•å…¥ã§ãã‚‹ï¼Ÿï¼
		- ãªã‚‹ã»ã©ã“ã‚Œã¯å½¹ã«ç«‹ã¤
		- https://gpt-index.readthedocs.io/en/latest/examples/query_engine/custom_query_engine.html
- An in-browser version of ChatGPT (or HF Chat), built with HuggingFace Transformers.js!
		- https://huggingface.co/spaces/mithril-security/blind_chat
		- webllmã¨ã¯é•ã£ãŸãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ã®local LLMå®Ÿè£…ã€transformer.jsã‹ã‚ã€ãã£ã¡ã‹ã‚‰HFä½¿ã†ã‚“ã ã€‚
- RSJ2023ã€ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿãƒ­ãƒœãƒƒãƒˆå¿œç”¨ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«2ï¼ˆæ¾å°¾ç ”ï¼‰
	- https://speakerdeck.com/tmats/rsj2023-ji-pan-moderunoshi-robotutoying-yong-tiyutoriaru2-shi-robotutoyong-noji-pan-moderuwozuo-tutehuo-yong-surufang-fa
	- æ—¥æœ¬ãƒ­ãƒœãƒƒãƒˆå­¦ä¼š [#RSJ2023](https://twitter.com/hashtag/RSJ2023?src=hashtag_click) ã®ã€ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿãƒ­ãƒœãƒƒãƒˆå¿œç”¨ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼ˆå¾ŒåŠï¼‰ã®è³‡æ–™
	- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’æ•´ç†ã—ãŸã‚ã¨ï¼Œãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹é ˜åŸŸã§ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—æ´»ç”¨ã™ã‚‹æ–¹æ³•ã«é–¢ã—ã¦ã‚µãƒ¼ãƒ™ã‚¤
- **Building RAG with LLMs and Prompts**ã€€by **Jerry Liu, LlamaIndex**
	-  @FlowGPTOfficial workshop today I gave talks on how to build RAG response generation and a simple router module using only LLMs and prompt
- llamaindexã®RAGã«ãŠã‘ã‚‹ã€é¡ä¼¼æ¤œç´¢èªã®post processingæ§˜ã€…ã€é †ç•ªå¤‰ãˆã‚‹ã¨ã‹ã‚ã‚Šãªã®ã‹ãƒ»
	- https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html#longcontextreorder
-  LLMãŒæŒã¤/æŒãŸãªã„/æŒã¡ã†ã‚‹å‰µé€ æ€§ã«ã¤ã„ã¦ã®è«–æ–‡
	- On the Creativity of Large Language Models
	- https://arxiv.org/abs/2304.00008
	- ãƒœãƒ¼ãƒ‡
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIwMjczNjY0NDQsMTYxNTgyMDQ1OSw5NT
gyMTMwOTQsNzYxMDkxNDQsLTE0OTY1NTE3NDIsLTIwNjYwMzE3
NzQsLTE1NDI1NzkxMzMsLTE4NTQ3Nzg5NzEsMjM3ODYzMjUyLC
0yMTIzMjAwMzUwLDEwMjU0MTgxOTYsNDY5MDM3Mzc2LC0xMjQ3
MDM5Mjg0LDI0NjAwMDUxNywxMDE3MjAxOTk0LC0xMzU4NDYyND
gxLDk0OTQ5MTY0NSwzMDk2NTI2MCwtMjM5NzY2MjMxLC0xOTQ5
NTY5NTUxXX0=
-->