# ひたすらLLM関連情報を追う、
これは、個人のtwitter bookmarkを毎週おさらいしている。


## 10/30

- 7bのフルファインチューニングがcolabで動く？VRAM 32G程度で行けると
	- https://x.com/Sakkusakumura/status/1716158933319246289?s=20
- Character-LLM: A Trainable Agent for Role-Playing
	- https://aiboom.net/archives/57223
	- 特定の人物、例えばベートーヴェンやクレオパトラなどの行動や感情を模倣させるよう訓練する新しいフレームワーク『Character-LLM（キャラクターLLM）』
	- 訓練されたLLMは、特定の人物としての行動や感情を効果的に模倣できることが確認されました。
-  Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts
	- https://cxh0519.github.io/projects/Progressive3D/?ref=aiartweekly
	- Progressive3D brings region specific object manipulation through text with a DALL-E 3 like level of prompt understanding to the table.
	- ３Dモデルに対して、様々な加工を言語で行う
- Courtland Leer et al., "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models"
	- https://arxiv.org/abs/2310.06983
	- 「心の理論（Theory of Mind）」をメタ認知能力をつかって向上できる。
	- 心理学における「Violation of Expectation（期待違反）：VoE」理論を適用


## 10/23

今週は、NIIからllm-jp-13b-v1.0が公開されたのが話題でした、さっそくcolabで使った例が公開されたり、4bit量子化版がhuggingfaceで公開されたりと、盛り上がってます。関係者の努力とABCIの活躍に頭が下がります。LLM活用アプリの性能を考えるときに、RAGでもそうなんだけど、LLMとembeddingの組み合わせをちゃんと評価するってのが最初にあるべきなのかも。使ったことないけどもReplicateはそこんところうまくついたサービス展開といえる。LLMをソフトウエアエンジニアリングで活用できるという論文が話題に。OpenAI、限りなくAGIに近いとうわさのArrakisの開発断念？映画Dune２(Arrakisという星が部隊）の公開も春にずれ込んだから、似たような運命をたどるのか？マッキンゼーのレポート、生成AIにより、AIの作文力が人間の上位25%を超える時期の予測が25年前倒しというのは驚いた、つまり我々は25年先の技術を今見ていることになる、そりゃ（多くの人にはLLMの凄さが）分らんわな。スタンフォード大の「科学論文の査読」に、大規模言語モデル（LLM）が有用であるという論文、これは朗報だ（誰得？）。「kaggle LLMコンペ　上位解法のまとめ」はこれはLLMプラクティショナーには必読だ。ちゃんとコンテキストを適切に与えることが重要。あたりまえだけど、それを行うのは難し。「世界モデル」に対するOpenAI共同設立者のIlya Sutskever氏の対談、大規模深層学習モデルは言葉を生成する何等かの表現（つまり世界モデル）を学習し、これから漏れ出るものがテキストであると言っている（ナウシカの「墓所の主」みたいなものか）。LLMの因果推論能力のベンチーマーク、fine-tuningすると性能はあがるが、少し表現を変えると性能が爆下がりって！、それがLLMなのよ！最新の言語理論である「ジェスチャーゲーム」で人間の言語能力が身についたとすると、LLMが示すテキスト生成能力は何？？Ilya Sutskever氏の対談の話と真っ向から対立する感じ。「言語ゲーム」といえばヴィトゲンシュタイン、ウィトゲンシュタイン研究を専門とする大谷先生の対話記事によると。LLMと言語ゲームって似たところがあるそうだ。なんか、楽しくなってきた。

- Ilya Sutskever氏LLMと世界モデルについて語る with Jensen Huang, CEO of Nvidia:
	- https://twitter.com/i/status/1713368556618887670
	- OpenAIの共同設立者であるIlya Sutskever氏とNvidiaのensen Huang社長との対談より
	- （巨大な）ニューラルネットが学んでいるのは、テキストを生成する「何か」に対する表現を学んでいる。その「何か」とは世界モデルであり、それが射影されたものが生成されたテキストなのである。
- Jonas Belouadi et al., "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ"
	- https://arxiv.org/abs/2310.00367
	- LLMを活用し人間のように科学的な図を生成するツール『AutomaTikZ』
	- テキストから科学的なベクターグラフィックスを生成する 
	- LLaMAをDaTikZデータセットで微調整
-  Can Large Language Models Infer Causation from Correlation?
	- https://arxiv.org/abs/2306.05836
	- https://ai-scholar.tech/articles/large-language-models/llm_causal_inference_skill
	-  LLMに因果推論能力はあるか？
	- 大規模言語モデルの因果推論能力をテストするベンチマークデータセットを提案  
	- 17の既存の大規模言語モデルを評価  
	- 現状のモデルは因果推論能力が低いことがわかった
	- fine-tuningにより性能向上が見られる一方で，少し表現を変えただけで性能が下がる現象も見られる
- Yijun Tian et al., "Graph Neural Prompting with Large Language Models"
	- https://arxiv.org/abs/2309.15427
	- LLMにナレッジグラフ（知識グラフ）を連携させることで、タスク遂行能力を大幅に向上させるフレームワーク『Graph Neural Prompting（GNP）』
	- GNPは、LLMに有益な知識を効果的にエンコードし、パフォーマンスを大幅に向上させることができる
- 大規模言語モデルがどのように動いてるかを視覚的に説明するインフォグラフィックが素晴らしいと
	- https://ig.ft.com/generative-ai/
	- Fanatical Timesのインフォグラフィック
- Large Language Models for Software Engineering: Survey and Open Problems
	- https://arxiv.org/abs/2310.03533
	- LLMをソフトウエアエンジニアリング(SE)にどうやって適用するか？
	- 要求エンジニアリング/デザイン、コード生成、テスト、運用/デプロイ、ドキュメント生成。またリサーチ領域での活用なども
	- 伝統的なSEとLLMを融合したはハイブリッドにより信頼ある効率的なLLMベースのSEが実現できた
- JapaneseEmbeddingEval　日本語におけるembeddingの評価
	-  https://github.com/oshizo/JapaneseEmbeddingEval
	- multilingual-e5っていい線いってるのか。。
- PaLI-3 Vision Language Models: Smaller, Faster, Stronger
	- https://huggingface.co/papers/2310.09199
	- Googleによる、高性能で小さいvision language model (VLM)
- マッキンゼーから発表されたAI動向に関するレポートがなかなか衝撃的
	- https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-AI-the-next-productivity-frontier#introduction
	- 生成AI（というかChatGPTに代表されるLLM)の登場により、AIの作文力が人間の上位25%を超える時期の予測が25年前倒しになった
		- 2017年の予測：2050年 ・2023年の予測：2024〜2025年 
- Xinyun Chen et al, "Teaching Large Language Models to Self-Debug"
	- https://arxiv.org/abs/2304.05128
	- GPT-4などLLMのコード生成能力にデバッグ機能を追加する『SELF-DEBUGGING（セルフデバッギング）』
	- LLMに自己デバッグの能力を教えることで、コード生成の性能が向上する
- ChatGPTを用いてコーディングを学ぶ方法について（慶応義塾大学）
	- https://speakerdeck.com/keio_smilab/keio-univ-intro-to-ml-02-coding
	- なんと、学生向けに、ChatGPTを用いてPythonなどのコーディングを学ぶという授業が、、
	- ChatGPTネイティブな学生は、ChatGPTでコーディングを学ぶのか。。
- Andrew Ng先生から、deeplearning.aiの「生成AI」の講義の宣伝
	- https://www.deeplearning.ai/courses/generative-ai-for-everyone/
- OpenAI、次世代LLMである、Arrakisの開発を断念？
	-  OpenAI Dropped Work on New ‘Arrakis’ AI Model in Rare Setback
	- 限りなくAGIに近いとうわさされる次世代のLLM、
	- どううも開発中（学習中）の性能評価で思ったほど性能が出なかったため。
	- https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback
- llamaindexのLiuさんより、“Evaluation Driven Development” (EDD)の提案
	- https://x.com/jerryjliu0/status/1713936561480610104?s=20
	- まずは、LLM＋Embeddingの組み合わせをちゃんと評価するところから始めようみたいな。
- Replicateを利用すると、任意のLLMとembeddingの組み合わせを簡単に評価できる
	- https://replicate.com/explore
	- つまりhuggingfaceのモデルをダウンロードして動かす手間を、少し省くサービスを提供、
	- ナイスだな。
- NIIから、LLM-jp-13B が公開される
	- LLM-jp （LLM 勉強会）は、日本語と英語を中心に事前学習した130億パラメータの大規模言語モデルをオープンなライセンスで公開
	- https://llm-jp.nii.ac.jp/release/
	- インストラクションデータでチューニングしたモデルや訓練・チューニングに用いたソフトウェアも公開
- データでできることのレベル感を理解する（デジタル庁の人のスライドより）
	- https://speakerdeck.com/hik0107/data-design-and-government?slide=10
	- 現状の把握(lv.1)、分解と差異の把握(Lv.2)、原因の把握(Lv.3)、対策の把握(Lv4)
-  Google Colab で LLM-jp-13B を試す by npakaさん
	- https://note.com/npaka/n/n60b0abf54ed5?sub_rt=share_sb
	- T4 ハイメモリで動作確認
	- 早速試されている
- BEYOND MEMORIZATION: VIOLATING PRIVACY VIA INFERENCE WITH LARGE LANGUAGE MODELS
	- https://arxiv.org/pdf/2310.07298v1.pdf
	- Redditの匿名ポストのテキストから、GPT-4はその人のプロファイル（収入、性別、住所）を85%の正確さで、かつ人間の1%のコストで当てた。。
	- A paper that really illustrates both the unexpected power, and unexpected risks, that come from LLMs.
-  InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
	- https://arxiv.org/abs/2310.07713
	- pre-train LLMs with Retrieval Augmentation
-  An Emulator for Fine-Tuning Large Language Models using Small Language Models
	- https://huggingface.co/papers/2310.12962
	- Emulator for Fine-Tuning(EFT)は、大規模な事前学習済みモデルを小規模な微調整済みモデルとアンサンブルすることで、大規模な事前学習済みモデルを微調整した結果をエミュレートするという、アップスケーリングが可能になった
-  Can large language models provide useful feedback on research papers? A large-scale empirical analysis
	- https://arxiv.org/abs/2310.01783
	- 「科学論文の査読」に、大規模言語モデル（LLM）が有用な可能性がある
	- 米スタンフォード大らが検証　参加者の80％以上「AI査読は有益」
	- https://www.itmedia.co.jp/news/articles/2310/19/news072.html
	- Nature系列のジャーナルにおけるフィードバックの結果、GPT-4が提供したコメントの57.55％は、全体の査読者の中で少なくとも1人の人間の査読者が記載していた
- A quantized version of the mistral that is instruction following over 32k tokens.
	- https://huggingface.co/TheBloke/MistralLite-7B-AWQ
	- mistralって性能がよいと先週評判になってたやつの、4bit量子化版が公開？
- llm-jp-13b-v1.0も早速GPTQ版が公開される
	- https://huggingface.co/mmnga/llm-jp-13b-v1.0-4bit-g128-GPTQ-calib-ja-1k
	- llm-jp-13b-v1.0を、 日本語のキャリブレーションセットで生成したGPTQモデル
-  言語はこうして生まれる―「即興する脳」とジェスチャーゲーム―
	- https://www.shinchosha.co.jp/book/507311/
	- 言語の生得性を否定し、文化進化や語用論的な観点から言語獲得を論じています
	- 歴史：ノーム・チョムスキーは「普遍文法」という概念を導入し、「人間の遺伝的青写真には、言語を支配する抽象的な数学的原理が内包」しているといった
	- 歴史：心理学者スティーブン・ピンカーがさらに『言語を生みだす本能』（NHKブックス）へと発展させる
	- 主張：「ジェスチャーゲーム」。言語は遺伝的に決定されたものなどではなく、身振り手振り、発声、あるいはその両方で自分の意思を双方向的に伝え合うジェスチャーゲームのようなものが起源なのではないかという斬新なアイデアだ。そこには普遍文法が入り込む余地などない。
- LLM（大規模言語モデル）は「言語ゲーム」的か  東京女子大学現代教養学部准教授・大谷弘氏に聞く（１）
	- [IT批評の記事](https://it-hihyou.com/recommended/llm%EF%BC%88%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%89%E3%81%AF%E3%80%8C%E8%A8%80%E8%AA%9E%E3%82%B2%E3%83%BC%E3%83%A0%E3%80%8D%E7%9A%84%E3%81%8B-%E2%80%95/)より
	- LLMって、パターンから学び、その背後には人間があるから、ヴィトゲンシュタインのいう「言語ゲーム」に似ている、らしい。記号接地してないという批判にも、学習データの背後の人間のあたりで接地しているのかもともいう。
- 多様体上の最適化理論
	- https://www.amazon.co.jp/exec/obidos/ASIN/4274231186?&linkCode=sl1&tag=mathlang09-22&linkId=bd145734052442298eb01413d823ca91&language=ja_JP&ref_=as_li_ss_tl
	- 多様体上の最適化理論について、基礎となる数理から応用例までを解説
-  Introducing CliffordLayers: Neural Network layers inspired by Clifford / Geometric Algebras.
	- https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/articles/introducing-cliffordlayers-neural-network-layers-inspired-by-clifford-geometric-algebras/
	- MS研究所から、クリフォード代数にインスパイアされた新しいNNレイヤの発明
-  OpenAgents: An Open Platform for Language Agents in the Wild
	- https://arxiv.org/abs/2310.10634v1
- llamaindexより、Unifying Text-to-SQL and RAG with our SQLRetrieve
	- https://docs.llamaindex.ai/en/latest/examples/index_structs/struct_indices/SQLIndexDemo.html
	- SQLデータベースに対して、RAGを行うRetriverについて、動いたぞ、役に立つぞ。
- kaggle LLMコンペ　上位解法まとめ
	- https://zenn.dev/yume_neko/articles/7347ba6b081e93
	- 科学分野の5択問題を解くLLMの精度を規則コンペのべスプラ
	- 今回のコンペで上位に行くにはRetrievalが最もキーだったように思います。やはり正解情報を直接参照できるので、contextをより良くすることが重要だったのではないかと思います。
- llama2のpretrainingを試す
	- https://zenn.dev/if001/articles/6c507e15cd958b
	- 小さいサイズのllama2を日本語でpre_trainingしてみます
	- pre_trainingからhuggingfaceへのuploadまでを行ってみました。
	- 小さいサイズであればgoogle colabで学習できる
- llm-jp-13b-v1.0-gguf
	- https://huggingface.co/mmnga/llm-jp-13b-v1.0-gguf
	- llm-jpさんが公開しているllm-jp-13b-v1.0のggufフォーマット変換版
	- ブランチらしい、LLama.cppが、なんかの変更を行うとggufが動かなくなるらしい、怖っ

## 10/16

RAGシステムの性能向上は依然もりあがっている。StanfordのDSpy、どうもLLMのプロンプト利用を別の次元に引き上げる画期的な開発のように見えるが追いつけない。RAGとFinetuningを組み合わせることによる性能向上がいままで抜けていたとは。LLMの心の理論(ToM)についての論文では、他人の心の状態の推定というのが肝なのか。zephyr-7b-alphaとか、Japanese StableLM Instruct Alpha v2 とか、ローカルで使いものになるLLMもどんどん出てきた。スタンフォードAIの、State of AI Report 2023、 KaggleのAI Report 2023、それぞれの立場で最新のAIを取り巻く様々な視点をまとめてくれている。アナロジー（類推）でプロンプトを生成する「アナロジカル・プロンプティング」は、人間の手間を省けるか？組み込み(embeding)の違いによるRAG性能の違いの検証から、やっぱe5(intfloat/multilingual-e5-large)が当面最強なのか？PFNのインターン生の成果などがいくつか公開。それにしてもPFNのインターン生つよつよだろう、ちょっとうらやましい。DeepMindのYasunagaさんやエジンバラ大学のMatsubaraさんなどの日本人の活躍もちらほら。


- Large Language Models (in 2023)
	- https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0
	- OpenAIのHyung Won ChungさんによるLLMの現状をまとめたスライド
	- The biggest progress in the past 10 years (or even more) can be summarized as
		- Create weaker inductive biases and scale up
		- Do not teach machines how we think we think. Let it learn in a machine’s way
- Masking PII Data in RAG Pipeline
	- https://betterprogramming.pub/masking-pii-data-in-rag-pipeline-326d2d330336
	- PII(Personal Identification Information)をマスキングする方法を、RAGにおいて行う方法
	-  LlamaIndexの NERPIINodePostprocessorを活用するのがみそ
- Jerryより、RAGシステムの性能向上に関する、様々な手法のブックマーク集
	- Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex
		- https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5
	- Building Performant RAG Applications for Production
		- https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/dev_practices/production_rag.html
	- Multi-Document Agents
		- https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html
	- Finetuning
		- https://docs.llamaindex.ai/en/stable/end_to_end_tutorials/finetuning.html
- Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models
	- https://arxiv.org/abs/2310.04406
	- Substantially improving over the existing prompting methods such as Reflexion, e.g., 68.1% -> 86.9% on HumanEval with GPT-3.5
- Ida Momennejad et al., "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval"
	- https://arxiv.org/abs/2309.15129
	- 人間の測定法と似たフレームワークでLLMの認知機能を調査した論文
	- LLMの「認知マップ」と「計画能力」を評価。
		- 認知マップ：外部環境を内部に表現する機能 
		- 計画能力：目標に向かって計画を立てて遂行する能力
	- GPT-3.5、GPT-4、Bard、LLaMA-13Bなど
	- 結果
		- ① 認知マップの理解や計画能力は「箱から出してすぐに」は持っていない 
		- ② 認知マップの欠如が理由で計画タスクに失敗する可能性が高い 
		- ③ 新しい評価プロトコル（CogEval）は有望である 
		- ④ LLMのアーキテクチャやトレーニングには工夫の余地がある 
		- ⑤ LLMの認知機能を向上させるには、メモリ（記憶容量）の拡張などが有効
- FireAct: Toward Language Agent Fine-tuning
	- https://fireact-agent.github.io/
	- LLM AgentとFintuningの合わせ技についての検証、ReActの性能をfine-tuningで向上できた
	- FireAct is a novel way to finetune LMs w/ agent trajectories of a mix of tasks & prompting methods.
	- Fine-tuning >> Prompting:
		- Notably, small LMs benefit most --- Llama2-7B improves 77% after fine-tuning!
- Pei Zhou et al., "How FaR Are Large Language Models From Agents with Theory-of-Mind?"
	- https://arxiv.org/abs/2310.03051
	- LLMの「心の理論(ToM:Theory of Mind)」における能力を評価するフレームワーク『Thinking for Doing (T4D)』
		- ① 他者の心の状態（信念、願望、意図など）についてどれだけ効果的に推論できるか
		- ② 推論した上でいかに行動に移せるか
	- 従来の心理学的テストではLLMのToM能力の評価は十分には出来ないとされています。
	- 「Foresee and Reflect (FaR)」という新しいフレームワーク
		- ① 将来のイベントを予測（Foresee） 
		- ② それに対する行動を考慮（Reflect）
	- 「FaR」フレームワークと評価パラダイム「Thinking for Doing (T4D)」の組み合わせによって、効率的にLLMのToM能力を評価することができる
- 7 Query Strategies for Navigating Knowledge Graphs With NebulaGraph and LlamaIndex
	- https://www.nebula-graph.io/posts/Knowledge-Graph-and-LlamaIndex
	- NebulaGraph を使ってグラフ構造に対する、Q&Aを実現する方法について via Llamaindex
- StanfordのDSpyを用いることによる、Q&Aのファインチューニングが簡単になる？
	- https://x.com/lateinteraction/status/1712135660797317577?s=20
- KaggleのAI Report 2023
	- https://www.kaggle.com/AI-Report-2023
	- これはAIの現状に関するエッセイコンペの結果をまとめたもの、最新のAIを取り巻く様々な視点からの見方がわかる。
- HuggingFaceにおけるLLM評価で、zephyr-7b-alphaがChatLlama 70Bを上回る性能をだしたらしいのでllamaindexで確かめてみた
	- https://colab.research.google.com/drive/16Ygf2IyGNkb725ZqtRmFQjwWBuzFX_kl?usp=sharing#scrollTo=lMNaHDzPM68f
	- We found that it is the ONLY open 7B model atm that does well on advanced RAG/agentic task
- DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation
	- https://huggingface.co/papers/2309.16653
	- ここで３Dモデル作成を試せる、なんかすごいぞ。
		- https://huggingface.co/spaces/jiawei011/dreamgaussian
-  Multimodality and Large Multimodal Models (LMMs)
	- https://huyenchip.com/2023/10/10/multimodal.html
	- マルチモーダルモデルに関するサーベイ。重要論文としてCLIPとFlamingoを解説した上で、今後の方向性として他のモダリティの追加、出力のマルチモーダル化、ベンチマークの整備などを挙げている
- LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression
	- https://arxiv.org/abs/2310.06839
	- Gains a performance boost of up to 17.1% on NaturalQuestions over the original prompt with ~4x fewer tokens
- MatGPT: A Vane of Materials Informatics from Past, Present, to Future
	- https://onlinelibrary.wiley.com/doi/10.1002/adma.202306733?af=R
	- **GPT AI**の出現は、科学研究分野が「データ」を基本要素とし、「アルゴリズム + 計算能力」を核心生産力とする知能文明時代に入ったことを示している。
	- 事前学習モデル、指向性設計モデル、協調学習、実験ロボットなど
- PFNのインターン発表： 遺伝⼦に関するグラフを利⽤したモデルの開発
	- https://tech.preferred.jp/ja/blog/model-learning-using-gene-graph/
	- RNAからProteinを予測するタスクにおいては、学習サンプル数が限られ、かつ使用できる特徴量が少ない状況においては、予測対象モダリティの制御に関与する特定のグラフ構造を用いることで性能の改善が認められました。
- サイバーエージェントがOpenCaml2を開発中らしい
	- https://aws.amazon.com/jp/blogs/news/open-calm-and-openai-chatgpt-accuracy-on-jaqket-experiment-in-amazon-sagemaker/
- RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation
	- https://arxiv.org/abs/2310.04408
	- LLMでのRAGの性能向上のために、2つの圧縮器(重要部分抽出・複数文書要約)を使うRECOMP法の提案。各圧縮器は学習させる必要有
- 機械学習波動関数？？
	- https://www.nature.com/articles/s41524-023-01130-4
	- 従来は1種類の構造しか訓練に使えませんでしたが、ハミルトニアンを対称性に基づくパラメータで記述することで様々な構造を訓練でき、転位がある約5000原子セルの電子状態予測を実現した
- Google Colab で Japanese StableLM Instruct Alpha v2 を試す by npakaさん
	- https://note.com/npaka/n/n0e463dbbce11?sub_rt=share_sb
	- 「Stability AI Japan」が開発した7Bパラメータの日本語LLM
	- 商用利用を制限しないデータセットのみを利用することで、同等レベルの性能を持つ**商用利用が可能**
	- Colab無料枠(T4)で動作する模様
- StanfordAIによる、 State of AI Report 2023
	- https://www.stateof.ai/2023-report-launch
	- OpenAIの**GPT-4**は、すべてのベンチマークや人間向けの試験において他のLLMを凌駕している。
	- Meta AIはオープン（な）AIのチャンピオンとして登場し、LLaMaモデルファミリーを最も強力な公開アクセス可能なOpenAI代替品となっている
	- LLMや拡散モデルは、特にライフサイエンス分野で実用的なブレイクスルーをもたらしてお
	- 生成AIが、低迷している、テック界隈のVCを救う。
	- 安全性はAI研究界で中心的なテーマとなり、世界中の政府や規制機関が対策を講じ始めた。
	- 標準的なLLMは頑健性に問題があり、最先端モデルの評価が困難になっている
- Michihiro Yasunaga et al., "Large Language Models as Analogical Reasoners"
	- https://arxiv.org/abs/2310.01714
	- 人間の「過去の類似事例」と「自らの知見」を組み合わせるアプローチに倣った、LLMの優れたプロンプトフレームワーク
	- LLMの推論能力を向上させるCoTは有用ですが、手間がかかります。 手動のプロンプト作業を少しでも軽減することが求められています。 
	- そこで研究者らは、人間のように自動的に知識を生成する「アナロジカル・プロンプティング」を発明しました。
- Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems
	- https://arxiv.org/pdf/2310.07680.pdf
	- エジンバラ大学の松原さんの論文
	- infinite-dimensional Hamiltonian system behind Bayesian inference.
	- ベイズ推論の裏に、無限次元のハミルトニアンシステムがあるという、、
- Zijun Liu et al., "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization"
	- https://arxiv.org/abs/2310.02170
	- 複数のエージェントに協力して仕事を開始させ、タスクの進行に応じて重要なエージェントを取捨選択する『Dynamic LLM-Agent Network（DyLAN）』フレームワーク
	- タスクに応じて動的にエージェントを選択する方式を考えました。
- LangChain を使った RAG における埋め込みモデルの比較
	- https://note.com/alexweberk/n/ncccfdab3f4bb
	- Wikipedia 記事を LangChain の CharacterTextSplitter を使って、４種類の埋め込みモデルを使ってベクトル化し、RAG による質問応答を試行
	- `intfloat/multilingual-e5-large` >= `pkshatech/GLuCoSE-base-jap` > `cl-nagoya/sup-simcse-ja-large` >= `openai/text-embedding-ada-002` というような感触
	- 4つの埋め込みモデルを使ったRAGを試してみました: 
		- intfloat/multilingual-e5-large 
		- cl-nagoya/sup-simcse-ja-large 
		- pkshatech/GLuCoSE-base-ja 
		- openai/text-embedding-ada-002
- OpenAI gpt-3.5-turbo と gpt-3.5-turbo-instruct モデルの違いについて
	- https://corp.langcore.org/media/chatgpt-instruct
	- gpt-3.5-turbo モデルは会話に秀でているので対話をさせるのであればこちらを使う方がよいです。
	- 会話以外のタスクの場合だと**一問一答のような単純な課題を解くケースでは gpt-3.5-turbo-instruct の方が期待する出力になる可能性**があります。
- Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction
	- https://arxiv.org/abs/2310.05627
	- IJCAIでLLM(chatGPT)使った株価リターン予測の論文
	- LLMによるテキストの埋め込みと株式の特徴を同じsemantic spaceで配置させる強化学習の枠組みを導入している。
- Zhiyu Chen et al., "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting"
	- https://arxiv.org/abs/2310.07146
	- GPT-4をセラピストとして実行し、人々の「認知の歪み」を診断させるためのフレームワーク『Diagnosis of Thought (DoT)』
	- ① DoTは、「認知の歪み」評価と分類で高性能を示した 
	- ② GPT-4は、「認知の歪み」分類で特に高い性能を示した 
	- ③ 専門家によってGPT-4による本診断方法は「包括的である」と評価された（84.5%）
-  Large Language Models can Learn Rules
	- LLMがルールを学習できる？
	- https://arxiv.org/abs/2310.07064
	- LLMs can learn (sometimes uncommon) rules with 2 stages: (1) induction: generate and verify rules from exemplars; (2) deduction: utilize the rule library for new problems. 11-27% gain on reasoning tasks that require rule learning.

## 10/10

function callを含むLLMのファインチューニングをOpenAIが導入されたり、LLMのRAGに対するファインチューニングについての考察があったりと、性能面での評価を含めRAG関係は成熟してきた感じ。LLMがどれだけ論理的かという検証も「逆転の呪い」を例に行われているが、LLMが「物事がどのように位置づけられ、時間がどのように進行するかを理解」しているという実験はこれからのLLMを用いた複雑なタスクの開発に弾みでもある。マルチモーダルでは、早速GPT-4Vに対抗するOSSであるLLaVAが登場した、LLaVA-1.5はすでに試せる模様。スタンフォード大学がAIに関する多角的なデータのレポートすばらしい。OpenAIは、GPT-4Vに引き続き、DALL·E 3に対する品質カード(System Card)を公開、安全な画像生成をアピール。MSのDeepSpeedチームの科学的基盤モデルっても頑張ってるな、気象予想に使えそう。MicrosoftのH100 GPU対抗チップのATHENA、本当にやる気があるのか？


- 『逆転の呪い』:「AはBである」と学習したLLMは、「BはAである」と学習しづらくなる。
	- https://arxiv.org/abs/2309.12288
	- LLMがどれだけ論理的か？という問いに対して、LLMの苦手な点を挙げる
	- 『逆転の呪い』LLMは、知識を構造化し、”帰結を主語にして同じことを言う”のが自動的にはできない
	- LLMの「逆転の呪い」を認識した上ですべきことの考察
-  Knowledge Graph Construction w/ WikiData Filtering  by llamaindex
	- https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/knowledge_graph2.html
	- REBELを用いて、文章あから知識グラフを抽出する方法において、Wikipediaをフィルタとして用いることで、春市ネーションを抑えれる
- Ronen Eldan et al., "Who's Harry Potter? Approximate Unlearning in LLMs"
	- https://arxiv.org/abs/2310.02238
	- LLMの記憶の一部を意図的に忘却させる
	- 約1GPU時間の微調整で、モデルはHarry Potter関連のコンテンツを生成または回想する能力を効果的に消去
-  Fine-tuning with Retrieval Augmentation  by llamaindex
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_retrieval_aug.html
	- https://arxiv.org/abs/2310.01352
	- gpt-4とDatasetGeneratorをつかって、正解qaデータを生成
	- gpt-3.5-turboを正解qaデータをつかて、RAGのコンテキストでファインチューニング
	- 結果correctnesは、素のLLM＝3.2、ファインチューニング後＝3.65、
- 非侵襲の脳活動センシングによる、音声のデーコーディング
	- Decoding speech from non-invasive recordings of brain activity
	- https://huggingface.co/papers/2208.12266
	- contrastive learningというのをつかって、脳波からスピーチを推定
- OpenAIが、function calling fine-tuning機能を新たに追加　by llamaindex
	-  Fine Tuning with Function Calling
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/openai_fine_tuning_functions.html
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/finetuning/openai_fine_tuning_functions.ipynb
	- 構造化されたデータ出力をLLMから得たいときに、functio/n callをつかうらしいが、この機能をfine-tuneすることができる
- LLMは世界モデルをもっているか？
	-  Language Models Represent Space and Time
	- https://arxiv.org/abs/2310.02207
	- LLMはシンプルに統計（確率）から次のテキストを生成しているのではなく、「物事がどのように位置づけられ、時間がどのように進行するかを理解」している可能性が示唆されました。 つまり、LLMが"世界モデル"を形成しているかもしれないという報告
	- 世界、米国、NYCの地名、歴史的人物、芸術作品、ニュースヘッドラインなどを含む6つのデータセットを用意
	- 空間と時間の理解度は、LLMのニューラルネットワークにおける階層を半分まで進んだところで品質が向上し、そのあと限界点に達する
	- LLMが「世界モデル」を形成している可能性が高いのであれば、LLMがより高度な認知タスクに対応できることに繋がります。 例えば自動運転車のソフトウェアにLLMを活用するのは優れた戦略である可能性があります
- huggingface/transformers v4.34の更新はかなりagressive
	- https://github.com/huggingface/transformers/releases/tag/v4.34.0
	- tokenizerの挙動を細かく制御していた人たちにとってはうれしいかも
- ModuLoRA is the first method to finetune 3-bit LLMs
	- 3-bitや2-bitに量子化したLLMの話題の裏にあるアルゴリズムModulLoRAが公開
	- https://browse.arxiv.org/pdf/2309.16119.pdf
- RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS
	- https://arxiv.org/abs/2310.03025
	- NVIDIAよりRAGとContext Window (CW)のパフォーマンス比較論文。4K CWのLLM＋RAGは、16K CWのLLMと同等、32K CWのLLaMA2-70B＋RAGは長いContextのタスクにおいてGPT-3.5-turbo-16kより優れていると事を実証分析 
- llama.cpp 単体で LoRA 作れる機能が追加
	- https://github.com/ggerganov/llama.cpp/pull/2632
- Why you should build RAG from scratch - with Jerry Liu from LlamaIndex
	- LlamaIndexの中の人に聞く回。ファインチューン、RAG、ReAct、ベクトル検索やハイブリッド検索等々についてJerryがどう考えてるか聴ける。RAGはハックだと言い切ってて面白い。
	- https://www.latent.space/p/llamaindex?utm_campaign=post&utm_medium=web
-  Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study
	- https://arxiv.org/abs/2307.08072
	- 量子化されたLLMについて、一般的にLLMで発現するとされているin-context learning、chain-of-thought, instruction-followingといった能力がどの程度保てているかを検証した研究。結果として4-bitまでの量子化であれば性能の劣化が見られないことを確認
- OpenAIのSuper aligment
	- https://openai.com/blog/introducing-superalignment
	- “Superintelligence will be the most impactful technology humanity has ever invented.”
	- Superintelligence "could lead to ... human extinction. ... We believe [superintelligence] could arrive this decade."
- 早速GPT-4Vに対抗するOSSであるLLaVAが登場
	-  LLaVA: Large Language and Vision Assistant
	- https://llava-vl.github.io/
	- Haotian Liu et al., "Improved Baselines with Visual Instruction Tuning"
	- https://arxiv.org/abs/2310.03744
	- お試しできる、https://llava.hliu.cc/
- How to build ChatGPT for your company data? by ABACUS AI
	- llama2を使うのが良いみたい　
	- https://x.com/Saboo_Shubham_/status/1710505571072278932?s=20
- 正則化項付き線形回帰は真の偏回帰係数を推定しているのか？
	- https://bob3.hatenablog.com/entry/2023/10/06/224133
	- 正則化項付き線形回帰（Ridge、LASSO、Elastic net）で真の偏回帰係数を推定できるのか？を実験してみました。
- RAGにおけるchankサイズについて
	- https://docs.google.com/presentation/d/18Z7H3WSncPzLOTHKZAj36w0E7HSGY78VkDooSzvvySE/edit#slide=id.g286c47b4bb8_1_0
	- More chunks ≠ better (lost in the middle problems / context overflows)
	- Reranking retrieved chunks doesn’t necessarily improve results, in fact can worsen them.
- Science Behind Why LLMs Can Easily Be Tricked And Are Predictably Gullible
	- https://x.com/bindureddy/status/1710504584496779675?s=20
	- while large language models exhibit impressive linguistic abilities, their lack of true understanding, combined with the intricacies of data-driven learning, makes them susceptible to errors and easy to fool.
- 新しいOSSのembeddingモデルgte-tinyが登場、OpenAIのtext-embedding-ada-002なみの能力をもちつつ、小さくて軽い
	- https://huggingface.co/TaylorAI/gte-tiny/tree/main
- OpenAI, "DALL·E 3 System Card"
	- https://openai.com/research/dall-e-3-system-card
	- DALL·E 3での安全対策
	- OpenAIは、DALL·E 3の論文を通して「画像生成AIの安全性は前進した」ことを報告
- Artificial Intelligence Index Report 2023
	- https://arxiv.org/abs/2310.03715
	- スタンフォード大学がAIに関する技術・法律・経済・環境・世論などの多角的なデータを収集してまとめた報告書「AI index Report 2023」をarxivに公開
- MSのDeepSpeedチームの基盤モデルの科学応用を目指したDeepSpeed4Scienceプロジェクト
	- https://deepspeed4science.ai/
	- https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/japanese/README.md
	- 科学的基盤モデル(SFM)とよぶらしい
	- ClimaXは、さまざまな気象および気候モデリングタスクを実行するために設計された最初の基盤モデルです
	- 分子動力学と機械学習型力場
	- 天気 from Microsoft Start
- Google ColabについにAI機能が来てる？
	- Proにしか来てないもよう。
- Best Practices for LLM Evaluation of RAG Applications A Case Study on the Databricks Documentation Bot
	- https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG
	- RAG（Retrieval Augmented Geneneration）の評価、特に"LLMを使った時代評価の観点"からベストプラクティス
- 様々なLLMが何ができるかの比較表 by llamaindex
	- https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking
	- ちょっと、llama2-7b-4bitが悲しい結果に。。
		- OpenAI models (gpt-3.5-turbo, gpt-3.5-turbo-instruct, gpt-4)
		-  Anthropic models (claude-2, Claude-instant-2)
		- llama2-chat-7b 4bit
		- Mistral-7b
- Microsoft、Nvidia GPU依存へのコスト削減につながるAIチップを来月デビューへ
	- https://texal.jp/2023/10/08/microsoft-is-developing-its-own-ai-chip-and-working-with-amd-to-stop-nvidias-monopoly/
	- 「**Athena**」１１月の開発者会議で発表予定？
	- NVIDIAのH100 GPUと同等に設計されている


## 10/2

今週は、いや、今週もいろいろありすぎて、消化しきれない。GPT-4V(ision) デビュー、画像理解とか、ついにLLMが眼を持った（カンブリア紀）、これってAGIの前触れ？　GPT-4V初期ユーザーからは、霞が関パワポを入力したり、天一のマークも「侵入禁止」標識と誤認識はしない、サイゼリアの「間違え探し」は苦手、という報告も。ChatGPTにもvisionや音声対話機能が来週からロールアウトする(Plus以上のユーザー）。Amazonは、生成AIのAnthropicに5900億円出資。Quoraが提供する [poe.com](http://poe.com) で試用できる。GoogleのGPT-4超えのGeminiは水曜(10/4)に発表される。LLMでLLMを評価するLLM-as-a-judge がはやり。一方OpenAIの次世代LLMであるArrakisはAGIだといううわさも（コードネームは「砂の惑星」からきている？？）。特許検索だけからウイルス薬を発見したり、過去データから安定な準結晶の化学組成をあきらかにしたりと、バイオ・材料系でLLMは大活躍。PFN の PLaMo-13B 、4 bit 量子化するとColab 無料版で動くぞ。機械学習により偏微分方程式を解く話、データから逆設計できるならば画期的すぎる。 Gaussian Splatをつかった三次元生成の論文とGitHub公開が同時に２か所で！。LINEのインターン生による量子化による大規模言語モデル軽量化の効果測定、ここまで６週間でできるのか。ChatGPTの検索プラグイン復活、どうも本来ペイウォールで守られている記事であっても全文が表示されてしまうという報告で停止してものに対策が打たれた模様。RAG関係の進捗も、時系列データやMergeRetrieverなど進展がある。

- Agents: LLMをつかった新しいagentフレームワークとツール軍
	- https://github.com/aiwaves-cn/agents
	- **Agents** is an open-source library/framework for building autonomous language agents. The library is carefully engineered to support important features including **long-short term memory**, **tool usage**, **web navigation**, **multi-agent communication**, and brand new features including **human-agent interaction** and **symbolic control**.
- llamaindexからneo4jを使ったグラフagent
	- https://llamahub.ai/l/tools-neo4j_db
	- The `Neo4jQueryToolSpec` class provides a way to query a Neo4j graph database based on a provided schema definition.
-  LLM Fine-Tuning (東大松尾研LLM講座 Day5資料)
	- https://speakerdeck.com/schulta/llm-fine-tuning-dong-da-song-wei-yan-llmjiang-zuo-day5zi-liao
- OSSのLLMはだGAFAMのLLMに勝ち目がいないかあるか？
	- https://x.com/bindureddy/status/1706092114063639035?s=20
	- OSSのLLMは、AIの民主化と透明性のためには必要という話
-  LLMを用いたLLMの自動評価について 〜可能性と注意点
	- https://engineers.ntt.com/entry/2023/09/25/091245
	- LLM-as-a-judge では、**人手評価に匹敵するクオリティの評価を、お金や時間、労力をかけずに機械的に行える**ことが期待できます。
-  Community-developed checklists for publishing images and image analyses(Nature)
	- https://www.nature.com/articles/s41592-023-01987-9
	- 画像や画像解析結果を報告する際のベストプラクティスに関するNature Methods誌の記事
	- 画像のフォーマットや注釈、色の選択、データの利用可能性、画像解析ワークフローの報告に関する重要な推奨事項が提供されています。
- OpenAIからGPT-4V(ision) が発表、ついでに品質カードSystem Cardも公開
	- https://cdn.openai.com/papers/GPTV_System_Card.pdf
	- GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities
	- 複雑な標識を読み取る、https://x.com/petergyang/status/1707169696049668472?s=20
	- サイゼリアの「間違えさがし」の正答率は１割、https://x.com/cumulo_autumn/status/1707574932153282728?s=20
	- GPT-4V vs. 霞が関　https://x.com/horromary/status/1707373718534824305?s=20
- 外部知識によりパーソナライズされた対話システム
	- https://www.jstage.jst.go.jp/article/jjske/22/2/22_TJSKE-D-22-00053/_article/-char/ja/
	- 様々な概念に対するユーザーの関心を推定し，知識グラフをパーソナライズする手法を用いて，雑談における共感性や情報提供を目指す
- ChatGPT(Pllusユーザー以上）に、来週から新機能をroll-outするとの発表
	- Voice Capabilities:
	- Image Interaction
	- New Text-to-Speech Model:
	- Collaboration with Spotify
-  Amazon、生成AI新興に5900億円出資　Microsoftに対抗
	-  Claude-2-100kは、Anthropicの最も強力なモデルで、コンテキストウィンドウが10万トークン（約75,000語）
	- ばっちり日本語にも対応しQuoraが提供する [poe.com](http://poe.com)  で実際に使ってみることができます。
- llamaindexのAuto Merging Retriever
	- https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html
	- 木構造で整理されたドキュメントに対して類似する枝から順にマージして見せるらしい。
	- RAGを評価する教師データをGPT4で生成する、DatasetGeneratorもついでに紹介。いわゆる、 LLM-as-a-judge の一種をlllamaindexがnativeサポートした
- 特許から分子データを抽出
	-  Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures
	- https://arxiv.org/abs/2309.08765v1
	- ChatGPTを使って特許から10万件の分子と関連するキーワードを高精度に抽出、このデータベースを学習したモデルからウイルス薬を逆探索するとそれっぽい分子を抽出できた
	- 特許分析だけから、、、
- ChatGPT-4V公開、iOSやAndroid版にも搭載、様々な評価が報告される
	- デモの画像と言語を交えたインタラクションは未来感ある。構造化文書を画像で見せてもある程度理解できる模様。
	- 人の見た目に対する言及など新たなリスクも評価・対策済みとのこと
	- 英語のほうがOCR精度が良いし色々試してるけど、シンプルな図表のReasoningはかなりできる。図表に含まれない背景情報も、GPT内部の知識で補えるのが強力。
- Calibrating LLM-Based Evaluator
	- https://huggingface.co/papers/2309.13308
	-  LLMベースの評価器の校正: 大規模言語モデル（LLM）を自然言語生成の品質評価に利用する方法を提案し，人間の評価との一致度を高めるための校正手法を提案する．
- Sam Altman氏、「社内内部的には、AGIは完成した」とtweet。
	- am Altman says "agi has been achieved internally" at OpenAI.
	- 噂ではOpenAIはArrakisという限りなくAGIに近いany-to-any modelを開発しており、サムアルトマンらしきアカウントがAGIの開発に成功した(追記: まぁ落ち着こうや) みたいなことを言ったという報告もある。
	- サンフランシスコで予定されている開発者会議（11/6）に何かしらの発表がある。
- 【続】Flash Attentionを使ってLLMの推論を高速・軽量化できるか？
	- https://qiita.com/jovyan/items/5716cd83e246df4a158e
	- 最近公開されたhuggingfaceから直接公式実装のFlash Attention2を使える機能（from_pretrainedでuse_flash_attention_2=Trueを指定）についても実験
- 『LogiCoT』GPT-4などのLLMに「自らの論理的な整合性をチェック」させるフレームワーク
	- "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic"
	- 前提（Premise）、考え（Thought）、検証（Verification）について明確に指示する
- 統語的評価データセット JCoLA が https://huggingface.co/datasets/shunk031/JGLUEに追加
	- JGLUE の全てのデータセットがそろったらしい
- ChatGPT の検索プラグイン(Plus用？）が復活
-  Pair Programming with a Large Language Model
	- https://www.deeplearning.ai/short-courses/pair-programming-llm/
	- DeepLearningAIより、ショートコースが公開。LLMとペアプロとは
- llamaindexのTimescaleDBとの連携
	- https://medium.com/llamaindex-blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0
- 大学における数理・データサイエンス・AI 教育 の中での統計科学の教育について（日本学術会議）
	- https://www.scj.go.jp/ja/info/kohyo/pdf/kohyo-25-k230926-24.pdf
	- (1) 数理・データサイエンス・AI 分野の理論的基礎としての統計科学の位置付け
	- (2) 数理・データサイエンス・AI 分野の再教育(リスキリング)の推進
	- (3) 学士課程及び大学院教育が必要とする統計教員の育成
	- (4) 初等・中等教育における教材、ソフトウェア、デジタル環境の整備と統計教育の さらなる充実
	- きっと、データサイエンティストが主人公のアニメが必要だと思うぞ。
- RAGをOSSだけで構築する方法(llamaindex)
	-  Building RAG from Scratch (Open-source only!)
	- https://gpt-index.readthedocs.io/en/latest/examples/low_level/oss_ingestion_retrieval.html
	- Sentence Transformers as the embedding model
	- Postgres as the vector store (we support many other vector stores too!)
	- Llama 2 as the LLM (through llama.cpp)
- Google Colab で Preferred Networks の PLaMo-13B を試すby npaka
	- https://note.com/npaka/n/n19ff9dd4a537?sub_rt=share_sb
-  機械学習アルゴリズムが発見した初めての準結晶(統計数理研究所）
	- https://www.ism.ac.jp/ura/press/ISM2023-05.html
	- これまでに合成されてきた準結晶や関連物質のパターンを読み解き、熱的に安定な準結晶を形成する化学組成を予測する機械学習技術を開発
- PFN の PLaMo-13B を 4 bit 量子化するとColab 無料版の T4 15GB でも推論できるらしい
	- https://colab.research.google.com/drive/1vgHInjIL5dJYoaIXL-s6ickbp3cwIQti?usp=sharing
- DreamGaussianが 無料Colabで試せる。5分ほどで完成
	- https://github.com/camenduru/dreamgaussian-colab
-  Mastering Customer Segmentation with LLM
	- https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41
	- テーブルデータをLLMのembeddingで数値化し、k-meansやt-SNEでクラスタの特徴を探る流れの良い解説記事
- デジタル庁のITコンサル/PM/週5日/一部リモート/デジタル庁IT支援の求人が話題に
	- 単価は、1,54万円/万
	- 体調が安定しており病欠が少ない方
- 機械学習により偏微分方程式を解く論文
	-  Neural Operators for Accelerating Scientific Simulations and Design
	- https://arxiv.org/abs/2309.15325v1
	- 入出力のマッピング演算子を学習するニューラル演算子。数値計算を高速化できるだけでなく、実験データからの学習や逆設計までできるそうです。
- ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning
	- https://huggingface.co/papers/2309.16650
	- ３Dの状況を概念モデルとして理解するための語彙を提供、これはメタファーの世界か。。
- Gaussian Splat＋三次元生成の論文が一つどころか二つ同時に出ているのが戦国時代っぽいところ
	- https://gsgen3d.github.io/
	- https://dreamgaussian.github.io/
	- Gaussian Splatting は、3D シーンを、ガウシアン関数で表された点群の集合として表現します。この点群の集合を、レンダリング時に、光線に沿ってサンプリングすることで、シーンをレンダリングします。
- lama_indexの AutoMergingRetrieverを図解した絵が素晴らしい
	- https://x.com/clusteredbytes/status/1707864519433736305?s=20
- OpenAPIの新しいinstructモデルでは、なにか機能が落ちた模様
	- OpenAI is removing the ability to evaluate P(completion | prompt) for user-provided completions to the `gpt-3.5-turbo-instruct` model.
- Google、新LLM　Geminiを 10月4日に発表か、
	- Gemini might be coming out on Wednesday
	- "plus few more surprizes"とinvitationに書いてあるらしい
-  7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex
	- https://betterprogramming.pub/7-query-strategies-for-navigating-knowledge-graphs-with-llamaindex-ed551863d416
- 【インターンレポート】量子化による大規模言語モデル軽量化の効果測定
	- https://engineering.linecorp.com/ja/blog/quantization-lightweighting-llms
	- LINEの技術職 就業型コースのインターンシップ生の発表
	- 6週間程度のインターン期間らしい
	- FP8による影響まとめ
		-  大きなモデルで最大1.2倍の推論高速化
	- GPTQによる量子化モデルの効果測定
- StreamlitとGithub CodespacesでブラウザのみでChatGPT API開発をする
	- https://corp.langcore.org/media/codespaces

## 9/25

相も合わらず、RAG(Retrieval Augmented Generation)関係が多いのはご容赦。上位のLLM(GPT-4とか）をつかって正解をつくって、RAGを評価する仕組みとか、この評価の仕組みをつかって別のLLＭ(gpt-3.5-turboとか)をRAG向けにfine-tuningするなんてのが、e2e(end-to-end)の手法として当たり前になりつつある。「知識は樹木のようなもの」とのたまうスクエニの三宅さんの話はいつも面白い。SOPをつかったAgentsというのはagentの可制御性という意味で面白い。Transformers.jsをつかったWeb LLMの新手が登場。Xwin-LM-70BがGPT-4超えか？というのがもっぱらの話題。LLMが創造性を持つか？の論文での創造性の３つの基準（価値、新規性、驚き）って、特許提案と同じだよね、LLMが特許提案できるか？に置き換えても同じ。instructorというopenai function callingにpydanticを組み合わせられるライブラリ使ってみたい。RAGでもメタ情報抽出にpydantic使ったりとか、この辺りも定番化か。ChatGPTの知識が、2022年1月までの知識までアプデされた。LLMの利用サーベイ、「５位：ビジネス戦略立案」ってのは笑ったね。gpt-3.5-turbo-instructというのが出てるのね、コンパクトで、言語生成に適したモデル（チャット用ではない）、これはfine-tuning用なのか？？、LLM向けAI半導体「SN40L」ってのも期待。

- ちょっとした気配りで皆を幸せにする GitHub の使い方
	- https://qiita.com/squid-cat/items/7166317e60d3ff96ccb7
	- PR がレビューされない環境を作らない
- 米国のAI企業公聴会より、Nvidiaの証言が素晴らしい
	- https://x.com/Yampeleg/status/1703774531771363738?s=20
	- OpenAI: AI will kill us. 
	- Anthropic: AI will kill us. 
	- InflectionAI: AI will kill us. 
	- Nvidia: Fortunately uncontrollable Artificial General Intelligence is Science Fiction not reality.
- 知識と技術の継承としてのAI by スクエニ三宅さん
	- https://togetter.com/li/2226417
	- その分野の専門家が持つそういった知識体系が、その教授なり専門家の価値なわけであるが、実際のところ、近くにいて話しかけなければ、自分にとって価値あるものを引き出せない。だからこそ、研究室があり学生がある。しかし、そういった知の体系は、万人に開かれるべきだ
	- AIによって日々積み重なる論文や発表資料、講演録を吸収し、知の系統樹を作らせる。我々はそれが巨大な樹木となっていくのを見ながら、欠けているピースや来るべき枝葉を準備する
- Intel/Llama-2-70b-chat-hf-onnx-int4
	- https://huggingface.co/Intel/Llama-2-70b-chat-hf-onnx-int4
	- high-quality, INT4, ONNX models for all LLama2 variants (base vs. chat, 7B to 70B).
- Best Practices for LLM Evaluation of RAG Applications by DataBricks
	- https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG
	- Human and GPT-4 judges can reach above 80% agreement on the correctness and readability score. And if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can reach above 95%.
-  Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?
	- https://arxiv.org/abs/2309.08963
	- structure-aware fine-tuning method, applied to Llama-7B, which significantly outperform other model like GPT-3.5/4 and Vicuna-13B.
- Azure Cognitive Search のハイブリッド+セマンティックランキングは、純粋なベクターサーチよりもパフォーマンス良かったそうで！
	- https://techcommunity.microsoft.com/t5/azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167
- "Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality"
	- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321
	- ① GPT-4ありの集団は以下のように優れていた ・タスクの完了数が平均で12.2%多い ・タスクの完了速度が平均で25.1%早い ・タスクの品質が平均で40%高い 
	- ② もともと成績のよくない人が目覚ましく向上した
- GPT-3.5-turbo を Fine-tuning して GPT-4 相当の性能を獲得する
	- https://tech.drobe.co.jp/entry/2023/09/19/140000
	- Lambda で GPT-4 を叩きつつ、入力と出力のペアを json 形式で Cloudwatch に落とします。
	- データをダウンロードしたらここを参考に Fine-tuning のデータの準備と validation を行います。
	- Fine-tuning の実施は簡単です。OpenAI の API を利用して以下を実施します。
		- 1.  トレーニングデータをアップロード
		- 2.  アップロードしたデータを指定しつつトレーニングを開始
	- Fine-tuning すると結果が GPT-4 に近づく事が観測できた
- Let's Verify Step by Step
	- https://arxiv.org/abs/2305.20050
	- LLMが複雑な問題を推論できるのは、学習中に推論方法（解き方）にアクセスし、その解き方を学んでいるからといえる
- 自律言語エージェントを構築するためのフレームワーク Agents を試す by npakaさん
	- https://note.com/npaka/n/n089614881df8
	- 「**Agents**」は、**自律言語エージェントを構築するためのフレームワーク**
	- 「**SOP**」(Standard Operation Process) を通じて言語エージェントにきめ細かい制御とガイダンスを提供できることです。「SOP」は**タスク全体のサブゴール / サブタスクを定義**し、ユーザーが言語エージェントのきめ細かいワークフローをカスタマイズできるようにします。
-  Benchmarking `gpt-3.5-turbo-instruct` on agents doing question-answering over tabular data
	- https://github.com/langchain-ai/langchain-benchmarks/blob/main/csv-qa/pandas_agent_instruct.py
	- It performed roughly the same as gpt-3.5-turbo (the chat model) with roughly ~67% accuracy
	- It errored twice due to misformatted output - without function prompting for output format becomes much more important
- StableDiffusionで生成した画像から3Dモデルを"AIで"作成し、Unity上でキャラクターを動かすまで【CSM AIの使い方】
	- https://note.com/okp_/n/n89b96384e0cb?sub_rt=share_b
- llamaindexのチュートリアル、“building RAG from scratch” -
	- https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/low_level/root.html
- SambaNova、最大5兆個のパラメータモデルを実行可能なLLM向けAI半導体「SN40L」を発表
	- https://news.mynavi.jp/techplus/article/20230920-2775419/
	- Ceruleanアーキテクチャ。NVIDIA H100の24台分の性能で、GPUに搭載されてる様な高速メモリが不要でメモリ大容量化が可能！DDRが使える
- sam altman氏、DALE 3のデモ画像を自慢する
	- https://x.com/sama/status/1704561613070893428?s=20
- OpenAI本家で、Fine-tuning用のweb pageが公開された
	- https://x.com/OfficialLoganK/status/1704181284036300970?s=20
	- 誰でも簡単にモデルの微調整ができ
- JSONの可視化ツール jsoncrack
	- https://jsoncrack.com/
- GPT-4などの大規模言語モデルで化学研究を行うにあたっての､現状・課題・展望を整理した論文
	- Prompt engineering of GPT-4 for chemical research: what can/cannot be done?
	- https://www.tandfonline.com/doi/full/10.1080/27660400.2023.2260300
	- GPT-4は、化学研究における言語処理やドメイン知識の組み込みに有効なツールとなり得ます。
	- 以下が必要
		- 分子構造や実験データを扱えるようにするためのプラグイン
		- マルチモーダルモデルの開発最新の化学情報を学習できるようにするためのローカルモデル
		- 推論や計画能力を向上させるためのアルゴリズムやハードウェアの革新
- llamaindexにて、RAGにおいて、カスタムプロンプトをつかったQueryを使う方法、
		- RAGStringQueryEngineというので、任意のpromptを投入できる？！
		- なるほどこれは役に立つ
		- https://gpt-index.readthedocs.io/en/latest/examples/query_engine/custom_query_engine.html
- An in-browser version of ChatGPT (or HF Chat), built with HuggingFace Transformers.js!
		- https://huggingface.co/spaces/mithril-security/blind_chat
		- webllmとは違ったブラウザベースのlocal LLM実装、transformer.jsかあ、そっちからHF使うんだ。
- RSJ2023「基盤モデルの実ロボット応用」チュートリアル2（松尾研）
	- https://speakerdeck.com/tmats/rsj2023-ji-pan-moderunoshi-robotutoying-yong-tiyutoriaru2-shi-robotutoyong-noji-pan-moderuwozuo-tutehuo-yong-surufang-fa
	- 日本ロボット学会 [#RSJ2023](https://twitter.com/hashtag/RSJ2023?src=hashtag_click) の「基盤モデルの実ロボット応用」セッションのチュートリアル（後半）の資料
	- 基盤モデルの特徴を整理したあと，ロボティクス領域での基盤モデルを構築し活用する方法に関してサーベイ
- **Building RAG with LLMs and Prompts**　by **Jerry Liu, LlamaIndex**
	-  @FlowGPTOfficial workshop today I gave talks on how to build RAG response generation and a simple router module using only LLMs and prompt
- llamaindexのRAGにおける、類似検索語のpost processing様々、順番変えるとかありなのか・
	- https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html#longcontextreorder
-  LLMが持つ/持たない/持ちうる創造性についての論文
	- On the Creativity of Large Language Models
	- https://arxiv.org/abs/2304.00008
	- ボーデンの３つの基準（価値、新規性、驚き）や他の哲学的理論に基づいて、LLMの創造性を検証
	- LLMは価値を持つ作品やアイデアを生成することができますが、新規性や驚きについては弱い
	- LLMは人間と同じような創造性を持っているとは言えません
	- 異なる学習方法や適応能力を持つモデルを開発することで、探索的や変革的な創造性を実現することができるかもしれません
	- LLMは人間と協働することで、人間の創造性を補完したり刺激したりすることができます
-  RAG is more than just embedding search
	- https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/
	- シンプルなベクトルサーチベースの課題を述べながら、instructorというopenai function callingにpydanticを組み合わせられるライブラリを紹介している記事
	- 課題の一つ、-   **Query-Document Mismatch**:ドキュメントと質問のembbedingって同じ空間でないと意味ないよね（地産地消の場合を除く）
- Xwin-LM-70BがGPT-4超え？
	- https://www.itmedia.co.jp/news/articles/2309/21/news085.html
	- Xwin-LMは米Metaが公開したAI「Llama2」をベースにしており、教師ありファインチューニング、報酬モデル、リジェクトサンプリング、強化学習などを使って調整したものという。パラメータ数はLlama2と同じく70億、130億、700億の3つのモデルを用意。中でも最大である700億の「Xwin-LM-70B-V0.1」は、AlpacaEvalの評価基準である「Text-Davinci-003」（GPT-3のモデルの一つ）に対する勝率で95.57％を記録。勝率95.28％のGPT-4を追い抜いたとしている。
- ChatGPTの知識が、2022年1月までの知識も反映した模様
	- https://old.reddit.com/r/ChatGPT/comments/16m6yc7/gpt4_training_cutoff_date_is_now_january_2022/
- e2e(end-to-end) LLM/RAG、RAG評価を含めてLLMでやるという話、について
	- raysummit2023でのチュートリアル、jupyternotebookあるよ
	- https://github.com/anyscale/ray-summit-2023-training/blob/main/Ray-LlamaIndex/notebooks/02_evaluation.ipynb
- RAGを構成するときに、メタデータを与えるってのは役に立つわけだが、それをPydantic ＋LLMで一発でできるという話、
	- extract a full Pydantic object from any doc with 1 LLM call.
	- https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/PydanticExtractor.html
- Text generation web UI で Xwin-LM-13B をロードして色々推論して遊んでみます。
	- https://note.com/sa1p/n/n51170c4d1a1f
	- 「Text generation web UI」は、oobabooga氏による**大規模言語モデル用の無料のWeb UI**
	- ただし、ローカルに、GPUなどが必要**Windowsの場合NVIDIA製のグラボでのみ動作する**
- Exploring ReAct Agent for Better Prompting in RAG Pipeline
	- https://betterprogramming.pub/exploring-react-agent-for-better-prompting-in-rag-pipeline-b231aae0ca7c
	- use ReAct Agent to analyze Amazon's recent disclosures and attitudes towards LLMs in their SEC Exhibits 99.1 filings
- RAGの評価、正解と答えとの比較評価で、従来のBLEU/ROUGEとかでなくて、単に類似性評価でよいという簡易は方法を提示
	- https://gpt-index.readthedocs.io/en/latest/examples/evaluation/semantic_similarity_eval.html
- OpenAI謹製の、RAG(Q&A)のチュートリアル
	- https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb
- BQMLの時系列分析、ARiMAを適当なラグ設定のもとで40モデルほどぺぺっと推定してくれて、かつAICも推定してくれるので鬼便利
	- https://x.com/behemuhemulove/status/1705629318439907451?s=20
- LLMって何に使われているかのサーベイ
	- https://x.com/dmvaldman/status/1705350469177295273?s=20
	- １位：プログラムのエラーと解消法について、２位：AIのソフトウエアについての質問、３位：旅行関係、４位：テキスト要約とか改善、５位：ビジネス戦略立案
- GPT-3.5-Turbo-Instruct
	- https://chatgpt-lab.com/n/n2ed70597dfbf
	- 既存の「GPT-3.5-Turbo」とは違ってチャットに特化したモデルではないため、モデルが広範な自然言語処理タスクを扱うことを可能にします
	- OpenAIのテストでは、175Bのパラメータを持つGPTモデルよりも、1.3Bのパラメータを持つInstructGPTモデルの方が、100倍小さいにもかかわらず、人々に好まれることが示されている

## 9/19

GPT-4を活用して、データセットをつくって、他のＬＬＭをファインチューニングするとか、色々出ているが、MetaやAppleがGPT-4越えのLLMを来年に向け開発中。Appleが出遅れているのは、自動運転とかそっちにリソースを割かれているかとも、でもM2もっているし、ポテンシャルはある。あほなSiriの代わりになるのか？。RestGPTは、ReActの発展形、「APIの理解」ってのができるらしい。やっぱり企業利用ならば、RAG(Retrieval Augmented Generation)関係で、元となるテキストのチャンキングの仕方とか、ベクトルＤＢの選び方とか、スクラッチからのRAGの作成とか、地道活動も拾ってます。AstroLLaMA、今後様々なタスクや分野に特化したLLMがどんどんできてくるかも。LiteLLMっていうLLMの抽象化を使うと、アプリコードが再利用できるのか、作った人天才。GPT4による生産性向上にういての定量評価、資料として色々使えるな。仏教対話AIって、聖人をどれだけ復活させても幸せになれない気がする。きっと故人のChatBot作成サービスって葬儀業界ですぐにでも出てきそうだ。いや、2021年にマイクロソフトが[特許化していた](https://edition.cnn.com/2021/01/27/tech/microsoft-chat-bot-patent/index.html)。。


- Meta、GPT-4と同程度の性能を目指すモデルの学習を計画
	- https://www.theverge.com/2023/9/10/23867323/meta-new-ai-model-gpt-4-openai-chatbot-google-apple
	- AIトレーニングチップを買い集め、データセンターを構築
	- 2024年の早い時期に新しい大規模言語モデルの学習を開始する予定
	- 企業がAIツールを作成するために、再びこのモデルを無料にするよう働きかけている
- Fine-tuning to Memorize Knowledge
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_knowledge.html
	- GPT4で、内部ドキュメントに対する、Q&Aを生成させてこれをつかって、LLMをファイチューニングする話。
	- “bake in knowledge”と呼ぶらしい。
- OpenIntepreterを使っていると、OpenAIのAPIコールで10ドルが一瞬で溶ける。（デジタル庁楠さん）
	- https://x.com/masanork/status/1701381113506329083?s=20
	- つまり、ChatGPT Plusの月額課金が気前いいことになっている。
-  RestGPT: Connecting Large Language Models with Real-World RESTful APIs
	- https://restgpt.github.io/
	- ReActの発展形か、、
	- https://zenn.dev/carnot/articles/7f87b613a0a637
		- **言語のみの指示から複数のAPIを呼び出すことが可能**
		- RestGPTではプランニング・APIの理解・APIの選択をそれぞれのモジュールが独立で行うため、複雑なユーザ要求にも柔軟に対応することが可能になっています。
- 来年にはGPT-4を上回る能力を持つとされる３つのモデル
	- ① OpenAI: GPT-4.5/GPT5 
	- ② Google: Gemini 
	- ③ Apple: Ajax
	- Apple is reportedly spending ‘millions of dollars a day’ training AI
	- https://www.theverge.com/2023/9/6/23861763/apple-ai-language-models-ajax-gpt-training-spending
- 仏教対話AIの多様化に成功―親鸞ボットと菩薩ボットの増産―(京大）
	- https://www.kyoto-u.ac.jp/ja/research-news/2023-09-12-0
	- 生成系AI「ChatGPT 4」と宗教を掛け合わせた新型チャットボット「親鸞ボット」と「世親ボット」を共同開発し、仏教対話AIの多様化に成功しました。
	- [会話事例](https://www.itmedia.co.jp/news/articles/2309/14/news083.html)が、地獄にしか見えないのは気のせい？
- リクルートにおける数理最適化の 活用事例と産学連携の取り組み
	- https://speakerdeck.com/recruitengineers/rikurutoniokerushu-li-zui-shi-hua-no-huo-yong-shi-li-tochan-xue-lian-xi-noqu-rizu-mi
	- 企業における数理最適化専門グループって、大変なのよね。
-  生成AIブームで多発の可能性　「PoC貧乏」
	- https://forbesjapan.com/articles/detail/65744/page2
	- 「生成AIで何かビジネスを作ってみて」と上層部が丸投げし、成果が出ないまま人件費がかさむ、ゆるやかなPoC貧乏が頻発することが考えられます。
	- まあ、生成AIに限らないわけだが。。
- Calls out of chaos: the adaptive significance of nonlinear phenomena in mammalian vocal production
	- https://www.sciencedirect.com/science/article/abs/pii/S0003347201919128
	- 赤子の泣き声がカオス的なダイナミクスで、複雑さと予測不可能性によって親に無視させないようにする適応的意義があるらしい
- 自然言語処理で扱うテキストのchunkingについて
	- https://zenn.dev/hijikix/articles/f414b067e29a57
	- Adjacent Sequence Clustering
	- 全体の文章をセンテンスに分割した後、チャンクに詰めていくのだが、その際に直前のセンテンスと処理中のセンテンスの意味的類似度を比較して、意味が離れているものは次のチャンクに詰める
- llamaindexのRAG作成チュートリアル（ローレベル）
	- https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/low_level/root.html
	- ローレベルというのは、プリミティブな処理で構成するという意味。
- llamaindexのResponseの作り方
	- Building Response Synthesis from Scratch
	- https://gpt-index.readthedocs.io/en/latest/examples/low_level/response_synthesis.html
	- promptをカスタマイズできるのが素敵。
- Vector databases (Part 4): Analyzing the trade-offs
	- https://thedataquarry.com/posts/vector-db-4/
	- ベクトルDBのトレードオフを分析した記事。挿入vs読取速度、取りこぼし（Recall）vsレイテンシー、インメモリvsオンディスク、全文検索vsベクトルハイブリッド検索等の観点から比較・分析を実質
- AstroLLaMA: Towards Specialized Foundation Models in Astronomy
	- https://arxiv.org/abs/2309.06126
	- 特定分野に特化したLLMが大量発生する予感。
- 東京都の 「文章生成AI利活用ガイドライン」
	- https://www.metro.tokyo.lg.jp/tosei/hodohappyo/press/2023/08/23/14.html
	- プロンプトの具体例も豊富でわかりやすい
- llamaindexがLiteLLMをサポート、＋１００のLLｍが利用可能に？？
	- https://gpt-index.readthedocs.io/en/stable/examples/llm/litellm.html
	-  (OpenAI, Cohere, AnthropicAI, huggingface, etc.)に対して同じインターフェイスを提供。
	- というか、LiteLLMすごいな。
- Announcing the Preview of OpenAI Whisper in Azure OpenAI service and Azure AI Speech
	- https://techcommunity.microsoft.com/t5/azure-ai-services-blog/announcing-the-preview-of-openai-whisper-in-azure-openai-service/ba-p/3928388
	- Azure OpenAIサービスおよびAzure AI SpeechでのOpenAI Whisperのプレビューを発表しました
- Discover the LLMs
	- https://llm.extractum.io/
	- LLM の VRAM や Context Len が一覧表示できて便利
- BCGとハーバードやMIT等によるGPT4を使用したタスク実験
	-  Centaurs and Cyborgs on the Jagged Frontier
	- https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged
	- BCGのコンサルティング758名で実験 
	- 18種類のコンサルタスクが対象 
	- AIを使用したコンサルは 、12.2％多く仕事を終え、 25.1％早く仕事を完了し、 40％高い品質
-  Optimizing LLMs From a Dataset Perspective
	- https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html
	- LLMsの最適化について、データセットの側面からまとめたブログ。人手で高品質なデータセットを作るグループや、LLMから大量のデータセットを生成するグループなど、いくつかの側面が簡潔にまとまっている
- InstaGraph
	- https://github.com/yoheinakajima/instagraph
	- 任意のドキュメントから知識グラフ作れるらしい。
	- 例：https://x.com/yoheinakajima/status/1701351068817301922?s=20

## 9/11

8/23に公開されたGPT-3.5-turboのfine-tuning API、RAGとの比較、証券報告書のQ&Aアプリの具体例、など、面白い記事がたくさん出てきた。Open Interpreterも相も変わらず熱い。デジタル庁のChatGPTの業務利用ハンズオン、いいな、こういうリテラシーを持てる人が増えないと。。大規模コンテンツ・行動モデル（LCBM）って、記号接地問題にさらに近づこうとしているのか？LLMをつかった様々なエージェントの作り方、いろんなデータ専門のエージェントがたくさんそろってくると、そろそろOrchestratorが必要かな。**Production-Ready LLM Applications**ってのは必読なスライドですね。ICML2023のまとめもあった。RAGを対象としたLLMの比較、フレームワークになってありがたい。ChatGPTの複数出力とか、性能が落ちたのでは？という疑惑など、何が起きているのか、起こそうとしているのか。

-  東京大学理学部オープンキャンパス2023 講演「生成型AIの数理と倫理」佐藤一誠教授
	- https://www.youtube.com/watch?v=n6NDlgJVug8&t=5s
-  Mustafa Suleyman on getting Washington and Silicon Valley to tame AI
	- https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/
	- DeepMindの共同創業者で、世界最高水準のAIスパコンを構築中のAI開発会社「Inflection AI」の設立者でもあるスレイマン氏によれば、今後18ヶ月程度でGPT-4の学習に使用された計算回数の10倍〜100倍がAIモデルの学習に使用され、次の3年程度でGPT-4の1000倍の計算回数が学習に使われるだろう、とのこと
- LangChain Cheat Sheet
	- https://www.kdnuggets.com/2023/08/langchain-cheat-sheet.html
- llamaindexより、Summary Index(旧List Index)の紹介
	- https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/index/index_guide.html#summary-index-formerly-list-index
- AI Agents – Build and Host LLM Apps At Scale
	- LLMを活用さいた様々なエージェントの作り方についての記事、なるほど
	- https://blog.abacus.ai/blog/2023/08/31/supercharge-productivity-accomplish-10x-more-with-ai-agents/
- Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior
	- https://huggingface.co/papers/2309.00359
	-  **大規模コンテンツ・行動モデル（LCBM）とコンテンツ・行動コーパス（CBC）**：本文書では、行動トークンをLLMの訓練に再導入する初期的な試みを行う。LCBMと呼ばれる新しいモデルは、コンテンツ理解タスクにおいてLLMと同等の性能を示すとともに、行動シミュレーション、コンテンツシミュレーション、行動理解、行動ドメイン適応といった能力も持つ。さらに、LCBMの研究を促進するために、コミュニケーター、メッセージ、受信者行動を含む新しいコーパスであるCBCを公開する。
- ChatGPTを業務に組み込むためのハンズオン
	- デジタル庁が一般公開しているChatGPTの入門
	- https://www.digital.go.jp/assets/contents/node/information/field_ref_resources/5896883b-cc5a-4c5a-b610-eb32b0f4c175/82ccd074/20230725_resources_ai_outline.pdf
	- なかなかのやり手が書いている、ここまで試行できる人は少ないのでは？？
	- プロンプトの書き方のコツ
		- できる限りコンテキストを明確にして書くこと
		- GPTの理解度(?)を確認しながら進める
		- 最初はマニュアルを読むより、まず自分でやってみて感覚をつかみことを推奨
- 最近のLLMの学習法のまとめ - SFT・RLHF・RAG　by npakaさん、
	- https://note.com/npaka/n/n862786604dc3
	- とりあえず、どれだけ知ってる？だけでもリトマス試験紙になる、むろん私はRAG派
	- SFT : Supervised Fine-Tuning
	- RLHF : Reinforcement Learning from Human Feedback
	- RAG : Retrieval Augmented Generation
- LangChain を使ったRAGを Elyza 7b instruct モデル
	- https://note.com/alexweberk/n/n3cffc010e9e9
	- 無料のT4ではメモリーオーバーで動かないんだが。。。
- SEC Insights
	- llamaindexを活用して、米国証券取引委員会への報告書(SEC-10)にたいするQ&Aアプリを作る例
	- https://github.com/run-llama/sec-insights
	- https://www.secinsights.ai/
-  Streamlit 入門  by npakaさん
	- https://note.com/npaka/n/n29b5e8088fe5
	- 「Streamlit」は、機械学習およびデータサイエンスのためのWebアプリケーションフレームを簡単に作成して共有できるPythonライブラリ
	- もうちょっとどうにかならんのか？
- **Production-Ready LLM Applications**
	- llamaindexのCEOより、
	- https://docs.google.com/presentation/d/1uzhz1aFWbyXSrWBzQ1FPQWtVjMgJqAYGoGoVzEnNmAg/edit#slide=id.p
		-  Fine-tuning: LLMs + embeddings
		-  Better Data + Retrieval Techniques for Production RAG
- ELYZA-7bは、M1 MacBook Airでもサクサク動くらしい
	- https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/blob/main/README.md
- やっぱりOpenInterpreterが熱い
	- https://github.com/KillianLucas/open-interpreter
- LLMをホストするAnyScaleのllamaindexでの利用例
	- https://gpt-index.readthedocs.io/en/latest/examples/llm/anyscale.html
	- run + finetune open-source LLMs through an API
	- そういうビジネスができるのか。。
-  Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data
	- https://betterprogramming.pub/fine-tuning-gpt-3-5-rag-pipeline-with-gpt-4-training-data-49ac0c099919
	- どうも、8/23にOpenAIがGPT-3.5-turboのfine-tuning APIを公開して、即座にllmaindexがこれに対応したらしい
	- じゃあ、Q&Aアプリを作るのに、RAGとFine-tuningどちらが高性能か？ということへの考察記事
	- こちらは、llamaindexをつかったGPT-3.5-turboのfine-tuningのcolab
		- https://colab.research.google.com/drive/1NgyCJVyrC2xcZ5lxt2frTU862v6eJHlc?usp=sharing
- Hierachical Agent	
	- 対象ドキュメントの内容が階層構造であるような場合のQ&Aの作り方。
	- https://colab.research.google.com/drive/1qIb09SyuLeiwGy_FGcRcQpM78yQ2p0_3?usp=sharing
- Discover LlamaIndex: Custom Tools for Data Agent
	- https://www.youtube.com/watch?v=lcuL6Gqw_-g
- 【速報】OpenAI APIでGPT-3.5-turboがfine-tuningできるようになりました！
	- https://dev.classmethod.jp/articles/openai-gpt35turbo-fine-tuning/
	- 学習するサンプルは最小10個必要で、50～100個で明確な改善が見られる
	- gpt-3.5-turboでfine-tuningが利用可能に
	- gpt-3のモデルであるbabbage-002とdavinci-002も新しいfine-tuningでサポート（モデルもGPT baseという扱い）
- グラフニューラルネットの 2023年まとめ (ICML2023)
	- 軽量 Transformer の介入や Diffusion for Molecules などの実世界利用、幾何学的な利用が記載されている
	- https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc
- Open Inerpreterの利用例、「nikkei225の10年分をプロットして」と滅入れすればあとは自動で、、、
	- https://twitter.com/NuCode/status/1700679106814501132?s=20
- ChatGPTが、可能性のある答えを複数ていじするようになった、RLHFやらせようとしているのかと話題に
	- https://twitter.com/GrantSlatton/status/1700662574315090351?s=20
- LLMの評価、特にRetrieval Augmented Generation (RAG) パイプラインを評価するためのOSSフレームワークragas
	- https://github.com/explodinggradients/ragas
- Agent deconstructedに、llmaindex agentが統合された？
	- https://github.com/shoggoth13/agents-deconstructed/blob/main/notebooks/react_chat.ipynb
	- ReActができるようになったのか。。、いろんなindexをもつLLM同士が会話して問題解決。。
- 【デモ付き】Embeddingsで独自データをChatGPTに理解させる
	- https://corp.langcore.org/media/embeddings
	- LangCore SaaSを使ってインフラ不要で手軽にEmbeddingsを活用した独自データの活用、らしい

## 9/4

GoogeからGPT-4対抗のGeminiが発表、GPT-4 の 2023 倍の計算能力を持つ？。LLMのファインチューニング関係で、様々な紹介がある。llamaindex周りの記事が多いが、それだけRAG(Retrieval-Augmented Generation)って需要があるということか。Embeddingもしっかり性能評価やファインチューニングすると性能があたる。llamaindexでQ&Aの性能を上げるためのTipsが詳しく書いてある、これは役立つ。ローカルLLMの試行も熱い、なんとCode interpreterもどきも動くという。最近のLLMでは、ELYZAが一番の模様(by shi3z)。理論関係では、transformerにおける自己注意はSVMと等価なのか？、確率過程の新刊も気になる。

- LLMのファインチューニング で 何ができて 何ができないのか
	- https://note.com/npaka/n/nec63c01f7ee8
- code llama がhuggingfaceのchatに登場
	- https://huggingface.co/chat/
- Llamaで、出力を指定するためのgrammar-based sampling
	- https://python.langchain.com/docs/integrations/llms/llamacpp#grammars
- Google 「Gemini」は、ChatGPT-4 Enterprise プラットフォームの直接の競合相手
	- https://www.theinformation.com/articles/the-forced-marriage-at-the-heart-of-googles-ai-race
	- GPT-4 の 2023 倍の計算能力を持つ
- llamainexでembeddingをファインチューニングする
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding.html
-  論文紹介 / Llama 2: Open Foundation and Fine-Tuned Chat Models　by NTT西田さん
	- https://speakerdeck.com/kyoun/llama-2-open-foundation-and-fine-tuned-chat-models
-  ご家庭用LLMでストリーミングする方法
	- https://note.com/shi3zblog/n/n66ae41af7c64
	- "elyza/ELYZA-japanese-Llama-2-7b-instruct"　利用
-  LlamaIndexの性能向上のためのテクニックガイド by npaka
	- https://note.com/npaka/n/n33e28a9e1409
-  Discover LlamaIndex: Introduction to Data Agents for Developers
	- https://www.youtube.com/watch?v=GkIEEdIErm8
	- first-ever video tutorial on LlamaIndex Data Agents
-  ChatGPT vs BERT：どちらが日本語をより理解できるのか？
	- https://fintan.jp/page/9126/
-  LlamaIndex の QAプロンプト と Refineプロンプト のカスタマイズ
	- https://note.com/npaka/n/ne878095d5bda
- llama2-13b-128k、論文を全部理解して要約を吐き出す方法
	- https://gist.github.com/alfredplpl/33fd6dd6d623d4da959f1ca8aabc88fe
- 「データ分析のための統計学入門」
	- http://www.kunitomo-lab.sakura.ne.jp/2021-3-3Open(S).pdf
- 【ローカルLLM】text-generation-webUIのAPI機能を試す
	- https://note.com/bakushu/n/na4e51d377ae7
	- LLM用のウェブUIであるtext-generation-webUIにAPI機能が付属しているので、これを使ってExllama＋GPTQのAPIを試してみた。
- 最近のLLMの性格 by shi3z
	- https://twitter.com/madyagi/status/1697949115190255951?s=20
	- ELYZAが良いみたい。
-  Transformers as Support Vector Machines
	- https://arxiv.org/abs/2308.16898
- fine-tuned a gpt-3.5 ReAct agent to be better at chain-of-thought
	- https://gpt-index.readthedocs.io/en/latest/examples/finetuning/react_agent/react_agent_finetune.html
-  機械学習のための確率過程入門
	- https://www.ohmsha.co.jp/book/9784274231087/
-  ローカルPCのターミナル上でLLM生成コードを実行できるOpen Interpreterを試す
	- https://note.com/hamachi_jp/n/n05ae28b76d9d
	- ChatGPTのコードインタープリター（Advanced Data Analysis）と同様な機能をローカル環境で実行可能な Open Interpreter 
	- llamaに差し替えることも可能
- 

## 8/28

先週発表された、松尾研の“Weblab-10B”に対する量子化やローカル環境での実行も花開くが、やっぱり今週はメタによるCode Llamaの発表がポイントになっている。
「LLM によるプログラムベース推論」的な考え方ってLLMをつかったアプリ作成には絶対必須な考え方になると思う。品質保証では、ガードレールとか、推論過程のガイドが必要だったり、得手不得手をちゃんと理解したうえでガイドするみたいな感じ。emergent機能とはLLMを動かしていて、予測していたのとは違う機能が創発するという話、欧州ＡＩ規制でも言及される、仕組みの解明と対策が急務。llamaindexから、外部検索と組み合わせる新しい、Metaphor機能がリリース。なんかどこのURLを見ればよいかのDBをつかってやるみたいな感じ。。HuggingFaceでは、LLMをWebベースで、ファインチューニングできる機能が公開されたらしい。結果はそのままHuggingFaceに乗るみたいなノリ。LLMをつかったQ&AであるRAGフレームワークで、類似データをtop-kでとってくる仕組みがうまくいかないときの工夫など、納得感ある。メタからCode Llamaが発表、コード生成ができる。さっそく、量子化されたり、llama.cppでローカルに動かしたりと、あっというまに、誰でも使えるようになる。コミュニティはすごいな。理論面では、emergentスキルに関して、通常の汎化理論に反する「スリングショット汎化」の提唱、ＬＬＭをつかった帰納的学習法というのも、従来の予測を書き換えるか。ＡＩ規制に対するパブコメをＡＩで分析など面白いかも。。

- 言語モデルにおける複雑なスキルの創発に関する理論　A Theory for Emergence of Complex Skills in Language Models
	- https://note.com/daichi_mu/n/n72b6265b09f6
	- 言語モデルのスケールアップに伴う新たなスキルの出現について、統計的枠組みと数学的分析を用いて分析する。能力レベルが通常の汎化理論に反する「スリングショット汎化」の概念を導入
- LLM によるプログラムベース推論
	- https://speakerdeck.com/smiyawaki0820/2023-dot-08-dot-07-geography-and-language-mian-qiang-hui-number-4
	- LLM 開発における評価・品質担保に関係、ガードレールや、推論過程のガイドなど最後はVisProg紹介
	- 東北大の宮脇さん、地理空間情報をLLMをつかいながら推論する仕組みについて。
- AIが「理解」するから、API仕様書のコピペでアプリができあがるローコード開発環境「Flowise」
	- https://internet.watch.impress.co.jp/docs/column/shimizu/1523766.html
- **[chatux-server-llm](https://github.com/sotokisehiro/chatux-server-llm)**
	- ローカル環境で動作する文章生成 AI チャットボットです。 CPU だけで動作します。
	- LINE の japanese-large-lm-3.6b-instruction-sft を CTranslate2 化
- Vicuna 13B v1.5 、text-generation-webui じゃなくて以前試作した llama.cpp の HTTP サーバー機能を使ってみたら普通に LLaMA 2 13B と遜色ない結果出してくれた
	- https://twitter.com/izutorishima/status/1693468524222861589?s=20
- Metaの大規模言語モデル「LLaMA」のトレーニングにも使用されたAIの学習用データセット「Books3」が削除される
	- https://gigazine.net/news/20230821-books-3-ai-data-set/
	- 知的財産権や著作権に対する侵害の疑いが指摘されていたらしい
- LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs
	- https://medium.com/llamaindex-blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f
	- Metaphor was trained to predict links on the internet, given how people talk about things on the Internet
	- インターネット検索とllamaindexの融合？結合の新たな形としてのメタファー？
- 【ローカルLLM】Gradio+CTranslate2で日本語LLMのチャットUIをつくる
	- https://note.com/bakushu/n/nba6e9c353ee4
	- [line-corp-japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft)を利用
	- CTranslate2で量子化
	- あとはgradioでWebUI生成！
- Generally Intelligence社、米国商務省国家電気通信情報庁（NTIA）が実施したAI規制に関するパブリックコメントの約1450件の回答の分析を開始。
	- https://generallyintelligent.com/perspectives/ntia-rfc-analysis/
	- https://twitter.com/kanjun/status/1693819078866354376?s=20
- llamaindexのMetaphorサーチのお試しができるらしい。
	- https://twitter.com/jerryjliu0/status/1693773766797746649?s=20
	- https://colab.research.google.com/drive/1PTnJTVmLAI-V8JJu8GsbUvbk8vs203kA?usp=sharing
- Stanford大学のHAIから、Create AI Actを連邦政府が法案をとおすべきである、米国のため
	-  We Must Pass the Create AI Act
	- https://hai.stanford.edu/news/we-must-pass-create-ai-act?utm_source=twitter&utm_medium=social&utm_content=Stanford%20HAI_twitter_StanfordHAI_202308220803_sf181078680&utm_campaign=&sf181078680=1
- Open AI でスーパーアライメントを4年以内に完了させることを目標として率いているJan Leike氏の対談
	- https://80000hours.org/podcast/episodes/jan-leike-superalignment/
-  Inductive-bias Learning: Generating Code Models with Large Language Model
	-  **帰納的学習法**：大規模言語モデル（LLM）を用いて、説明変数から目的変数を予測するモデルを生成する新しい学習法。この学習法は、教師あり学習とメタラーニングの要素を持つ。
	- https://arxiv.org/abs/2308.09890
-  日本語が使えるようになったGoogle PaLM2を試す
	- https://note.com/eurekachan/n/n62b15394b5dc
	- BigQuery のSQLなんかも日本語で生成をお願いすることが出来ます。
	- LangChainからも呼び出したりできるようです
- ANYONE can fine-tune (almost) any LLM available on Hugging Face
	- Hugging Faceで簡単にLLMをファインチューニングできるAPIが公開
	- https://twitter.com/abhi1thakur/status/1693619860050153958?s=20
- RAGシステムで、top-k 抽出がうまくいかないときの工夫について
	- https://twitter.com/jerryjliu0/status/1694013501323563101?s=20
	- Metadata Filters + Auto Retrieval:
	- Store Document Hierarchies (summaries -> raw chunks) + Recursive Retrieval
- 今村・松井の『ベイズ最適化』
	- 第4章までよめらば、ベイズ最適化が理解できるらしい。
	- https://www.kindaikagaku.co.jp/book_list/detail/9784764906631/
- メタが、Code Llamaを公表
	- https://ai.meta.com/blog/code-llama-large-language-model-coding/
	- Foundation base models (Code Llama) 
	- Python specializations (Code Llama - Python), 
	- Instruction-following models (Code Llama - Instruct)
- 【ローカルLLM】Colabの標準GPUで「CodeLlama-34B-GGUF」を動かす
	- https://note.com/bakushu/n/n21cb30a15f27
	- 量子化は「GPTQ」ではなくて、CPU＋GPUで実行できる「GGUF(旧GGML)」
	- 標準GPU（Tesla T4）で動くのがみそ
- Weblab-10Bを量子化(GPTQ)して簡単に動かすことがhugging faceでできる
	- transformersにGPTQが統合されたおかげで、無料Colabでそのままでは動かなかったWeblab-10Bもらくらく動くようになってた。
	- dahara1/weblab-10b-instruction-sft-GPTQ
	- https://github.com/webbigdata-jp/python_sample/blob/main/weblab_10b_instruction_sft_GPTQ_sample.ipynb
- 【まとめ】Google Colab で Code Llama を試す
	- https://note.com/npaka/n/n51ed424b2943
- CodeLlama model now work w/ llama-cpp-python
	- [@TheBlokeAI](https://twitter.com/TheBlokeAI)さんによる
	- llama.cpp GGUFの組み合わせで動くということ
	- https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main
	- https://github.com/abetlen/llama-cpp-python
- CodeLamaの、colabでの実行とビデオ
	- https://colab.research.google.com/drive/1lyEj1SRw0B9I2UUI2HOrtiJ_fjvbXtA2?usp=sharing
	- https://www.youtube.com/watch?v=rlCe_lG4uhk

## 8/21
暑くて溶けそうなのに、電力はどうにかもっている夏です。松尾研からの国産LLMである“Weblab-10B”の発表。なお、松尾研には夏休み中の総理も訪問され講座を受講（なにか修了証書をもらってたな）、もっと国としてのサポートが期待できるかも。GPT-4は、暗号化されたプロンプトも理解できるぐらい優れているらしいが、特定の「脱獄プロンプト」に弱い面も。Trustworthy LLM、LLMの信頼性などの研究も進む、社会規範への整合とかそういう側面もある。スタンフォード大学のLLMの安全性のベンチマークとの比較も気になる。あいもかわらず知識グラフ系のLLM応用がちらほら、知識グラフ抽出や知識グラフをつかったRAG(Retrieval-Augmented Generation)などもあるが、知識の活用かそれともファインチューニングか？みたいな第２世代(エキスパートシステム）と第３世代（データがすべて）のAIの対比みたいな絵面だなあ。MRIスペクトルから分子を予想みたいな素朴な応用がもっとあっていい気もする。TRL(Transformer Reinforcement Learning)は、強化学習を用いたLLMの最適化を簡単にできるようになるらしい、DPO(Direct Preference Optimization)なんか斬新じゃん。元Googleトップ研究者による「Sakana AI」にはびっくり、めざす「自然からインスピレーションを得たインテリジェンスに基づいた新しいタイプの基礎モデル」とはどんなものになるのか？日本はコンテンツだけでなくて、人材リソースとしてもまだ魅力がある？？

- ローカルデータに対するQ&Aなどするときに、知識を活用したRAGで構成するのがよいのか、いや、目的に対してLLMをファインチューニングするがいいのかというはなし
	- Knowledge Graphs & LLMs: Fine-Tuning vs. Retrieval-Augmented Generation
	- https://neo4j.com/developer-blog/fine-tuning-retrieval-augmented-generation/
- LLMをつかったsemantic searchのDeeplearning.aiの無料コース
	- Large Language Models with Semantic Search
	- https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/
- GPT-4のセーフガードを故意に突破する脱獄プロンプトに関する研究
	-  "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
	- https://jailbreak-llms.xinyueshen.me/
- 「汎用的なAIってやつ」を作ったところで、それで十分なレベルまで収益化を実現させるのはそれなりに難しいという話(TJO
	- https://twitter.com/TJO_datasci/status/1691112696685719553
- GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher
	- https://arxiv.org/abs/2308.06463
	- GPT-4 can understand ciphertext, which introduces the risk of generating unsafe content.
- CSVにたいするQ&Aエージェントのベンチマーク
	- https://github.com/langchain-ai/langchain-benchmarks/tree/main/csv-qa
-  Knowledge Graph RAG Query Engine (RAG: Retrieval-Augmented Generation)
	- https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_rag_query_engine.html
	- augmenting LLMs with context from a graph database
-  Large Language Models with Semantic Search
	- https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/
	- Deeplearing.aiからのsemantic searchの無料コース、Cohereの人がでている？
- 知識グラフ抽出のデモ
	- text to graph playground
	- https://auto-graph.streamlit.app/
-  Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment
	- LLMの信頼性に関するサーベイ論文
	- 用語や概念を整理し，実際に8つの観点からLLMの信頼性を検証
	- https://arxiv.org/abs/2308.05374
	- reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness.
	- 目的は：reliable and ethically sound deployment of LLMs in various applications.
- RWKVについて解説
	- https://agirobots.com/rwkv/
	- RNNの利点である高速な推論と処理可能なシーケンス長を大幅に向上
- LLMに関して起きている訴訟について
	- https://twitter.com/srush_nlp/status/1691845245074620915?s=20
- LLMでMRIスペクトルから分子を予測
	- https://chemrxiv.org/engage/chemrxiv/article-details/64d5e4ccdfabaf06ff1763ef
	- NMRスペクトルを文字列で表現、これを言語モデルへ入力し分子を予測することで67%の精度
- 松尾研究室100億パラメータサイズ・日英2ヶ国語対応の大規模言語モデル“Weblab-10B”をオープンソースで公開
	- https://weblab.t.u-tokyo.ac.jp/100%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%BA%E3%83%BB%E6%97%A5%E8%8B%B12%E3%83%B6%E5%9B%BD%E8%AA%9E%E5%AF%BE%E5%BF%9C%E3%81%AE%E5%A4%A7%E8%A6%8F%E6%A8%A1/
	- https://huggingface.co/matsuo-lab/weblab-10b
	- 日本語のベンチマークであるJGLUE評価値が事前学習時と比べて大幅に改善（66→78%
	- 早速オープンソース警察が、商用に使えないのにオープンソースとは言わないとの突っ込みが。。
- 岸田首相、東京大で生成AIの講座受ける　「百聞は一見にしかず」
	- https://www.asahi.com/articles/ASR8G6X84R8GUTFK002.html
	- 松尾豊・東大大学院教授の講座を受けた。AIを学習させるプログラミングも体験し、受講
- LLMをつかった文書検索では、メタデータを入れることで性能が改善する
	-  Building Production-Ready LLM Apps with LlamaIndex: Document Metadata for Higher Accuracy Retrieval
	- https://betterprogramming.pub/building-production-ready-llm-apps-with-llamaindex-document-metadata-for-higher-accuracy-retrieval-a8ceca641fb5
- GoogleのトップAI研究者2人が東京でAI企業立ち上げを発表
	- 「自然からインスピレーションを得たインテリジェンスに基づいた新しいタイプの基礎モデルを開発する」
	- ジョーンズ氏とハー氏が新AI企業「Sakana AI」を東京に設立
	- うち1人は、生成AI革命のきっかけとなった論文の著者の一人
	- 日本で研究者を募り、生成AIの基盤モデル開発を目指す
	- https://www.nikkei.com/article/DGXZQOUC186TM0Y3A810C2000000/?n_cid=SNSTW001&n_tw=1692351448
	- 起業の地に日本を選んだ理由として、米国で生成AIの人材獲得競争が過熱している点をあげた。
-  TRL - 強化学習によるLLMの学習のためのライブラリ
	- TRL - Transformer Reinforcement Learning
	- https://note.com/npaka/n/nbb974324d6e1
	- 強化学習を使用してTransformer言語モデルを学習できます。このライブラリはHuggingFace Transformersと統合されています。
-  DPO による Llama 2 のファインチューニング(npaka)
	- https://note.com/npaka/n/nfe7391a1d28d
	- 「Direct Preference Optimization」では、既存の手法で使用されているRLベースの目標を、単純なバイナリクロスエントロピー損失を介して直接最適化できる目標に切り替える
	- LMを改良するこのプロセスが大幅に簡素化

## 8/14
お盆ですが、膨大にならないうちに更新します。ところで、「大規模言語モデル入門」(技術評論社ISBN 978-4-297-13633-8）いいですね、Huggingfacesをつかって、日本語データセットをつかった、ファインチューニングなど見所が多い。
さて今週は、先週に引き続き vicuna-v1.5関係の記事が多かったわけですが、stability.aiから日本語のStableLLMがリリースされたがのが大きなニュースでした。LLMベンチマークもColab環境でできるらしい。Metaの公表した生成AIのガイドとか、FacToolなんか、AIの安全性やリスクなんかに対してちゃんと取り組んでいる。日FR本のAI戦略の、開発促進に偏った姿勢とは一線を画している（つまり余裕がないということ）。FacToolによる分析の結果、GPT-4はやっぱりすごいんだな。Llmaindexのllmがgpt-3.5-turboにやっと変更されたらしい、そんなに使いにくかったのか。。LLMをプロダクションで使うための色々なTipsが公表されてたり、一方Andrew Ngさんは、LLMが世界を理解しているというブログを開陳。LLM時代の医療へのAI利用のベネフィットとリスクについてのランサー記事とか、数学者Terence Taoさんの、LLMをつかったAIが数学論文の共著者になりうるという興味深い予測も。産総研のAIセミナー、あっという間に満杯に。興味だけは大きいのに、手が動かない人が多すぎないか。。まあ、LLMでいくら頑張てもChatGPTでよくない？みたいな意見もある。様々な面で、日本はLLM開発で遅れてきていて、もはや以前のような横綱相撲をするような感じではないのに政府はそうは言えないのか、しかし民間は頑張っている。

- LlamaIndexでAutoGPTQモデルを使う（vicuna-13B-v1.5-GPTQ）
	- https://zenn.dev/libratech/articles/1979874b223895
	- 4bit化など軽量化されたllmをllamaindexで使う方法、ローカル環境とか
	- Colabの無料版(T4インスタンス)でも動作する
- LLama2公開にあわせて、Metaから"responsible generative AI"に関するガイドが出ている.、
	- https://ai.meta.com/static-resource/responsible-use-guide/
- text-generation-webui で TheBloke/vicuna-13B-v1.5-GPTQが動く
	- https://twitter.com/smorce1/status/1688250856129646592?s=20
- llama2をつかって、ローカルにQ&Aを実行する手法について via llamaindex
	- https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemoLlama-Local.html
- LLMを試すのに、「ガンダムテスト」というのがあるらしい、vicuna-13b-v1.5-16kは優秀らしい
	- https://twitter.com/NuCode/status/1688455649091608576?s=20
- 内閣府AI戦略会議(8/4)の資料が一部公開、AI関連施策は開発振興一本足に近くリスク対応が申し訳程度
	- https://www8.cao.go.jp/cstp/ai/ai_senryaku/4kai/shisaku.pdf
-  IPA「ITパスポート試験 シラバス」に、生成AIの仕組み、活用例、留意事項等に関する項目・用語例を追加
	- https://www.ipa.go.jp/shiken/syllabus/henkou/2023/20230807.html
- 「JP Language Model Evaluation Harness」によるLLM性能評価 by stabilityAI
	- https://note.com/npaka/n/nedf4dacd4037
	- Colab(T4)で12時間もかかる、できるらしい
- llama-2-13bのJGLUE、言語モデルの評価と関係
	- https://huggingface.co/HachiML/Llama-2-13b-hf-qlora-dolly-ja-2ep/blob/main/benchmark_jglue/JGLUE_Llama-2-13b-hf-qlora-dolly-ja-2ep.ipynb
- GPTQの元論文はこちら、
	- https://arxiv.org/pdf/2210.17323.pdf
	- GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS
- ストックマークは最近の話題にも詳しいGPT-NeoXをベースとした14億パラメータの日本語のLLMをOSS公開
	- https://stockmark.co.jp/news/20230808
- HuggingFacesとNVIDIAが提携、企業向けのサービスを展開？
	- https://www.nvidia.com/ja-jp/about-nvidia/press-releases/2023/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing/
	- HuggingFaceにあるAIモデルのトレーニングとか微調整ができる企業向けのサービスで、GPUとしてNVidiaのクラウドGPUが選べるようになるらしい。
- 悲報？：産総研、LLMのセミナー「シミュレーションとAIの融合技術とその最新事例」、すぐに定員いっぱいになる
	- https://www.airc.aist.go.jp/seminar_detail/seminar_069.html
- Stability.ai、 日本語言語モデル「Japanese StableLM Alpha」をリリース(8/10)
	- https://ja.stability.ai/blog/japanese-stablelm-alpha
- 早速Japanese Stable LLMを、Colab無料環境から利用するnotebookが公開
	- https://colab.research.google.com/github/mkshing/notebooks/blob/main/stabilityai_japanese_stablelm_alpha_7b.ipynb
	- huggingfacesにログインしないといけない、、が動くぞ！
	- ガンダムテストしてみたが、なんか、学習時につかったデータが表示される。
- 生成AIによって生成されたテキストを判別する方法についての論文
	- https://arxiv.org/abs/2306.15666
	- Testing of Detection Tools for AI-Generated Text
	- ■文章のスタイルを変化させられている場合（例えば子供っぽくなど）、識別が困難になる 
	- ■言い換えや書き換えによって段階的に文章を変更されると、識別がかなり困難になる
	-  ■AI生成コードの検出はAI生成テキストの検出よりもさらに困難になる
- Langchainのテキスト分割の様子を目視できる、playgroundが爆誕
	- https://langchain-text-splitter.streamlit.app/
- Google Colab で Japanese StableLM Alpha + LlamaIndex の QA を試す
	- https://note.com/npaka/n/n5c80ca661357
- 「とっきょ」広報誌で、こち亀の内容が、拒絶通知の理由になった事例が紹介。。
	- https://www.jpo.go.jp/news/koho/kohoshi/vol57/07_page1.html
	- 拒絶を避けるべく、特許出願する前にはこち亀を全巻読破する必要があるのか、、、
	- 審査官の趣味という気もするが、、
-  ChatGPTの新機能カスタム指示の面白い使い方
	- https://note.com/it_navi/n/nca4643390969
	- カスタム指示は、ChatGPTの**役割、回答方針、出力形式など**を予め設定することができます。
- LLMは世界を理解しているか？by Andrew Ng
	- https://www.deeplearning.ai/the-batch/issue-209/
	- Othelo-GPTの例から、答えは YESらしい。
- 生成AIの文章やコード、論文が“事実か”チェックする技術　米Meta含む研究者らが開発
	- https://www.itmedia.co.jp/news/articles/2308/09/news064.html
	-  FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
	- https://arxiv.org/abs/2307.13528v2
		- 研究者らはベンチマークを開発し、知識ベースのQA、コード生成、数学の問題解決、科学論文のレビュー執筆の4つのタスクで実験を行った。その結果、GPT-4はChatGPT、Bard、Claude-v1、Vicunaと比較して、事実精度が最も優れていた。Vicuna-13Bは、知識ベースのQAではそれなりに良好な事実性を示したが、コード生成、数学の問題解決、科学論文のレビュー執筆など、より困難なシナリオではパフォーマンスが低い結果となった。
- llamaindexのv0.8がリリース
	- https://github.com/jerryjliu/llama_index/blob/main/CHANGELOG.md
	- [1] The default LLM is now gpt-3.5-turbo
	- [2] Speaking of changing prompts, we’ve changed the default question-answering templates for both our create and refine strategy as well as tree_summarize.
	- [3] Our default text splitter is now our brand-new sentence text splitter.
	- [4] Added llama.cpp and @huggingface as fallbacks if openai key is not set.
	- [5] Some new features: a `SentenceWindowNodeParser` and `MetadataReplacementNodPostProcessor` 
- チュートリアル、Create a CustomGPT And Supercharge your Company with AI – Pick the Best LLM
	- https://blog.abacus.ai/blog/2023/08/10/create-your-custom-chatgpt-pick-the-best-llm-that-works-for-you/
-  Building LLM applications for production
	- https://huyenchip.com/2023/04/11/llm-engineering.html
	- LLMをプロダクションで使うための色々なTipsがまとまった記事
- いろいろLLMをいじってみても、結局ChatGPTでよくない？みたいな
	- https://twitter.com/mr_bay_area/status/1689868431900975104?s=20
-  AI in medicine: creating a safe and equitable future
	- Lancerの記事、LLM時代における、医療分野へのAI適用のメリットとリスクについてまとめ
	- https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(23)01668-9/fulltext
-  Embracing change and resetting expectations by Terence Tao@microsoft
	- https://unlocked.microsoft.com/ai-anthology/terence-tao/
	- He predicts that AI will be a trustworthy co-author in mathematical research by 2026, when combined with search and symbolic math tools.
	- 2026年までには、数学研究において、AIが信頼できる共著者になりうるとの予測


## 8/7

llama2ベースのVicuna v1.5で盛り上がっている、langchainやllamaindexとの組み合わせでも動く模様。ReActなどのAgent機能もちゃんとうごくらしい。llama2を手ごろに試せるcolab noteもたくさん公開、ローカルGPUで動かす報告も。なおllama2本家も申請すればを直接使うこともできる。マイクロソフトはwindows上でのllama2というネタでメタとパートナーとのこと、二股かけてる？マイクロソフトがAzure OpenAIをつかったChatGPTもどきのサンプル実装を公開、カニばってない？文章から知識を抽出する方法、llamaindexでも知識グラフ(KG)を抽出するKnowledgeGraphIndexがあったが、REBELという外部のtransformerを利用する方法もあるのか。用途に合わせて選択、細かい調整が必要かな。UCバークレーのDynalang、AIエージェントと紹介されているが、論文タイトルからするとLLMで世界モデルを構築しようとしている（「二重過程モデル」の真ん中に出てくるやつ？記号接地モデルというかそういうやつ）。自コンテンツをつかったChatBotの作り方についてわかりやすい説明があった。JSTの生成AIのまとめ、日本の生きる道は、「第4世代AI」「信頼されるAI」「AI・データ駆動科学」ということらしい。「第４世代AI」とはSystem1とSystem2が連動する、「二重過程モデル」のことらしい、Dynalangの話ともつながった！

> NeurIPS2019で、Bengioの基調講演の「二重過程モデル」（即時的なSystem1と熟考的なSystem2の二重モデル、間に、世界モデルが入る）。知覚系の深層学習(System1)によって眼前の状況に対する世界モデル（World Model）が得られるが、それを使って言語・知識系が適切な手順を組み立てるのがSystem2。カーネマンのFast & Slowとも関連がありそう。。

- Google Colab で Vicuna-v1.5 + LlamaIndex の QA を試す
	- npakaさんより、ハイメモリでないと動かないのか。。
	- https://note.com/npaka/n/n931319f17b34
-  Google Colab で Llama 2 + LlamaIndex の QA を試す
	- npakaさんより、llma2利用には申請が必要なのか、
	- Q&Aテンプレに修正が必要なもよう
	- https://note.com/npaka/n/n3e1b59d1ac9e
- vicuna-7b-v1.5の一番簡単な利用方法by npakaさｎ
	- https://huggingface.co/lmsys/vicuna-7b-v1.5
	- https://twitter.com/npaka123/status/1686872443305295878?s=20
- ChatGPTの小改良が順次リリースされるとの告知
	- https://twitter.com/OpenAI/status/1687159114047291392?s=20
	- prompt exampleとか、Plus会員にはGPT-4がデフォルトになるとか、そういうｙつ
- UCバークレー、アルファ碁とChatGPTを混ぜて強くしたようなAIエージェント「Dynalang」
	- https://arxiv.org/abs/2308.01399
	- Learning to Model the World with Language
- マイクロソフト社、Azure OpenAIで、ChatGPTもどきを作るサンプル実装を公開
	- https://github.com/microsoft/azurechatgpt
	- 企業利用が加速するか。。いやplaygroundで十分？
- 人工知能研究の新潮流2　～基盤モデル・生成AIのインパクト～
	- JSTのまとめ、生成AI研究の動向報告書
	- https://www.jst.go.jp/crds/report/CRDS-FY2023-RR-02.html?fbclid=IwAR0KQ7bg5BRLIblzI154AHYheNrF1SPPzm-xn4z1PuQBUPK2Kia2qT4PMxU
	- 「第4世代AI」「信頼されるAI」「AI・データ駆動科学」
- 雇用判断にAIを使うのは、EU規制上禁止？
	- 禁止ではなくて、ハイリスクAIに相当するから、守るべきことを守らないといけないということ
	- https://twitter.com/umiyuki_ai/status/1687639267273748480?s=20
- 南極の氷が、今年は急激にとけているらしい　via 安宅さん
	- https://www.economist.com/graphic-detail/2023/08/02/the-rapid-loss-of-antarctic-sea-ice-brings-grim-scenarios-into-view
- REBELという関係抽出トランスフォーマーをつかって知識グラフを抽出して推論する例
	- https://twitter.com/jerryjliu0/status/1687607838539927553?s=20
	- llamaindexの人による紹介、なんか抽出する知識の密度を調整したいところ
- Google Colab で LangChain + Vicuna-v1.5 のエージェント機能を試す
	- https://note.com/npaka/n/nb3c02ce2d4c5
	- npakaさんより、serpAIとmathをツールとして、ReActが試せるらしい。ハイメモリが必要。。
-  Google Colab で Llama.cpp + Vicuna-v1.5 を試す
	- npakaさんより、Colabでこんなこともできるのか？
	- https://note.com/npaka/n/n280ffc0d5ff0
- llama-2-7bをつかって、colabでchatbodを作る例、
	- 動くんだ、、、というか動くぞ！
	- https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-7b-chat.ipynb
- 自分のコンテンツを学習したカスタムChatBotを作る方法
	- https://zenn.dev/karaage0703/articles/c8baa66c40f9b7
	- そうか、いつもやってるやつは、Retrieval-Augmented Generation（RAG）ってよばれているのか？
-  LLMがローカルで動くパラメータ数どこまで？Metaの「Llama 2」を試してみた
	- https://pc.watch.impress.co.jp/docs/column/nishikawa/1519390.html
	- 西川さんが組むとは、だいぶ民主化が進んだのか。
	- Colabでも結構簡単にうごくが、ローカルなGeForce RTX 4070 Ti(12GB)でも動かす事例が(西川 和久)
-  Llama 2ベースのLLM FastChat/Vicuna v1.5をローカルで動作
	- https://jweb.asia/26-it/ai/91-fastchat-vicuna-v1-5-on-llama-2.html

## 7/31

いやあ、暑くなって１週間さぼったら、それなりにまとめるのがつらい。メタのLLaMa2リリースが大きな話題、岡野原さんの解説が良いかも。さっそくggml化、webui対応、LanChain組み込みが行われる。LangChainの統合開発環境LangSmith、よくLangChainの紹介動画に出てきてやつが正式リリースか。メタはマイクロソフトと組んでOSS化するとのこと、マイクロソフト無敵だな。OpanAI x Azureの人は、マイクロソフトの「ChatGPT - Azure OpenAI 大全」は参考になるか。ChatGPTの性能が初期に比べて劣化しているとの報告も。「生成AIと著作権に関する論点整理」の図は素晴らしい。OpenAIのCEOであるSam Altman氏が共同創業したWorldCoinプロジェクトが7/24に仮想通貨WLDをローンチした、日本にも虹彩認証Orbが複数設置されるも認知度は今一歩か、AIで得られた利益を配る、BIプロジェクトの一旦とのこと。

- LLaMa2をリリース、商用利用が可能に
	- https://ai.meta.com/llama/
- LLaMa2を早速ggmlに変換された
	- https://huggingface.co/TheBloke
- メタ社LLaMa2を、Microsoftと組んでOSS化すると発表
	- https://twitter.com/alex_valaitis/status/1681348531834044426?s=20
- Llama2-70B-Chatモデルは、なんと有用性評価でGPT-3.5TurboのChatGPTを打倒！
	- https://twitter.com/umiyuki_ai/status/1681361453838929923?s=20
- LangChaiの統合開発環境LangSmith正式版発表
	- https://blog.langchain.dev/announcing-langsmith/
	- おっと、正式発表されたのか
- Llama2は学習データを2Tトークンに増やしコンテキスト長を4KにしGQAを採用。報告書では有用性と安全性の向上に向けたSFTとRLHFの詳細が充実している。
	- 岡野原さんの解説
	- https://twitter.com/hillbig/status/1681436336451125257?s=20
- BigChat Enterpriseを発表
	- https://blogs.microsoft.com/blog/2023/07/18/furthering-our-ai-ambitions-announcing-bing-chat-enterprise-and-microsoft-365-copilot-pricing/
	- ユーザーとビジネスデータは暗号化され、組織外に流れることはありません。またチャット履歴は保存されずMicrosoftから見れません
- LLaMA2、ネット上のデモだとあんま日本語強くない印象だけど、ローカルでggml 4bit版の13B chat動かした感じ想像以上にまともに会話できるな、という印象
	- https://twitter.com/RosaRugosaBeach/status/1681554704701194240?s=20
- 東大の大規模言語モデルサマースクール
	- https://deeplearning.jp/llm2023/
- ChatGPTの性能が、初期リリースに比べて最近低下しているとの論文が
	- https://arxiv.org/pdf/2307.09009.pdf
- GitHubのcopilotがVSCodeから可能に
	- https://twitter.com/code/status/1682435342610079761?s=20
- TypeChat、マイクロソフトによる、プロンプトの代わりにType(型）をつかったChat、スキーマエンジニアリングともよぶらしい。
	- https://github.com/microsoft/TypeChat
- LLaMa2は、洞察とメタ認知に優れている
	- https://arxiv.org/pdf/2307.10928.pdf
- LangChainのLLaMa2インターフェイス
	- https://python.langchain.com/docs/integrations/chat/llama_api
- llama2 13B chat 4bit
	- https://twitter.com/manjiroukeigo/status/1683047350141599744?s=20
- 京大、仏典をGPT-４で学習した、ブッダポッドプラスを発表
	- https://ledge.ai/articles/buddha_bot_plus_kyoto_university
	- GPT-4で仏典を解釈 わかりやすく回答
- TheBloke/Llama-2-70B-Chat-GGML
	- https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML
- 生成AIによるコード生成とCode Interpreter活用ハンズオン with PLATEAU
	- https://connpass.com/event/290745/
- Abstraction and Analogy: The Keys to Robust Artificial Intelligence
	- https://www.eventbrite.co.uk/e/abstraction-and-analogy-the-keys-to-robust-artificial-intelligence-tickets-675075728677?aff=oddtdtcreator
- MicrosoftによるOpenAI　Azure大全
	- https://speakerdeck.com/hirosatogamo/chatgpt-azure-openai-da-quan
	- GPTの全体像、MicrosoftとOpenAIの関係、プロンプトエンジニアリングなど全て学べます
- llama2-webui
	- https://github.com/liltom-eth/llama2-webui
	- Run Llama 2 locally with gradio UI on GPU or CPU from anywhere
- DeepMindから強化学習で核融合炉（トカマク）を制御する話
	- https://arxiv.org/abs/2307.11546
- Google Colab で Llama 2 + LangChain の RetrievalQA を試す
	- https://note.com/npaka/n/n6d33c2181050
- 医療のあらゆるタスクで最優秀スコアを獲得する医療特化の大規模言語モデル「Med-PaLM M」
	- https://arxiv.org/pdf/2307.14334.pdf
- 「生成AIと著作権に関する論点整理」
	- なんと詳細な図が、、
	- https://www.bunka.go.jp/seisaku/bunkashingikai/chosakuken/hoseido/r05_01/?fbclid=IwAR06f_2GFjUTlVn6Ofot52SfMhcJuyjTtkzF-D7DczgB75d0d5iCC9ucGnQ
- World Coinの発表（Sam Altmanが関係している）、日本でも認証Orbが設置
	- 代官山のサイトに行ってみたが、人はぼちぼち、日本では今一歩の認知度か。暑かった
	- https://twitter.com/umiyuki_ai/status/1685323501069299713?s=20
	- 200万人がオーブ認証済みとか言ってたのに、予約者さえまだ32万人

## 7/18

暑くてすでに夏バテです。あいも変わらずcode interpreterの事例が続々、来年度の講義資料もこれで作るか。LLM時代のリテラシーって何という問い、教育もそうだし、リカレントもそう。Promptflowみたいな、（一見）思い付きのスタートアップがタケノコのように出てくるだろう。AlphaFoldがFoldItというゲームから名前がきているとは知らなかった、集合知ね。GoogleのNotebookLLM、エンジニアノートバッドという従来からの夢が、一歩実現に近づくか。普通に使っているEmbeddingなんかも、もちゃんと振り返って、カスタマイズの余地がある。

- ChatGPTのcode interpreterをつかて、講義の一部を作成（東大、強化学習、今井先生）
	- https://twitter.com/ImAI_Eruel/status/1678378444441387010?s=20
- What Should Data Science Education Do with Large Language Models?
	- https://arxiv.org/abs/2307.02792v2
	- LLMにより教育の変革、LLM-informed creativity, critical thinking, AI-guided programming.
- AlpacaEvalなるLLMベンチマークがあった、OSS系では、Vicuna-33Bがトップ
	- https://tatsu-lab.github.io/alpaca_eval/
-  AI tools are designing entirely new proteins that could transform medicine
	- https://www.nature.com/articles/d41586-023-02227-y
	- RFdiffusionという拡散モデルによるタンパク質の合成が、AlphaFoldなどのハルシーネーションベース？の手法より優れているとの論文
- GPT-4でワークフロー自動化「Promptflow」開発、Carnot（カルノー）が8,500万円をプレシード調達
	- https://thebridge.jp/2023/07/carnot-pre-seed-round-funding
	- 雨後のタケノコのようにスタートアップが立ち上がるか？？
- LLamaindexにおける、RAGの説明 by npakaさん
	- https://note.com/npaka/n/n27a36f784fb3
	- LLMとカスタムデータを組み合わせるための「RAG」(Retrieval Augmented Generation) パラダイム
- ストラング先生の線形代数講義のグラフィカルなノート、行列演算を極める。
	- https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra
- DeepMindのHasabisさんのインタビュー
	- AlphaGoのあとにAlphaFoldに着手したのは、FoldIt（集合知で折り畳み問題を解くゲーム）に着想を得たとのこと
	- https://podcasts.apple.com/us/podcast/a-i-could-solve-some-of-humanitys-hardest-problems/id1548604447?i=1000620748039
- OpenAIから、embeddingのカスタマイズする、ノートブック。デフォルトでは使えない？
	- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb
- Google Labs、言語モデル「NotebookLM」の提供開始を発表--まず米国から
	- https://japan.zdnet.com/article/35206577/
	- NotebookLMではユーザーのノートや情報源を『土台にして』言語モデルが稼働
- Bardにマルチモーダル機能が。。	
	- https://twitter.com/i/status/1680237703676190722
- 

## 7/10

OpenAIからGPT plusユーザー向けに、code interpreterが開放された。これで、データサイエンティストの仕事がなくなる？Plusじゃない人も、まずは手始めにLangChainのビデオをみて、データとのチャットを体感してみるといいかも。楔文字の翻訳など、様々な学問領域にLLMが侵食してゆく。OpenAIはアラインメント問題をAIで解くみたいなそっちの方向（結果としてAIの基盤整備が進む）。LLMのベンチマークとして、性格診断(Big5)というのは面白いアプローチ。使う人の性格判断と合わせるとマッチングが取れたりして。

- llamaindexにてtext-to-SQLの大幅なアップデート
	- https://twitter.com/llama_index/status/1676002583381692421?s=20
- 大規模言語モデルの"性格"特性を分析＆調整するフレームワーク、DeepMind、ケンブリッジ大学、慶応大学
	- Personality Traits in Large Language Models
	- https://arxiv.org/abs/2307.00184
- タスクの複雑さが増すとLLMの性能が急速に低下する現象を丁寧に検証
	- Faith and Fate: Limits of Transformers on Compositionality
	- https://arxiv.org/abs/2305.18654
- LangChainにおけるSpacy Embeddingの利用例
	- OpenAIやHuggingFace以外、
	- https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/spacy_embedding
- EUのAI規制の、最終案の一つ前の和訳が、総務省の「AIネットワーク社会推進会議」で公開
	- https://www.aplawjapan.com/publications/20220725
- IPAに「デジタル基盤センター」新設、デジタル庁と協力して基盤整備
	- https://xtech.nikkei.com/atcl/nxt/news/18/15517/
	- 古巣の社会基盤センターが改組されて、「デジタル基盤センター」になり、デジタル庁の影響を受けるようになった。。。悲しい。
- CAMEL-5B と SentenceTransformers で LlamaIndex を試す
	- https://note.com/npaka/n/n2e408cded4ac
- DeepLearningAIから、新コース、LangChain: Chat with Your Dataを無償リリース
	- https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/
- OpenAI、計算リソース利用の２０％をアライメント問題に割くことを表明
	- アラインメント問題自体をAIで自動化するようにも見える（つまり目には目を、AIにはAIを）
	- https://openai.com/blog/introducing-superalignment
- NP困難といわれる、3次元パッキング問題を、MITが解く？
	- https://news.mit.edu/2023/chore-packing-just-got-faster-and-easier-0706
	- FFTを利用しているとのこと
- 楔形文字の解読にトランスフォーマを駆使して成功
	- https://academic.oup.com/pnasnexus/article/2/5/pgad096/7147349?login=false
- OpenAI Code Interpreterを、GPT plusユーザーに解放。
- 

## 7/4

暑くてバテてました。LLMって、人間の知能を模擬するならば、Agentが実装できるというが、実装が近づいてきた。計画問題も直接解かせるよりも、計画問題を生成させるという組み合わせも面白い。形式言語なんか振り返ってみるのも面白いかも。LLMをComputer Visonへの応用、言語と画像の区別はなくなるのか？GoogleのKaggleチャレンジって、LLMの品質保証では重要な要素。ノン・セミパラメトリック統計ってのがあるのか？ 岡野原さんの『大規模言語モデルは新たな知能か』はおすすめ。記号接地って、LLMで実現できてんじゃない？みたいなのがじわじわと語られつつある(MLSE2023合宿より）。DeepMindのGemini、本当に出るのか？

- OpenAIのilian WengによるLLMをつかった、Agentの良解説記事
	- https://lilianweng.github.io/posts/2023-06-23-agent/
- LLMを使ってプランニング問題を解く、PDDLと呼ばれるプランニング言語に変換させた上でソルバーに解かせる。LLM単独より正確。
	- https://arxiv.org/abs/2304.11477
- Relicの社内勉強会での生成AI解説７０P資料
	- https://qiita.com/hedgehog051/items/b1308e8baf7b0f551548
- 形式言語とは何か（現代思想）
	- http://www.seidosha.co.jp/book/index.php?id=3821&status=published
	- 「正しい文とは何だろうか。。。」から始まる
- Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language
	- https://huggingface.co/papers/2306.16410
- 学習済みモデルから特定のデータの影響を消すKaggleチャレンジ by Google
	- https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html
- 研究者の資質と教員の仕事 by 谷中教授
	- https://twitter.com/verypluming/status/1674445457463062534?s=20
- コンテキストを気にした文書分割
	- https://twitter.com/RLanceMartin/status/1674817117475188737?s=20
- ノン・セミパラメトリック統計
	- https://www.kyoritsu-pub.co.jp/book/b10031225.html
	- 分布関数、密度関数や回帰関数について、一定の滑らかさのみを仮定して、ノンパラメトリックな推定と検定を行う方法を紹介する
- DeepMindの次世代AI「Gemini」はChatGPTを凌駕する？
	- https://wired.jp/article/google-deepmind-demis-hassabis-chatgpt/?utm_medium=social&utm_source=twitter
	- 「GeminiはAlphaGoのようなシステムの強みと大規模言語モデルの卓越した言語能力を組み合わせたもの」
- 岡野原『大規模言語モデルは新たな知能か』の個人的着目ポイントは、transformerで交互に積層する自己注意機構とパーセプトロンについて、前者が「短期記憶」、後者が長期記憶として機能するとの解説（p.108）
- 言語モデルに物理化学特徴量を取り入れた物性予測 by IBM
	- 分子の物理化学的特徴量を選定し、言語モデルを微調整する　
	- https://arxiv.org/abs/2306.14919v1
- VARモデル ＋ グレンジャー因果性の統計的仮説検定による、時系列データの因果探索
	- https://twitter.com/kenken26679105/status/1675281986917900288?s=20
	- 非ガウスモデル・VAR-LiNGAMであれば、同時刻も分析可能
	- https://twitter.com/kenken26679105/status/1675306307849699328?s=20
- Chains vs Agents" webinar by LangChain
	- https://www.youtube.com/watch?v=bYLHklxEd_k
- 結局記号接地ってなんだったけ？ by 丸山＠MLSE
	- https://twitter.com/maruyama/status/1675813852947308544?s=20
	- 実世界への参照なしで、言語空間内での埋め込みだけで意味を操作

## 6/26

あいもかわらずOpenAIのFunction APIの利用について、具体例が増える、Pydatanicと組み合わせればほぼ無滝の情報抽出ができそうだし、抽出した情報をつかったQ&Aなど、ちょっと説明性もあがるか？ヘルスケア分野でのGoogleAIの発表は衝撃的、眼科検診で様々な病気が見つかる。。。OpenLLAMaがでてきて、あっというまにFlanのデータでファインチューニングしたものが、商用利用できるのか？生成AIの研究や仕事への影響についてまとまった資料がぼちぼちでてきた。世界モデルに基づくプランニングなんかもLLMならではの研究か。LLMのコンパクト化も引き続き、マイクロソフトの取り組みがある。MITの試験問題をGPT4に解かせる話が不正という記事が、ちょっと悲しいが、オープンサイエンスの成果か。。


- OpenLLaMAは、LLaMaのオープン版（商用利用が可能？）GPU RAMは26.5GBで動作の模様
	- https://huggingface.co/openlm-research/open_llama_13b
	- https://github.com/openlm-research/open_llama
- Google	 ピーチャイ氏の講演、ヘルスケア分野で、AIがCTとかMRIとかを代替するかも（眼底検査で代替できる？）
	- https://twitter.com/alvinfoo/status/1670599368930656257?s=20
- OpenAIのFunction callとpydantic 	を組み合わせた例や再帰構造への対応など、情報抽出がこんなに便利に
	- https://twitter.com/jxnlco/status/1670764386447953921?s=20
	- https://twitter.com/matchaman11/status/1670799349004083200?s=20
	- https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html
- エンコーダーとデコーダについてわかりやすい解説
	- https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder
- LangChainで、	**MarkdownHeaderTextSplitter**を使えば、引用元つきのQ&Aが簡単に
	- https://note.com/hamachi_jp/n/nf23b75d14068
- マッキンゼーによる、生成AIの生産性への影響レポート
	- 生成AIで従業員の時間を6~7割節約可能 
	- 生成AIのビジネスインパクトが高いのは2枚目画像の右上の領域
	- 産業×用途別のインパクト評価(3枚目) 
	- 特定領域の具体的な用途と経済価値(4枚目)
	- https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights
- GPT４ALLベースのcopilotが登場？
	- https://morph.so/
- Debate　Tree、議論の構造をビジュアライズする
	- https://debatetreeofthoughts.streamlit.app/
- 基盤モデル・生成AIの科学研究への影響に関する資料、文科省 基礎研究振興部会(第11回)
	- https://www.mext.go.jp/b_menu/shingi/gijyutu/gijyutu27/siryo/mext_00007.html
- LlamaIndex	で、function call+pydatnicを組み合わせて、	query planningが可能に、
	- https://gpt-index.readthedocs.io/en/latest/examples/agent/openai_agent_query_plan.html
- 水口画伯なくなる、合掌
	- https://twitter.com/AKZ161/status/1671498721287352320?s=20
- PyRCA、Pythonをつかった、ルート原因分析
	- https://github.com/salesforce/PyRCA
- OpenAIのFunction APIの解説
	- https://every.to/chain-of-thought/gpt-4-can-use-tools-now-that-s-a-big-deal
- Flan-Open-Llama-7b、OpenLLaMaを、Flanのデータセットでチューニングした？
	- https://huggingface.co/conceptofmind/Flan-Open-Llama-7b
- 第2回LLM勉強会
	- https://llm-jp.nii.ac.jp/llm/2023/06/20/study-group-2.html
- local llmでsentence embeddingどれ使えば良いんだっけ
	- https://note.com/if001/n/n25d795afe571
- OpenAIのEmbbedingをつかって文章の類似度を計算
	- https://techblog.gmo-ap.jp/2023/06/22/embeddings_api_calc_sentence_similarity/
- CVPR2023より、疑似確率が確率になるという問題への回答
	- https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Class_Adaptive_Network_Calibration_CVPR_2023_paper.pdf
- マイクロソフトから小規模LLMに関する論文 Textbooks Are All You Need
	- 13億パラメータ"しか”ないモデル(phi-1)を、The StackとStackOverflowのデータを教科書品質にした60億トークンとGPT-3.5で生成した10億トークンをNVIDIA A100 8台・4日間で学習
	- https://arxiv.org/abs/2306.11644
- Flan-Open-Llama-3b
	- https://huggingface.co/conceptofmind/Flan-Open-Llama-3b
- Reasoning with Language Model is Planning with World Model
	- LLMを使ってプランニングを必要とするタスクを解く際、現在の状態をLLMを使って把握するようにして(「世界モデル」)、取るべき行動に対する報酬をLLMを使って見積もった上でモンテカルロ木探索によって行動を決定する手法(RAP; Reasoning via Planning)
	- https://arxiv.org/abs/2305.14992
- OpenAIのCookbookにllama_indexをつかった、文書分析の例が載る
	- https://github.com/openai/openai-cookbook/blob/main/examples/third_party_examples/financial_document_analysis_with_llamaindex.ipynb
- GPT-4がMITの試験問題を正しく解いたという論文が手続き的にも本質的内容面でも不正との指摘
	- 正解がでるまで何度も聞いた等の不正があった模様。。
	- http://people.csail.mit.edu/asolar/CoursesPaperStatement.pdf

## 6/19

今週は、6/12日にCEOの慶応大学での講演。OpenAIのAPIのアップデートの話題がありました。トークン数の拡大（青空文庫の短編クラスなら取り扱える）とか、Function callの追加。これで、LangChainのReActエージェントを使わなくても、OpenAI Agentで外部ツールとLLMが連携したソリューションが手軽に作れるようになるのは恐ろしいこと。品質評価の視点では、OpenAI Evalとかの話題も。欧州ＡＩ規制が、欧州議会の投票で採択され、次の段階（トリローグ）を経て年内に成立か。さっそくスタンフォード大学のHAIチームが、既存LLMの規制への対応状況をベンチマーク。データサイエンス系の型は、関数データ解析などは目からうろこではないか。ベイズ派と頻度派の争いには巻き込まれたくないもの。知識グラフとLLMの融合も、整理された論文が出てきた。

- 「OpenAI CEO Sam Altman氏と塾生との対話」開催(6/12)
	- https://www.keio.ac.jp/ja/news/2023/6/15/27-139184/
	- 会場となった西校舎ホールには約700名の学生が集まり、約40分にわたり活発な質疑応答が行われました。またその様子は519教室にも配信され、1,000名以上の学生にとって貴重な機会となりました。
- 「二つの分散:不偏推定量と最尤推定量のどち らを使うべきか」井手さん(IBM)
	- 頻度派に対する執拗な攻撃は続く。。
	- https://ide-research.net/book/Which_variance_should_I_use.pdf
	-  データサイエンスの本質をひと言で答えろと言われたら、「観 測データに対してもっとも当てはまりの良いモデルをつくるために、最尤推 定を使ってパラメターを決めること」と答えればよい」
-  Evaluating the Social Impact of Generative AI Systems in Systems and Society
	- https://huggingface.co/papers/2306.05949
- 「見たくないものをみる」（PFNの丸山さん）
	- https://note.com/hiroshi_maruyama/n/n7890a1fb7aef
	- 新たな倫理規範の確立について
	- 「人間中心のAI」に違和感を抱き、「人間が（知能の面で）万物の霊長でないかもしれない」という「都合の悪い真実」を直視すべきという話
- 「デジタル庁のサイトやばすぎる」
	- https://qiita.com/mu_tomoya/items/f78f1fad3a8b57ac7dc3
	- やばいくらい参考になるらしい。
-  米OpenAIのCEO「AIはさらに賢く」　慶大で意見交換（日経）
	- OpenAIの強みはresearch culture  
	- コーディング or 理論解析に強い人が成功している
	- AI技術は急速に進歩・応用されており、このような時代にAIに関われる今の学生はlucky generation
	- https://www.nikkei.com/article/DGXZQOUC1037N0Q3A610C2000000/
- 平均・分散・相関が変わらない、X,Yの様々な事例。。
	- まあ有名な奴だけどゴジラはよく考えたな。
	- https://twitter.com/docmilanfar/status/1668093023895568386?s=20
- ヒントン先生にたいするルカンの所感
	- 人間並みのAIを実現するには、２つが必須で、（今はたらない）
		- (1) learning world models from sensory inputs like video, 
		- (2) an architecture that can reason and plan (not just auto-regress).
- Dockerコンテナをwebassemblyに変換して実行できるツール？
	- https://www.publickey1.jp/blog/23/dockerwebassemblywebcontainer2wasm03.html
- 欧州のデータスペースに関する、JRCのレポート
	- European Data Spaces - Scientific Insights into Data Sharing and Utilisation at Scale
	- https://publications.jrc.ec.europa.eu/repository/handle/JRC129900
- GPTにFunction Callが追加
	- 出力の整形とか、あるいは、自然言語から、関数のAPI呼び出しを作ったりとか、つまり、LangChainでいうところのAgentが簡単にできるようになる。。
	- https://openai.com/blog/function-calling-and-other-api-updates
- CV（コンピュータビジョン）の最新刊は、生成AI、巻頭言がよかったとのこと
	- https://www.amazon.co.jp/dp/B0C6JW6T6B?ref_=cm_sw_r_cp_ud_dp_Q44X6Q8W7NPXKP46168A
	- イマドキノ拡散モデル：拡散モデルに関する最近の研究動向を紹介。基本技術、条件付き生成への拡張、生成の高速化について述べ、拡散モデルを学ぶうえで役立つリソースを紹介。
- OpenAIのモデルを評価するフレームワークEval
	- https://github.com/openai/evals
	- **特定の課題に対してどれぐらい高精度で生成できているかを評価**できます。
- DADCのスマートビルガイドラインの補足資料が公開、
	- 補足資料って、最初から説明が足らなかっただろうに。
	- https://www.ipa.go.jp/digital/architecture/Individual-link/ps6vr7000001x8o0-att/smartbuilding_guideline_appendix.pdf
- 最近引退されたMITのストラング先生の、"Ther Art of Linear Algebra"の和訳、たった１４P、全理系は涙して読むべし
	- 「行列5分解」「行列の世界」「固有値地図」の視覚的解説
	- https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra/blob/main/The-Art-of-Linear-Algebra-j.pdf
- GTP-calls:コールセンターの会話を分析するアプリ、マイクロソフト
	- https://arxiv.org/abs/2306.07941
- シンボリック回帰と深層学習を組み合わせることで、データから方程式を見つける。
	- https://arxiv.org/abs/2207.00529
- GPT3.5 APIのアプデで使えるようになった16kトークンで何ができるか？
	- 青空文庫のちょっとした短編ならば、分析が可能になったレベルらしい
	- https://note.com/mahlab/n/n99577fabf16e
- GPTでのfunction callの良例
	- https://gist.github.com/hotchpotch/364cb8ae188e40f4e9ff1273232bc918
- OpenAI API の 関数呼び出し を試す、npakaさんの記事
	- **外部APIを呼び出して質問に答えるチャットボットの作成**
	- **自然言語をAPI呼び出しに変換**
	- **テキストから構造化データを抽出**
	- https://note.com/npaka/n/n917463f55b8a
- 欧州AI規制における、生成モデル、一般目的AIに対する義務事項
	- https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai
	- ban on AI for biometric surveillance, emotion recognition, predictive policing 
	- registration of models with EU 
	- detailed summary of training data 
	- requirement to identify deepfakes
-  第6回LangChainもくもく会開催レポート
	- https://note.com/mahlab/n/nc6ec4a9bd3c5
	- Grounded GenerationサービスVectara、LLMホスティングサービスBeam、SQLiteでベクトルDB検索が可能になるsqlite-vss、PostgreSQLのベクトル検索拡張pgvector、LangChain AI Handbookの話
- OpenAIのFunction Callをつかうと、LangChainのReAct Agentのようなこともできるという話（すげー、というか、エコシステム壊してないか？？？）
	- https://github.com/jerryjliu/llama_index/blob/main/docs/examples/agent/openai_agent.ipynb
	- llma_indexには、openai agentが組み込まれる予定らしい。
	- https://twitter.com/llama_index/status/1668995628146257921?s=20
- 「関数データ解析の概要とその方法」滋賀大学、松井先生
	- https://speakerdeck.com/hidetoshimatsui/guan-shu-detajie-xi-nogai-yao-tosonofang-fa
	- デーサイエンスで習う、回帰、クラスタリング、などのすべてが、データを関数として取り扱う枠組みで、再構成されている。なんともすがすがしいスライド。夏休みのお供に！
- 機械学習サービスにおけるONNXの活用と応用　〜ONNXテキスト形式の拡張〜
	- https://www.sportip.jp/blogs/onnx
	- やっぱり、ONNXにして、WebGPUつかって、ブラウザで動か用になるのね、
- Rinna-3.6B で 文脈付きの質問応答 を試す npakaさん記事より
	- https://note.com/npaka/n/n3bb60c61ef94
	- 「JSQuAD」は文脈付きの質問応答タスクで、53.42と半分以上正解
- 欧州AI規制に、現状のLLMはどれぐらい対応できているかのベンチマーク(スタンフォード題）
	- https://crfm.stanford.edu/2023/06/15/eu-ai-act.html
	- 現状特に著作権保護学習データ開示等が行われていないこと、DSA的透明性確保の非対称規制提言など
	- すごすぎでしょう。
- 知識グラフのLLMの統合についてのロードマップ論文
	- Unifying Large Language Models and Knowledge Graphs: A Roadmap
	- https://arxiv.org/abs/2306.08302
	- Combining the advantages of LLMs and knowledge graphs (KGs) is a promising direction.
- 欧州でEV電池規制　リチウムは8割再資源化、31年までに
	- 6/14日に欧州議会の投票を通過したという話、
	- EV,主要材料のリチウムは使用済み電池から2027年までに50%、31年までに80%を再資源化する必要がある。
	- 「電池パスポート」の導入も決まった
	- https://www.nikkei.com/article/DGXZQOGR1706S0X10C23A6000000/
- レヴィ＝ストロースの70年来の謎を進化シミュレーションで解明- 文化人類学の基礎「親族の構造」を数理モデルで生成 -
	- https://www.u-tokyo.ac.jp/focus/ja/press/z0109_00325.html
	- これって、LLMで同じことが多分1年以内のできるようになる。
- 　応用行動分析「死人テスト」死人にもできることを行動目標にしたいという話
	- https://twitter.com/81I6VVboj7h2Bqy/status/1667893285883621376?s=20
	- 「会議で余計な発言しない」、「廊下で走らない」、などは死人にもできる目標なので、それはまちがえであるということ。。その行動、死人にもできるのでは？

## 6/12

熊本で開かれた人工知能学会全国大会の話題もちらほら。偉い先生のまとめスライドが役に立つ。ローカルでLLMを動かす動きも相も変わらず活発。ggML形式のLLMならば、gpt4allのチャット用のソフトでllmを入れ替えて動くらしい。npaka氏のローカルＬＬＭのまとめは良記事。それにしても、4.75bitのSpQRって本当か？タンパク質やプロプロテインなどの研究対象の操作などができるチャットシステムも登場、そういう応用はこれからもたくさんでそう。データサイエンス界隈は、自らの存在意義的に、Noteableプラグインがよほど応えたらしい。ついにMSからChatでOffice製品を制御できる技術が発表、パワポも作ってくれるのか？その間googleのBardは、裏でコード実行する仕組みを取り入れ、苦手な計算とか論理などの精度が向上。今度はＧＡＳとの連携か。東芝福本氏の製造業における生成ＡＩの活用は一読の価値あり。倫理とか公平性という、上から目線より、「卵のためのＡＩ」に、わたしはなりたい。

- データ分析の効率が10倍上がるデータサイエンティストのためのChatGPTの活用術
	- https://qiita.com/ot12/items/96b5783568196d3320fe
	- さいごはNoteableなのか。。
- ChatGPTのように狙いの分子やタンパク質を編集できるChatDrug
	- https://arxiv.org/abs/2305.18090v1
- 「rinna」の日本語言語モデルを試用、メモリ32GBあればCPUだけでも動くぞ！
	- https://internet.watch.impress.co.jp/docs/column/shimizu/1503707.html
- GPT4ALL周りのソフトは、ggML準拠のモデルならば、gpt4allでなくても動くようになった！
	- The GPT4All Chat UI supports models from all newer versions of `ggML`, `llama.cpp` including the `LLaMA`, `MPT` and `GPT-J` architectures. T
	- https://docs.gpt4all.io/gpt4all_chat.html
- どうやらパラメータ数130億(13B)でChatGPT(GPT-3.5)クラスの性能が出せることがMSから発表
	- https://huggingface.co/papers/2306.02707
- こんどはProteinChat、構造があれば何でもよいのか。。
	- ProteinChat: Towards Achieving ChatGPT-Like Functionalities on Protein 3D Structures
	- https://www.techrxiv.org/articles/preprint/ProteinChat_Towards_Achieving_ChatGPT-Like_Functionalities_on_Protein_3D_Structures/23120606/1
- 確率的熱力学に経済学のツールを用いることで、熱力学と情報理論の間の相互作用について定量的に調べた
	- https://arxiv.org/abs/2306.00449
- 「大規模言語モデル入門」７月２９日発売予定
	- https://www.amazon.co.jp/dp/4297136333
- Microsoftの研究者らが新たに開発したAIシステム「Semantic Interpreter」は、Officeを操作、パワポが作れる。。
	- https://arxiv.org/abs/2306.03460
- DeepMindのAlphaDev、人の作りしソートアルゴリズムよりも高速なアルゴリズムを生成。
	- https://www.nature.com/articles/s41586-023-06004-9
	- といっても最適化しているだけだとか、ChatGPTでも同様な最適化ができたとの報告が続く。
	- https://chat.openai.com/share/95693df4-36cd-4241-9cae-2173e8fb760c
- 医療現場での、構造化されてない医療メモをつかったLLM
	- https://www.nature.com/articles/s41586-023-06160-y
- LlamaIndexの、JSON Query Engineの紹介ビデオ
	- https://www.youtube.com/watch?v=4tDyfAaIqEw
- 前篇　AIは「ジェスチャーゲーム」を知らない
	- 今井むつみ先生と、高野秀行の対談
	- 『言語の本質　ことばはどう生まれ、進化したか』の今井先生の対談
	- https://kangaeruhito.jp/interview/756531
- Googleの「Bard」が「暗黙的なコード実行」を導入、文字列の操作や論理・推論を含む複雑なタスクに対する回答精度が向上
	- やっぱBardやるね。
	- https://gigazine.net/news/20230608-google-bard-implicit-code-execution/
- Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners
	- https://arxiv.org/abs/2305.14825
- 東芝福本氏による、製造業で生成AIはどんな役割を果たすのか？ ドイツで見たMSやシーメンスらの取り組み
	- ハノーバーメッセでの展示の現地報告は貴重だ
	- https://www.sbbit.jp/article/st/115632
- 壁のためのAIと卵のためのAI
	- JSAI2023での学生企画、卵のためのAIになりたい。
	- https://speakerdeck.com/yukinobaba/ai-for-wall-and-ai-for-egg
- 松尾研による、「基盤モデルの技術と展望」
	- GPT-3など基盤モデルの技術的な動向について、数多くの文献をもとに整理されている
	- https://speakerdeck.com/yusuke0519/jsai2023-tutorial-ji-pan-moderunoji-shu-tozhan-wang
- 状態空間モデルを活用した時系列データのCausalImpact分析
	- https://speakerdeck.com/stakaya/zhuang-tai-kong-jian-moderuwohuo-yong-sita-shi-xi-lie-detafalsecausalimpactfen-xi
	- まあ、Rでなくても再現できそうだ。
- llamaindexのKnowledge Graphインデクス機能が大幅にパワーアップ？
- https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.ipynb
- 欠損値を平均値を代入するというプラクティスが議論に、
	- 問題ないという人もいるが、問題ないならそもそも除外しても同じなのでは？
	- https://twitter.com/kenken26679105/status/1667288891949453312?s=20
- JSAI2023、人の位置情報の時系列をトークン列に置き換えてGPT-2で学習、人の移動軌道を生成する研究
	- https://confit.atlas.jp/guide/event/jsai2023/subject/2H5-OS-8a-02/tables?cryptoId=
- ローカルでlangchain経由で簡単につかえてそこそこ日本語も喋れるのwizard-vicuna-13 q8_0
	- https://twitter.com/if_004/status/1667474091564204033?s=20
- ローカルLLMのまとめ
	- https://note.com/npaka/n/nd95fba328b65
- 表形式データの行を1文と見て，差分プライベートに言語モデルを学習させ，そこから合成データを生成する手法を提案．複数のデータセットで既存のグラフィカルモデルベースのものと同等の性能
	- https://arxiv.org/abs/2306.04803
- 4.75bit 相当の量子化で、16fp と比べ損失ゼロの推論が可能
	- SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression
	- https://arxiv.org/abs/2306.03078
## 6/5
相も変わらず、4bit化とか、Rinna-3.6BのLoRaとか、ローカルでLLMを動かす、作る可能性が広がっている。まあ現状のLLMって実は疎なのではという特異値分解の結果も。じゃらんでChatGPT活用サービス試行開始。DeepLearningAIより、LangChainのショートコース無料開始、作者登場で豪華なことに。DeepAI OpenAIのLLMの苦手な計算問題での、「プロセス監視報酬モデル(PRM)」による改良。数値シミュレーションの世界でもLLM活用が。OpenAIのsecurity Portal発表。Grokking「過学習してしばらく経ってから、急に汎化誤差が下がり始める（正解率が上がり始める）」という現象への手がかりも。。言語学会からもLLMに対する元気のよい発信や出版が多数。ドイツ連邦データ保護当局（BfDI）の生成AIについての声明、これは読むべきか。日本のAI戦略会議の議論との温度差はいかんともしがたい。新聞記事の本論の前段の記事間違えを鬼の首を取ったように叩く、狭量な日本のDSメンタリティも興味深い。この政府にしてこの国民アリという感じか、逆か。

- DeepLearningAIより、LangChainのショートコースが、無料
	- 作者自身の登場で、豪華な構成に、
	-  LangChain for LLM Application Development
	- https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/
- biomedGPT: マルチモーダルな医療用のGPT
	- https://arxiv.org/abs/2305.17100
- AI戦略会議による、暫定的論点整理、日本はノーローなのか？これでいいの？
	- https://www8.cao.go.jp/cstp/ai/index.html
- LLM自身がPythonによるツール作成？Large Language Models as Tool Makers
	- https://arxiv.org/abs/2305.17126
- Google Colabで、ローカルランタイムでの実行ができるようになった。。
	- https://research.google.com/colaboratory/local-runtimes.html
- Transcendental Style in Film(映画における超越的様式)
	- https://twitter.com/routemopsy/status/1663396967417024513?s=20
	- ホウ・シャオシェンがタルコフスキー領域にあるのは解せない。
- 「 Google Colab で Rinna-3.6B のLoRAファインチューニングを試す」
	-  なんと14Gでできるなら、無料枠？
	- https://note.com/npaka/n/nc387b639e50e
- Large Language Models are not Fair Evaluators
	- https://arxiv.org/pdf/2305.17926.pdf
-  How To Finetune GPT Like Large Language Models on a Custom Dataset
	- Macbookでも簡単にfinetuneできるようになった
	- https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/
- 「ChatGPTの仕組みと社会への影響」、京大黒橋先生のわかりやすいといわれる講義、１９分でさくっと
	- https://www.youtube.com/watch?v=aKqIPlDyWhs
- 「ChatGPTとNoteableによる科学技術情報分析」
	- https://speakerdeck.com/hayataka88/chatgpttonoteableniyoruke-xue-ji-shu-qing-bao-fen-xi
	- 噂のNoteable。ついに、お話しするだけで、EDAから回帰まで、、
- Let's Verify Step by Step by OpenAI
	- LLMが苦手な計算問題をとかせるために、process supervisionというのをどうにゅ
	- 「プロセス監視報酬モデル(PRM)」というらしい。
	- https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf
- NIIのオープンハウス(6/1-6/3)、ChatGPTネタ大杉。
	- https://www.nii.ac.jp/event/openhouse/2023/
- 因果推論のコースマテリアル
	- https://arxiv.org/abs/2305.18793
- Rinnaすごいかも。japanese-gpt-neox-3.6b-instruction-ppo
	- https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo
- 局所詳細釣り合い、ゆらぎの定理、Jarzynski等式と拡散モデルの関係
	- https://zenn.dev/xiangze/articles/6e8ce8b8d43d08
	- そういうものらしい
- rinna/japanese-gpt-neox-3.6b について、ベース、SFT、RLHFで動かした例 on colab
	- https://note.com/npaka/n/ne4a38239f420
	- ベースなら無料枠で動く？
- じゃらんでAIチャットサービス開始
	- https://note.com/npaka/n/ne4a38239f420
- Rinna-3.6B を llama.cpp で CPU 動作のメモ
	- https://zenn.dev/syoyo/articles/946c17666e10fb
	- CPUだけでも十分動くのか。。。
- Berry: A code for the differentiation of Bloch wavefunctions from DFT calculations
	- https://arxiv.org/abs/2006.02744
	- DFT計算で得た波動関数を微分するためのオープンソース
- What Is ChatGPT Doing … and Why Does It Work?
	- https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
	- Wolfman AlphaのWolfmanさんの記事、
	- 「現在のChatGPTの場合，事態はもっと極端で，各トークンの出力を生成するためのニューラルネットはループのない純粋な「フィードフォワード」ネットワークであるため，自明でない「制御フロー」を持ついかなる計算も行うことができない
- Googleによる図式を理解するLLM
	- Foundation models for reasoning on charts
	- https://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html
-  Physics-constrained machine learning for scientific computing
	- https://www.amazon.science/blog/physics-constrained-machine-learning-for-scientific-computing?_amp=true
	- 保存則と境界条件の制約を守りつつ偏微分方程式の解を求めるディープラーニングモデル。Amazon ScienceからICMLとICLRで発表
- LLMでドメイン特化言語を作りまくり？
	- https://huggingface.co/papers/2305.19234
	- Grammar Prompting for Domain-Specific Language Generation with Large Language Models
- OpenAI のCEOのインタビュー、GPUリソースが世界的に足らないのはGPT-4のマルチモーダル学習中のため？
	- https://humanloop.com/blog/openai-plans
- 「AIに脅かされる「個人」　情報を断ち切る規制必要」　政治季評
	- https://www.asahi.com/articles/ASR5065XLR5YUSPT006.html
	- 中身は議論されず、最初のChatGPTはオープンソースのところに、鬼首をとったようにかみつくDS界隈
- QCDの１方程式から多様な世界が作り出されるチャート
	- http://suganuma-hideo.o.oo7.jp/hideo/index.files/main.files/HQCD.pdf
- “According to . . . ” Prompting Language Models Improves Quoting from Pre-Training Data
	- Wikipediaによると、、、を付け加えるプロンプトテクニック？
	- LLMが事前学習データから直接引用するように誘導し、生成される情報の信頼性を向上
	- https://arxiv.org/pdf/2305.13252.pdf
- inna-3.6b-instruction-oppのggml 4q_2を作って、LangChainのsummarize chainで使ってみました…
	- https://twitter.com/8hmVmEGJ6nFyUE5/status/1663936372363898880?s=20
	- やっぱりggmlと4bitが最強なのか。。モデルサイズが2Gって、あーた
- LLMをつかって、微分方程式から保存則を抽出する？？
	- Discovering New Interpretable Conservation Laws as Sparse Invariants
	- https://arxiv.org/abs/2305.19525
- OpenAIがsecurity portalを公開
	- https://trust.openai.com/
- つい最近引退された、ストラング教授（線形代数他）のインタビュー記事
	- https://news.mit.edu/2023/gilbert-strang-made-linear-algebra-fun-0531
- GPT4ALLをつかって、GPUなしで、ローカルPCでLLMを動かす
	- https://gpt4all.io/index.html
	- A free-to-use, locally running, privacy-aware chatbot. **No GPU or internet required.**
- ChatGPTプラグイン「Notable」だけでデータ分析コンペに挑戦してみた話
	- https://qiita.com/ot12/items/ba74fa150e160d94a71f
	- やっぱりNoteableは最強の件、来年のデータサイエンス特論のネタにしよう！
- A Mechanistic Interpretability Analysis of Grokking
	- 学習が進むと突然、未見のデータに一般化するように学習する現象のメカニズムの解明だそうだ
	- https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking
- カモシカ-LoRaから、OpenCALM 7B, 3Bをファインチューニングして作成したアダプタを公開
	- https://twitter.com/kam0shika/status/1663906516276051969?s=20
- Transformer.js、ブラウザやnodejsからhuggingfaceのtransformerが使える
	- https://github.com/xenova/transformers.js
- 特異値分解で30%もLLMを圧縮しても性能が変わらなかった
	- LLMってやっぱり疎なのね
	- ~30% Compression Of LLM (Flan-T5-Base) With Low Rank Decomposition Of Attention Weight Matrices
	- https://smashinggradient.com/2023/05/23/30-compression-of-llms-with-low-rank-decomposition-of-attention-weight-matrices/
- LQML;
	- LMQL (Language Model Query Language) is a programming language for large language model (LM) interaction. 
	- https://docs.lmql.ai/en/stable/
- Andrew Ngさんによる米軍AIドローンシミュレーション（操作者を殺すという結論）への反駁
	- https://twitter.com/AndrewYNg/status/1664694504476102680?s=20
- スタンフォード大学による、機械学習もろもろチートシート
	- https://github.com/afshinea/stanford-cs-229-machine-learning
- LangChainをサポートするvicuna-13bモデルをrinnaが公開
	- https://huggingface.co/rinna/vicuna-13b-delta-finetuned-langchain-MRKL 
	- https://note.com/hamachi_jp/n/n97d368a617ac
- A Survey on Large Language Models for Recommendation
	- https://arxiv.org/abs/2305.19860v2
- 分子生物学にLLMが最適な件
	- https://towardsdatascience.com/large-language-models-in-molecular-biology-9eb6b65d8a30
- GPT4ALLとLangChainとChromaをつかった、ローカルに動く最小限のQ&A
	- https://twitter.com/AssemblyAI/status/1661747770108305409?s=20
- 「ChatGPTの出現は自然言語処理の専門家に何を問いかけているか」
	- 言語学会の乾先生の巻頭言
	- 「では，これで自然言語処理は終わるのか？ もちろん，終わらない．解くべき課題，新たに生まれる問いは山ほどある．」
	- https://www.anlp.jp/topics/topic230601.html
- 「言語の本質　ことばはどう生まれ、進化したか (中公新書)」
	- 今井むつみ, 秋田喜美の本、
	- https://www.amazon.co.jp/dp/B0C4XF523T?ref_=k4w_ss_dp_lp
-  Langchain・Semantic Kernel・guidanceでエージェント機能を実装して比較してみた
	- https://qiita.com/sakue_103/items/6ffee0bc267e71eafd60
- 時系列データにおける特徴量エンジニアリング by NRI
	- https://datascience.nri.com/entry/2022/10/12/155350
- ドイツ連邦データ保護当局（BfDI）の生成AIについての声明（5月22日）、 by 生貝先生
	- https://www.bfdi.bund.de/SharedDocs/Downloads/DE/DokumenteBfDI/Stellungnahmen/2023/StgN_Generative-K%C3%BCnstliche-Intelligenz.pdf?__blob=publicationFile&v=2
	- GDPR的なリスクベースアプローチとDSA的なシステミックリスクアプローチの対比など興味深い。青少年学習データのフィルタリングやAI規則の川上川下問題なども
- Jupyter AIが出た！試した！！すごい！！！
	- https://qiita.com/moritalous/items/a270d5932ebee18d0ba8?utm_content=buffer352b5&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
	- すごいらしい
- 

## 5/29
Microsoft BuildでWindowsとGPTとの統合とか、BingでもChatGPTのプラグインが使えるようになるとか、相も変わらずMicrosoftはどうやって投資を回収できるのか不明。Adamを超えるSophiaの登場や、4bit化のQLoRaの登場など、個人や企業でのLLM作成には朗報であるが、LLMの民主化って危険もあるよね。なので、DeepMindやMicrosoftは倫理性やリスクに関する研究をちゃんと続けて公開している。Voyagerすごい、研究開発のタスクももGPT-4でできるのでは？WebGPUを使ったwebllm、nodejs版でも動く模様。microsoftはguidanceでモデル利用の効率化の工夫を行っているとの報告も。アブダビから謎の巨大LLMであるFalcon-40Bが発表されるも、OSSとうたいつつ実は謎ライセンスであっというまに叩かれる。SQLとかNotableとかデータサイエンス系のChatGPTの活用が本格的に。祝！SIAMの賞の受賞、蔵本先生！！！。欧州AI規制の最終投票を目前に、OpenAI、欧州AI規制遵守が困難と判断されれば、欧州からサービス引き上げとの記事。ChatGPTアプリが日本でもiPhoneに登場、似たような名前のアプリがたくさんあって、、、。

- MicorsoftのAIが倫理的であるかどうかを評価するツールキット
	- https://github.com/microsoft/responsible-ai-toolbox
- 知識グラフのneo4jと、LangChainからの利用、Cypher問い合わせを自動生成する
	- https://python.langchain.com/en/latest/modules/chains/examples/graph_cypher_qa.html
- LIMA: Less Is More for Alignment
	- Lucan先生によると、LLaMA 65B + 1000 supervised samples = {GPT4, Bard} level performance
	- https://arxiv.org/abs/2305.11206
- scikit-llm: scikit-learnとLLMをシームレスつにつなげる
	- https://github.com/iryna-kondr/scikit-llm
- LangChainからAzure OpenAI を使うメモ
	- https://qiita.com/tmiyata25/items/7a04096342241d8a2b4c
- Textually Pretrained Speech Language Models：なんか音声をいれると音声を出力するLLM!!
	- https://pages.cs.huji.ac.il/adiyoss-lab/twist/
- ImageBind　by Meta、マルチモーダルな学習
	- https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/?utm_source=twitter&utm_medium=organic_social&utm_campaign=blog&utm_content=card
- open-calm-7b を databricks-dolly-15k-ja で LoRA したのをマージして ggml にして 4bit 量子化して redpajama.cpp で MacBook ローカルで動く日本語高速チャットボット
	- https://twitter.com/niw/status/1660894493867134976?s=20
-  LLaMAベースの日本語大規模言語モデル(LoRaした)公開
	- https://llm.msuzuki.me/
- Microsoft Build開催、OSとLLMが融合？
	- https://blogs.microsoft.com/blog/2023/05/23/microsoft-build-brings-ai-tools-to-the-forefront-for-developers/
- LLM向けの学習最適化エンジンSophia、アダムを超えるか
	-  Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training
	- https://arxiv.org/abs/2305.14342
- QLoRa: HuggingFaceのモデルが、4bit化されたものが使えるようになる？
	- https://huggingface.co/blog/4bit-transformers-bitsandbytes
- Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks
	- https://huggingface.co/papers/2305.14201
- 自然言語のpromptによるLLMの利用は、LLMの本来の能力を生かしきれてない
	- https://arxiv.org/abs/2305.13264v1
- QLoRaを使えば、普通のcolabにて、数時間でLLMができるという報告。4bit最強。
	- 33B-parameter LLM on Google Colab in a few hour
	- https://twitter.com/ItakGol/status/1661714548594823174?s=20
- OpenCALM-7BをLoRAでFine tuningして対話ができるようにする
	- https://note.com/masuidrive/n/n0e2a11fc5bfa
- Reasoning with Language Model is Planning with World Model
	- CoT on GPT-4との比較で勝るとのこと
	- https://arxiv.org/abs/2305.14992
- Voyager: 長期的な探索をGPT-4でやらせる例。Minecraftをやらせたら、、（研究開発も。。。）
	- https://github.com/MineDojo/Voyager
- 祝！蔵本先生、SIAMでJürgen Moser Lecture賞受賞！受賞講演
	- https://www.youtube.com/watch?v=2P-EgTSa-E4&feature=youtu.be
- DeepMindから、一般的なAIモデルが潜在的に持ちうる有害なリスクを評価するフレームワーク
	- https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks?utm_source=twitter&utm_medium=social&utm_campaign=ModelEval
- WebGPUをつかってLLMを動かす仕組み、WebLLMが、nodejsでも動く？？
	- https://github.com/mlc-ai/web-llm
- LangChainからDatabricksを使う
	- https://python.langchain.com/en/latest/modules/models/llms/integrations/databricks.html
- アブダビの研究所からFalcon-40Bが発表、オープンソースなのに、ライセンス料が必要みたいな罠が見つかる。
	- https://huggingface.co/tiiuae/falcon-40b
	- 「売上の10%をロイヤリティとして12ヶ月毎に支払わなければならない」
- microsoft/guidance(LangChainのようなもの）をつかって、Agentを定義して、動かす
	- https://note.com/explaza_inc/n/n7cb8043506bd
- OpenAIがgptサービスの５月の速度低下をレポートするものが
	- https://twitter.com/helicone_ai/status/1662325356563496961?s=20
- Googleが生成AIの無料講座を公開
	- https://www.cloudskillsboost.google/journeys/118
- SQLを活用したデータ分析におけるChatGPTの活用法
	- https://speakerdeck.com/hikarut/sqlwohuo-yong-sitadetafen-xi-niokeruchatgptnohuo-yong-fa
- ChatGPTのデータサイエンス向けのプラグインNotableが便利との記事
	- https://secon.dev/entry/2023/05/27/170000-noteable-iris/
- Lucan先生、GAFAMの代わりに、MAGMAを造語。 Meta, Amazon, Google, Microsoft, App
	- https://twitter.com/ylecun/status/1662375684612685825?s=20
- OpenAIのアルトマンCEO、「EU AI Act順守が困難ならEUでの事業は停止する
	- https://www.itmedia.co.jp/news/articles/2305/26/news106.html

## 5/22
ChatGPT以外のOSSのLLMでは、googleのFLAN-20B with UL2 ぐらいならば、なんとか同等の性能がでるという報告も(A100が必要)。privateGPTや、GPT4ALLなどの、ローカル環境で動かせるOSSのLLMもだいぶそろってきました。PyTorchをブラウザ環境'(TypeScriptで）動かす仕組みも登場。しかし本命は、WebGPUをつかって、ブラウザ以外からもLLMをローカルで高速に動かす試みには期待したいところ。いっぽうTinyStoriesなど、どれだけLLMを小さくできるかな？的なアプローチも続く。Tramnsformerも偏微方程式を解くなど、物理モデルの領域に広げる試みも。日本からは日本語版LLMが複数出現、実力のほどは？？LLMの説明性やバイアス対策なども。ChatGPTがついにIPhoneに乗る（USのみ）。MicrosoftはLLMを使いやすくするフレームワークGuidanceを発表、SemanticKernelの立場は？？Marvinのような、LLMとプログラミングの融合パラダイムには可能性がありそうです。

- LLMのバイアスをあぶりだす、Constructive Input Decoding(CID) by google
	- https://arxiv.org/abs/2305.07378
- privateGPT:ローカル環境で動く最小限のGPT、LangChain, GPT4All, LlamaCpp, Chroma and SentenceTransformersを活用
	- https://github.com/imartinez/privateGPT
- TinyStories:３～４才ぐらいが理解できる短い文書のデータセット、どれだけLLMを小さくできるかを評価するためのもの by Microsoft
	- https://arxiv.org/abs/2305.07759
- Google/OpenAIがオープンソースのLLMを開発している。
	- https://www.theinformation.com/articles/open-source-ai-is-gaining-on-google-and-chatgpt
- Marvin:プログラミングとLLMの補助を組み合わせた新しいパラダイム、LMQLみたいな感じ？スキーマに従ってデータ抽出など
	- https://note.com/hamachi_jp/n/na1960fc9d6d3
	- https://www.askmarvin.ai/
- Excelとチャットする、titnanicの例で、前処理のところをチャットで実現
	- https://github.com/Anil-matcha/Chat-With-Excel/blob/main/Data_analysis_with_langchain.ipynb
- Physics Informed Token Transformer(PITT)：偏微分方程式(PDE)をトークン化してエンベディングし、PDEの解を求める機械学習手法として有名なFourier Neural Operator(FNO)の補正として利用
	- https://arxiv.org/abs/2305.08757v1
- Abbeel教授によるHinton教授へのインタビュー、NYTimesの記事依頼、全世界から２分毎ｎ取材依頼が来たらしい
	- https://www.youtube.com/watch?v=rLG68k2blOc
- 医療分野に特化した言語モデル「Med-PaLM2」の論文、現役の医者もPaLM2の回答のほうを評価
	- https://arxiv.org/abs/2305.09617
- rinna、日本語に特化した36億パラメータのGPT言語モデルを公開
	- https://rinna.co.jp/news/2023/05/20230507.html
- MicrosoftがLangchainみたいな、Guidanceを発表
	- https://github.com/microsoft/guidance
- CyberAgentが日本語版ローカルLLMを発表
	- https://huggingface.co/cyberagent
- Google の FLAN-20B with UL2 レベルならば、ChatGPT APIのように使えるらしい	
	- https://qiita.com/sakasegawa/items/7394fe68eb0087b3c4a5
- Google、自社のcolabratoryに、コード生成機能を搭載するらしい
	- https://blog.google/technology/developers/google-colab-ai-coding-features/
- Transformer.js: Hugging Faceのtransformerを、ブラウザで動かすことができる、ONIX runtimeを利用、WebGPU対応は不明
	- https://github.com/xenova/transformers.js
- Graph Neural Network(GNN)で、巡回セールスマン問題を解く、スタンフォード大学の講義での事例、CS224W
	- https://medium.com/stanford-cs224w/tackling-the-traveling-salesman-problem-with-graph-neural-networks-b86ef4300c6e
- OpenCALM-7Bをdolly-15k-jaでLoRAしたら、ある程度会話できるようになった
	- https://twitter.com/masuidrive/status/1659089478781227008?s=20
- LLMの出力の説明に関する論文らしい、Explaining black box text modules in natural language with language models by microsoft
	- https://huggingface.co/papers/2305.09863
- TokenHawk、WebGPUを活用して、ローカルで、WebでLLMを動かすことができる仕組み、GoogleのDawnエンジン利用
	- https://github.com/kayvr/token-hawk
-  ChatGPTがiPhoneで動くようになる(米国)
	- https://openai.com/blog/introducing-the-chatgpt-app-for-ios
- Trasnformerを制御に用いる、# A Generalist Dynamics Model for Control、by DeepMind
	- https://huggingface.co/papers/2305.10912
- LangChainから、Spark SQL Agent
	- https://python.langchain.com/en/latest/modules/agents/toolkits/examples/spark_sql.html
- LangChainから、ローカルにダウンロードしたGPT4ALLの使い方改善
	- https://python.langchain.com/en/latest/modules/models/llms/integrations/gpt4all.html
- Language Models Meet World Models: Embodied Experiences Enhance Language Models
	- https://arxiv.org/abs/2305.10626
- WebGPU-pytorch、pytorchが、webGPUの上で動く（学習、推論とも）
	- https://github.com/praeclarum/webgpu-torch
-  Hugging FaceのモデルをLangChainで使う方法を調べた、Hubを使うか、ローカルにダウンロードして使うか、
	- https://www.mattari-benkyo-note.com/2023/05/19/langchain_hugging_face/
- Stanford大学のTransformerの事業CS２５が最強の件
	- https://web.stanford.edu/class/cs25/
- Stanford大学、文字列のアライメントライブラリstring2string
	- https://github.com/stanfordnlp/string2string
- Self-Queringという手法、による文書検索Weaviate、スキーマを与えると、検索結果に、情報抽出の結果も出してくれる（曲のratingとかgeneとかのメタデータなど）もやってくれる。おおすごい
	- https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/weaviate_self_query.html
- BCGがまとめた日本企業の変革を阻む「チェンジモンスター」資料、ポケモン的なキャラクター付け
	- https://web-assets.bcg.com/img-src/japan%20tembo-146-change%20monster_1oct2002_tcm9-169992.pdf


## 5/15
最新のLLMに関する情報は、Transformer論文のアーキテクチャの議論や応用プロンプトエンジニアリングの探求、アッセンブリ理論の応用、AGIの悲観論、Shap-Eのデモサイト、LLamaindexの要約機能追加、GPT-4のニューロン説明試み、分子生成モデル改良、医師国家試験合格報道、WebGPUでのLLM実行、PaLM 2の発表、Q&A向けretreaver、BardとGPT-4性能比較、フォント問題、HumanML3Dデータセット、DeepL日本拠点計画、3D Tilesプロジェクト、日本語T5モデル、LeCun講演、ChatGPT Plugin提供、分子励起状態予測、機械学習理論発展、GTモデル作成、推薦システム研究、量子機械学習研究者転向、Helion Energyへの電力購入契約、牧野先生の不偏分散解説、Scikit-learnデータセット変更、Vicuna-13B-4bit実行方法、LangChainのretriever追加など多岐にわたります。

- オリジナルのTransformer論文のアーキテクチャ構成の絵が、本文と合ってないと記事が、
	- https://arxiv.org/abs/2002.04745
- few-shot learningで満足できない人の応用プロンプト集
	- https://cameronrwolfe.substack.com/p/advanced-prompt-engineering
- アッセンブリ理論、有機化合物で分子の結合の複雑さの評価、LLMの評価にも使える？
	- https://www.quantamagazine.org/a-new-theory-for-the-assembly-of-life-in-the-universe-20230504/
- AGIが人類を壊滅させる可能性はほぼ100%といった強い悲観論、AI Alignment Centerの人の話によると、
	- https://note.com/bioshok/n/n43041a52a529
- OpenAIが公開した、プロンプトから3Dモデルを作るShap-Eのデモサイトがhuggingfaceに。
	- https://huggingface.co/spaces/hysts/Shap-E
- LLamaindexに新しいドキュメント要約の仕組みが導入？
	- https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec
- OpenAI: GPT-4で、GPT-2の個々のニューロンの働きの説明というか、意味づけを行う？？	
	- https://openai.com/research/language-models-can-explain-neurons-in-language-models
- VAEによる分子生成のモデルの改良の話
	- https://arxiv.org/abs/2305.03041v1
- GTP-4日本の医師国家試験で合格？
	- https://news.yahoo.co.jp/articles/60da4c733c2a03a9829bc598f8dcc246e4d10b00
- LLamaindexにて、正式にhuggingfaceの LLM support　される
	- https://github.com/jerryjliu/llama_index
- WebGPUで、LLMをローカルに動かす動きが活発に、LaMA, Alpaca, Vicuna, and Dol
	- https://github.com/mlc-ai/web-llm
- Google I/OでPaLM 2を発表
	- https://ai.google/static/documents/palm2techreport.pdf
- Wikipediaに対するQ&Aを可能にするretreaverを提供するCoheare?
	- https://github.com/menloparklab/cohere-weaviate-wikipedia-retrieval
	- https://github.com/weaviate/weaviate
- Google I/Oが大収穫だった模様、Bardは日本語、韓国語対応、 Bard、PaLM 2
	- https://www.gizmodo.jp/2023/05/google-io23-ai-outline.html
- BardとGPT-4の性能比較、結構GPT-4に肉薄している模様。
	- https://qiita.com/kumag0r0/items/77dbe743643183ae3e98
- Bard発表のプレゼンで、「日本語」のフォントが残念と話題に、、、
	- https://www.itmedia.co.jp/news/articles/2305/11/news178.html
- HumanML3D:Human motion language Dataset
	- https://github.com/EricGuo5513/HumanML3D
- DeepL日本に拠点を置く？
	- https://newsdig.tbs.co.jp/articles/-/480597?display=1
- GoogleのPhotorealistic 3D Tilesを[http://deck.gl](https://t.co/j5x1oduUK1)で表示、軽いらしい
	- https://twitter.com/syanseto/status/1656586481094520838?s=20
	- deck.lg(TerrainExtension) + Google Photorealistic 3D Tiles 
	-  Google 3D tileで読み込んだ3Dモデルの上にTerrainExtensionを使ってGeoJSONポリゴンをオーバーレイ
- 日本語T5モデルの公開 by レトリバ
	- https://note.com/retrieva/n/n7b4186dc5ada
- LeCun先生の講演、LeCun: Towards Machines That Can Understand, Reason, & Plan
	- https://www.youtube.com/watch?v=_JfEScYyVCE
- OpenAI、ChatGPT Plusユーザー全体に、5/12よりPluginが使えるようなるとアナウンス
	- https://help.openai.com/en/articles/6825453-chatgpt-release-notes
- 拡散モデルを用いることで２次元の分子グラフからでも同等の励起状態の予測精度が得られるという話らしい
	- https://arxiv.org/abs/2304.12233v2
- 機械学習理論発展、Hyperbolic Poincaré distributions = Probability distributions with support the Poincaré disk
	- https://arxiv.org/abs/2205.13984
- Graph Transformer (GT)を作る例題
	- https://arxiv.org/pdf/2012.09699.pdf
- 推薦において、ユーザーの嗜好って、LLMは本当に理解してるんだったけ論文。
	- https://arxiv.org/abs/2305.06474
- 量子機械学習の研究者が、軒並み量子をやめて機械学習にいってるという、組合せ最適化の大家である湊先生の嘆き
	- https://twitter.com/MinatoYuichiro/status/1657243184064499712?s=20
- GoogleのPhotorealistic 3D Tiles（左）と国交省の3D都市モデルPLATEAUの3D Tiles（右）の比較
	- https://twitter.com/syanseto/status/1656964913913540608?s=20
- ChatGPTとOSSのLLM達とガチタスクでの比較、いい線言ってるらしい。Vicuna-13B, ChatGPT (3.5), MPT-7B-Chat
	- https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610
- PrivateGPT:単にOSSのLLMをダウンロードしてチャットに仕立てる、LangChain and GPT4All and LlamaCpp
	- https://github.com/imartinez/privateGPT
- OpenAIの Sam Altman氏の、謎のツイート"summer is coming"
	- https://twitter.com/sama/status/1657405294354518017?s=20
- 東大吉田塁（酒場の人ではない）先生の、「教員向けChatGPT講座」が分かりやすいと評判に、
	- https://www.youtube.com/live/lwccHzqfuvc?feature=share
- Pluginを開発するOSSである、PlugnPlai and LangChainの例
	- https://github.com/edreisMD/plugnplai/blob/master/examples/plugins_step_by_step.ipynb
- HuggingFaceから、自然言語でAgentに指示を出したら画像でも文章でも音声でも出力してくれるモデルを勝手に選んで出力してくれるTransformers  Agent発表、
	- https://huggingface.co/docs/transformers/transformers_agents
- Microsoft社、Sam Altman氏が出資する核融合スタートアップであるHelion Energyと2028に電力購入契約
	- https://www.businessinsider.jp/post-269773
	- 加速器で、重水素とHe-3を加速させて衝突時に、磁場で圧縮して、融合させて、膨張の力による磁場の変化から直接電力を（水とか蒸気とかを使わずに）得るという仕組み。
	- OpenAIはますます、Microsoftと一蓮托生に、、、、
- 神戸大学、「牧野」先生、不偏分散の自由度がn-1である理由を失念。
	- https://twitter.com/jun_makino/status/1657229042121314304?s=20
	- 牧野先生ご紹介の「美しい導出」https://manabitimes.jp/math/1205
- Scikit-learnの組み込みデータセットから、ボストン住宅価格が、ポリコレのため削除されてた
	- https://twitter.com/tokoroten/status/1394192087453638662?s=20
	- （授業で使っている人要注意）
- Stable Vicuna-13B-4bitがcolabで動作する、ローカルにダウンロードしてWebUIを上げる
	- https://zenn.dev/tatsuromurata/articles/8e523cf2d0c2bc
	- https://note.com/it_navi/n/nceffc6e8df35
- LangChainに、arxiv用のretrieverが追加、Q&Aなどができる
	- https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/arxiv.html

## 5/8
LLamaIndex 0.6.0がリリースされ、データに対する新しいクエリインターフェイスが導入されました。ChatGPT Code Interpreterが登場し、プログラムの解釈と実行が可能になりました。
Andrew Ngのプロンプトエンジニアリングの講義が提供され、開発者向けにプロンプトエンジニアリングのスキルが教授されます。Transformerのenc-dec間にinformation bottleneckを導入したVAE的な表現の正則化に関する研究が行われました。"Are Emergent Abilities of Large Language Models a Mirage?"と題された論文が公開され、LLMの新たな能力に関する議論が提起されました。
JDLAでは、生成AIの利用ガイドラインが提供され、AIの利用に関する指針が提案されましたLangChainとOpenAIのGymnasiumが連携し、エージェントシミュレーションに関する利用事例が紹介されました。ディープラーニングによる自然言語処理に関する書籍が出版され、NLPに興味を持つ方に向けたリソースが提供されます。"Causal Reasoning and Large Language Models: Opening a New Frontier for Causality"という論文が公開され、因果推論とLLMの関連についての研究が行われました。自己アテンション機構を使用して多電子系のシュレディンガー方程式を第一原理的に解く研究が行われOpenLLAMAが公開され、LLMを活用したデータクエリエンジンが提供されます。G.HintonによるGAI（General Artificial Intelligence）に関するインタビューがCNNで公開され、AIの未来についての議論が展開されました。"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings"という記事が公開され、LLMの性能評価に関する情報が提供されました。"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis"と題された研究が行われ、テキストからヒューマンモーションを生成する技術が提案されました。

- LlamaIndex 0.6.0 - データに対する新しいクエリインターフェイス
	- https://note.com/npaka/n/n4254fc549dc0
- ChatGPT Code Interpreter
- Andrew Ngのプロンプトエンジニアリングの講義
	- https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/
- Transformerのenc-dec間にinformation bottleneckを入れてVAE的に表現の正則化
	- https://openreview.net/forum?id=6QkjC_cs03X
- Are Emergent Abilities of Large Language Models a Mirage?
	- https://arxiv.org/abs/2304.15004
- JDLAでは、「生成AIの利用ガイドライン」
	- https://www.jdla.org/document/?utm_source=prtimes&utm_medium=referral
- LangChainとOpenAIのGymunasiumの連携
	- https://python.langchain.com/en/latest/use_cases/agent_simulations/gymnasium.html
- ディープラーニングによる自然言語処理
	- https://www.amazon.co.jp/dp/4320125029/
- Causal Reasoning and Large Language Models: Opening a New Frontier for Causality
	- https://arxiv.org/abs/2305.00050
- 自己アテンション機構をつかって多電子系のシュレディンガー方程式を第一原理的に解く
	- https://arxiv.org/abs/2211.13672
- OpenLLAMA
	- https://github.com/openlm-research/open_llama
- G.Hintonによる、GAIインタビュー @CNN
	- https://www.youtube.com/watch?v=FAbsoxQtUwM
-  Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings
	- https://lmsys.org/blog/2023-05-03-arena/
- TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis
	- https://mathis.petrovich.fr/tmr/
- LLMs & Causal Reasoning
	- https://arxiv.org/abs/2305.00050
- LangChainのv0.0.139からv0.0.151までの差分を整理（もくもく会向け）
	- https://note.com/mahlab/n/ne29d4bfb1d45
- LLAMAindexの新しい抽象化API,brand new “router” abstraction in order to build powerful, generalizable, LLM-powered query engines over your data.
	- https://colab.research.google.com/drive/1KH8XtRiO5spa8CT7UrXN54IWdZk3DDxl?usp=sharing
- ホワイトハウスNew Actions to Promote Responsible AI Innovation that Protects Americans’ Rights and  Safety
	- https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/
- OpenAlpaca, an instruction-following model based on OpenLLaMA
	- https://github.com/yxuansu/OpenAlpaca
- 「LlamaIndex」が0.6.0で大きな変更があったので更新しました。
	- https://note.com/npaka/n/n50475d6c3118
- ChromaDB Self-Querying Retriever
	- https://github.com/hwchase17/langchain/blob/master/docs/modules/indexes/retrievers/examples/chroma_self_query_retriever.ipynb
- experimental CodeChain、LangChainの上でPythonを実行できるらしい。
	- https://langchain-ai.github.io/kork/
- Unifying LLM-powered QA Techniques with Routing Abstractions
	- https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0
- PandasAI、またまたpandaベースのチャット解析ツール、OpenAI以外のLLMの使えそう。
	- https://github.com/gventuri/pandas-ai

## 4/10
LLMの倫理的なふるまいを評価するためのマキャベリベンチマークが発表されました。LLaMA-Adapterと呼ばれる、軽量なLoRAのようなシステムが紹介されました。DeepMindからは、Transformersのための形式的なアルゴリズムに関する研究が発表されました。LLMに対する心理学的な評価やセラピーを行うための枠組みが提案されました。リーガルなGPT-4ベースのサービスであるHarveyが公開されました。京大2回生の統計力学の期末試験の問題が論文になった話があります。AzureのOpenAIがEmbeddingのバージョン2をリリースし、トークン数が2,048から8,191に増加しました。MatChaと呼ばれるシステムが、グラフなどの入力から推論やQ&Aを行うことができるようになりました。gpt4allの公式チャットUIがリリースされました。Microsoft ResearchのSparks of AGI: early experiments with GPT-4に関する説明がYouTubeで提供されています。

-   LLMの倫理的なふるまいをさせるための、マキャベリベンチマーク
    -   [https://arxiv.org/abs/2304.03279](https://arxiv.org/abs/2304.03279 "https://arxiv.org/abs/2304.03279")
-   LLaMA-Adapter:軽量なLoRAみたいなしくみらしい。
    -   [https://arxiv.org/abs/2303.16199](https://arxiv.org/abs/2303.16199 "https://arxiv.org/abs/2303.16199")
-   DeepMindから "Formal Algorithms for Transformers"
    -   [https://arxiv.org/abs/2207.09238](https://arxiv.org/abs/2207.09238 "https://arxiv.org/abs/2207.09238")
-   LLMに対して心理学的な評価（セラピー？）を行う枠組み
    -   [https://arxiv.org/abs/2207.09238](https://arxiv.org/abs/2207.09238 "https://arxiv.org/abs/2207.09238")
-   リーガルなGPT-4ベースのサービス、Harvey（米ドラマのSUITSの主人公の一人がハーベイ）
    -   [https://harvey-ai.notion.site/Careers-Harvey-c9e804fe422e4316bdfde9fe74ed6b06](https://harvey-ai.notion.site/Careers-Harvey-c9e804fe422e4316bdfde9fe74ed6b06 "https://harvey-ai.notion.site/careers-harvey-c9e804fe422e4316bdfde9fe74ed6b06")
-   京大２回生の統計力学の期末試験の問題が、論文になった話、
    -   [https://www.t.u-tokyo.ac.jp/press/pr2023-04-05-001](https://www.t.u-tokyo.ac.jp/press/pr2023-04-05-001 "https://www.t.u-tokyo.ac.jp/press/pr2023-04-05-001")
-   AzureのOpenAI、Embeddingのバージョン２が登場、トークン数が2,048→8,191と激増
    -   [https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models-1](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models-1 "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models-1")
-   MatCha: グラフとかの入力、からも推論やQ&Aができる。 by GoogleAI
    -   [https://arxiv.org/abs/2212.09662](https://arxiv.org/abs/2212.09662 "https://arxiv.org/abs/2212.09662")
-   gpt4allの公式チャットUIがリリース
    -   [https://github.com/nomic-ai/gpt4all-ui](https://github.com/nomic-ai/gpt4all-ui "https://github.com/nomic-ai/gpt4all-ui")
-   MS ResearchのSparks of AGI: early experiments with GPT-4の著者による説明。。
    -   [https://www.youtube.com/watch?v=qbIk7-JPB2c&t=1023s](https://www.youtube.com/watch?v=qbIk7-JPB2c&t=1023s "https://www.youtube.com/watch?v=qbik7-jpb2c&t=1023s")

## 4/17
今井むつみ先生の講演「AI時代に必要な学びと教育ー認知科学からの視点」が2023年3月29日にYouTubeで配信されます
DatabricksからDoly2.0がリリースされました。Doly2.0はオープンソースであり、商用利用も可能なインストラクション調整型LLMです。
CMUの化学者による、LLMを使った合成実験の危険性に関する論文が公開されました。
OpenAIの研究者[@ilyasu](https://twitter.com/ilyasut)による、LLMにおけるビジョンの重要性に関するツイートとGPT-4にビジョンが含まれていることを示すビデオがTwitterで公開されています。
GPT4ALLを使用したApatch2ライセンスのチャットボットOSSが公開されました。
触媒開発にGPTを活用する研究が行われており、ベイズ最適化とLLMを組み合わせて合成条件を見つける方法が紹介されています。さらに、in context learningを使用することでチューニングが不要とされ、ガウス過程回帰と同等の性能を実現しています。
    
-   AlpacaにCoT（Commonsense Transformers）とStorytellingを強化したモデルAlpacino30bが公開されました。詳細は[こちら](https://huggingface.co/digitous/Alpacino30b/tree/main)からアクセスできます。
-   今井むつみ先生の講演「AI時代に必要な学びと教育ー認知科学からの視点」(2023年3月29日)がyoutube配信される
    -   [https://www.youtube.com/playlist?list=PLMITB-DRUs7N10WLl_4zDUWfBkLd6z_Em](https://www.youtube.com/playlist?list=PLMITB-DRUs7N10WLl_4zDUWfBkLd6z_Em "https://www.youtube.com/playlist?list=plmitb-drus7n10wll_4zduwfbkld6z_em")
-   DatabircksからDoly2.0がリリース(OSSかつ商用利用可)
    -   [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm "https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm")
-   CMUの化学者による、LLMを使った合成実験に係る危険性についての露文
    -   [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm "https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm")
-   [@OpenAI](https://twitter.com/OpenAI "https://twitter.com/openai")の大天才研究者[@ilyasu](https://twitter.com/ilyasut "https://twitter.com/ilyasut")による、LLMにおけるvisonの重要性と、GPT-4にはvisonも入っているよビデオ
    -   [https://twitter.com/i/status/1645752089140957187](https://twitter.com/i/status/1645752089140957187 "https://twitter.com/i/status/1645752089140957187")
-   GPT4ALLを使ったApatch2ライセンスのチャットボッドOSSが公開
    -   [https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all "https://github.com/nomic-ai/gpt4all")
-   GPTを用いた触媒開発。ベイズ最適化とLLMを組み合わせて、合成条件を見つける。しかもin context learningを使うので、チューニングも不要！！！（素のGPTでOKということ）。ガウス過程回帰と同程度の性能。逆設計も可能
    -   [https://arxiv.org/abs/2304.05341v1](https://arxiv.org/abs/2304.05341v1 "https://arxiv.org/abs/2304.05341v1")
-   AlpacaにCoTとStorytellingを強化した、Alpacino30b公開
    -   [https://huggingface.co/digitous/Alpacino30b/tree/main](https://huggingface.co/digitous/Alpacino30b/tree/main "https://huggingface.co/digitous/alpacino30b/tree/main")

## 4/24
最近、MicrosoftはSemantic KernelのPythonバインディングを発表し、これによりSemantic KernelをPythonで使用できるようになりました。また、gist tokenを使用してプロンプトを効果的に圧縮する方法を示した論文も登場しており、26倍の効果があるとされています。
さらに、新しいプロジェクト「LLaVA」が登場し、これは言語とビジョンを組み合わせたもので、画像とビデオを操作するための新しいアシスタントです。Google ColabでDolly2.0を試す方法や、自律エージェントに関する詳細なガイドも公開されています。
CMUからは大規模言語モデル（LLM）に関する興味深い論文が登場し、自律的な科学研究能力について探究しています。また、数学のワードプロブレムを解決するためにシンボリックソルバとLLMを組み合わせる方法に関する研究も進行中です。
テキストから人間のビデオを生成する新しい技術であるText2Performerも注目されています。さらに、Transformerを超える可能性があるとされる新しい系列モデルS4とその発展形であるH3に関する情報もあります。
一方、Stability AIはオープンソースのLLMであるStableLMのスイートを導入し、LLMの安定性に関する議論が進行中です。MicrosoftはビジュアルなLLMを使用したアプリ開発環境を提案し、LLMを使用したアプリ開発に関するベストプラクティスについての記事も公開されています。
Googleは長期間の時系列予測に特化したTime-Series Dense Encoderを紹介し、プログラミング言語生成ツールであるBardを発表しました。化学の分野でも、生成モデルが進化し、新たなパラダイムとして取り上げられています。
また、大規模言語モデルの驚異と脅威についての特別講演が東京工業大学の岡崎直観教授によって行われ、SQLクエリ生成など、ChatGPTの新しい応用例も紹介されています。
最後に、LLM向けのプログラミング言語であるLMQL（Language Model Query Language）やLangChainの新機能であるContextual Compression Retrieverについても言及されています。これらの情報は、LLM技術の最新動向を把握するのに役立つでしょう。


- Microsoft のSemantic KernelのPythonバインディングが発表
	- https://github.com/microsoft/semantic-kernel/blob/main/python/README.md
- gist tokenによりプロンプトを圧縮する論文(26倍?)
	-  https://arxiv.org/abs/2304.08467
- LLaVA:　Language-and-Vision Asistant、 画像とvicunaをくっつけた
	- https://llava.hliu.cc/
- Google ColabでDolly2.0を試す方法
	- https://note.com/npaka/n/nac386bf799b6
- The Complete Beginners Guide To Autonomous Agents
	- https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents
- 例のCMUの論文：Emergent autonomous scientific research capabilities of large language models
	- https://arxiv.org/abs/2304.05332v1
- シンボリックソルバとLLMを組み合わせるSolving Math Word Problems by Combining Language Models With Symbolic Solvers
	- https://arxiv.org/abs/2304.09102
- Text2Performer: Text-Driven Human Video Generation
	- https://yumingj.github.io/projects/Text2Performer.html
- Transformerを超えるんじゃないかと言われてる新たな系列モデル（と理解してる）S4とその更なる発展であるH3
	- https://techblog.morphoinc.com/entry/2022/05/24/102648
- Stability AI	真のOSSのLLM?
	- https://ja.stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models
- Bing ChatがLaTeXの式を成型できるように進化
- Neo4jの知識をエージェントとして、LLMに組み込む話(LangChainの話）
	- https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7
- MicrosoftのLow-code LLM、ビジュアルなLLMを使ったアプリ開発環境？
	- https://arxiv.org/abs/2304.08103
- LLMを使ったアプリ開発で気を付けること（良記事）Building LLM applications for production
	- https://huyenchip.com/2023/04/11/llm-engineering.html
- GoogleのTime-Series Dense Encoder、長いスケールの時系列予測ができるのか？
	- https://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html
- GoogleのBardがPythonなどのコード生成ができるようになった
	- https://blog.google/technology/ai/code-with-bard/
- 化学における生成モデルの活用サーベイ？# Generative Models as an Emerging Paradigm in the Chemical Sciences
	- https://pubs.acs.org/doi/10.1021/jacs.2c13467
- 特別講演「大規模言語モデルの驚異と脅威」、岡崎 直観（東京工業大学情報理工学院 教授）
	- https://youtu.be/PUuk4Cv-ycg
- ChatGPTでSQL queryを生成する例
	- https://www.promptingguide.ai/applications/coding
- LMQL(Language Model Query Language)とLlamaIndexを接続してみる
	- https://note.com/mahlab/n/n34ac7ebf0387
- **LMQL（Language Model Query Language）**はこのような問題を解決するために開発されている大規模言語モデル（LLM）向けのプログラミング言語です。
	- https://note.com/mahlab/n/n11b15b323c87
-  ChatGPT + LangChain で、膨大な PDF ドキュメントの内容を爆速で把握する
	- https://qiita.com/hiroki_okuhata_int/items/7102bab7d96eb2574e7d
- Generative Agents: Interactive Simulacra of Human Behavior
	- https://arxiv.org/abs/2304.03442
- 物性予測モデルの悪意ある使用を防ぐための論文。Censoring chemical data to mitigate dual use risk
	- https://arxiv.org/abs/2304.10510v1
- LangChainの新機能Contextual Compression Retriever
	- https://note.com/mahlab/n/n7d72e83904cc

> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjAyOTUzMTE5LDQzNTEwOTQ0NywxNjQ1Nz
YzODQ1LC0yMDg3NDY0MTMwLC0xMjIwMDkwOTUwLDE5NzQ4OTQw
ODUsLTEwNDMyMTU3MjMsLTQyNDQyNDI1NiwtMTIzMDczOTIzOS
wxNTU4MTI4MTk1LDk1MjUyODA2MSwtMTQ2MDMzMzE4NCw5Nzgy
MjQyMDAsLTE3NTc1NTI5MTksMTk1MjM1OTU3LDExMzcxNjkzMj
MsLTE2MzcxNTQwNjQsLTE0MTk1NzM5ODAsMjE0MDM3MjM0LC0x
NDE2NTQzNTZdfQ==
-->