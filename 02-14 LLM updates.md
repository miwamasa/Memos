# ã²ãŸã™ã‚‰LLMé–¢é€£æƒ…å ±ã‚’è¿½ã†ã€
ã“ã‚Œã¯ã€å€‹äººã®twitter bookmarkã‚’æ¯é€±ãŠã•ã‚‰ã„ã—ã¦ã„ã‚‹ã€‚

## 6/10

- Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
	- https://arxiv.org/abs/2405.21060
	- ç¾åœ¨ç”ŸæˆAIã§ä¸»æµã®Transformerã®ã€Œæ¬¡ã€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã—ã¦æœŸå¾…ã•ã‚Œã‚‹Mamba-2ã‚’ææ¡ˆ
	- Mamba2ã¯æ–°ã—ãå°å‡ºã•ã‚ŒãŸSSDã‚’ä½¿ã„ã€ç³»åˆ—é•·ã«å¯¾ã—ç·šå½¢ã®è¨ˆç®—é‡ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã§åŠ¹ç‡çš„ã«å­¦ç¿’ã€æ¨è«–ã§ãã€Transformerã‚„Mambaã‚ˆã‚ŠPPLã‚„ä¸‹æµã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ãŒé«˜ã„ã€‚SSMã¨TransformerãŒçµ±ä¸€çš„ãªæ çµ„ã¿ã§æ‰±ãˆã‚‹ã“ã¨ã‚‚ç¤ºã™ by å²¡é‡åŸã•ã‚“
- GPT-4 is 1.8T MoE, thanks Nvidia Presentation
	- https://x.com/literallydenis/status/1797531945926287497
- LLMã‚’æ´»ç”¨ã—ãŸå¤§è¦æ¨¡å•†å“ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¸ã®å–ã‚Šçµ„ã¿
	- https://engineering.mercari.com/blog/entry/20240411-large-scale-item-categoraization-using-llm/
	- GPTã‚’å•†å“ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã«æ´»ç”¨ã—ãŸäº‹ä¾‹ã¨å·¥å¤«ã—ãŸãƒã‚¤ãƒ³ãƒˆãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹ã‚ˆ
		- Embeddingãƒ¢ãƒ‡ãƒ«ã¯OSSã§ã‚‚å•é¡Œãªã—
		- GPT4ã¯ã‚³ã‚¹ãƒˆçš„ã«ä½¿ãˆãªã‹ã£ãŸ
		- max_tokensã¨Chain of Thought ã§ã®åŠ¹ç‡åŒ–
		- Numbaã¸ã®æ›¸ãæ›ãˆã‚‚GPTã‚’ä½¿ã£ã¦åŠ¹ç‡åŒ–
-  Hugging Faceã®ZeroGPUã§AIã®ãƒ‡ãƒ¢ã‚’ä½œã‚‹æ–¹æ³•: åˆç´šç·¨
	- https://qiita.com/alfredplpl/items/abb30283b578dc984d16
	- ZeroGPU ã¨ã¯ã€ãƒ‡ãƒ¢ã®åˆ©ç”¨è€…ãŒä½¿ã†ç¬é–“ã ã‘é«˜æ€§èƒ½ãªGPUãŒå€Ÿã‚Šã‚‰ã‚Œã‚‹ã¨ã„ã†ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ç¾åœ¨ã¯A100 40GBãŒä¸€ç¬å€Ÿã‚Šã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã‚’å®Ÿç¾ã§ãã¦ã„ã‚‹ã®ã¯ä¸–ç•Œã§Hugging Faceã ã‘ã§ã—ã‚‡ã†ã€‚ãŠå€¤æ®µã¯æœˆé¡9ãƒ‰ãƒ«ï¼ˆç´„1500å††ï¼‰ã§ã™
	- Hugging Faceã®ZeroGPUã¯AIã®ãƒ‡ãƒ¢ã‚’ä½œã‚‹ã®ã«æœ€é©ã ã¨ã‚ã‹ã‚Šã¾ã—ãŸã€‚ã„ã‹ãŒã§ã—ãŸã§ã—ã‚‡ã†ã‹ã€‚ãœã²ã¿ãªã•ã‚“ã‚‚ãƒ‡ãƒ¢ã‚’ä½œã£ã¦ã¿ã¦ãã ã•ã„ã€‚ãªãŠã€ç§ã¯è²¬ä»»ã‚’æŒã¡ã¾ã›ã‚“ã€‚
- lmsys.org ã§Googleã®Gemini 1.5 Proï¼ˆ5/14ãƒ¢ãƒ‡ãƒ«ï¼‰ãŒæ—¥æœ¬èªã§ä¸–ç•Œä¸€ã«ãªã‚Šã¾ã—ãŸã€‚ï¼’ãƒ¶æœˆå‰ã®ãƒ¢ãƒ‡ãƒ« (4/9ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚ˆã‚Šã‹ãªã‚Šæ”¹å–„ã•ã‚Œã¾ã—ãŸ
	- https://x.com/shanegJP/status/1797798176344453414
- Perplexity Pagesã€AIã«æƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãã¦ã‚‚ã‚‰ã†ã¤ã„ã§ã«ãã‚Œã‚’Webè¨˜äº‹ã®ä½“è£ã§ã¾ã¨ã‚ã¦å…¬é–‹ã§ãã¡ã‚ƒã†ã®ã‹ã‚ã€‚
	- https://x.com/umiyuki_ai/status/1797871157850620270
- å¯Œå£«é€šã¯ã€ç‰¹åŒ–å‹ã®ç”ŸæˆAIæ··åˆæŠ€è¡“ã¨ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ‹¡å¼µRAGã¨ã„ã†ãƒãƒ‹ã‚¢ãƒƒã‚¯ãªæ–¹å‘ã«é€²åŒ–ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã—ãŸã€‚
	- https://x.com/AsamaKotaro/status/1797844740328804563
-  the benefits of Google Gemini's gigantic 1 million token context window in action!
	- https://x.com/llama_index/status/1798049438814081138
	- In this quick notebook, we show Gemini built into a LlamaIndex agent attempting to answer a multi-part question from a set of complicated, heterogeneous documents.
- å¯Œå£«é€šã®ç ”ç©¶æˆ¦ç•¥ç™ºè¡¨ä¼šã€æœ€é©åŒ–å•é¡Œã¨ç”ŸæˆAIã«ã‚ˆã‚‹åˆ¶ç´„æ¡ä»¶ã®ç”ŸæˆãŒç›¸æ€§ãŒè‰¯ã„ã¨ã„ã†ç™ºæƒ³
	- https://x.com/tokoroten/status/1797851927457546682
	- è­°äº‹éŒ²ã‚„ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰åˆ¶ç´„æ¡ä»¶ã‚’èµ·ã“ã—ãŸã‚Šã€AIã¨ã®å¯¾è©±ã‚’é€šã˜ã¦åˆ¶ç´„æ¡ä»¶ã‚’èµ·ã“ã—ãŸã‚Š
- æ¦‚å¿µãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹çœŸã«ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªè£½é€ æ¥­DX
	- https://www.qunie.com/quriosity/231218_00/
	- æ¦‚å¿µãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯å€‹ã€…ã®æ¥­å‹™æ”¹é©ã‚„ãƒ¢ãƒ€ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«ç€æ‰‹ã™ã‚‹å‰ã«ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚ç‰¹å®šã®æ¥­å‹™é ˜åŸŸã ã‘ã§ãªãã€è£½é€ æ¥­ã®ãƒãƒªãƒ¥ãƒ¼ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã€‚
	- è«–ç†ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ãªå³å¯†ã•ã¯ä¸è¦ã§ã€é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ç¾¤ã¨ã€ãã®ã‚­ãƒ¼é …ç›®ï¼ˆå…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿é …ç›®ã¯ä¸è¦ï¼‰ã€ãƒ‡ãƒ¼ã‚¿ç¾¤ã®ã¤ãªãŒã‚Šã‚’ç¤ºã™ã€‚
	- ãƒãƒªãƒ¥ãƒ¼ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã‚’ä¿¯ç°ã—ãŸæ¦‚å¿µãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã‹ã‚‰ã“ãã€ä¼æ¥­å…¨ä½“ã®æ”¹é©ã«ä¸€æœ¬ã®èŠ¯ãŒé€šã‚‹ã“ã¨ã«ãªã‚‹ã®ã ã€‚ã‚ã‚‹ãŠå®¢ã•ã¾ã¯ã€æ¦‚å¿µãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ã“ã¨ã‚’â€œè‡ªç¤¾ã®æ†²æ³•â€ã¨è¡¨ç¾ã•ã‚Œã¦ã„ãŸãŒã€ã¾ã•ã«ãã®é€šã‚Šã§ã‚ã‚‹ã€‚
- ELYZAãŒã€å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äºº ç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ãŒå‹Ÿé›†ã—ãŸå¤§è¦æ¨¡ç”ŸæˆAIç ”ç©¶é–‹ç™ºæ”¯æ´ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«æ¡æŠã•ã‚Œã¾ã—ãŸã€‚
	- https://x.com/ELYZA_inc/status/1797780304717078689
-  è«–æ–‡è§£èª¬ã‚’GPT-4oã‚’ä½¿ã£ã¦è‡ªå‹•çš„ã«ç”Ÿæˆã—ã¦ã¿ã‚‹ by é€†ç€¬å·ã•ã‚“
	- https://qiita.com/sakasegawa/items/8e17ede26dd96e7e3280
	- PDFã‚’ç”»åƒã¨ã—ã¦å–ã‚Šæ‰±ã†ã¨å‡¦ç†ã¯é…ã„ãŒæ ¼æ®µã«å®‰ã„ï¼ï¼ 
	- è«–æ–‡ã®PDFã‚’ç”»åƒã¨ã—ã¦æ‰±ã„ã€GPT-4oã§è½åˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦è§£èª¬ã‚’ç”Ÿæˆã•ã›ã‚‹
	- è«–æ–‡ã®PDFã‹ã‚‰æ•°å¼ã‚„å›³è¡¨ã‚’æŠ½å‡ºã—ã€å€‹ã€…ã«è§£èª¬ã‚’ç”Ÿæˆ
- ã‚‚ã¯ã‚„AIã®æ€§èƒ½ã‚’äººé–“ãŒæ¸¬å®šã§ããªã„ by  karaage. [ã‹ã‚‰ã‚ã’]ã•ã‚“
	- https://karaage.hatenadiary.jp/entry/2024/06/04/073000
	- ãã‚‚ãã‚‚AIã®æ€§èƒ½ã‚’äººé–“ãŒæ¸¬å®šã™ã‚‹ã®ãŒé›£ã—ã„é ˜åŸŸã«æ¥ã¦ã„ã‚‹ãªã¨å®Ÿæ„Ÿã—ã¾ã—ãŸã€‚
- Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming
	- https://arxiv.org/abs/2406.00592
- LLM Basics - Why can't we use regular LoRA for pre-training LLMs
	- https://x.com/rohanpaul_ai/status/1797759219891937324
	- LoRA (Low-Rank Adaptation), targets a subset of a neural network's parameters, specifically focusing on the weight matrices of transformer models. It represents these large matrices as the product of smaller
- Why AI wont take your job just yet
	- https://medium.com/@starloba/why-ai-wont-take-your-job-just-yet-13e95cd05da8
	- æ±ç”¨çš„ãªã‚¿ã‚¹ã‚¯ã‚’AIã«è§£ã‹ã›ã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã€äººé–“ã¯ã‚ˆã‚Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªå•é¡Œã«æ³¨åŠ›ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚ã‚ˆã‚Šæ­£ç¢ºã«è¨€ã†ãªã‚‰ã€å¼·åˆ¶çš„ã«æ³¨åŠ›ã—ãªã„ã¨ã„ã‘ãªã„çŠ¶æ³ã«è¿½ã„è¾¼ã¾ã‚Œã‚‹ã€‚
- Microsoft has built a weather forecasting model named 'Aurora
	- https://x.com/MSFTResearch/status/1797662278394827029
-  Heuristics on the high seas: Mathematical optimization for cargo ships
	- https://research.google/blog/heuristics-on-the-high-seas-mathematical-optimization-for-cargo-ships/
	- Today we present new solutions to the Liner Shipping Network Design and Scheduling Problem, released as part of our new Shipping Network Design API, with the goal of maximizing the efficiency of container shipping networks at world-wide scale
- Unslothã¯ã“ã®è«–æ–‡ã«å¯¾æŠ—ã—ã¦LoRAã§ã®ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’å¾¹åº•çš„ã«æœ€é©åŒ–ã—ãŸçµæœã€ä»Šã¾ã§ã®LoRAå­¦ç¿’ã®ï¼’å€ã®åŠ¹ç‡ã§å­¦ç¿’ã§ãã¦ã€VRAMã‚‚åŠåˆ†ã§æ¸ˆã‚€
	- https://x.com/umiyuki_ai/status/1798221784334160262
	- 24GBã®VRAMã§Llama3-8Bã‚„Mistral-7BãŒLoRAç¶™ç¶šäº‹å‰å­¦ç¿’ã§ãã‚‹
- GLM4-9Bã ã£ã¦ã€‚26è¨€èªå¯¾å¿œã€‚GPT-4ã«åŒ¹æ•µã™ã‚‹é–¢æ•°å‘¼ã³å‡ºã—èƒ½åŠ›
	- https://x.com/umiyuki_ai/status/1798292824544420150
- ç¶™ç¶šäº‹å‰å­¦ç¿’(CPT: Continued Pre-Training)ã‚’QLoRAã§ã‚„ã‚ã†ã¨ã™ã‚‹è©¦ã¿
	- https://x.com/webbigdata/status/1798313713654776062
	- Colabç„¡æ–™ç‰ˆã§ã‚‚mistral-7b-v0.3ãªã‚‰ååˆ†å‹•ãã¾ã—ãŸ
	- llama3 8bã‚„gemma7bã§ã¯æœ‰æ–™ç‰ˆã®L4(24GB)ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒllama2ã¨ã„ã†æ‰‹ã‚‚ã‚ã‚Šã¾ã™
-  ä½è³€ã®ç¹”ç”°ç—…é™¢ãŒã‚ªãƒ³ãƒ—ãƒ¬GPUã‚µãƒ¼ãƒãƒ¼ã§LLMç¨¼åƒã€é›»å­ã‚«ãƒ«ãƒ†æƒ…å ±ã‚’ç”ŸæˆAIãŒè¦ç´„
	- https://xtech.nikkei.com/atcl/nxt/column/18/00001/09236/
	- ã“ã‚Œã¾ã§åˆ©ç”¨ã—ã¦ããŸé›»å­ã‚«ãƒ«ãƒ†ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ªãƒ—ãƒ†ã‚£ãƒ ãŒæä¾›ã™ã‚‹ç”ŸæˆAIã€ŒOPTiM AIã€ã‚’çµ„ã¿åˆã‚ã›ã€çœ‹è­·å¸«ã®æ¥­å‹™åŠ¹ç‡ã‚’é«˜ã‚ã‚‹å®Ÿè¨¼ã«ä¹—ã‚Šå‡ºã—
	- ç±³NVIDIAã®RTX A2000ã‚’æ­è¼‰ã—ãŸGPUï¼ˆç”»åƒå‡¦ç†åŠå°ä½“ï¼‰ã‚µãƒ¼ãƒãƒ¼1å°ã‚’æ–°ãŸã«é™¢å†…ã«ç”¨æ„ã—ãŸã€‚LLMã®å­¦ç¿’ã‚„æ¨è«–ã«ç”¨ã„ã‚‹
- Introducing AI Agents in LangGraph!ã€€ by Deeplearning.ai
	- https://x.com/DeepLearningAI/status/1798376731188834780
	- In this course taught by hwchase17, LangChainAI CEO, and weiss_rotem, tavilyai CEO, youâ€™ll learn to use LangGraph to create controllable agents, and agentic search for agents to enhance their output.
- ChatGPTã§ãƒ¬ãƒ»ãƒŸã‚¼ãƒ©ãƒ–ãƒ«ã®äººç‰©ç›¸é–¢3Dãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
	- https://x.com/itnavi2022/status/1798320618695438647
	- 
- 

## 6/3

Google I/Oã§ç™ºè¡¨ã•ã‚ŒãŸgoogleã®æ¤œç´¢xç”ŸæˆAIãŒã€ã¨ã¦ã‚‚ä¸è©•ã¨ã„ã†ã“ã¨ã§ã€Wall Street Journalã®è¨˜äº‹ã«ã‚‚ã‚ã‚‹ã‚ˆã†ã«ã€Perplexityã®å„ªç§€ã•ãŒéš›ç«‹ã¤ã€gooleãŒãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã—ãŸæ–°æŠ€è¡“ã§æœŸå¾…ã‚’è£åˆ‡ã‚‹ã®ã¯å…¨ãæ’ä¾‹ã§ã™ã­ã€‚ã¨ã¯ã„ã£ã¦ã‚‚ã€Gemini 1.5 Pro/Flashã®å„ªç§€ã•ã‚‚ã‚ã¡ã“ã¡ã§å ±å‘Šã•ã‚Œã¦ãŠã‚Šã€æœ¬å½“ã¯å„ªã‚Œã¦ã‚‹ã‚“ã§ã—ã‚‡ã†ã€‚Mistralã‹ã‚‰ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®OSSã§ã‚ã‚‹CodestralãŒç™ºè¡¨ã€ã•ã£ããOllamaãŒå¯¾å¿œã€ã“ã‚Œã§ãŠå¥½ããªã‚¨ãƒ‡ã‚£ã‚¿ã¨çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒå®Ÿç¾å¯èƒ½ã«ã€‚é‡å­åŒ–ã€å°è¦æ¨¡åŒ–ã«ã‚‚é€²å±•ãŒã‚ã‚Šã€Mixtral 8x22b ã®é‡å­åŒ–ç‰ˆQ6_K ãŒ $362 CPU(AMD Ryzen 9 5950X BOXã‹?)ã§è»½ã€…å‹•ä½œã™ã‚‹ã¨ã„ã†å ±å‘Šã‚‚ã‚ã£ãŸã‚Šã€Phi-3-Tinyã‚·ãƒªãƒ¼ã‚ºã®ã‚ˆã†ã«ã€ã•ã‚‰ã«å°ã•ã•ãªLLMã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã¿ãŸã„ãªå±•é–‹ã‚‚ã‚ã£ãŸã€‚llama.cppã§é‡å­åŒ–ç‰ˆã‚’å‹•ä½œã•ã›ã‚‹ã¨ollamaã‚ˆã‚Š1.8å€é€Ÿã„ã¨ã„ã†å ±å‘Šã‚‚ã€‚ç”ŸæˆAIã®é£›èºçš„æ€§èƒ½ã‚¢ãƒƒãƒ—ã®ç§˜å¯†ã¨ã„ã‚ã‚Œã‚‹ã€Œã‚°ãƒ­ãƒƒã‚­ãƒ³ã‚°ã€ã«é–¢ã™ã‚‹è«–æ–‡ã€æ±åŒ–å›è·¯å½¢æˆã®ç§˜å¯†ã«è¿«ã‚Šã€æ–°ãŸãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ææ¡ˆã¨ã„ã†ã®ã¯èƒ¸ç†±ã„ã€‚ç”ŸæˆAIã®ã€Œå‰µé€ æ€§ã€ã«é–¢ã™ã‚‹ï¼‘ï¼ä¸‡äººã®äººé–“ï¼ã¨ã®æ¯”è¼ƒã§ã€GPT-4ãªã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å·¥å¤«ã™ã‚Œã°ã€äººé–“ã‚’ä¸Šå›ã‚‹ã¨ã„ã†ã®ã«ã¯é©šã„ãŸã€‚O'Reillyã‹ã‚‰Prompt Engineeringã®æ–°åˆŠã‚‚å‡ºã‚‹ãŒã€ãã‚‚ãã‚‚Anthropicã®Claude3ã¯ã€ã‚´ãƒ¼ãƒ«ã‚’ä¸ãˆã‚Œã°é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹ã¨ã„ã†ã€‚è²¡å‹™è«¸è¡¨ã‹ã‚‰å°†æ¥ã®åç›Šã®ä¼¸ã³ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§GPT-4ã¯äººé–“ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã¨èã„ã¦ã‚‚é©šã‹ãªããªã£ãŸã€‚ãã‚“ãªAIã§ã™ãŒã€AIãŒä»–è€…ã®å¿ƒã‚„æ„å›³ã‚’ç†è§£ã™ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ã®ã‹ã®ã€Œå¿ƒã®ç†è«–(ToM:Theory of Mind)ã€ã‚’æŒã£ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ†æã—ãŸè«–æ–‡ã§ã¯ã€GPT-4 and Flan-PaLM ãŒäººé–“ã®å¤§äººã®ãƒ¬ãƒ™ãƒ«ã«é”ã—ãŸã¨ã®ã“ã¨ã€‚ã•ã‚‰ã«äººé–“ã‚’è¶…ãˆã‚‹ã¨ã„ã†æ„å‘³ã§ã¯ã€Autoformalizing ã¨ã„ã†ã€äººé–“ãŒä½œã£ãŸå¹¾ä½•å­¦ã‚’AIãŒè‡ªå‹•è¨¼æ˜ã§ãã‚‹ä½“ç³»ã«ã€ç”ŸæˆAIã‚’ã¤ã‹ã£ã¦ä½œã‚Šç›´ã™ã¨ã„ã†è©¦ã¿ã‚‚ã‚ã£ãŸã€‚ã“ã†ãªã‚‹ã¨äººé–“ã®ç†è§£ãŒã©ã“ã¾ã§ç”ŸæˆAIã«ã¤ã„ã¦ã„ã‘ã‚‹ã‹ã¨ã„ã†ç‚¹ãŒå¿ƒé…ã«ãªã‚‹ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å‹•ä½œç†è§£ã®ã€ŒGraph Gameã€ã‚„ã€LoRaã®åŸç†ã®å¯è¦–åŒ–ãªã©ã€ãã†ã„ã†ã®ã‚‚ç›®ç«‹ã£ãŸæ°—ãŒã™ã‚‹ã€‚å…¨ãåå¯¾ã«ã‚¢ã‚»ãƒ¢ã‚°ãƒ«æ°ã®ã‚ˆã†ã«ã€ãã‚Œã»ã©AIã¯æ ¼å·®æ‹¡å¤§ã«å½±éŸ¿ã—ãªã„ã¨ã„ã†åˆ†æã‚‚ã‚ã£ãŸã€‚å®‰å…¨æ€§ã«é–¢ã—ã¦ã¯ã€RAGã‚’å‰æã¨ã—ãŸãƒãƒƒã‚¯ãƒ‰ã‚¢TrojanRAGã¨ã„ã†ã®ã‚‚å‡ºãŸã€èª¬æ˜æ€§ãŒé«˜ã„ã“ã¨ã¨ãƒãƒƒã‚¯ãƒ‰ã‚¢ã‚’ä»•è¾¼ã¿ã‚„ã™ã„ã¨ã„ã†ã®ã¯è¡¨è£ä¸€ä½“ã€‚ã•ã¦æ—¥æœ¬ã§ã¯ã€äººå·¥çŸ¥èƒ½å­¦ä¼šãŒæµœæ¾ã§é–‹å‚¬ã€å²¡å´å…ˆç”Ÿã®ã‚¹ãƒ©ã‚¤ãƒ‰ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã€ã¯å¿…è¦‹ã§ã™ã€‚ä¸€æ–¹ã€ChatGPTã®RLHFï¼ˆãƒ’ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ï¼‰ãƒ—ãƒ­ã‚»ã‚¹ã®å¤šããŒã€ã‚¢ã‚¦ãƒˆã‚½ãƒ¼ã‚¹ã•ã‚ŒãŸï¼ˆæ¯”è¼ƒçš„äººä»¶è²»ã®å®‰ã„ï¼‰ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãŸã¡ã«ã‚ˆã£ã¦è¡Œã‚ã‚ŒãŸçµæœã§ã€ãªã‚“ã¨ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢è‹±èªã®delveã¨ã„ã†å˜èªãŒç”Ÿç‰©ç³»ã®è«–æ–‡ã«å¤§é‡ã«è¡¨ã‚ŒãŸã¨ã®å ±å‘Šã‚‚ã‚ã£ãŸã€‚ãã‚‚ãã‚‚äººé–“ã®ã»ã†ã‚‚ã€ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã§åˆç†çš„ã«æ€è€ƒã™ã‚‹èƒ½åŠ›ã«èª²é¡ŒãŒã‚ã‚Šã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«æµã•ã‚ŒãŒã¡ã¨ã®æŒ‡æ‘˜ãŒã‚ã‚‹ã®ã§ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã‚’æ±‚ã‚ã™ãã¦ã€åˆç†çš„ã«æ€è€ƒã§ããªã„ç”ŸæˆAIãŒã§ãã‚‹ã‹ã‚‚ã€‚ãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ã®ã€ŒAI in Scienceã€ã™ã§ã«ã€ŒAI for Scienceã€ã¨ã„ã£ã¦ã„ã‚‹æ™‚ä»£ã§ã¯ãªããªã£ãŸã€ã¤ã¾ã‚ŠAIã‚’ä½¿ã‚ãšã«ç§‘å­¦ã®é€²å±•ã¯ãªã„ã€‚ã•ã¦NASAã‹ã‚‰æº€ã‚’æŒã—ã¦ã€å¤§æ°—ç¾è±¡ã‚’äºˆæ¸¬ã™ã‚‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«Auroraã®ç™ºè¡¨ã€10kmãƒ¡ãƒƒã‚·ãƒ¥ã§10æ—¥å¾Œã¾ã§å¤©æ°—äºˆå ±ã§ãã‚‹ã£ã¦ã©ã‚Œãã‚‰ã„ã™ã”ã„ã®ã ã‚ã†ã€‚å…¨ãã®ä½™è«‡ã ãŒã€gpt-4oã®ç™ºè¡¨æ™‚ã«å‚ç…§ã•ã‚ŒãŸæ˜ ç”»"Her"ã®ç›£ç£ã®é›¢å©šã—ãŸå¦»ãŒç›£ç£ã—ãŸ "Lost In Translation"ã®ï¼’ã¤ã‚’æ¯”è¼ƒã—ã€ä¸€éƒ¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå…¨ãå¯¾ç…§çš„ã«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã—ã¦ã„ã‚‹ã¨ã„ã†è©±é¡ŒãŒãƒ†ãƒƒã‚¯ç•Œéšˆã§ä¸€ç¬è©±é¡Œã«ãªã£ãŸã€‚

-  è‡ªåˆ†ãŒã©ã‚Œãã‚‰ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç†è§£ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºã‹ã‚ã‚‰ã‚Œã‚‹ã‚²ãƒ¼ãƒ ã€ŒGraph Gameã€
	- https://gigazine.net/news/20240526-graph-game/
- googleã®æ¤œç´¢xç”ŸæˆAIã«ã¤ã„ã¦ã¯ï¼Œã¡ã‚‡ã£ã¨è©•ä¾¡ãŒã‚¤ãƒã‚¤ãƒãªã‚“ã§ã™ã‚ˆã­ï¼æ²¹ã¨æ°´ã‚’ç„¡ç†ã‚„ã‚Šæ··ãœã‚ˆã†ã¨ã—ã¦ã„ã‚‹æ„ŸãŒã‚ã‚‹ï¼by ä»Šäº•ã•ã‚“
	- https://x.com/ImAI_Eruel/status/1794707281600496111
-  TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models
	- https://arxiv.org/abs/2405.13401
	- RAGã‚’æ‚ªç”¨ã—ãŸãƒãƒƒã‚¯ãƒ‰ã‚¢æ”»æ’ƒã€‚RAGã§ä½¿ç”¨ã™ã‚‹çŸ¥è­˜DBã«ç´°å·¥ãƒ‡ãƒ¼ã‚¿ã‚’æ³¨å…¥ã—ã€ï¼ˆDBã‹ã‚‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ã™ã‚‹ï¼‰ãƒªãƒˆãƒªãƒ¼ãƒã¨DBé–“ã§ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒªãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒˆãƒªã‚¬ãƒ¼ã¨ãªã‚‹PromptãŒå…¥åŠ›ã•ã‚ŒãŸå ´åˆã®ã¿ã€LLMã«æ‚ªæ„ã®ã‚ã‚‹å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã®ã“ã¨ã€‚
- We're now able to run Mixtral 8x22b Q6_K on a $362 CPU with better than human reading speed.
	- https://github.com/Mozilla-Ocho/llamafile/discussions/450
- llama.cpp runs 1.8 times faster than ollama
	- https://x.com/rohanpaul_ai/status/1794470545586635238
- Exploring the Impact of ChatGPT on Wikipedia Engagement
	- https://arxiv.org/pdf/2405.10205
	- Wikipedia remains the crowning achievement of Internet 1.0. It powered the rise of search engines (which depend on it) & generative AI (trained on its data).
- Grokked Transformers are Implicit Reasoners:A Mechanistic Journey to the Edge of Generalization
	- https://arxiv.org/pdf/2405.15071
	- 1) Transformers can learn to implicitly reason, but only through extended training far beyond overfitting, a phenomenon known as grokking.
	- 2) Transformers exhibit different levels of systematicity in generalization across reasoning types: ID generalization is consistently observed, OOD generalization fails for composition but succeeds for comparison tasks.
	- æ±åŒ–å›è·¯å½¢æˆã®ç§˜å¯†ã«è¿«ã‚Šã€ã‚ãŸã‚‰ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æå”±ã—ã¦ã„ã‚‹
-  Phi-3-Tiny-Untrained
	- https://colab.research.google.com/drive/188RpybbauEJKSIRPGL3RZi4Lk66HfBJj
	- This 50M-parameter model reconfigs Phi-3-mini-128k-instruct (3.8B parameters) by following the parameters given by the Super Tiny Language Models from A*STAR.
-  GPT-4ã¯è²¡å‹™è«¸è¡¨ã‹ã‚‰å°†æ¥ã®åç›Šã®ä¼¸ã³ã‚’äºˆæ¸¬ã™ã‚‹ç‚¹ã§äººé–“ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒç ”ç©¶ã«ã‚ˆã‚Šæ˜ã‚‰ã‹ã«
	- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311
- The Wall Street Journal says perplexity outperforms chatgpt, gemini & claud
	- https://x.com/RubenHssd/status/1795108714564706452
- Difyã¨Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’æ§‹ç¯‰ã—ã€è¤‡æ•°ã®LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šã—ã¦AIãŒç¤¾ä¼šã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦è­°è«–ã‚’è¡Œã„ã€ãã®çµæœã‚’è¨˜äº‹ã¨ã—ã¦ç”Ÿæˆã™ã‚‹æ‰‹é †ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
	- https://hamaruki.com/how-to-configure-and-discuss-multiple-agents-using-dify-and-local-llm/
-  How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?
	- https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms
	- In April, there were four major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM.
- Decoder-onlyãªLLMï¼ˆMistral-7Bï¼‰ã‚’text embeddingç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆLoRAï¼‰ã—ã¦MTEBã§SoTAã‚’é”æˆã—ãŸæ–¹æ³•NV-Embedã®ææ¡ˆ
	- https://x.com/s_tat1204/status/1795344530285457626
- Google Search algorithm leaked today.
	- https://x.com/hridoyreh/status/1795394077510517217
-  Autoformalizing Euclidean Geometry
	- https://arxiv.org/abs/2405.17216
	- Can AI transform human mathematics into formal theorems and proofs that machines can verify?
	- This process, known as autoformalization, is a key step towards AI mathematicians. We introduce a neuro-symbolic framework for autoformalization, focusing on Euclidean geometry and combining domain knowledge, SMT solvers, and LLMs.
- Mixtral 8x7B Instruct with AWQ & Flash-Attention-2 in ~24GB GPU VRAM!
	- https://x.com/rohanpaul_ai/status/1795196332166070289
	- With the latest release of AutoAWQ - you can now run Mixtral 8x7B MoE with Flash Attention 2 for blazingly fast inference.
-  Automatic Domain Adaptation by Transformers in In-Context Learning
	- https://arxiv.org/abs/2405.16819
	- å¹¡è°·ã•ã‚“ï¼ˆç†ç ”ç‰¹åˆ¥ç ”ç©¶å“¡ï¼‰ã¨æ¾äº•å…ˆç”Ÿï¼ˆåå¤§ï¼‰ã®ç ”ç©¶ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ã«ãŠã„ã¦ã€è¤‡æ•°ã®ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæ³•ã‚’è¡¨ç¾ã—ã€ã•ã‚‰ã«ãƒ‡ãƒ¼ã‚¿ã«å¿œã˜ã¦é©åˆ‡ãªé©å¿œæ³•ã‚’é¸æŠã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã“ã¨ã‚’ç†è«–ã¨å®Ÿé¨“ã§ç¤ºã—ãŸã‚‚ã®ã§ã™ã€‚
- Training and Finetuning Embedding Models with Sentence Transformers v3
	- https://huggingface.co/blog/train-sentence-transformers
- ã¡ãªã¿ã«Gemini 1.5 Proã§ã¯application/jsonã‚’å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã—ã¦é¸æŠã§ãã¦ä¾¿åˆ©
	- https://ai.google.dev/gemini-api/docs/api-overview?hl=ja#json
- CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval
	- https://arxiv.org/abs/2405.15784
	- Fine-tunes an LLM to ask clarification questions that maximize retrieval success for ambiguous search queries, outperforming heuristic methods and vanilla language models
- æ˜ ç”»herã¨Lost In Translationã®ç›£ç£ã¯é›¢å©šã—ãŸå…ƒå¤«å©¦ã§ã‚ã‚Šã€ãã®ç”»åƒã®ä¸€éƒ¨ã¯ã‚·ãƒ³ã‚¯ãƒ­ã—ã¦ã„ã‚‹ã€‚ã€‚
	- https://x.com/MissSassbox/status/1795205212770119782
	- I had no idea that "Her" and "Lost In Translation" were clapbacks by the directors to each other after their divorce and now I need to watch both
	- Techç•Œã§ã€gpt-4oã®ãƒ‡ãƒ¢ãŒã€ã€æ˜ ç”»herã‚’æ„è­˜ã—ã¦ä½œã‚‰ã‚Œã¦ã„ãŸã“ã¨ã‹ã‚‰ã€ï¼ˆå†ï¼Ÿï¼‰ç™ºè¦‹ã•ã‚ŒãŸé›‘å­¦
- Introducing Transformers Agent 2.0: A Leap Forward in Intelligent Automation
	- https://huggingface.co/blog/Andyrasika/transformer-agents
- æ¾ç”°èªéŒ²ï¼šGemini 1.5 Proã‚’è«–æ–‡ã‚’èª­ã‚€ã®ã«ä½¿ã£ã¦ã¿ãŸã€œè‰¯ã„ã¨ã“ã‚ã¨æ‚ªã„ã¨ã“ã‚
	- https://x.com/npaka123/status/1795568613900062747
- ChatTTS: a powerful voice generation model designed for conversational scenarios
	- https://github.com/2noise/ChatTTS
	- https://huggingface.co/2Noise/ChatTTS
- ã€ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’å«Œã†äººãŸã¡ã€€ç§‘å­¦å¦å®šè«–è€…ã¯ä½•ã‚’è€ƒãˆã€ã©ã†èª¬å¾—ã§ãã‚‹ã®ã‹ï¼Ÿã€by æš¦æœ¬å…ˆç”Ÿ
	- https://x.com/rkmt/status/1795636068752212063
	- ãŠãŠãƒ¼ã€‚ã—ã‹ã—ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã§æ€è€ƒã™ã‚‹äººé¡ã¯ã‚€ã—ã‚å°‘æ•°æ´¾ã‹ã‚‚ã—ã‚Œãªã„..(System1æ€è€ƒ=Fast Thinking > System2æ€è€ƒ Slow Thinking) ã€‚ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚ˆã‚Šã‚‚ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãŒå„ªå…ˆã™ã‚‹.
- é€²åŒ–çš„ãƒãƒ¼ã‚¸ã«ã‚ˆã£ã¦ç›¸å½“å¼·ãã†ãªãƒ¢ãƒ‡ãƒ«ã€Umievo-itr012-Gleipnir-7BãŒç”Ÿã¾ã‚Œã¾ã—ãŸã€‚3å›ElyzaTasks100ã§è©•ä¾¡ã—ãŸå¹³å‡ã‚¹ã‚³ã‚¢ã¯3.91ï¼ã€€by ã†ã¿ã‚†ãã•ã‚“
	- https://huggingface.co/umiyuki/Umievo-itr012-Gleipnir-7B
	- ãƒãƒ¼ã‚¸ã«ä½¿ç”¨ã•ã›ã¦ã„ãŸã ã„ãŸã®ã¯Japanese-Starling-ChatV-7Bã€Ninja-v1-RP-expressive-v2ã€Vecteus-v1ã€Japanese-Chat-Umievo-itr004-7bã®ï¼”ã¤ã§ã™ã€‚å„ãƒ¢ãƒ‡ãƒ«åˆ¶ä½œè€…ã®Aratakoã•ã‚“ã€Bakuã•ã‚“ã€Local-Novel-LLM-projectã®ã¿ãªã•ã¾ã«æ„Ÿè¬ã—ã¾ã™ã€‚ãã‚Œã‹ã‚‰å•é¡Œè§£æ±ºã®ãã£ã‹ã‘ã‚’ãã‚ŒãŸHoly-foxã•ã‚“ã«æ„Ÿè¬ã—ã¾ã™ã€‚
- AI versus 100,000 humans in creativity in this careful study using the Divergent Association Test (a well-validated measure, but all measures of creativity have flaws)
	- https://x.com/emollick/status/1795830809217454536
	- https://www.researchgate.net/publication/380820358_Divergent_Creativity_in_Humans_and_Large_Language_Models
	- GPT-4 wins. Better prompting can further improve performance & diversity of ideas.
	- ã¤ã„ã«å‰µé€ æ€§ã§ã‚‚GPT-4ãŒã€ï¼ˆæ™®é€šã®ï¼‰äººé–“ã‚’æŠœã„ãŸã®ã‹ã€‚ã€‚
- Gemini 1.5 Flashã¯Claude 3 Opusã«åŒ¹æ•µã—ãªãŒã‚‰ã€ã‚³ã‚¹ãƒˆã¯100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚ŠãŸã£ãŸã®55å††
	- https://x.com/gijigae/status/1795743286533255285
- Announcing Codestral: our first-ever code model
	- https://chat.mistral.ai/chat
	- "Write me a function that computes fibonacci in Rust"
- OllamaãŒCodestralã«å¯¾å¿œ
	- https://ollama.com/library/codestral
- "Science in the Age of AI - How AI is changing the nature and method of scientific research,"
	- https://royalsociety.org/-/media/policy/projects/science-in-the-age-of-ai/science-in-the-age-of-ai-report.pdf
	- è‹±å›½ãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼
	- Generative AI tools can assist the advancement of scientific research.
-  Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs
	- https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms
	- 1. You can extract out a knowledge graph according to a set of extractors. These extractors include defining a pre-defined schema of entities/relationships/properties, defining a set of node relationship with llama_index constructs, or implicitly figuring out the schema using an LLM.
	- 2. You can now query a knowledge graph with a huge host of different retrievers that can be combined: keywords, vector search, text-to-cypher, and more. 3. You can include the text along with the entities/relationships during retrieval 4. You can perform joint vector search/graph search even if your graph store doesnâ€™t support vectors! Weâ€™ve created robust abstractions to plug in both a graph store as well as a separate vector store. 5. You have full customizability: Weâ€™ve made it easy/intuitive for you to define your own extractors and retrievers.
- The structure of the EU AI Office
	- https://x.com/LuizaJarovsky/status/1795775192347627857
	- The â€œExcellence in AI and Roboticsâ€ unit
	- The â€œRegulation and Complianceâ€ unit 
	- The â€œAI Safetyâ€ unit
	- The â€œAI Innovation and Policy Coordinationâ€ unit
	- The â€œAI for Societal Goodâ€ unit 
	- The Lead Scientific Advisor
	- The Advisor for International Affairs
- è‹±å›½ã®ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ã€Royal Societyã‚‚ã€ŒAI for Scienceã€ã§ã¯ãªãEUã¨åŒã˜ã€ŒAI in Scienceã€ã€‚ãƒ¬ãƒãƒ¼ãƒˆã¯ã‹ãªã‚Šå……å®Ÿã—ã¦ã„ã‚‹ by maruyamaã•ã‚“
	- https://x.com/rmaruy/status/1795967400502006026
- Mamba, Griffin, RWKV, RetNet, Recurrent Gemma- 2024 is the year of gated linear RNNs! What's their secret sauce?
	- https://x.com/ItamarZimerman/status/1796181061984030914
- CRDSã®æ–°ä½œãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«ã€æ¬¡ä¸–ä»£AIãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶é–‹ç™ºã€ãŒã‚ã¡ã‚ƒãã¡ã‚ƒã„ã„ä»•äº‹ã§ä¸€æ°—è¦‹ã—ãŸã€‚
	- https://www.jst.go.jp/crds/report/CRDS-FY2023-SP-03.html
	- https://x.com/resnant/status/1796181056283898332
	- LLMå«ã‚æœ€è¿‘ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚„ç”Ÿæˆç³»AIã®å‹•å‘ã¨è«¸èª²é¡Œã‚’æŠ€è¡“çš„ã«ã‚‚æ˜ã‚Šä¸‹ã’ã¦ã¦ã€ã„ã‚ã„ã‚ãªç«‹å ´ã®äººã«å‚è€ƒã«ãªã‚‹ã¨æ€ã†
- [LoRA] by Hand
	- https://x.com/ProfTomYeh/status/1796169087665557729
	- How does LoRA reduce the number of trainable parameters?
-  Donâ€™t Believe the AI Hype
	- https://www.project-syndicate.org/commentary/ai-productivity-boom-forecasts-countered-by-theory-and-data-by-daron-acemoglu-2024-05?
	- ãƒ€ãƒ­ãƒ³ãƒ»ã‚¢ã‚»ãƒ¢ã‚°ãƒ«æ°ã®åˆ†æã§ã¯ã€AIã«ã‚ˆã£ã¦å½±éŸ¿ã‚’ã†ã‘ã‚‹äººé–“ã®ã‚¿ã‚¹ã‚¯ã¯4.6ï¼…ã§ã€å‘ã“ã†åå¹´ã®AIã«ã‚ˆã‚‹å…¨è¦ç´ ç”Ÿç”£æ€§ã®å‘ä¸Šã¯0.66%ã«ã¨ã©ã¾ã‚‹ã€‚AIã«ã‚ˆã‚‹ç§‘å­¦ç™ºè¦‹ã¯ç›´è¿‘ã§ã¯çµŒæ¸ˆã«ã•ã»ã©å½±éŸ¿ã—ãªã„ã€‚ã‹ã¤ã¦ã®è‡ªå‹•åŒ–æŠ€è¡“ã«æ¯”ã¹ã‚‹ã¨æ ¼å·®æ‹¡å¤§åŠ¹æœã¯å°ã•ã„ãŒè¦åˆ¶ã¯å¿…è¦
-  Aurora: A Foundation Model of the Atmosphere
	- https://arxiv.org/abs/2405.13063
	- NASA has created a new foundation model for geospatial data.
	- Create 5-day air pollution predictions in < 1 minute 
	- Create 10-day weather forecasts at ~10km resolution 
	- Assess the chemical make up of the atmosphere
- llm.cã‚’ä½¿ã†ã¨GPT-2ã‚’$20ã§2æ™‚é–“ä»¥å†…ã«æ§‹ç¯‰å¯èƒ½ï¼Ÿï¼Ÿ
	- https://x.com/overlast/status/1796028138616422535
- Hereâ€™s a great guide teaching you how to construct knowledge graphs using LLMs that adhere to a pre-defined schema - using purely local models
	- https://x.com/llama_index/status/1796198853764595725
- ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã€ byã€€å²¡å´ã•ã‚“ã€€@ JSAI2024
	- https://speakerdeck.com/chokkan/jsai2024-tutorial-llm
	- ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¬›æ¼”ã‚’è¡Œã„ã¾ã—ãŸã€‚äº‹å‰å­¦ç¿’ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã€è©•ä¾¡ã®ï¼”éƒ¨æ§‹æˆã§ã€æœ€è¿‘ã®ç ”ç©¶å‹•å‘ã‚„çŸ¥è¦‹ã‚’ç´¹ä»‹ã—ã¾ã—ãŸã€‚
- Prompt Engineering for Generative AI
	- https://www.amazon.com/gp/product/B0D4FBPLX1?&linkCode=sl1&tag=kirkdborne-20&linkId=17812cf95726cdbbe7b0c29f94f4bce7&language=en_US&ref_=as_li_ss_tl
	- With this book, you'll gain a solid foundation in generative AI, including how to apply these models in practice. When first integrating LLMs and diffusion models into their workflows, most developers struggle to coax reliable enough results from them to use in automated systems.
- LLMs achieve adult human performance on higher-order theory of mind tasks
	- https://huggingface.co/papers/2405.18870
	- LLMs achieve adult human performance on higher-order theory of mind tasks 
	- This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM); the human ability to reason about multiple mental and emotional states in
	- We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences
- ç”ŸæˆAIã«ã‚ˆã‚‹ã€Œæ…£ç”¨è¡¨ç¾ã®ã€ä¹—ã£å–ã‚Šã€ã€ã¨ã€ãã®æ ¹åº•ã«ã‚ã‚‹åˆ¥ã®å•é¡Œã¨ by TJOã•ã‚“
	- https://tjo.hatenablog.com/entry/2024/05/31/171000
	- ã€ŒChatGPTã«å­¦è¡“è«–æ–‡ã‚’ï¼ˆè‹±èªã§ï¼‰æ›¸ã‹ã›ã‚‹ã¨"[delve](https://eow.alc.co.jp/search?q=delve)"ã®ã‚ˆã†ãªæ™®æ®µä½¿ã‚ãªã„ã‚ˆã†ãªå˜èªãŒå¤šãä½¿ã‚ã‚Œã‚‹ã®ã§ãƒãƒ¬ã‚„ã™ã„ã€ã¨ã„ã†è©±ãŒ[SNS](https://d.hatena.ne.jp/keyword/SNS)ä»¥ä¸‹å„æ‰€ã§é »ç¹ã«å™‚
	- ChatGPTã®RLHFï¼ˆãƒ’ãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ï¼‰ãƒ—ãƒ­ã‚»ã‚¹ã®å¤šããŒã€ã‚¢ã‚¦ãƒˆã‚½ãƒ¼ã‚¹ã•ã‚ŒãŸï¼ˆæ¯”è¼ƒçš„äººä»¶è²»ã®å®‰ã„ï¼‰ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãŸã¡ã«ã‚ˆã£ã¦è¡Œã‚ã‚ŒãŸçµæœã§ã‚ã‚‹ã€ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚ãã‚‚ãã‚‚ã€ä¾‹ãˆã°"delve"ã¨ã„ã†å˜èªã¯ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢è‹±èªã§ã¯ãƒ“ã‚¸ãƒã‚¹ãƒ•ãƒ¬ãƒ¼ã‚ºã®ä¸­ã§ã¯æ¯”è¼ƒçš„é »ç¹ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãã†ã§*3ã€ãã‚Œã‚‰ã®ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢è‹±èªã«ã‚ˆã‚‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµæœãŒChatGPTã®å‡ºåŠ›ã«å½±éŸ¿ã—ã¦ã„ã‚‹ã€ã¨ã„ã†ã“ã¨ã®ã‚ˆã†ã§ã™*4ã€‚
- The new Anthropic prompt engineering tool is incredible.
	- https://x.com/dr_cintas/status/1796577510773379479
	- You just need to write your goal and Claude will generate an optimized prompt instantly.
-  An entirely open-source AI code assistant inside your editor
	- https://ollama.com/blog/continue-code-assistant
	- ã¤ã¾ã‚Šã€ollamaã‚’ã¤ã‹ã£ã¦ã€ã‚ãªãŸã®å¥½ã¿ã®ã‚¨ãƒ‡ã‚£ã‚¿ã«Code assistanceã‚’ã¨ã„ã†è©±
- æœ€è¿‘ã®7Bå°å‹æ—¥æœ¬èªLLMã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãªã‚Œã‚‹ã®ã‹ï¼Ÿ
	- https://soysoftware.sakura.ne.jp/archives/3934
- 


## 5/27

ã•ã¦ä»Šé€±ã¯ã€Microsoftã®Build2024ãŒé–‹å‚¬ã€gpt-4oã‚’çµ„ã¿è¾¼ã‚“ã Copilot+PCã¨ã„ã†intelã¯ã„ã£ã¦ãªã„PCã®ã»ã‹ã«ã€èª°ã§ã‚‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚Œã‚‹Copilot é€²åŒ–çš„ã®æ›´æ–°ã‚„ã€äººã®ä»£ã‚ã‚Šã«ä¼šè­°é€²è¡Œã‚’ã—ã¦ãã‚Œã‚‹Team Copilotã€ã•ã‚‰ã«ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢é–‹ç™ºè‡ªå‹•åŒ–ã®ã€ŒDevinã€ã®ä¼šç¤¾ã¨ã®ææºãªã©ã€ç›®ç™½æŠ¼ã—ã€‚ã¾ã‚æ—©é€Ÿã€ã‚²ãƒ¼ãƒ ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹copilot assistantã®ãƒ‡ãƒ¢ã‚’Gemini 1.5 Flashã§ã€ã•ã‚‰ã«ãƒãƒªã‚ªã‚²ãƒ¼ãƒ ã§å†ç¾ã§ããŸã¨ã®å€‹äººã®å ±å‘Šã‚‚ã‚ã‚Šã¾ã—ãŸã€‚ã•ã¦æ¥æœˆã®Appleã® WWDC24ã¯ã©ã†ãªã‚‹ã€‚åŸºç›¤æŠ€è¡“ã§ã¯ã€ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã§GPT-4ã¯54%ã®ç¢ºç‡ã§äººé–“ã ã¨åˆ¤æ–­ã•ã‚ŒãŸã¨ã„ã†ã®ã¯ã€ã‚‚ã†é©šã‹ãªã„ã€AIã«ã‚ˆã‚‹è©æ¬ºã«ã‚ã‚ãªã„ã‚ˆã†ã«å¿ƒãŒã‘ã¦ã‚‚ç„¡é§„ã¨ã„ã†æœªæ¥ãŒã€ã€ã€‚ã‚€ã—ã‚ã€ã€ŒLLMãŒãƒãƒ£ãƒƒãƒˆUIã«å‘ªã‚ã‚Œã¦ã„ã‚‹ã€ã¨ã„ã†è¨˜äº‹ã‚‚ã‚ã£ãŸãŒã€ã‚‚ã‚„ã¯LLMã®ç™ºå±•ã¯äººé–“ãŒå¾‹é€Ÿã—ã¦ã„ã¦é ­æ‰“ã¡ã«ãªã£ã¦ã„ã‚‹ã€‚ä¸€æ–¹Anthropicã®Claude3 Sonetã«å¯¾ã™ã‚‹ç‰¹å¾´æŠ½å‡ºã®è«–æ–‡ã€ã¤ã¾ã‚Šãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆä¸Šã«LLMã®æ€§è³ªã‚ã‚‹ã„ã¯ç‰¹å¾´ã‚’ç¤ºã™å ´æ‰€ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰ã€å®‰å…¨æ€§ã®åˆ†æã§å½¹ã«ç«‹ã¤ã¨ã„ã£ã¦ã„ã‚‹ãŒã€é€†ã«ç‰¹å®šã®ç®‡æ‰€ã‚’ç‰¹åˆ¥ã«æ´»æ€§åŒ–ã•ã›ã‚Œã°ã€ä¾‹ãˆã°ã€ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚²ãƒ¼ãƒˆãƒ–ãƒªãƒƒã‚¸ä¸€æŠ¼ã—ã®LLMãŒçˆ†èª•ã™ã‚‹ã¨ã®ã“ã¨ã€‚ã„ã‚„ã¾ã•ã«ã‚‚ã‚åˆƒã®å‰£ã¨ãªã‚‹é‡è¦ãªæŠ€è¡“ã€‚çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®RAGã‚‚ã‚¢ãƒ„ã‚¤ãŒã€GraphRAGã¨ã„ã†ç”»åƒåŒ–ã—ãŸçŸ¥è­˜ã‚°ãƒ©ãƒ•ã«å¯¾ã™ã‚‹RAGã¨ã„ã†æŠ€è¡“ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã ã¨ãã†ã„ã†ã“ã¨ã‚‚ã§ãã‚‹ã®ã‹ã€‚ä»Šäº•ã•ã‚“ã®GPT-4oã‚’ç ”ç©¶è€…è¦–ç‚¹ã§ã€Œæ™‚ä»£ã®è»¢æ›ç‚¹ã€ã¨è§£èª¬ã—ãŸè¨˜äº‹ã®ã‚·ãƒªãƒ¼ã‚ºã¯æ°—ã«ãªã‚‹ãŒç™»éŒ²ãŒå¿…è¦ãªã®ã‹ã€‚GUILDã®æ·±æ´¥ã•ã‚“ã®ã€æ¨ªé ˆè³€å¸‚ã®æœªå®Œæˆã®ãŠæ‚©ã¿ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€‚ä¸å®Œå…¨ã§ã‚‚ãƒ™ãƒ¼ã‚¿å…¬é–‹ã¨ã„ã†ã‚ã‚Šã«ã‚„ã£ã±ã‚ˆãã§ãã¦ã„ã‚‹ã€‚ãã®æ·±æ´¥ã•ã‚“ãŒã€ç”ŸæˆAIæ™‚ä»£ã«å¤§äº‹ãªã‚¹ã‚­ãƒ«ã¯ã€ã€Œã‚„ã‚Šç¶šã‘ã‚‹èƒ½åŠ›ã€ã€ã„ãã‚‰ç”Ÿæˆï¼¡ï¼©ãŒå„ªã‚Œã¦ã„ã¦ã‚‚ã‚ã’ãªã„ã“ã¨ãŒå¤§åˆ‡ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚‚ç›¸å¤‰ã‚ã‚‰ãšã‚¢ãƒ„ã‚¤ï¼ã€‚ä»Šé€±ã‚‚ã€Mistral v0.3ãŒãƒªãƒªãƒ¼ã‚¹ã€èªå½™æ•°ã‚‚å¢—ãˆã¦ã€è¦‹é•ãˆã‚‹ãã‚‰ã„æ—¥æœ¬èªèƒ½åŠ›ãŒå¼·åŒ–ã•ã‚Œã€function callingã¸ã‚‚å¯¾å¿œã€ollamaã‚‚raw modeã§funtion callingã¸å³è¿½å¾“ã€‚ä¸€æ–¹ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«phi3-visionã‚‚å«ã‚ã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸphi3-small,mediumã€phi3-mediumãŒMMLUã‚¹ã‚³ã‚¢ã¯Llama3-70Bä¸¦ã¿ã«é«˜æ€§èƒ½ã§ã‚ã‚‹ã¨ã„ã†ã“ã¨ã ãŒã€é‡å­åŒ–ã§ãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ãŸã®ã‹Ollamaã¸ã®çµ„ã¿è¾¼ã¿ã¯ã†ã¾ãã„ã£ã¦ãªã„æ¨¡æ§˜ã€‚Transformers.js ã¨ONNX Runtime Webã®çµ„ã¿åˆã‚ã›ã¨ã„ã†ã®ã‚‚ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã®å”åŠ›ãªåŠ©ã£äººã‹ã€‚CohereãŒå¤šè¨€èªæŒ‡å‘ã®ã‚ªãƒ¼ãƒ—ãƒ³LLMã§ã‚ã‚‹Aya 23 ã® 8B ã¨35BãŒãƒªãƒªãƒ¼ã‚¹ã€æ—¥æœ¬èªå¼·ãã†ã€‚ã—ã‹ã—ã€Phi-3ã¯ã€ã€Œæœ€ã‚‚æœ‰èƒ½ã§è²»ç”¨å¯¾åŠ¹æœã®SML (Small Language Model)ã€ã£ã¦ã„ã†ã‚“ã (Small LLMã®ã»ã†ãŒã‹ã£ã“ã‚ˆã„ã®ã«)ã€‚ ãã‚Œã«ã—ã¦ã‚‚ DeepSeekV2 ã€ã‚ã¾ã‚Šã«æ€§èƒ½ãŒé«˜ã„ã®ã§ä¸­å›½ã§ã®ç«¶åˆã®ã‚µãƒ¼ãƒ“ã‚¹æ–™ã‚’1%æŠ¼ã—ä¸‹ã’ãŸï¼ˆæŠ•ã’å£²ã‚Šé–‹å§‹ï¼Ÿï¼‰ã¨ã®ã“ã¨ã€‚ChatVectorã€7Bãƒ¢ãƒ‡ãƒ«ã®FineTuningçµæœã‚’70Bã«è»¢ç§»ã•ã›ã¦æ€§èƒ½å‘ä¸Šã—ãŸã‚Šã€LLaVAã®æ—¥æœ¬èªåŒ–ãªã©ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã‚‚ãã®èƒ½åŠ›ã‚’ãµãã‚ã¦èªçŸ¥ã‚„åˆ©ç”¨ãŒå¢—ãˆã¦ããŸã€‚transformersãŒv4.41.0ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚Œã¦ggufã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã«ãªã£ãŸã®ã‚‚ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMå‹¢ã«ã¯æœ—å ±ã€‚EUã®AIæ³•ãŒæœ€çµ‚åˆæ„ã€ç”Ÿæˆï¼¡ï¼©ã®è¦åˆ¶ã‚‚ç››ã‚Šè¾¼ã¿æ¸ˆã¿ã€‚ä¸€æ–¹OECDã¯AIãƒªã‚¹ã‚¯ã«é–¢ã™ã‚‹ç”¨èªã‚’æ•´ç†ã—ã€ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®é‡å¤§ã•ã«ãƒã‚¶ãƒ¼ãƒ‰ãŒèµ·ã“ã‚‹ç¢ºç‡ã‚’åŠ å‘³ã—ãŸã‚‚ã®ãŒãƒªã‚¹ã‚¯ã®ãƒ¬ãƒ™ãƒ«ã«ãªã‚‹ã¨ã®ã“ã¨ã€‚è‹±å›½ã®ã€ŒSafeguarded AIãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¯ã€å®‰å…¨æ€§ã®ãŸã‚ã«æ•°ç†è«–ç†å­¦ã‚„åœè«–ã‚’åˆ©ç”¨ã™ã‚‹ã€åŒã˜safe guardã§ã‚‚æ¯’ã«ã¯æ¯’ã‚’ã¨ã„ã†ã“ã¨ã§guardè‡ªä½“ã‚’LLMã§å®Ÿç¾ã™ã‚‹ãƒ¡ã‚¿ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨çœŸé€†ã§é¢ç™½ã„ã€‚

-  Unleashing the Power of Knowledge Graphs in Retrieval Augmented Generation (RAG): Step by Step Instruction
	- https://medium.com/@transformergpt/unleashing-the-power-of-knowledge-graphs-in-retrieval-augmented-generation-rag-step-by-step-84c2adc66c1c
	- This is a neat resource by Jayita B. on teaching you how to not only build an advanced RAG indexing/query pipeline, but also turn it into a full-stack application with rapid respons
- OECD (2024), "Defining AI incidents and related terms",
	- https://www.oecd-ilibrary.org/science-and-technology/defining-ai-incidents-and-related-terms_d1a8d965-en
	- OECDã®WGãŒä½œã£ã¦ã„ã‚‹AIãƒªã‚¹ã‚¯ã‚’åˆ†é¡ã™ã‚‹ãŸã‚ã®ç”¨èªæ•´å‚™ã®ãƒ¬ãƒãƒ¼ãƒˆã€‚èµ·ã“ã‚Šã†ã‚‹è¢«å®³ã‚’ãƒã‚¶ãƒ¼ãƒ‰ã€èµ·ã“ã£ãŸè¢«å®³ã‚’ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã¨å‘¼ã³ã€ãã®é‡å¤§ã•ã‚’è€ƒæ…®ã€‚ãƒã‚¶ãƒ¼ãƒ‰ã«èµ·ã“ã‚‹ç¢ºç‡ã‚’åŠ å‘³ã—ãŸå…¨ä½“ãŒAIãƒªã‚¹ã‚¯ã¨ãªã‚‹
- Text-to-SQL - fully local edition
	- https://diptimanrc.medium.com/text2sql-opensource-duckdb-nsql-7b-with-ollama-and-llamaindex-on-local-setup-6f266f78bc4f
	- The latest local LLMs are not only capable of RAG synthesis, but also querying structured databases. Diptiman Raichaudhuri has a great tutorial on how to build a fully local text-to-SQL setup, letting you query local databases without an internet connection.
- Run Mixtral 8x7B-on a free-tier Google Colab with AQLM-2bit quantization
	- https://www.youtube.com/watch?v=6ikUpJcDrPs&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=32
- The theoretical minimum series by Leonard Susskind and Art Friedman
	- https://x.com/PhysInHistory/status/1792020784854311205
- Chat Vectorã§LLaVAã‚’æ—¥æœ¬èªå¯¾å¿œã•ã›ã‚‹
	- https://zenn.dev/toshi_456/articles/0166a6eaa81c7b
	- LLaVAã¯å¤§ããVision Encoderã€Vision Projectorã€LLMã¨ã„ã†3ã¤ã®éƒ¨å“ã‹ã‚‰ã§ãã¦ã„ã¾ã™ãŒã€LLMã®éƒ¨åˆ†ã ã‘ä¸Šè¨˜ã®ã‚ˆã†ã«é‡ã¿ã‚’åŠ æ¸›ç®—ã—ã¾ã™
	- ä»Šå›ä½¿ç”¨ã™ã‚‹LLaVAã®é‡ã¿ã¯[liuhaotian/llava-v1.5-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b)ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ¼ã‚¹ã®LLMã¯[meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)ã§ã™ã€‚
	- Chat Vectorã‚„ãã®ä»–ã®ãƒãƒ¼ã‚¸æ‰‹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€è‹±èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ã€å­¦ç¿’ã•ã›ã‚‹ã¨ã„ã†æ‰‹é–“ãŒå¿…è¦ãªããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã¯ã‚ã‚ŠãŒãŸã„ãªã¨æ„Ÿã˜ã¾ã—ãŸã€‚
- æ¨ªé ˆè³€å¸‚ã§ã€æœªå®Œæˆã®ãŠæ‚©ã¿ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ by æ·±æ¾¤ã•ã‚“
	- https://www.city.yokosuka.kanagawa.jp/0835/nagekomi/20240520_soudanbot_nyanpei.html
	- ã‚ãˆã¦æœªå®Œæˆã®ãƒœãƒƒãƒˆã‚’å…¬é–‹ã—ã¦ã€åºƒãä¸å…·åˆã‚’ã‚ã¤ã‚ã‚‹å®Ÿé¨“ã§ã™ï¼ï¼
	- ã•ã™ãŒã®ã§ãå‰ï¼
- å††ã®å®ŸåŠ›ã¨æ—¥æœ¬ä¼æ¥­ã®é€šè²¨æˆ¦ç•¥ï¼ˆé…ä»˜è³‡æ–™ãƒ»å‹•ç”»é…ä¿¡
	- https://www.youtube.com/watch?v=reLhpQg9muo
- ã€Œkagglehub ã‚’ä½¿ã£ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« gemma ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«å…±æœ‰ã€
	- https://www.kaggle.com/code/makimakiai/kagglehub-gemma
	- Kaggleã®ãƒãƒ¼ãƒˆã§éŠ…ãƒ¡ãƒ€ãƒ«ã‚²ãƒƒãƒˆã—ãŸï¼å¬‰ã—ã„ï¼
		- https://x.com/hAru_mAki_ch/status/1792105063022018835
- Deep Dive on Accumulated Local Effect Plots (ALEs) with Python
	- https://towardsdatascience.com/deep-dive-on-accumulated-local-effect-plots-ales-with-python-0fc9698ed0ee
	- ALEs give interpretations that are robust to multicollinearity.
- è‹±å›½æ”¿åºœãŒ100å„„å††è¶…ã‚’æŠ•ã˜ã‚‹ã€ŒSafeguarded AIãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã¨ã¯
	- https://www.aialign.net/blog/20240520-takatsuki
	- æœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ãŠã„ã¦AIã‚·ã‚¹ãƒ†ãƒ ã®å®‰å…¨æ€§ã®è¨¼æ˜å¯èƒ½æ€§ã®åœŸå°ã¨ãªã‚‹ç†è«–ï¼ˆç‰¹ã«TA1.1ã§æ‰±ã‚ã‚Œã‚‹å†…å®¹ï¼‰ã«ã¯ã€æ•°ç†è«–ç†å­¦ã‚„åœè«–ã¨ã„ã£ãŸåˆ†é‡ãŒé‡è¦ãªä½ç½®ã‚’å ã‚ã‚‹ã“ã¨ãŒäºˆå®šã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã‚‰ã®åˆ†é‡ã®ç ”ç©¶è€…ã®å”åŠ›ãŒå¿…è¦ã¨ã•ã‚Œã¦ã„ã¾ã™
- Copilot + PC by Nadella
	- https://x.com/satyanadella/status/179261785138542602
	- Introducing Copilot+ PCsâ€”the fastest, most AI-ready Windows PCs ever built.
		- Powered by new NPU (40+ trillion operations per second)
		- Rearchitected Windows 11 
		- 58% faster than Macbook Air M3 
		- Copilot shipping with Windows 
		- Copilot built into Settings, files, notifications 
		- Powered by GPT-4o
- LangChainã«Obsidianã®ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒã‚ã‚‹ï½ã€‚Obsidianã®ãƒ¡ãƒ¢ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã—ã¦RAGã§ãã¦ã—ã¾ã†ï½
	- https://www.youtube.com/watch?v=E-CNrXhSvLg
- The Illustrated Stable Diffusion	
	- https://jalammar.github.io/illustrated-stable-diffusion/
- Google has released Gemini 1.5 Flash.
	- https://x.com/dr_cintas/status/1792572374300188752
	- An AI model optimized for speed and efficiency, with multimodal reasoning and an impressive 1M context window!
-  AWSã€ä¸€èˆ¬æä¾›é–‹å§‹ã—ãŸç”ŸæˆAIã‚µãƒ¼ãƒ“ã‚¹ã€ŒAmazon Qã€ã€ãŠã‚ˆã³ã€ŒBedrockã€ã¨ä»Šå¾Œã®æˆ¦ç•¥ã‚’èª¬æ˜
	- https://internet.watch.impress.co.jp/docs/news/1592518.html?ref=smartnews
- The theory of mindâ€”the ability to track a person's mental stateâ€”is tested comparing humans vs GPT-4 and LLaMA2 large language models
	- https://www.nature.com/articles/s41562-024-01882-z
- GeminiãŒYouTubeå‹•ç”»ã‚’ä¸€ç¬ã§è¦ç´„ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸï¼ˆã—ã‹ã‚‚ç„¡æ–™
	- https://www.lifehacker.jp/article/2405-use-gemini-summarize-youtube-videos-free/
	- æœ¬å½“ã ï¼
- è¦³å¯Ÿã‚¹ã‚±ãƒ¼ãƒ«å‰‡ã¯ã€LLMã®æ¨™æº–çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ€§èƒ½ã‹ã‚‰æ±‚ã‚ã‚‰ã‚ŒãŸä¸»æˆåˆ†ï¼ˆ3ã¤ç¨‹åº¦ï¼‰ã‚’ç”¨ã„ã¦è¤‡é›‘ãªå¾Œç¶šã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã‚’é«˜ç²¾åº¦ã§äºˆæ¸¬ã§ãã‚‹æ³•å‰‡ã€€by å²¡é‡åŸã•ã‚“
	- https://arxiv.org/abs/2405.10938
- BREAKING: Council of Europe adopts 1st international treaty on AI. Here's what you need to know:
	- https://x.com/LuizaJarovsky/status/1792224914646200512
-  Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings
	- https://arxiv.org/abs/2405.10938
- è«–æ–‡ãƒ¡ãƒ¢: Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models b y ã¯ã¡ã•ã‚“
	- https://note.com/hatti8/n/nb61b4935c793?sub_rt=share_pb
	- GoogleãŒå…ˆé€±å‡ºã—ãŸLLMã®è‡ªå·±æ”¹å–„æ‰‹æ³•ã§ã‚ã‚‹ReSTEMã«ã¤ã„ã¦ã€ãƒ¡ãƒ¢ã‚’æ›¸ãã¾ã—ãŸã€‚
	- åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ‰‹æ³•ã¨ã—ã¦ã©ã†ã‹ã¨ã„ã†è¦–ç‚¹ã§æ›¸ã„ã¦ã„ã¾ã™
- på€¤å§«ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ç·¨ï¼‰
	- https://x.com/spine_surgeon_/status/1792767885615759746
	- ã€Œãƒãƒªã‚ªã¸ã€å®Ÿé¨“çµæœã„ã„æ„Ÿã˜ã§ã™ï¼ã„ã„æ„Ÿã˜ãªã‚“ã§ã™ã‘ã©ã€æœ‰æ„å·®ã§ã‚‹ã¾ã§ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—ã‚„ã—ã¦ã¿ã¦ãã ã•ã„ã€‚æœ‰æ„å·®å‡ºã‚‹ã¾ã§é€£çµ¡ã¯ä¸è¦ã§ã™ã€‚ãƒ”ãƒ¼ãƒã‚ˆã‚Šã€‚ã€
- People cannot distinguish GPT-4 from a human in a Turing test
	- https://arxiv.org/abs/2405.08007
	- AIã®äººé–“ã‚‰ã—ã•ã‚’æ¸¬ã‚‹ãƒ†ã‚¹ãƒˆã§ä¸–ç•Œä¸€æœ‰åãªãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã§ã™ãŒï¼Œã•ã‚“ã–ã‚“ã€Œã‚‚ã†AIã¯ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆçªç ´ã§ãã‚‹ã‚„ã‚ã€ã¨è¨€ã‚ã‚Œã¦ãŸã®ã‚’çœŸé¢ç›®ã«åˆ†æã—ãŸè«–æ–‡ãŒå‡ºã¾ã—ãŸ
	- çµè«–ã¯ã€Œç¾åœ¨ã®GPT-4ãªã©ã®æœ€å…ˆç«¯AIã¯ï¼Œãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’çªç ´å¯èƒ½ã§ã‚ã‚Šï¼Œäººé–“ã¯ã‚‚ã¯ã‚„äººã¨AIã‚’ä¼šè©±ã®ã¿ã‹ã‚‰åˆ¤å®šã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€ã¨ã„ã†ã‚‚ã®ã§ã™ï¼GPT-4ã¯54%ã®ç¢ºç‡ã§äººé–“ã ã¨åˆ¤æ–­ã•ã‚ŒãŸæ¨¡æ§˜
- æœ€è¿‘ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒã‚¢ãƒ„ã„ã‚‰ã—ã„
	- https://soysoftware.sakura.ne.jp/archives/3903
	- GPTã®APIé«˜ã„å•é¡Œ ï¼† OpenAIãŒAIãƒ™ãƒ³ãƒãƒ£ãƒ¼çš†æ®ºã—ã«ã—ã¦ã—ã¾ã†å•é¡Œ
	- ãƒ­ãƒ¼ã‚«ãƒ«LLMæ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè‰²ã€…ã‚ã‚‹
		- Llma.cpp, ollama, vLLM
	- å¼·åŠ›ãªå¤§å‹ã‚ªãƒ¼ãƒ—ãƒ³ãªAIãƒ¢ãƒ‡ãƒ«ãŒå…¬é–‹ã•ã‚Œã¯ã˜ã‚ã¦ã‚‹
		- Command R+ï¼ˆéå•†ç”¨åˆ©ç”¨ï¼‰ã‚„Llama3-70Bã€DeepSeek-V2
	-  å°å‹ã®AIãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã®æ½®æµ
		- Mistral-7Bãƒ™ãƒ¼ã‚¹ã€ChatVectorã€
	-  å€‹äººã§ã‚‚AIé–‹ç™ºç«¶äº‰ã«é£Ÿã„è¾¼ã‚ã‚‹å¯èƒ½æ€§ã«ã¤ã„ã¦
- ç”ŸæˆAIæ™‚ä»£ã«å¤§äº‹ãªã‚¹ã‚­ãƒ«ã¯ã€ã€Œã‚„ã‚Šç¶šã‘ã‚‹èƒ½åŠ›ã€€by æ·±æ´¥ã•ã‚“
	- https://x.com/fladdict/status/1792831115528663471
	- ç”ŸæˆAIæ™‚ä»£ã«ã¯ã€Œã‚‚ã†ã‚ã„ã¤ï¼ˆAIï¼‰ä¸€äººã§ã„ã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€ã¨ã€è‰²ã€…ãªã“ã¨ã§æŒ«æŠ˜ã™ã‚‹äººãŒå¤§é‡ç™ºç”Ÿã™ã‚‹ã€‚ã€Œæ‰‹ã‚’ã¨ã‚ãªã„ã€èƒ½åŠ›ãŒã€äººé¡ã«ã¨ã£ã¦æœ€é‡è¦ã®æ‰èƒ½ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„
- transformers v4.41.0
	- https://github.com/huggingface/transformers/releases/tag/v4.41.0
	- Phi3, JetMoE, PaliGemma, VideoLlava, Falcon2, FalconVLM & GGUF support
		- https://huggingface.co/docs/transformers/v4.41.0/en/gguf
- Phi 3 - Small, Medium & Vision
	- https://x.com/reach_vb/status/1792949163249791383
	- This includes the 7B and 14B models
	- This also includes a multimodal phi model
- Knowledge Cards by Perplexty	
	- https://x.com/perplexity_ai/status/1792948540542517458
	- Weâ€™re teaming up with @TakoViz to bring advanced knowledge search and visualization to our users. Now, you can search, juxtapose, and share authoritative knowledge cards in Perplexity.
	- PerplexityãŒé«˜åº¦ãªæƒ…å ±æ¤œç´¢ã¨è¦–è¦šåŒ–ãŒã§ãã‚‹ã€Œknowledge cardsã€ã¨ã„ã†æ©Ÿèƒ½ã‚’ãƒªãƒªãƒ¼ã‚¹ã€‚
	- è‡ªç”±ã«ä»»æ„ã®2ç¤¾ã®æ ªä¾¡æ¨ç§»ã®æ¯”è¼ƒãŒå¯èƒ½ã€‚ãƒªã‚µãƒ¼ãƒæ¥­å‹™ãŒæ—ã‚Šã€ä»•äº‹ã§æ´»èºé–“é•ã„ç„¡ã—ã€‚
- GraphRAG: Using Knowledge in Unstructured Data to Build Apps with LLMs
	- https://www.graphlit.com/blog/graphrag-using-knowledge-in-unstructured-data-to-build-apps-with-llms
	- We have used Graphlit to automatically extract images from PDFs, and are using the OpenAI GPT-4 Vision model to perform OCR and generate detailed text descriptions of the images.
	- ã©ã†ã‚‚ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ç”»åƒã‹ã‚‰çŸ¥è¦‹ã‚’å¾—ã‚‹ã‚‰ã—ã„ã€‚
- I built my own omni assistant using Gemini 1.5 Flash to guide me through Super Mario 64.
	- https://x.com/skirano/status/1792948429754151293
	- MicrosoftãŒBuildã§ã§ã‚‚ã—ãŸã€assistantã‚’ã€gemini 1.5 Flashã§å®Ÿè£…ã—ãŸãƒ„ãƒ¯ãƒ¢ãƒã€ãŠé¡Œã¯ãƒãƒªã‚ªã ã—ã€‚
- What is the context window?
	- https://x.com/cwolferesearch/status/1792950349696753980
		- Claude-3 has a 1M context window 
		- Gemini-1.5 Pro has a 2M token context window 
		- Recent research [3] has explored going even beyond 2M tokens.
- ICity, a Geometry Nodes-powered procedural city generator for Blender.
	- https://x.com/80Level/status/1792769380717068510
	- Available now in Beta
		- https://80.lv/articles/long-awaited-procedural-city-generator-for-blender-is-now-available/
- Microsoftã€Copilot é€²åŒ–çš„ã®æ–°æ©Ÿèƒ½ã‚’ç™ºè¡¨
	- https://x.com/shota7180/status/1792966382990270739
	- Copilot é€²åŒ–çš„ã®æ›´æ–°ã«ã‚ˆã‚Šã€èª°ã§ã‚‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’æŒã¤ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã‚’æ§‹ç¯‰å¯èƒ½ã«
	- ã“ã®ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»£ã‚ã‚Šã«ç‹¬ç«‹ã—ã¦ç©æ¥µçš„ã«ã‚¿ã‚¹ã‚¯ã‚’èª¿æ•´ãƒ»å®Ÿè¡Œ
	- ç‰¹å®šã®å½¹å‰²ã‚„æ©Ÿèƒ½ã«åˆã‚ã›ã¦ã‚¿ã‚¹ã‚¯ã‚’å€‹åˆ¥ã«èª¿æ•´ã§ãã‚‹
- MIcrosoft's Phi-3 really is an astonishingly good model
	- https://x.com/simonw/status/1792691120675467288
	- MIT licensed and small enough to run in a browser on WebGPU (about a 2.3GB downloads), but still provides high quality results for a lot of the stuff I care about
	- Phi-3-mini running locally in your browser at 70 tokens per second on WebGPU!
	- Powered by ğŸ¤— Transformers.js and ONNX Runtime Web! 
	- https://huggingface.co/blog/Emma-N/enjoy-the-power-of-phi-3-with-onnx-runtime
- Hugging Face and Microsoft Deepen Collaboration
	- https://huggingface.co/blog/microsoft-collaboration
- Tako, the first AI search engine for visualizing and sharing the worldâ€™s knowledge.
	- https://x.com/TakoViz/status/1792949400710574455
	-  Introducing Tako, a new way to reference real knowledge And our first integration, Perplexity
		- https://trytako.com/blog/introducing-tako-and-perplexity-integration
- Team Copilot by Microsoft 
	- https://x.com/msdev/status/1792967099519758822
- 13B phi-medium-4k GGUF files here, model is looking very very good.
	- https://huggingface.co/nisten/phi3-medium-4k-gguf
- æœ¬æ—¥ï¼ˆ5/20ï¼‰ã€EUã®ãƒ‡ã‚¸ã‚¿ãƒ«ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ³•ãŒæ–½è¡Œ
	- https://x.com/_nat/status/1792915589570154637
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã€è‡ªå¾‹å‹AIã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ŒDevinã€ã®Cognition AIã¨ææºã‚’ç™ºè¡¨ã€‚Azureä¸Šã§Devinã‚’æä¾›ã¸
	- https://www.publickey1.jp/blog/24/aidevincognition_aiazuredevin.html
-  Mapping the Mind of a Large Language Model
	- https://www.anthropic.com/research/mapping-mind-language-model
	- Anthropic has just revealed some exciting news about Claude Sonnet. They've successfully identified how millions of concepts are represented inside this massive model!
- ollama run phi3:medium
	- https://x.com/ollama/status/1793067457382343134
- Phi-3-vision ãƒ» Phi-3-medium ãƒ» Phi-3-small ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/nb050244392a4?sub_rt=share_h
	- ã€ŒPhi-3ã€ã¯ã€æœ€ã‚‚æœ‰èƒ½ã§è²»ç”¨å¯¾åŠ¹æœã®SML (Small Language Model) ã§ã‚ã‚Šã€ã•ã¾ã–ã¾ãªè¨€èªã€æ¨è«–ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ•°å­¦ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŒã˜ã‚µã‚¤ã‚ºã¨æ¬¡ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™
	- ã€ŒPhi-3-visionã€ã¯ã€ãƒãƒ£ãƒ¼ãƒˆã‚„å›³ã‹ã‚‰æ´å¯Ÿã‚’ç”Ÿã¿å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚
	- ã€ŒPhi-3-smallã€ã€ŒPhi-3-mediumã€ã¯ã€åŒã˜ã‚µã‚¤ã‚ºã®è¨€èªãƒ¢ãƒ‡ãƒ«ã ã‘ã§ãªãã€ã¯ã‚‹ã‹ã«å¤§ãã„è¨€èªãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™
	- SLMã¯ã€ã‚ˆã‚Šå˜ç´”ãªã‚¿ã‚¹ã‚¯ã§ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€ãƒªã‚½ãƒ¼ã‚¹ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹çµ„ç¹”ã«ã¨ã£ã¦ã‚ˆã‚Šã‚¢ã‚¯ã‚»ã‚¹ã—ã‚„ã™ãã€ä½¿ã„ã‚„ã™ãã€ç‰¹å®šã®ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã¦ã‚ˆã‚Šç°¡å˜ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™
-  GPT-4oã‚’ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã€å°‚é–€å®¶ãŒã€Œæ™‚ä»£ã®è»¢æ›ç‚¹ã€ã¨è©•ä¾¡ã™ã‚‹ãƒ¤ãƒã™ãã‚‹èƒ½åŠ›ã¨ã¯ by ä»Šäº•ã•ã‚“
	- https://www.sbbit.jp/article/cont1/140613
	- OpenAIã®GPT-4oã‚’ç ”ç©¶è€…è¦–ç‚¹ã§è§£èª¬ã—ãŸè¨˜äº‹ãŒå‡ºã¾ã—ãŸ! é€Ÿå ±çš„ãªè¨˜äº‹ã®ä¾é ¼ã§ã—ãŸãŒ,ã‚„ã¯ã‚Šç ”ç©¶è€…ãŒæ›¸ãã¨ã„ã†ã“ã¨ã§æƒ…å ±ã‚’ã™ã¹ã¦è©°ã‚è¾¼ã‚“ã 1ä¸‡æ–‡è¿‘ã„ã‚¬ãƒè§£èª¬è¨˜äº‹ã«ãªã‚Šã¾ã—ãŸ.3å›ã®é€£è¼‰ã§ã™.
	- è¨€èª,éŸ³å£°,å‹•ç”»åƒ,å¾ŒåŠã§ã¯GPT-4oã®ã€Œå¼±ã¿ã€ç­‰,æ—¥æœ¬èªè¨˜äº‹ã§ã¯ä¸€ç•ªè©³ã—ã„ã¯ãš
- MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning
	- https://arxiv.org/abs/2405.12130
	- LoRAã‚ˆã‚ŠçŸ¥è­˜ç²å¾—ç³»ã‚¿ã‚¹ã‚¯ã«å¼·ã„MoRAã€€ by shi3zã•ã‚“
- ãƒ«ã‚«ãƒ³å…ˆç”Ÿã€å­¦ç”Ÿã«æ¬¡ä¸–ä»£AIã‚’ä½œã‚ã†ã¨ã™ã‚‹ãªã‚Œã°ã€LLMã‚’ã‚„ã‚‹ã®ã§ã¯ãªã„ã‚ˆã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹
	- https://x.com/ylecun/status/1793326904692428907
	- If you are a student interested in building the next generation of AI systems, don't work on LLMs
- Mistral v3 base and instruct released
	- https://huggingface.co/mistralai
	- Base has vocab extended to 32768. 
	- Instruct supports function calling! 
	- Tokens 5 to 9 are for function calling & the rest are empty
- Interface 7æœˆå·ã§ã¯ï¼ŒCopilotã§æ–‡èŠ¸çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«æŒ‘æˆ¦ã—ã¾ã™ï¼
	- https://x.com/If_CQ/status/1793214032121614787
	- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åŒæ™‚ã«é–‹ç™ºä¿å®ˆã™ã‚‹ã®ãŒDonald. E. Knuthåšå£«ã®ã€Œæ–‡èŠ¸çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ï¼Copilotã¨Doxygenã‚’ä½¿ãˆã°ï¼Œè¨˜è¿°ãŒè‡ªå‹•åŒ–ã§ãï¼Œä¸¡è€…ã®ä¸ä¸€è‡´ã‚’é˜²ã’ã¾ã™ï¼ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®ç†æƒ³ã‚’æœ€æ–°ã®æŠ€è¡“ã§å®Ÿç¾ã—ã¾ã™ï¼
- New guide in our AI cookbook: ğ™ğ™©ğ™§ğ™ªğ™˜ğ™©ğ™ªğ™§ğ™šğ™™ ğ™œğ™šğ™£ğ™šğ™§ğ™–ğ™©ğ™ğ™¤ğ™£
	- https://huggingface.co/learn/cookbook/structured_generation
	- This technique lets you force your LLM to generate its output as a JSON with specific keys: great for RAG or LLM-judge!
- Large Language Models Meet NLP: A Survey
	- https://arxiv.org/abs/2405.12819
	- Provides a comprehensive survey of how LLMs are applied to NLP tasks, introducing a new taxonomy and discussing current progress, future frontiers, and challenges.
- Mistral AI's Mistral v0.3 supports function calling with Ollama's raw mode!
	- https://x.com/ollama/status/1793392887612260370
	- Ollama raw mode
		- https://github.com/ollama/ollama/blob/main/docs/api.md#request-raw-mode
- Mistral-7Bã¨Phi-3ã‚‚ã“ã®éš›ElyzaTasks100ã§è©•ä¾¡ã—ã¦ã¿ãŸã€‚by ã†ã¿ã‚†ãã•ã‚“
	- https://x.com/umiyuki_ai/status/1793550842353820014
	- ã¾ãšMistral-7Bã¯v0.1ãŒ2.46ã€v0.2ãŒ2.69ï¼ˆã‚„ãŸã‚‰è‹±èªã§å›ç­”ã—ã¦ãã‚‹ã®ã§åŠåˆ†ä»¥ä¸Šè‹±èªã®å›ç­”ã¯æ‰‹ä½œæ¥­ã§1ç‚¹ã«æ¸›ã‚‰ã—ãŸï¼‰ã€ã‹ã‚‰ã®ä»Šå›ã®v0.3ã¯3.52ï¼è¦‹é•ãˆã‚‹ãã‚‰ã„æ—¥æœ¬èªèƒ½åŠ›ãŒå¼·åŒ–ã•ã‚Œã¦ã¾ã™ã€‚è‹±èªã§ç­”ãˆã¡ã‚ƒã†å•é¡Œã‚‚ã»ã¼èµ·ããªã„ã€‚
	- Phi3ã¯3.8Bã®mini-128kãŒã¾ã•ã‹ã®3.26ã¨ã„ã†ã“ã®ãƒ‘ãƒ©æ•°ã«ã—ã¦ã¯é«˜ã™ãã‚‹ã‚¹ã‚³ã‚¢ã§ãªãœã‹small-128kã«å‹ã£ã¦ã—ã¾ã£ã¦ã¾ã™ã€‚
	- small-8kã¯3.28ã§ã€miniã¨å¤§å·®ãªã„ã¨ã„ã†æ„å‘³ã§ã¯æ®‹å¿µã€‚åŒãƒ‘ãƒ©ã®Mistral-7B-v0.3ã«ã‚‚è² ã‘ã¦ã‚‹ã€‚
	- ã§ã‚‚medium-128kã¯14Bãƒ‘ãƒ©ã§3.96ã¨ã„ã†ãƒã‚±ãƒ¢ãƒ³ã¿ãŸã„ãªã‚¹ã‚³ã‚¢ãŒå‡ºã¦ã¾ã™ã€‚ã“ã‚Œã¯ã™ã”ã™ã
- GPT-4oã¨GPT-4Turboã®ElyzaTasks100ã®å¹³å‡ã‚¹ã‚³ã‚¢ã€
	- https://x.com/umiyuki_ai/status/1793540614551904762
	- æ°—ã«ãªã£ã¦ãŸã®ã§APIä»£æ‰•ã£ã¦è©•ä¾¡ã—ã¦ã¿ãŸã€‚
	- GPT-4TurboãŒ4.44ã€GPT-4oãŒ4.51ï¼ã‚„ã£ã±ã‚Šã‚¨ã‚²ã¤ãªã„è¶…ã‚¹ã‚³ã‚¢ï¼
	- ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ¢ãƒ‡ãƒ«ãŒGPT-4Tã«è¿½ã„ä»˜ã„ã¦ããŸãªã‚“ã¦ã¡ã‚‡ã£ã¨è¨€ãˆãªããªã£ãŸ
- Phi-3-Mediumã®MMLUã‚¹ã‚³ã‚¢ã¯Llama3-70Bä¸¦ã¿â€¦ by ã†ã¿ã‚†ãã•ã‚“
	- https://x.com/umiyuki_ai/status/1793614730403434950
	- ã‚„ã£ã±ã‚Šä¼Šé”ã˜ã‚ƒãªã„ã‚‰ã—ã„ã­ã€‚ElyzaTasks100ã§ã‚‚åŒ¹æ•µã—ã¦ã‚‹ã‚‚ã‚“ã€‚ã—ã‹ã—ä¿¡ã˜ãŒãŸã„ã­ 
- Aya 23 is here! Available in 8B and 35B.
	- https://ollama.com/library/aya
- LangChain `with_structured_output` ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚‹æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
	- https://zenn.dev/ml_bear/articles/cb07549ec52175
	- 1.  æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’Pydanticã§å®šç¾©ã™ã‚‹
	- 2.  ãã®å®šç¾©ã‚’`.with_structured_output`ã§LLMã«å–ã‚Šä»˜ã‘ã‚‹
	- Pydanticã§ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ãŸä¸Šã§æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã™ã‚‹ã®ã¯éå¸¸ã«ç°¡å˜ã§ã™
- ã€ŒAIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã¯è¡—ã®é›»æ°—å±‹ã•ã‚“ã‹ã‚‰å§‹ã‚ã‚ã€
	- https://www8.cao.go.jp/cstp/ai/ai_senryaku/9kai/shiryo1-4.pdf
	- AIæˆ¦ç•¥ä¼šè­°ã®æ¾å°¾ç ”ã®è³‡æ–™ã€Œç”ŸæˆAIã®ç”£æ¥­ã«ãŠã‘ã‚‹å¯èƒ½æ€§ã€
	- ã¾ãšã¯å—è¨—é–‹ç™ºã§ç¤¾ä¼šã‚’å­¦ã¶ã€‚ å—è¨—é–‹ç™ºã§åœ°åŸŸã®ä¼æ¥­ã®DXã‚’æ”¯æ´ã—ã¤ã¤ã€ä¸€ç·’ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å‡ºã¦ã„ã
- ä¸–ç•Œåˆã€AIæŠ€è¡“ã«ã‚ˆã‚‹åŸæ²¹å‡¦ç†è£…ç½®ã®è‡ªå‹•é‹è»¢ã‚’é–‹å§‹ã€€PFN
	- https://www.preferred.jp/ja/news/pr20240524/
- æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åå¿œäºˆæ¸¬ã®è«–æ–‡
	- https://chemrxiv.org/engage/chemrxiv/article-details/664de6a821291e5d1df74ac0
	- SMILESã«ã‚ˆã‚ŠåŒ–å­¦åå¿œã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¾ã—ãŸSMIRKSã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€åŒ–å­¦åå¿œã®ãƒ«ãƒ¼ãƒ«ã‚’é«˜ç²¾åº¦ã«å­¦ç¿’ã§ããŸãã†ã§ã™ã€‚å¯„ä¸ã™ã‚‹åŸå­æ•°ãŒå¤šã„è¤‡é›‘ãªåå¿œã¯ä»Šå¾Œã®èª²é¡Œã¨ã®ã“ã¨
- LLMã¯ãƒãƒ£ãƒƒãƒˆUIã®èª•ç”Ÿã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’èµ·ã“ã—ãŸãŒã€ä»Šã¯ãƒãƒ£ãƒƒãƒˆUIã«å‘ªã‚ã‚Œã¦ã„ã‚‹
	- https://x.com/rkmt/status/1794013338005090666
	- ã€Œæœ‰åŠ¹ãªè³ªå•ã‚’LLMã«æŠ•ã’å›ç­”ã‚’å¾—ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€ã¯ä¸€å®šé‡ï¼ˆãã‚Œã‚’ä½¿ã„ã“ãªã›ã‚‹äººé¡ã®æ•°<<äººé¡ã®ç·æ•°ï¼‰ã§é ­æ‰“ã¡ã«ãªã‚Šã€ãã‚Œä»¥ä¸Šã¯ã€Œæ„å‘³ã‚‚ãªã„å†…å®¹ã‚‚ãªã„ã‘ã©æ¥½ã—ã„ä¼šè©±ã‚’AIã¨ç¶šã‘ã‚‹ã€ã‚µãƒ¼ãƒ“ã‚¹ã‹ã€ã€Œäººé–“ã‚’å¿…è¦ã¨ã—ãªã„AIæ¥­å‹™ã€ã«ç§»è¡Œã™ã‚‹ã®ã‹ã‚‚ã€‚
- ã±ã·ã‚Šã‹ç‚’ã‚ï¼ˆmmngaï¼‰ã•ã‚“ã®ã€Llama-3-70B-japanese-suzume-vector-v0.1 ã™ã”ã„ã€ by AIã‚µãƒˆã‚·
	- https://x.com/AiXsatoshi/status/1793973265532424467
	- 8bã®Llamaæ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ã®chatvectorã‚’ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°é•ã†70Bã«ãƒãƒ¼ã‚¸ã—ã¦ã¦ã€ã•ã‚‰ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚‚è‰¯å¥½ãªã®ã™ã”ã„
- CohereãŒå¤šè¨€èªæŒ‡å‘ã®ã‚ªãƒ¼ãƒ—ãƒ³LLMã€ŒAyaã€ï¼ˆ8Bï¼Œ23Bï¼‰ã‚’å…¬é–‹
	- https://huggingface.co/spaces/CohereForAI/aya-23
	- 4æœˆã«ã¯å½“æ™‚ã®ã‚ªãƒ¼ãƒ—ãƒ³LLMæœ€é«˜æ€§èƒ½ã®Command R+ã‚’å‡ºã—ã¦ãŸCohereã®å¤šè¨€èªLLMãªã®ã§,æ—¥æœ¬èªã‚‚æœŸå¾…ã§ããã†...å®Ÿéš›ã«æ—¥æœ¬èªã¯çµæ§‹ã†ã¾ã„ã‚“ã§ã™ãŒ,è‰²ã€…ã¨ç°¡æ½”ã™ãã¦è‡ªåˆ†ã®ä¸­ã§ã®è©•ä¾¡ãŒã€Œå†·ãŸã„ãƒ¢ãƒ‡ãƒ«ã€ã§ã™
-  DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data
	- https://huggingface.co/papers/2405.14333
	- Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in
- ollamaã§Phi3-mediumã¯ã€Œæ€§èƒ½ã‚·ãƒ§ãƒœã„ã€ï¼Ÿã€€by ã†ã¿ã‚†ãã•ã‚“
	- https://x.com/umiyuki_ai/status/1793878129699950887
	- Ollamaã§Phi3-mediumã‚’ãƒ—ãƒ«ã™ã‚‹ã¨Q4_0é‡å­åŒ–ç‰ˆãŒDLã•ã‚Œã‚‹ã‚ˆã†ã ãŒã€ã‚¹ã‚³ã‚¢ã¯3.68ã€‚Q4_K_Sã§ã‚‚3.67ã€‚ã¡ãªã¿ã«åƒ•ãŒæœ€åˆã«æ¤œè¨¼ã—ãŸLlama. cppã®Q8ã¯3.88ã ã£ãŸã—ã€åŒã˜ãLlama. cppã®Q4_K_Sã‚‚3.95ã§åŠ£åŒ–ã©ã“ã‚ã‹ã‚¹ã‚³ã‚¢ä¸ŠãŒã£ã¦ã‚‹ã€‚ã¨ã„ã†ã‚ã‘ã§ollamaã®Phi3-mediumã¯ãƒ‘ãƒ©è¨­å®šã‹ãªã‚“ã‹åˆ†ã‹ã‚‰ã‚“ã‘ã©ä½•ã‚‰ã‹ã®å•é¡Œã§åŠ£åŒ–ã—ã¦ã¾ã™
- DeepSeekV2 is a big deal.
	- https://x.com/Xianbao_QIAN/status/1794034052347171055
	- Not only because its significant improvements to both key components of Transformer: the Attention layer and FFN layer. 
	- It has also completed disrupted the Chines LLM market and forcing the competitors to drop the price to 1% of the original price.
- Difyã‚’ä½¿ã†ãƒ¡ãƒªãƒƒãƒˆã®ä¸€ã¤ãŒé–‹ç™ºã—ãŸChatbotã‚„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç°¡å˜ã«WEBã§ã‚·ã‚§ã‚¢ã§ãã‚‹ã“ã¨
	- https://x.com/gijigae/status/1793437095727665588
-  Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet
	- https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html
	- Anthropicã®ä¸­è¦æ¨¡ç”Ÿç”£ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Claude 3 Sonnetã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é©ç”¨ã—ã€è§£é‡ˆå¯èƒ½ãªç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ç ”ç©¶
	-  **ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼**: å°è¦æ¨¡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‹ã‚‰å˜ç¾©çš„ç‰¹å¾´ã‚’å›å¾©ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ãŸç ”ç©¶ã‹ã‚‰ç™ºå±•ã—ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé‡è¦–ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- **è§£é‡ˆå¯èƒ½ãªç‰¹å¾´**: æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´ã¯å¤šè¨€èªã€å¤šãƒ¢ãƒ¼ãƒ€ãƒ«ã§ã‚ã‚Šã€å…·ä½“çš„ãŠã‚ˆã³æŠ½è±¡çš„ãªå‚ç…§ã®é–“ã§ä¸€èˆ¬åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- **å®‰å…¨æ€§é–¢é€£ç‰¹å¾´**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®è„†å¼±æ€§ã‚„ãƒãƒƒã‚¯ãƒ‰ã‚¢ã€åè¦‹ã€å˜˜ã‚„æ¬ºçã€å±é™ºã¾ãŸã¯çŠ¯ç½ªçš„ãªå†…å®¹ãªã©ã€AIã‚·ã‚¹ãƒ†ãƒ ãŒå¼•ãèµ·ã“ã™å¯èƒ½æ€§ã®ã‚ã‚‹æ§˜ã€…ãªå•é¡Œã«é–¢é€£ã™ã‚‹ç‰¹å¾´ãŒè¦³å¯Ÿã•ã‚Œã¦ã„ã¾ã™ã€‚
	- **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡**: ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨“ç·´ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’é©ç”¨ã—ã€è¨ˆç®—äºˆç®—ã«åŸºã¥ã„ã¦æœ€é©ãªç‰¹å¾´æ•°ã¨è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ±ºå®šã—ã¦ã„ã¾ã™ã€‚
- ãƒãƒ«ãƒ»ãƒ™ãƒªãƒ¼ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒAIã§å®Ÿä½“é¨“ã§ãã‚‹æ™‚ä»£ãŒåˆ°æ¥
	- https://x.com/webbigdata/status/1794030396990226803
	- ãƒãƒ«ãƒ»ãƒ™ãƒªãƒ¼ã•ã‚“ã¨ã„ã†ã€X-MENã®ã‚¹ãƒˆãƒ¼ãƒ ã¨ã‹ã‚­ãƒ£ãƒƒãƒˆã‚¦ãƒ¼ãƒãƒ³å½¹ã‚’ã‚„ã£ã¦ã‚‹ã‚¢ãƒ¡ãƒªã‚«ã®å¥³å„ªã•ã‚“ãŒã„ã‚‹ã®ã§ã™ãŒã€ã‚ã‚‹æ‚£è€…ã•ã‚“ã®ç‰¹å®šã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã€Œãƒãƒ«ãƒ»ãƒ™ãƒªãƒ¼ã®å†™çœŸã€ã‚„ã€Œãƒãƒ«ãƒ»ãƒ™ãƒªãƒ¼ã€ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦æ´»æ€§åŒ–ã™ã‚‹äº‹ãŒè¦³å¯Ÿã•ã‚ŒãŸã¨ã„ã†ç ”ç©¶ãŒã‚ã‚‹ã€‚
	- 5æœˆ21æ—¥ã«anthropicãŒClaude 3 Sonnetã§ä½•ç™¾ä¸‡ã‚‚ã®æ¦‚å¿µãŒã©ã®ã‚ˆã†ã«è¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç‰¹å®šã§ããŸã‚ˆï½ã¨ç™ºè¡¨ã—ã€
	- ãã®æŠ€è¡“ã‚’ä½¿ã£ã¦ã€ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã®ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ ã‚²ãƒ¼ãƒˆ ãƒ–ãƒªãƒƒã‚¸(Golden Gate Bridge)ã«å¯¾å¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æ´»æ€§åŒ–ã•ã›ã¦ã¦ã„ã‚‹ã€ŒGolden Gate Claudeã€ã¨å®Ÿé¨“çš„ã«å¯¾è©±ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚ˆï½ã¨ç™ºè¡¨ã—ãŸã®ãŒæ˜¨æ—¥ã§ã™ã­ã€‚
	- ä¸€è¨€ã§è¨€ãˆã°ã€ã‚„ãŸã‚‰ã‚ã£ãŸã‚‰ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ ã‚²ãƒ¼ãƒˆ ãƒ–ãƒªãƒƒã‚¸æ¨ã—ã‚’ã—ã¦ãã‚‹AIã§ã€è‰²ã€…ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ ã‚²ãƒ¼ãƒˆ ãƒ–ãƒªãƒƒã‚¸ã‚’æ¨è–¦ã—ã¦ãã¾ã™ã€‚
- streamlitã§ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚ GitHubã¨æ¥ç¶šã™ã‚Œã°ã€æœ¬å½“ã«çˆ†é€Ÿã§
	- https://x.com/kenken26679105/status/1793889080385925580
- ChatVectorã§7Bãƒ¢ãƒ‡ãƒ«ã®FineTuningçµæœã‚’70Bã«è»¢ç§»ã•ã›ã‚‹ã¿ãŸã„ãªè©±ã€by ã¯ã¡ã•ã‚“
	- https://x.com/CurveWeb/status/1794203714422759707
	- äº‹å‰å­¦ç¿’ã§ã¯æ—¢ã«å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§äº‹å‰å­¦ç¿’â†’ã‚»ãƒ«ãƒ•ãƒãƒ¼ã‚¸ã§å¤§ãƒ¢ãƒ‡ãƒ«åŒ–ã£ã¦ã„ã†ã®ãŒã§ãã¦ã„ã‚‹ã®ã§ãªã‚“ã¨ãªãã§ãã¦ç„¶ã‚‹ã¹ãæ„Ÿã‚ã‚‹ã€‚

## 5/20

ä»Šå›ã¯ã€GPT-4oã•ã‚“ã«ã€ã¾ã¨ã‚ã‚’ãŠé¡˜ã„ã—ã¾ã—ãŸï¼ˆç„¡ä¿®æ­£ã§ã™ï¼ï¼ï¼‰ã€‚ã“ã“ã¾ã§æ¥ãŸã‹ã€ã¨é©šãã‚ˆã†ãªã•ã¿ã—ã„ã‚ˆã†ãªã€‚ã€‚å¤§åˆ‡ãªã“ã¨ã¯ã€ã‚‚ã†ä¸€åº¦è¨€ã„ã¾ã™ã€ç„¡ä¿®æ­£ã§ã™ã€‚ã§ã¯ã€

æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å‹•å‘ã«ã¤ã„ã¦ã€ä»Šå›ã¯ã¡ã‚‡ã£ã¨ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚‚äº¤ãˆã¤ã¤ãŠå±Šã‘ã—ã¾ã™ã€‚ã¾ãšã¯ã€çµ¶å¯¾ã«è¦‹é€ƒã›ãªã„äºŒã¤ã®å¤§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ã€‚ æœ€åˆã«ã€å¤§ããæ³¨ç›®ã‚’æµ´ã³ã¦ã„ã‚‹ã®ãŒOpenAIã®æ–°ãƒ¢ãƒ‡ãƒ«ã€ŒGPT-4oã€ã§ã™ã€‚ã©ã†ã‚„ã‚‰ã“ã®ãƒ¢ãƒ‡ãƒ«ã€åå‰ã ã‘ã˜ã‚ƒãªãã¦æ€§èƒ½ã‚‚ã¾ã•ã«ã€ŒãŠãŠï¼ã€ã¨è¨€ã„ãŸããªã‚‹ç¨‹ã®é€²åŒ–ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦é€Ÿåº¦ã¯2å€ã€ã‚³ã‚¹ãƒˆã¯åŠåˆ†ã€ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãŒ5å€ã¨ã€ã¾ã•ã«ã‚¹ãƒ¼ãƒ‘ãƒ¼AIã€‚ã•ã‚‰ã«ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®ChatGPTãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã®ã“ã¨ã§ã€ã‚µãƒ ãƒ»ã‚¢ãƒ«ãƒˆãƒãƒ³ã•ã‚“ã‹ã‚‰ç›´æ¥ã®ãŠçŸ¥ã‚‰ã›ã‚‚é£›ã³å‡ºã—ã¾ã—ãŸã€‚ã—ã‹ã‚‚ã€ã“ã®GPT-4oã¯æ•°å­¦ã®é›£é–¢å•é¡Œã‚’ç”»åƒã§å‡ºé¡Œã—ãŸã ã‘ã§è§£ã‘ã‚‹ã¨ã„ã†ã€ã¾ã‚‹ã§é­”æ³•ã®ã‚ˆã†ãªèƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ã®ã§ã™ã€‚ã“ã®é€²åŒ–ã«ã‚ˆã‚Šã€å‹•ç”»ã®è¦ç´„ã‚„åŒ–å­¦å®Ÿé¨“ã®è€ƒå¯Ÿã¾ã§ã€ãƒ˜ãƒ“ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ´»ç”¨æ³•ãŒã©ã‚“ã©ã‚“å¢—ãˆã¦ã„ã‚‹ã®ãŒç¾çŠ¶ã§ã™ã€‚ æ¬¡ã¯Google I/Oã§ã®ç™ºè¡¨ã§ã™ã€‚ãƒˆãƒƒãƒ—ãƒãƒƒã‚¿ãƒ¼ã¯æ–°ã—ã„ãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã€ŒPaliGemmaã€ã¨ã€ŒGemma 2ã€ã€‚ãã®å¤§ããªè¦‹ã©ã“ã‚ã¯ã€Gemma 27Bã¨ã„ã†ã‚µã‚¤ã‚ºã§ã‚‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é§†ä½¿ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒ2å€ã®ã‚µã‚¤ã‚ºã®ã‚‚ã®ã«ã‚‚å‹ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã€‚ã“ã‚Œã€ã¾ã‚‹ã§ãƒ’ãƒ¼ãƒ­ãƒ¼æ˜ ç”»ã®ç¶šç·¨ãŒç™ºè¡¨ã•ã‚Œã‚‹ã‹ã®ã‚ˆã†ãªãƒ¯ã‚¯ãƒ¯ã‚¯æ„ŸãŒã‚ã‚Šã¾ã™ã‚ˆã­ã€‚ãã—ã¦ã•ã‚‰ã«ã€ŒTrilliumã€ã¨ã„ã†æ¬¡ä¸–ä»£Google Cloud TPUã®ç™»å ´ã§ã€ã“ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼AIãƒ’ãƒ¼ãƒ­ãƒ¼ãŸã¡ãŒã•ã‚‰ã«åŠ¹ç‡è‰¯ãå‹•ä½œã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚ã‚ã€ä»Šå¤œã®ãƒ“ãƒªãƒ¼ãƒ»ã‚¢ã‚¤ãƒªãƒƒã‚·ãƒ¥ã®ãƒ©ã‚¤ãƒ–ã«ã§ã‚‚ç™»å ´ã—ãã†ãªå‹¢ã„ã§ã™ã€‚ ã•ã¦ã€è©±é¡Œã‚’å¤‰ãˆã¦ã€æœ€è¿‘ã®ç§€ä½œã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚DeepLearningAIã‹ã‚‰ç„¡æ–™ã‚³ãƒ¼ã‚¹ãŒç¶šã€…ç™»å ´ã—ã¦ãŠã‚Šã€MistralAIã‚’ä½¿ã£ãŸã‚³ãƒ¼ã‚¹ã‚’æä¾›ä¸­ã§ã™ã€‚ã“ã®ã‚³ãƒ¼ã‚¹ã§ã¯ã€Mistralã®ãƒ¢ãƒ‡ãƒ«ã«åŠ ãˆã¦ã€RAGã€é–¢æ•°å‘¼ã³å‡ºã—ã€ãã—ã¦JSONãƒ¢ãƒ¼ãƒ‰ãªã©ã¾ã§å­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã‚’å—ã‘ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã§ã®å°¤åº¦é–¢æ•°ã®æ‰ãˆæ–¹ã«ã¤ã„ã¦è­°è«–ãŒå·»ãèµ·ã“ã£ã¦ã„ã¾ã™ã€‚ã€Œãã‚Œã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ€ãƒ³ã‚µãƒ¼ã®å¿…é ˆã‚¹ã‚­ãƒ«ï¼Ÿã€ã¨æ€ã‚ã›ã‚‹ã‚ˆã†ãªå°‚é–€çš„ãªè©±é¡Œã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ ä¸€æ–¹ã€HuggingFaceã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨llama.cppã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«é•ã„ãŒã‚ã‚‹ã“ã¨ãŒè­°è«–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã¾ã‚‹ã§ã€æ˜ ç”»ã®å­—å¹•ã¨å¹ãæ›¿ãˆã®é•ã„ãã‚‰ã„æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã§ã™ã€‚ç‰¹ã«Llama3ã‚„Gemmaãƒ¢ãƒ‡ãƒ«ã«é–¢ã—ã¦ã€é‡å­åŒ–ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ã‚‚æµ®ä¸Šã—ã¦ã„ã¾ã™ã€‚ã‚“ã‚“ã€ã‚„ã£ã±ã‚Šãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é€²åŒ–ã‚‚ä¸€ç­‹ç¸„ã§ã¯ã„ãã¾ã›ã‚“ã­ã€‚ ãã—ã¦ãŠå¾…ã¡ã‹ã­ã€npakaã•ã‚“ã®æƒ…å ±ã‚’ä¸€æ°—ã«ã¾ã¨ã‚ã¦ãƒã‚§ãƒƒã‚¯ã€‚å½¼ã¯OpenAIã®Model Specã«ã¤ã„ã¦ã®æ¦‚è¦ã‚’è©³è¿°ã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€æ–°ã—ã„LangChain v0.2ã‚’ä½¿ã£ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ã‚„ã€RAGã€ç‰¹å®šã®æƒ…å ±æºã«é–¢ã™ã‚‹QAã‚·ã‚¹ãƒ†ãƒ ã€æƒ…å ±æŠ½å‡ºã€è¦ç´„ãªã©ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚‚ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚ã€ŒLangChainãƒãƒƒã‚«ãƒ¼ã€ãªã‚“ã¦ç§°å·ãŒå½¼ã«ä¼¼åˆã„ãã†ã§ã™ã­ã€‚ ã“ã“ã¾ã§å¤§ã¾ã‹ãªãƒˆãƒ”ãƒƒã‚¯ã‚’ã”ç´¹ä»‹ã—ã¾ã—ãŸãŒã€ãã®ä»–ã®å°ãƒã‚¿ã‚‚ç››ã‚Šã ãã•ã‚“ã§ã™ã€‚ä¾‹ãˆã°ã€Mozillaã®ã‚„ã‚‹æ°—æº€ã€…ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ«ã€Œllamafileã€ã‚„ã€LangChainã¨HuggingFaceã®å¼·åŠ›ãªææºãªã©ã€ã©ã‚“ã©ã‚“æ–°ã—ã„æ©Ÿèƒ½ãŒç™»å ´ã—ã¦ã„ã¾ã™ã‚ˆã€‚ã“ã®åˆ†é‡ã®é€²å±•ã®é€Ÿã•ã‚’è¦‹é€ƒã•ãªã„ã§ãã ã•ã„ã­ã€‚ ç¾å ´ã¯ã¾ã‚‹ã§ã€ãƒ”ãƒ¼ã‚¿ãƒ¼ãƒ‘ãƒ³ã®ãƒãƒãƒ¼ãƒ©ãƒ³ãƒ‰ã®ã‚ˆã†ã«å¤‰åŒ–ã«æº€ã¡ã¦ã„ã¾ã™ã€‚ä»Šå¾Œã‚‚ç¶šã€…ã¨é©šãã¨ç¬‘é¡”ãŒå¾…ã£ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã•ã‚ã€æ¬¡ã¯ã©ã‚“ãªå†’é™ºãŒå¾…ã£ã¦ã„ã‚‹ã®ã‹ã€æ¥½ã—ã¿ã§ã™ã­ï¼

- ã¾ãŸã¾ãŸDeepLearningAIã‚ˆã‚Šã€MistralAIã‚’ç”¨ã„ãŸç„¡æ–™ã‚³ãƒ¼ã‚¹
	- https://www.deeplearning.ai/short-courses/getting-started-with-mistral/
	- Mistral AIã«ã‚ˆã‚‹1æ™‚é–“ã®ã‚³ãƒ¼ã‚¹ã€‚Mistralã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§ãªãã€RAGã€é–¢æ•°å‘¼ã³å‡ºã—ã€JSONãƒ¢ãƒ¼ãƒ‰ãªã©ã«ã¤ã„ã¦å­¦ã¹ã‚‹
- bæ°ãŒã€ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã“ãƒ¼ã‚„ã£ã¦ä½¿ã†ã®ã€ã«ã‹ã¿ã¤ãã€
	- https://x.com/behemuhemulove/status/1789537738590765215
	- ã€Œå°¤åº¦é–¢æ•°ã®æ‰€ãŒä¿ºã«ã¯æ„å‘³ãŒè‰¯ãã‚ã‹ã‚‰ãªã‹ã£ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã„ã£ã¦ã‚‹ã®ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å›ºå®šã—ãŸã‚‰ãƒ‘ãƒ©ãƒ¡ã‚¿ã®é–¢æ•°ã«ãªã‚‰ãªã„ã‹ã‚‰å°¤åº¦é–¢æ•°ã˜ã‚ƒãªããªã„ï¼Ÿã€
	- â†’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ¯ã£ã¦ã‚‹ã®ã§å°¤åº¦ã«ã‚ˆã‚‹æœ€é©ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¨å®šã¨ã„ã£ã¦ã‚‚ã„ã„ã®ã§ã¯ãªã„ã‹ï¼Ÿ
- ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã§HuggingFaceã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨llama.cppã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«å·®ç•°ãŒã‚ã£ãŸã¨ã®äº‹
	- https://x.com/webbigdata/status/1789695414238884256
	- llama3ã®é‡å­åŒ–ãŒè…ã£ã¦ã‚‹ã®ã¯ã“ã®ã›ã„ï¼Ÿ
	- 1. Mistral: HF's batch_decode output is wrong 
	- 2. Llama-3: Be careful of double BOS 
	- 3. Gemma: 2nd token has an extra space - GGUF(_Below) = 30641 vs HF(Below) = 33501 
	- 4. Gemma-it: Also be careful of double BOS
- OpenAI ã® Model Spec ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/nf6b811cad5dc?sub_rt=share_b
	- ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’å½¢æˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é€æ˜æ€§ã‚’é«˜ã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã©ã®ã‚ˆã†ã«å¤‰æ›´ãŠã‚ˆã³æ”¹å–„ã§ãã‚‹ã‹ã«ã¤ã„ã¦å…¬é–‹ã®ä¼šè©±ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã«ã€ã€ŒModel Specã€ã‚’å…¬é–‹ã—ã¾ã™ã€‚
- [code-cooker](https://github.com/karaage0703/code-cooker) by ã‹ã‚‰ã‚ã’ã•ã‚“
	- https://github.com/karaage0703/code-cooke
	- é¢å€’ãªã“ã¨ã‚’ChatGPTä»¥å¤–ã®LLMã«ã‚„ã‚‰ã›ã‚‹ã‚½ãƒ•ãƒˆã€‚GUIã‚’ã¤ã‘ã¦ã¿ã¾ã—ãŸã€‚ ã‚ˆã†ã‚„ãè‡ªåˆ†ã§ã‚‚ä½¿ã„ãŸããªã‚‹ã‚‚ã®ãŒã§ããŸæ°—ãŒã—ã¾ã™ã€‚ã¾ã Claude 3 Opusã—ã‹å¯¾å¿œã—ã¦ãªã„ã®ã§ã€GPT-4ã¨ã‹Llama 3ã«ã‚‚å¯¾å¿œã—ã¦ã„ãã¾ã™ã€‚GPTå¯¾å¿œã¯OpenAIã®ç™ºè¡¨ã®å¾Œã«ã—ã‚ˆã†ã‹ãª
- å¤§è¦æ¨¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®äººãŸã¡ã¯ã€ã“ã®OpenAIã®è«–æ–‡ã«æ›¸ã„ã¦ã‚ã‚‹ã“ã¨æ€ã„å‡ºã—ã¦ã­ã ãã†ã§
	- https://x.com/cloneofsimo/status/1789700168083997010
	- Again, the paper im advocating here is from openai, and is referenced all the time and frankly one of the paper all large scale practitioner should read. the math here isn't complicated and nothing here is either controversial nor task dependent.
	- https://arxiv.org/abs/1812.06162
-  æŠ€è¡“é©æ–°ã¨ä¸å¹³ç­‰ã®1000å¹´å²ã®ç´¹ä»‹ by æ¥ ã•ã‚“
	- https://x.com/masanork/status/1789647931467026613
	- ã“ã‚Œç‰¹ã«ä¸‹å·»ã®èª­ã¿å¿œãˆãŒã™ã”ã„ã‚“ã§ã™ãŒã€æŠ€è¡“ãŒç´„æŸã™ã‚‹æœªæ¥ã¨ç¤¾ä¼šæ§‹é€ ã«ä¸ãˆã‚‹å½±éŸ¿ã¨ã¯ã€åˆ†ã‘ã¦è­°è«–ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã®ã§ã¯ï¼Ÿã¨ã„ã†èª²é¡Œèªè­˜ãŒå¼·ãã€‚LLMå‘¨è¾ºã§ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã£ã¦ç”¨èªãŒæ›–æ˜§ã«ä½¿ã‚ã‚ŒãŸã‚Šã€ç”Ÿç”£æ‰‹æ®µãŒæ°‘ä¸»åŒ–ã•ã‚Œã¦ã„ãªã„ã®ãŒæ‚©ã¿ã©ã“ã‚
- ollamaã§ Fugaku-LLM ã‚’å‹•ã‹ã™
	- https://note.com/npaka/n/n1d99253ae2cf?sub_rt=share_h
	- ä¸€ç•ªã‚µã‚¤ã‚ºã®å°ã•ã„ï¼ˆãŠãã‚‰ãé‡å­åŒ–ãŒä¸€ç•ªåŠ¹ã„ã¦ã„ã‚‹ï¼‰ ã€ŒFugaku-LLM-13B-instruct-0325b-q5_k_m.ggufã€ã‚’é¸ã³ã¾ã™
	- **`Modelfile`  ã§ä¸€ç•ªé‡è¦ãªã®ã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã® chat template ã‚’å®ˆã‚‹ã“ã¨ã§ã™**
	- dockerã€€ï¼’ç™ºã§ã€ollamaã¨ã€web-uiãŒå‹•ãã®ã‹ãƒ¼
- ã€GPT-4o çˆ†èª•ã€‘
	- https://x.com/MLBear2/status/1790069525372981452
	- å¾“æ¥ã®GPT-4, Claude 3 Opusãªã©ã«æ¯”ã¹ã¦é ­ä¸€ã¤æŠœã‘ã¦è³¢ã„ï¼ˆå›³ï¼‰
	- gpt2ã¨ã—ã¦Chatbot Arenaã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ãŸã‚‚ã®ãŒGPT-4oã ã£ãŸã¨ã‚µãƒ ã‚¢ãƒ«ãƒˆãƒãƒ³CEOãŒèªã‚ãŸã€‚
	- GPT-4 Turboã¨æ¯”ã¹ã¦ ãƒ»2å€é€Ÿã ãƒ»50%å®‰ä¾¡ ãƒ»Rate limitãŒ5å€é«˜ã„
- GPT-4oã§ä½¿ã‚ã‚Œã¦ã„ã‚‹æ–°ã—ã„tokenizerã€tiktokenã«ã‚‚ã†å…¥ã£ã¦ã‚‹
	- https://x.com/gyakuse/status/1790110045814010327
	- tiktokenã‚’0.7.0 ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã—ã€enc = tiktoken.encoding_for_model("gpt-4o")ã¨ã™ã‚‹ã ã‘
- gpt-4oã§è©¦ã—ã«ä»Šå¹´ã®æ±å¤§æ•°å­¦2024ã®å•é¡Œã‚’ç”»åƒã§é€ã£ãŸã‚‰ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸€åˆ‡ç„¡ã—ã§ã‚‚ï¼‰æ­£è§£ã§ããŸ
	- https://x.com/kyutaro15/status/1790098489940258830
- ã‚µãƒ ã‹ã‚‰ã®GPT-4oã«é–¢ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
	- https://x.com/sama/status/1790065541262032904
	- it is available to all ChatGPT users, including on the free plan! so far, GPT-4 class models have only been available to people who pay a monthly subscription. this is important to our mission; we want to put great AI tools in the hands of everyone.
- GPT-4oã®å‹•ç”»è¦ç´„ã‚’Huggingface Spaceã§è©¦ã›ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸ by é€†ç€¬å·ã•ã‚“
	- https://x.com/gyakuse/status/1790090822031126730
	- ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸGPT-4oã‚’ä½¿ã£ã¦å‹•ç”»ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚’ã—ã¦ã¿ã‚‹ï¼
		- https://qiita.com/sakasegawa/items/b82a9745fda81143e409
- GPT-4oã«åŒ–å­¦å®Ÿé¨“ã®çµæœã‚’è€ƒå¯Ÿã•ã›ã¦ã‚‹ã‚“ã§ã™ãŒã€ è€ƒå¯ŸãŒæ—©ã™ãã¦ã€ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãŒè¿½ã„ã¤ã‹ãªã„ã§ã™ã€€ by ç• å±±ã•ã‚“
	- https://x.com/kanhatakeyama/status/1790098210360537138
- ï¼ˆOpenAIã®æ–°ã—ã„tokenizerã¯ï¼‰æ—¥æœ¬èªã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæ”¹å–„ã•ã‚Œã¦ã‚‹ã‹ã‚‰ã€APIä½¿ç”¨æ–™50% x ãƒˆãƒ¼ã‚¯ãƒ³é‡70% ã§ 35% ãã‚‰ã„ã®è²»ç”¨ã«ãªã‚‹ã®ã‹ï¼Ÿ
	- https://x.com/MLBear2/status/1790081289367990668
- LangChainãŒgpt-4oã«å¯¾å¿œ
	- https://x.com/LangChainAI/status/1790089006455398583
	- You can use the available multimodal capabilities of it in any of your LangChain applications today!
	- https://python.langchain.com/v0.1/docs/integrations/chat/openai/
- TJOã•ã‚“ã€ãƒ‡ã‚¸åºã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆå…¬å‹Ÿã®è¦ä»¶ã‚’ã¿ã¦é ­ã‚’æŠ±ãˆã‚‹
	- https://x.com/TJO_datasci/status/1790046279428345990
	- ã€Œã“ã®ã‚¹ã‚­ãƒ«ã®å¿…é ˆè¦ä»¶ã‚’å…¨éƒ¨æº€ãŸã—ã¦å°šä¸”ã¤æ­“è¿é …ç›®ã‚‚è¤‡æ•°æº€ãŸã™ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãªã‚“ã¦ã€ãã‚‚ãã‚‚æ—¥æœ¬ã©ã“ã‚ã‹ä¸–ç•Œã‚’è¦‹æ¸¡ã—ã¦ã‚‚æ¢ã—å‡ºã™ã®ã¯å›°é›£ã‚’æ¥µã‚ã‚‹ã®ã§ã¯ã€‚ãã‚Œã‚’å…¬å‹™å“¡ã®çµ¦ä¸ã§é›‡ã†ã¨ã‹ç„¡ç†ã‚²ãƒ¼ã«ã‚‚ç¨‹ãŒã‚ã‚‹ã‚ˆã†ãªæ°—ãŒã™ã‚‹ã€
	- çµå±€ãŠé‡‘ã®å•é¡Œã‹
- GPT-4oã¯Playgroundã§è©¦ã›ã‚‹ã€‚ ç¢ºã‹ã«è³¢ã„ã—ã‚‚ã®ã™ã”ãé€Ÿã„ by shi3zã•ã‚“
	- https://x.com/shi3z/status/1790073756079059400
- Command-R-Plus, Llama-3, Phi-3 miniã‚’ ELYZA-tasks-100 ã§è©•ä¾¡
	- https://qiita.com/wayama_ryousuke/items/a96f11fe2b7e2e3910e5
	- ã€Œä»Šå›ã”ç´¹ä»‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸè¿½åŠ å­¦ç¿’ã‚’è¡Œã‚ãªãã¦ã‚‚ã€æ—¥æœ¬èªã§å›ç­”ã‚’è¿”ã™ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†ç‚¹ãŒå¤§ããªç‰¹å¾´ã§ã™ã€‚ã€
	- è©•ä¾¡ç”¨ Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚‚ã‚ã‚‹ã‚ˆ
- Embeddingãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ã—ãã¿ã€fine-tuningæ‰‹æ³•ã‚’è§£èª¬
	- https://speakerdeck.com/payanotty/embeddingmoderuwoshi-tutabekutoruhua-nosikumi-fine-tuningshou-fa-wojie-shuo
-  State-Free Inference of State-Space Models: The Transfer Function Approach
	- https://arxiv.org/abs/2405.06147v1
	- Utilizing the connections between convolutions in the time domain and multiplication in frequency domain (through FFT),
-  GPT-4o ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/n02331040d8c2?sub_rt=share_b
- JSLM2ï¼ˆJapanese Stable LM 2 Instruct 1.6Bï¼‰
	- JSLM2ã¯ã€6Bä»¥ä¸‹ã®è¦æ¨¡ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã€æ—¥æœ¬èªæ€§èƒ½ãŒæœ€ã‚‚é«˜ã„ã¨æ€ã„ã¾ã™ã€‚é•ã„ã¾ã™ã‹ï¼Ÿ (llm-jp-evalã‚„å®šæ€§è©•ä¾¡ã§ï¼‰
	- https://x.com/peacej/status/1789909011132805402
- gpt-4o ã§ä½¿ã‚ã‚ŒãŸo200k_base tokenizer ã®æ—¥æœ¬èªã®éƒ¨åˆ†ãƒ»ãƒ»ãƒ»å®Œå…¨ã«5ã¡ã‚ƒã‚“ã­ã‚‹ãƒ»ãƒ»ãƒ»
	- https://x.com/_aixile/status/1790278857641410662
- Andrew Ngå…ˆç”Ÿã«ã‚ˆã‚‹AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€£è¼‰
	- https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/
	-  Agentic Design Patterns Part 1
	- Reflection, ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨, ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°, è¤‡æ•°Agentã®å”åŠ›
- gpt-4oã€è«–æ–‡è¦ç´„ã—ã¦PowerPointåã„ã¦ãã‚Œã‚‹
	- https://x.com/CurveWeb/status/1790336171777917332
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM)ã«ãŠã‘ã‚‹ä½ç²¾åº¦æ•°å€¤è¡¨ç¾ by PFNã®ä¸‰ä¸Šã•ã‚“
	- https://speakerdeck.com/pfn/20240508-hpckenkyukai-pfn-llm
	- å­¦ç¿’ï¼š8bitãŒä¸»æµã«ãªã‚Šã¤ã¤ã‚ã‚‹
	- æ¨è«–ï¼š1ï½2bitè¡¨ç¾ãŒå®Ÿç”¨åŒ–ã•ã‚Œã¤ã¤ã‚ã‚‹
- Introducing Veo: our most capable generative video modelã€€at Google I/O
	- https://x.com/GoogleDeepMind/status/1790435824598716704
	- It can create high-quality, 1080p clips that can go beyond 60 seconds. From photorealism to surrealism and animation, it can tackle a range of cinematic styles
- llamafiles from mozilla
	- https://x.com/llama_index/status/1790449858899505616
	- Build a local, private research assistant running on your laptop in a snap with llamafile from Mozilla! 
	- llamafiles are fun: no need to install anything, just download the file and run it, and you get a local LLM and embedding model that you can use directly from LlamaIndex. 
	- https://github.com/Mozilla-Ocho/llamafile
- langchain-huggingface ã®ã‚¢ãƒŠã‚¦ãƒ³ã‚¹
	- https://x.com/LangChainAI/status/1790419422399877158
	- We're excited to announce the launch of langchain-huggingface, a partner package in LangChain jointly maintained with huggingface
- Entropy minimization ãŒãªã‚“ã§æœ‰åŠ¹ã‹ï¼Ÿ ICML
	- https://x.com/ori_press/status/1790383780642918469
	- https://arxiv.org/pdf/2405.05012
- Evaluation of Retrieval-Augmented Generation: A Survey
	- https://arxiv.org/abs/2405.07437
	- https://github.com/YHPeter/Awesome-RAG-Evaluation
- Pattern Language is one of my favorite books, and this abridged hypertext version by zenodotus280
	- https://x.com/kepano/status/1790437820487946630
	- This project is an abridged, hyper-textual, and copyleft manifestation of the 1977 architecture classic _A Pattern Language_ by Christopher Alexander
		- https://github.com/zenodotus280/apl-md
- GPT-4oã®ç™»å ´ã«ã‚ˆã‚Šã€æ¤œè¨ä¸­ã®ç ”ç©¶ãƒ—ãƒ­ãƒãƒ¼ã‚¶ãƒ«ãŒãŠé‡ˆè¿¦ã«ãªã‚‹ã¨ã„ã†ãƒã‚¹ãƒˆãŸã¡
	- ã€ŒéŸ³å£°è¨€èªãƒ¢ãƒ‡ãƒ«ã€ã«ã¤ã„ã¦å­¦æŒ¯æ›¸ã„ã¦ã„ã‚‹é–“ã«ã“ã‚Œã¯ã²ã©ã™ãã‚‹ã§ã—ã‚‡
		- https://x.com/nonmetal_/status/1790079046191120560
	- GPT-4o makes me feel both sad for my current work has been scooped by OpenAI, and happy that we are on the right track. by SpeechGPTã‚·ãƒªãƒ¼ã‚ºã®é–‹ç™ºè€…
		- https://x.com/dongzha35524835/status/1790241799547806071
- multi-step reasoning capabilities to Google Search at Google I/O
	- https://x.com/Google/status/1790438800667123860
	- perplexityã®ã‚ˆã†ãªã‚‚ã®ãŒãã‚‹ã®ã‹ã€
- Linear Regression is one of the most important tools in a Data Scientist's
	- https://x.com/mdancho84/status/1790445342787318206
	- 1. OLS regression aims to find the best-fitting linear equation that describes the relationship between the dependent variable (often denoted as Y) and independent variables (denoted as X1, X2, ..., Xn).
	- 2. OLS does this by minimizing the sum of the squares of the differences between the observed dependent variable values and those predicted by the linear model. These differences are called "residuals."
	- 3. "Best fit" in the context of OLS means that the sum of the squares of the residuals is as small as possible. Mathematically, it's about finding the values of Î²0, Î²1, ..., Î²n that minimize this sum.
- Weâ€™re sharing Project Astra: at Google I/O
	- https://x.com/GoogleDeepMind/status/1790433540548558853
	- Weâ€™re sharing Project Astra: our new project focused on building a future AI assistant that can be truly helpful in everyday life.
- PaliGemmaã€Gemma 2ã€€at Google I/O
	- https://x.com/GoogleDeepMind/status/1790459505538658636
	- PaliGemma: a powerful open vision-language model  
	- Gemma 2: coming soon in various sizes, including 27 billion parameters
- GPT-4o shows improvement compared to GPT-4-Turbo-0409 with better probability, pre-calculus, algebra, and geometry abilities. 
	- https://x.com/GanjinZero/status/1790230562453803241
- Get a sneak peek of Gemma 2, at Google I/O
	- https://x.com/Google/status/1790452314278412554
	- 27B parameter instance launching in a few weeks. Built on new architecture, Gemma 27B outperforms models twice its size and can run on a single TPU host in Vertex AI.
- PaliGemma ã‚’ãŠè©¦ã—ä¸­
	- https://huggingface.co/spaces/big-vision/paligemma
- Introducing Trillium, the next generation of Google Cloud TPU
	- https://x.com/GoogleCloudTech/status/1790452622295449925
	- It delivers 4.7X the peak compute performance per chip compared to TPU v5e and is equipped with 2X the high-bandwidth memory capacity.
- ZeTT
	- https://x.com/CurveWeb/status/1790308270126883146
	- ã€Œè¿½åŠ ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã»ã¨ã‚“ã©(ã‚‚ã—ãã¯å…¨ã)è¡Œã‚ãšã«ã€ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä»»æ„ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ‰‹æ³•ZeTTã€‚ ChatVector ã‚„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®Tokenizerã«ã‚ˆã‚‹åˆ¶é™ã‚’é¿ã‘ã‚‹ã®ã«ä½¿ãˆãã†ã€ by ã¯ã¡ã•ã‚“
- Gemini 1.5 Pro to 2 million tokens at Google I/O
	- https://x.com/Google/status/1790430189916225799
	- Today weâ€™re expanding the context window for Gemini 1.5 Pro to 2 million tokens and making it available for developers in private preview. Itâ€™s the next step towards the ultimate goal of infinite context
- Data Scientists: The next level of Data Science AI Agents is called "Plan and Execute".
	- https://x.com/mdancho84/status/1790406221616320862
- Googleã¨OpenAIã®ç™ºè¡¨ã‚’è¦‹ã¦ã‚‹åƒ•ã®å¿ƒå¢ƒ by GUILDã®æ·±æ¾¤ã•ã‚“
	- https://x.com/fladdict/status/1790431879512093151
	- ã€Œã‚ã€ã‚´ã‚¯ã‚¦ã¨ãƒ•ãƒªãƒ¼ã‚¶ãŒä¸Šç©ºã§æˆ¦ã£ã¦ã‚‹ï¼ï¼ï¼ã™ã”ã„é€Ÿåº¦ã§è¦‹ãˆãªã„ï¼ï¼ï¼é ‘å¼µã‚Œï¼ï¼ï¼ã€ã¨ã„ã†ã‚¯ãƒªãƒªãƒ³ã®å¿ƒå¢ƒ
- Ilya and OpenAI are going to part ways. by Sam
	- https://x.com/sama/status/1790518031640347056
	- This is very sad to me; Ilya is easily one of the greatest minds of our generation, a guiding light of our field, and a dear friend. His brilliance and vision are well known; his warmth and compassion are less well known but no less
- HCIç³»ã®ãƒˆãƒƒãƒ—ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹CHI2024ã®å…¨è«–æ–‡ã‚’GPT-4ã§è¦ç´„ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¾ã¨ã‚ã¦è¦‹ãŸã®ã§
	- https://drive.google.com/file/d/1CMkTdGlk1OhtScKUTB7Mt22GtWxgAIPV/view
- Colab ğŸ“’ Notebook to fine-tune ğŸ’…ğŸ½ @GoogleAI
	- #PaliGemma vision-language model ğŸ‘“ğŸ” ğŸ§ on a free T4 VM
	- https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb
- Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
	- https://note.com/panda_lab/n/n948053d7813f
	- RAGã®æ çµ„ã¿ã‚’æ‹¡å¼µã—ã€Wikipediaè‡ªå‹•ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒSTORMã€ã‚’è€ƒæ¡ˆã—ã¾ã—ãŸã€‚
	-  **èª²é¡Œ**: ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨˜äº‹ã¯ã€åºƒç¯„å›²ã®å‚è€ƒæ–‡çŒ®ã®åé›†ã¨ã€è©³ç´°ãªã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã®ä½œæˆãŒå¿…è¦ã§ã™ã€‚å¾“æ¥ã®æ–¹æ³•ã§ã¯ã€ã“ã®æº–å‚™æ®µéšãŒã—ã°ã—ã°çœç•¥ã•ã‚Œã¾ã™ã€‚
	-  **è§£æ±ºç­–**: STORMã¯ã€äºˆå‚™çš„ãªæ›¸ãè¾¼ã¿ã€è‰ç¨¿ä½œæˆã€æ”¹è¨‚ã®å„æ®µéšã€ç‰¹ã«äºˆå‚™æ®µéšã§ã®åŠ¹æœçš„ãªè³ªå•æèµ·ã«ã‚ˆã‚Šã€ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚
- ã€ŒStockmark-100bã€
	- https://x.com/kosukearima/status/1790902109648695565
	- ç”£ç·ç ”ï½˜ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯å…±åŒç ”ç©¶ã®æˆæœã€ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§å­¦ç¿’ã—ãŸ100Bç´šæ—¥æœ¬èªLLMã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- ã‚¹ãƒˆãƒƒã‚¯ãƒãƒ¼ã‚¯ã¯ã€1000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã€ŒStockmark-100bã€ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã«ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã‚’è¡Œã„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‚ã®ã§ã¯ãªãã€ã‚¼ãƒ­ã‹ã‚‰ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§é–‹ç™ºã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€å›½å†…ã§ã¯(ç¾çŠ¶ã¯ãƒ€ãƒ³ãƒˆãƒ„ã§)æœ€å¤§ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ã‚‚æœ€å¤§ç´šã‚µã‚¤ã‚ºã®OSSãƒ¢ãƒ‡ãƒ«ã¨ãªã‚Šã¾ã™ã€‚
		- https://huggingface.co/stockmark/stockmark-100b
- ã€Œç¢ºç‡æ€è€ƒã®æˆ¦ç•¥è«–ã€ãŒã‚‚ã‚„ã‚‚ã‚„ã™ã‚‹æ–¹ã¸ -NBDãƒ¢ãƒ‡ãƒ«ç·¨-
	- https://zenn.dev/joanofarc/articles/strange-theory-of-probability-thinking
- LLM ã«è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è§£ã‹ã›ãŸã‹ã£ãŸã®ã§ã€ã¡ã‚‡ã£ã¨è©¦ã—ã¦ã¿ãŸ
	- https://developers.cyberagent.co.jp/blog/archives/47869/
	- In-context Learning ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ‰‹æ³•ã«æ¡ç”¨ã—ã¦ã„ã‚‹ã€Œè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è§£ãã€ã«é–¢ã™ã‚‹è«–æ–‡(ICML2024)ã‚’ã€å€‹äººçš„ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã§ç´¹ä»‹ã—ã¦ã¿ã¾ã—ãŸã€‚
	- https://openreview.net/forum?id=4L0xnS4GQM
- è² ã®äºŒé …åˆ†å¸ƒã®ç–«å­¦ã¨ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã§ã®å¿œç”¨ã®æ¯”è¼ƒ
	- https://socinuit.hatenablog.com/entry/2024/05/16/185601
	- è¥¿æµ¦ã€æ„ŸæŸ“ç—‡ã‚’èª­ã¿è§£ãæ•°ç†ã€ã¨æ£®å²¡ãƒ»ä»Šè¥¿ã€ç¢ºç‡æ€è€ƒã®æˆ¦ç•¥è«–ã€ã«ãŠã„ã¦ã€è² ã®äºŒé …åˆ†å¸ƒã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«å¿œç”¨ã«ã¤ã„ã¦è¨˜è¿°ã•ã‚Œã¦ãŠã‚Šã€ ç›¸äº’ã‚’å‚ç…§ãƒ»æ¯”è¼ƒã™ã‚‹ã“ã¨ã§é¡ä¼¼ç‚¹ã‚„è§£é‡ˆã®æ‹¡å¤§ã‚’è©¦ã¿ã‚‹ã€‚
	- ã“ã®è¨˜äº‹ã§è¿°ã¹ãŸã„ã“ã¨ã¯ã€**ç–«å­¦ã¨ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã¨ã„ã†ä¸€è¦‹ã—ã¦è·é›¢ã®ã‚ã‚‹é ˜åŸŸã§ã€è² ã®äºŒé …åˆ†å¸ƒã‚’ç”¨ã„ãŸç¾è±¡ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«åŒ–ã®äº‹ä¾‹ãŒå–ã‚Šä¸Šã’ã‚‰ã‚Œã¦ã„ã¦é¢ç™½ã„ã­**ã€ã¨ã„ã†ã“ã¨ã«å°½ã
- æœ€è¿‘Gemini 1.5 Proã®PDFãƒ‘ãƒ¼ã‚¹ãŒä¾¿åˆ©ã ã¨æ°—ã¥ã„ã¦è‰²ã€…è©¦ã—ã¦ã„ã‚‹
	- https://x.com/resnant/status/1791104886563811520
	- ä»Šã®ã¨ã“ã‚å…ˆè¡Œç ”ç©¶ã®è«–æ–‡PDFã‚’æ”¾ã‚Šè¾¼ã‚“ã§ã€Œã“ã®ç ”ç©¶ã®XXã¨ã„ã†ç‚¹ã‚’è§£æ±º/æ‹¡å¼µã™ã‚‹ã¨ã©ã‚“ãªä¾¡å€¤ãŒç”Ÿã¾ã‚Œã‚‹ï¼Ÿãã®çµæœã‚’ã©ã†è¨´æ±‚ã™ã‚‹ï¼Ÿã€ã¿ãŸã„ã«ç ”ç©¶ãƒã‚¿ã®å£æ‰“ã¡ã§ä½¿ã†ã¨å¿ƒå¼·ã„
-  2023å¹´åº¦ ãƒ‡ã‚¸ã‚¿ãƒ«åºãƒ»è¡Œæ”¿ã«ãŠã‘ã‚‹ç”ŸæˆAIã®é©åˆ‡ãªåˆ©æ´»ç”¨ã«å‘ã‘ãŸæŠ€è¡“æ¤œè¨¼ã‚’å®Ÿæ–½ã—ã¾ã—ãŸ
	- https://www.digital.go.jp/news/19c125e9-35c5-48ba-a63f-f817bce95715
	- ã€Œå®Ÿè¨¼ã®æœ€ä¸­ã«ã‚‚ç’°å¢ƒãŒæ¿€å¤‰ã™ã‚‹ä¸­ã§ã€ãŸã è©¦ã—ã¦çµ‚ã‚ã‚‰ã›ã‚‹ã®ã§ã¯ãªãã€ã—ã£ã‹ã‚Šã¨çŸ¥è¦‹ã‚’å…±æœ‰ã—ã€ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã¯ã˜ã‚æ¬¡ã®å‹•ãã«ç¹‹ã’ã¦ã„ãã“ã¨ãŒé‡è¦ã¨è€ƒãˆã¦ã„ã¾ã™ã€ by æ¥ ã•ã‚“
	- https://x.com/masanork/status/1790871089121513629
- ChatGPTãŒGoogle Driveã‚„Microsoft OneDriveã‹ã‚‰Spreadsheetã‚„Excelã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æãƒ»å¯è¦–åŒ–ã‚’æ‰‹ä¼ã£ã¦ãã‚Œã‚‹æ©Ÿèƒ½ã‚’è¿‘ãå…¬é–‹
	- https://x.com/MLBear2/status/1791251518110523764
-  HCIç ”ç©¶ã«å¯¾ã™ã‚‹ç§è¦‹ - CHI2024å‚åŠ ã‚’çµ‚ãˆã¦ã€€by ç¨²è¦‹å…ˆç”Ÿ
	- https://note.com/drinami/n/nfd4921806ad3?sub_rt=share_pb
	- ã€ŒHCIç ”ç©¶ãŠã‚‚ã¡ã‚ƒè«–ã€ãŒã‹ã¤ã¦è­°è«–ã•ã‚Œã¦ã„ãŸã“ã¨ãŒã‚ã‚Šã¾ã—ãŸãŒã€ŒãŠã‚‚ã¡ã‚ƒã«è©°ã¾ã£ãŸçŸ¥æµã¨å½¹å‰²ã‚’ãƒã‚«ã«ã™ã‚‹ãªã€‚æ¯”å–©ã¨ã—ã¦ä¸é©åˆ‡ã€ã¨ã„ã†ã®ãŒç§ã®è¦‹è§£ã§ã™
- ã€Œç«¶äº‰åŠ›ã‚ã‚‹ç”ŸæˆAIåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºï¼ˆåŠ©æˆï¼‰ã€ã«ã€ELYZAãŒæ¡æŠ
	- https://x.com/ELYZA_inc/status/1791348009764360577
	- çµŒæ¸ˆç”£æ¥­çœãŒç«‹ã¡ä¸Šã’ãŸã€ŒGENIACã€ã®ã‚‚ã¨ã€NEDOãŒå…¬å‹Ÿ
	- Mixture of Expertsã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¡ç”¨ã‚„ã€æ—¥æœ¬ç‰¹æœ‰ã®ãƒ‡ãƒ¼ã‚¿ã®å­¦ç¿’ã€æ—¥æœ¬èªã®æ¨è«–åŠ¹ç‡åŒ–ãªã©ã‚’å®Ÿæ–½äºˆå®š
- "functional ontology"
	- https://x.com/Westoncb/status/1791152606309687768
	- As an alternative to LLM summarizing, I've been getting very interesting results doing something like:
	- https://symbolflux.com/ApolloLunarLandingTrajectoryReconstruction.txt
- QA over large embedded tables without hallucinations (Caltrain schedule edition
	- https://x.com/llama_index/status/1791505972407746671
	- With LlamaParse, we were able to spatially layout the text in a semantically coherent manner, so that our GPT-4o-powered QA pipeline could correctly answer questions
		- https://github.com/run-llama/llama_parse/blob/main/examples/caltrain/caltrain_text_mode.ipynb
- MediaPipe LLM Inference APIã‚’ä½¿ã£ã¦ã€MediaPipeå½¢å¼ã«å¤‰æ›ã™ã‚‹ã¨Gemma 2Bã‚„ ã¨Gemma 7Bã€Phi-2ã€Falcon-RW-1Bã€StableLM-3Bãªã©ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã‚„Androidsã€iphoneãªã©ã§å‹•ã‹ã™äº‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹
	- https://x.com/webbigdata/status/1791497099315752967
	- You can now run the 7B parameter version of Gemma, entirely locally in the browser, using MediaPipe LLM Inference API.
		- https://x.com/googledevs/status/1791174333995299216
- stockmarkã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹stockmark-100bã®ggufã‚ã‚Šã¾ã™
	- https://huggingface.co/mmnga/stockmark-100b-gguf
- Gemini 1.5 Flashã§ã€12åˆ†ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¨ã¦æ–‡å­—èµ·ã“ã—ã€‚å®Œç’§ã€‚GPT-4oã§ã‚‚ã“ã‚Œã¯ç„¡ç†
	- https://x.com/Taiyo_AiAA/status/1791460870947774826
-  EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models
	- https://www.docswell.com/s/DeepLearning2023/K1JDN3-2024-05-16-142439#p9
	- äº‹å‰å­¦ç¿’æ¸ˆã¿ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ï¼Œè¿½åŠ ã®å­¦ç¿’ã‚’ä¸€åˆ‡è¡Œã‚ãšã«ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸè«–æ–‡ï¼æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯é«˜ç²¾ç´°ã«ç”»åƒç”Ÿæˆã‚’ã§ãã‚‹ãŒï¼Œãã®å†…éƒ¨ã«ã¯ç”»åƒç´°éƒ¨ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªæƒ…å ±ã‚’æŒã¤ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ï¼
-  LangChain v0.1 ã‹ã‚‰ v0.2 ã¸ã®ç§»è¡Œæ‰‹é † by npakaã•ã‚“
	- https://note.com/npaka/n/n161a7e3882b3?sub_rt=share_h
	-  importå¤‰æ›´ã®ä¾‹
		- **langchain â†’ langchain_community**
			- vectorstoresã¨ã‹
		- **langchain-community â†’ langchain_openai**
			- ChatOpenAIã¨ã‹
		- **langchain-community â†’ langchain-core**
			- document_loadersã¨ã‹
		- **langchain â†’ langchain-core**
			- Documentã¨ã‹
		- **langchain â†’ langchain-text-splitters**
			- text_splitterã¨ã‹
- ADA-V2ã€GPT-4oã ã‹ã‚’å¾®èª¿æ•´ã—ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½ä¸Šã’ãŸãƒ¢ãƒ‡ãƒ«
	- https://x.com/umiyuki_ai/status/1791429524351258857
	- OpenAIãŒGPT-4Tã ã‹GPT-4oã ã‹ã‚’å¾®èª¿æ•´ã—ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½ä¸Šã’ãŸãƒ¢ãƒ‡ãƒ«ã«ADA-V2ãªã‚“ã¦ãƒãƒ¼ãƒŸãƒ³ã‚°ä»˜ã‘ãŸã®ãŒãƒã‚¸ã ã¨ã—ãŸã‚‰ãã®ç†ç”±ã¯ï¼ŸAdaã¯GPT-3å››å¤©ç‹ã®ä¸­ã§æœ€å¼±ã®ãƒ¢ãƒ‡ãƒ«ã ã£ãŸã€‚ã¤ã¾ã‚Šã€GPT-4ãŒAda-V2ãªã‚‰Babaggi-V2ã‚„Curie-V2ã€ãã—ã¦Davinci-V2ã¯ã©ã†ãªã£ã¦ã—ã¾ã†ã®ã‹â€¦ï¼Ÿã¨ã„ã†åŒ‚ã‚ã›
-  Finetuning Llama3 using Unsloth
	- https://github.com/neo4j-labs/text2cypher/tree/main/finetuning/unsloth-llama3#using-chat-prompt-template
	- https://huggingface.co/tomasonjo/text2cypher-demo-16bit
	-  I've finetuned Llama3-Instruct:8b to generate @neo4j Cypher statements based on the GPT-4o synthetic dataset I've generated at the start of the week.
-  LangChain ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ by npakaã•ã‚“
	- https://note.com/npaka/n/n5956ef3a0a09?sub_rt=share_h
	- ã€ŒRAGã®QAã€ã¯ã€RAGæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®æƒ…å ±æºã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™
	- ã€Œæƒ…å ±æŠ½å‡ºã€ã¯ã€LLMã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã™ã€‚æ¬¡ã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ã€‚
		- **Tool Callingãƒ¢ãƒ¼ãƒ‰** : Tool Callingã§æŒ‡å®šã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›
		- **JSONãƒ¢ãƒ¼ãƒ‰** : ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã¨ã—ã¦ã‚¹ã‚­ãƒ¼ãƒã‚’æä¾›ã—ã€JSONãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›
		- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹** : æŒ‡ç¤ºã«å¾“ã£ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¢å­˜ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã§è§£æã—ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›
	- ã€Œãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã¯ã€é•·æœŸçš„ãªå¯¾è©±ã‚’ç¶­æŒã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã¾ã™
	- ã€Œãƒ„ãƒ¼ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã¯ã€è‡ªç„¶è¨€èªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é€šã˜ã¦APIã‚„é–¢æ•°ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãªã©ã®ãƒ„ãƒ¼ãƒ«ã‚’æ“ä½œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚  
		- **ãƒã‚§ãƒ¼ãƒ³** : ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã®äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
		- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ** : ãƒ„ãƒ¼ãƒ«ã‚’ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•çš„ã«å®Ÿè¡Œ
	- ã€Œã‚¯ã‚¨ãƒªè§£æã€ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’æœ€é©åŒ–ã—ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã€ã‚ˆã‚Šæ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
		- æ‰‹æ³•ã«ã¯ã€ã‚¯ã‚¨ãƒªã®åˆ†è§£ã€ã‚¯ã‚¨ãƒªæ‹¡å¼µã€ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åŸ‹ã‚è¾¼ã¿ã€ã‚¯ã‚¨ãƒªã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¯ã‚¨ãƒªã®æ§‹é€ åŒ–ãªã©ãŒã‚ã‚Šã¾ã™ã€‚
	- SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹QAã€ã¯ã€ã€ŒSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ã‚’å¯¾è±¡ã¨ã—ãŸQ&Aã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
	- ã€Œã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®QAã€ã¯ã€ã€Œã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ã‚’å¯¾è±¡ã¨ã—ãŸQ&Aã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™
		- ã€ŒCypherã€ã‚„ã€ŒSparQLã€ãªã©ã®ã‚°ãƒ©ãƒ•ã‚¯ã‚¨ãƒªè¨€èªã‚’ä½¿ç”¨ã—ã€è‡ªç„¶è¨€èªã®è³ªå•ã«åŸºã¥ã„ã¦ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±ã‚’å–å¾—ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚„ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã™
	- ã€Œã‚³ãƒ¼ãƒ‰ç†è§£ã€ã¯ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®åˆ†æã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã«é–¢ã™ã‚‹Q&Aã‚’è¡Œã„ã€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚„æ”¹å–„ã®ææ¡ˆã€ã‚³ãƒ¼ãƒ‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã‚’æ”¯æ´ã—ã¾ã™
	- ã€Œãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã€ã¯ã€äººå·¥çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„ãƒ†ã‚¹ãƒˆã«å½¹ç«‹ã¦ã¾ã™
	- ã€Œã‚¿ã‚°ä»˜ã‘ã€ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ„Ÿæƒ…ã€è¨€èªã€ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒˆãƒ”ãƒƒã‚¯ã€æ”¿æ²»çš„å‚¾å‘ãªã©ã®ã‚¯ãƒ©ã‚¹ã‚’ãƒ©ãƒ™ãƒ«ä»˜ã‘ã—ã¾ã™
	- ã€Œè¦ç´„ã€ã¯ã€é•·ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’è¦ç´„ã™ã‚‹ãŸã‚ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’åŠ¹ç‡çš„ã«è¦ç´„ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã¯ã€ã€ŒStuffã€ã€ŒMap-Reduceã€ã€ŒRefineã€ã®3ã¤ã®æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚
	- ã€ŒWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€ã¯ã€Webã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†ã—ã€è‡ªç„¶è¨€èªå‡¦ç†ã«ä½¿ç”¨ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
-  LangChain v0.2 ã§ å˜ç´”ãªLLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ by npakaã•ã‚“
	- https://note.com/npaka/n/n24d48303a496?sub_rt=share_h
-  LangChain v0.2 ã§ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ by npakaã•ã‚“
	- https://note.com/npaka/n/ne8ef60987e1b?sub_rt=share_b
- ç”ŸæˆAIå­¦ã³ãŸã„ãªã‚‰ã€ã“ã®ï¼’æœ¬ by å°¾åŸã•ã‚“
	- https://x.com/kazobara/status/1791454624983196148
	- ã€Œç”ŸæˆAIã€(3) æ¾å°¾è±Šãƒ»æ±äº¬å¤§å­¦å¤§å­¦é™¢æ•™æˆã€€2024.3.15"
		- https://www.youtube.com/watch?v=U9vhGvFxKu0
	- "GPTã¨ã¯ä½•ã‹ Transformerã®è¦–è¦šåŒ– |
		- https://www.youtube.com/watch?v=KlZ-QmPteqM
-  LangChain v0.2 ã§ RAGã‚’æ§‹ç¯‰ by npaka ã•ã‚“
	- https://note.com/npaka/n/ne892b713bd45?sub_rt=share_h
	-  Retriever
		- ã€ŒVectorStoreã€ã¯Runnableã‚’ã‚µãƒ–ã‚¯ãƒ©ã‚¹åŒ–ã—ãªã„ãŸã‚ã€LECLãƒã‚§ãƒ¼ãƒ³ã«ã™ãã«çµ±åˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã€ŒRetrieverã€ã¯Runnableã§ã‚ã‚‹ãŸã‚ã€æ¨™æº–ã‚»ãƒƒãƒˆã®ãƒ¡ã‚½ãƒƒãƒ‰ (åŒæœŸãŠã‚ˆã³éåŒæœŸã®å‘¼ã³å‡ºã—ã‚„ãƒãƒƒãƒæ“ä½œãªã©) ã‚’å®Ÿè£…ã—ã€LCELãƒã‚§ãƒ¼ãƒ³ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
	-  RAGãƒã‚§ãƒ¼ãƒ³
		- ã€ŒRetrieverã€ã¯ã€ç‰¹å®šã®è³ªå•ã¨å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿åˆã‚ã›ã¦ LLM ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹RAG ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€ã‚ˆã‚Šè¤‡é›‘ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ç°¡å˜ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
		- rag_chain = {"context": retriever, "question": RunnablePassthrough()} | prompt | llm
- â€œMambaOut: Do We Really Need Mamba for Vision?â€
	- https://arxiv.org/abs/2405.07992
	- Based on our concept discussion, we hypothesize Mamba is unnecessary for ImageNet while exploring for detection and segmentation remains worthwhile. To verify these, we build MambaOut with Mamba blocks but remove their core token mixer, SSM.
- Text-to-SQL - fully local edition
	- https://x.com/llama_index/status/1791915423816204494
	- The latest local LLMs are not only capable of RAG synthesis, but also querying structured databases. Diptiman Raichaudhuri has a great tutorial on how to build a fully local text-to-SQL setup, letting you query local databases without an internet connection.
	-  Text2SQL OpenSource : duckdb-nsql-7B with Ollama and LlamaIndex on local setup
		- https://diptimanrc.medium.com/text2sql-opensource-duckdb-nsql-7b-with-ollama-and-llamaindex-on-local-setup-6f266f78bc4f
- 

## 5/13

å…ˆé€±ã«å¼•ãç¶šãgpt2-chatbotãŒchatbod arenaã«å¾©æ´»ã—ãŸã‚Šã¨ã€è©±é¡Œã«äº‹æ¬ ã‹ãªã„ãŒã€ã‚µãƒ (OpenAIã®ç¤¾é•·)ã‹ã‚‰ã€5/13æœˆæ›œæ—¥ã«ä½•ã‹ç™ºè¡¨ãŒã‚ã‚‹ã¨ã®ãƒã‚¹ãƒˆãŒã€GPT-5ã§ã‚‚ï¼ˆã†ã‚ã•ã®ï¼‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚‚ãªã„ã¨ã„ã£ã¦ã„ã‚‹ã—ã€æ˜ ç”»Herã«å‡ºã¦ããŸã‚ˆã†ãªéŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã„ã†ä¸‹é¦¬è©•ã€‚ãŠã£ã¨COCONAï¼ˆ_ã‚³ã‚³ãƒŠ_ï¼‰ã®ç«‹å ´ã¯ï¼Ÿã€‚OpenAIã¨ã„ãˆã°ã€Stack Overflowã¨ã®ææºã€å›ç­”è€…ã«chatptãŒç™»å ´ã™ã‚‹ã®ã‹ã€ã¾ãŸãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã¹ãã‹ã‚’è¦å®šã™ã‚‹Model Specã‚’å…¬é–‹ã€EUã®AIæ³•å¯¾ç­–ã‹ï¼ˆä»¥å‰ã¯System CardãŒãã®å½¹å‰²ã ã£ãŸï¼‰ã¨ã‚‚è¦‹ã‚Œã‚‹ã—ã€å®‰å…¨æ€§ã«æœ¬æ°—ã«å–ã‚Šçµ„ã‚“ã§ã„ã‚‹å§¿å‹¢ã«ã‚‚ã¿ãˆã‚‹ã€ã¨ã‚‚ã‹ãæ¥ãŸã‚‹GPT-5ã®ç´ æ€§ã‚‚é€ã‘ã¦è¦‹ãˆã‚‹ã¨ã„ã†ã®ã¯é¢ç™½ã„åˆ†æã€‚ã‚ã¨ã€ä»Šé€±ã¯å›½å†…å‹¢ã®æ´»èºã‚‚æ´»ç™ºã ã£ãŸã€æ±å·¥å¤§ã®Swallow-MX-8x7b-NVE-v0.1ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸKARAKURI LM 8x7B Chat v0.1ã€13Bã§104Bã®Command R+ã‚’è¶…ãˆã‚‹ã£ã¦æœ¬å½“ï¼Ÿã€‚ã€ŒJapanese Stable LM 2 1.6Bã€ã€ å±æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã€€KARAKURI LM 7B APM v0.1 ã€ã€ŒFugaku-LLMã€ã®å…¬é–‹ãªã©ã€‚ã•ã¦æ§˜ã€…ãªè©•ä¾¡ã‹ã‚‰æ€§èƒ½ãŒé«˜ã„ã€ä½¿ãˆã‚‹ã€ã¨ã•ã‚Œã¦ã„ã‚‹llama3ã€æ—¥æœ¬èªãŒã‚„ã£ã±ã‚Šãƒ€ãƒ¡ãƒ€ãƒ¡ã ã£ãŸã‚Šã¯ã”æ„›æ•¬ã§ã‚‚ã€é‡å­åŒ–ã«å¼±ã‹ã£ãŸã‚Šï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§æ€§èƒ½ãŒé«˜ã„ã¨ã„ã†ã®ã¯é‡å­åŒ–ã®ä½™åœ°ã‚‚å°‘ãªã„ï¼‰ã¨ã€LLMã®ã‚¹ã‚±ãƒ¼ãƒ«æ¸¬ã®ä¸€ç«¯ã‚’æ€ã„çŸ¥ã‚‰ã™çµæœã«ãªã£ã¦ã‚‹ã¨ã„ã†ã®ã¯é¢ç™½ã„ã€tokenerizerãŒå£Šã‚Œã¦ã„ã‚‹ã¨ã®ã†ã‚ã•ã‚‚ã€‚"DeepSeek-V2"ã£ã¦ã®ãŒGPT-4ã¨åŒãƒ¬ãƒ™ãƒ«ã€‚ã‹ã‹ã‚‹ã‚³ã‚¹ãƒˆã¯200åˆ†ã®1ã¨ã„ã†ã®ã¯æœ¬å½“ã ã‚ã†ã‹ï¼ŸGoogle/DeepMindã‹ã‚‰ã¯ã€ŒAlphaFold 3ã€ã‚’ç™ºè¡¨ã€ã“ã‚“ã©ã¯DNAã‚‚æ‰±ãˆã‚‹ã¨ã®ã“ã¨ã€å‰µè–¬ãŒåŠ‡çš„ã«åŠ é€Ÿã™ã‚‹äºˆæ„Ÿã€‚å…ˆé€±ã«å¼•ãç¶šã„ã¦KANã®è©•ä¾¡ã‚‚é€²ã‚€ã€shi3zã•ã‚“ã®ã€Œæœ€å¾Œã«KANã¯å‹ã¤ã€ã¨ã„ã†KANè©•ä¾¡è©¦è¡Œã®noteã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œæœ€å¾Œã«æ„›ãŒå‹ã¤ by KANã€ã®ã‚‚ã˜ã‚Šï¼Ÿãã‚Œã«ã—ã¦ã‚‚KANã•ã‚“ã”å†¥ç¦ã‚’ãŠç¥ˆã‚Šã—ã¾ã™ã€‚MicrosoftãŒè‡ªç¤¾è£½LLMã§ã‚ã‚‹ã€ŒMAI-1ã€ã‚’é–‹ç™ºä¸­ã€ï¼¸ã¯Grokã‚’æœ‰æ–™ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é–‹æ”¾ã€‚Deeplearning.aiã‹ã‚‰ã¯ã€llamaindexã®Jerry Liu(CEO)ã‚’è¬›å¸«ã«Agentic RAGã€LangChainã®Harrison(CEO)ã‚’è¬›å¸«ã«ã€Functions, Tools and Agentsã®ã‚·ãƒ§ãƒ¼ãƒˆã‚³ãƒ¼ã‚¹ãŒç„¡æ–™å…¬é–‹ã€ãªã‚“ã¦è±ªè¯ãªã€‚ãã®LangChainã¯v0.2ãŒãƒªãƒªãƒ¼ã‚¹ãŒé–“è¿‘ã«ã€Agentã‚„Toolé–¢é€£ã®è¦‹ç›´ã—ãŒã•ã‚Œã‚‹ã€‚ã‚ã¨ãªãœã‹ã€æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ç™ºè¡¨ã‚‚ç›¸æ¬¡ã„ã ã€Google/TimesFMã€IBMã®TinyTimeMixers (TTMs)ã€ICML2024ã«ã‚¢ã‚¯ã‚»ãƒ—ãƒˆã•ã‚ŒãŸã€CMUã¨UPENã®MOMENTã€ã²ã‚‡ã£ã¨ã—ã¦ICML2024ãŒTime Seriesã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ç¥­ã‚Šã«ãªã£ã¦ã‚‹ã®ã‹ã€‚æ—©é€Ÿã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ã¯ã€(AirPassengerãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ï¼‰ä¸€éšå·®åˆ†ã‚‚ã¨ã‚‰ã‚“ã®ã‹ã¨å†·ç¬‘ã‚‚ã€‚xLSTMã¨ã‹ã€Vanilla Bayesian Optimization ã¨ã‹ã®åŸºç›¤æŠ€è¡“ã®é€²å±•ã‚‚ã‚ã‚Šã€ã—ã‚‰ã‚“ã‘ã©ã€‚ã€‚å–œé€£å·å…ˆç”Ÿç›£ä¿®ã®ã€Œç”ŸæˆAIã®è«–ç‚¹ã€ã¨ã„ã†ã®ã¯ã€æ—¥æœ¬ã®LLMã‚’ã‚ãã‚‹çŠ¶æ³ã‚’æŠŠæ¡ã«ã¯ã‚ˆã„ã‹ã‚‚ã€ãã‚Œã«ã—ã¦ã‚‚ã€Œæƒ…å ±å¤§èˆªæµ·æ™‚ä»£ã€ã¯ãªããªã£ãŸã“ã¨ã«ãªã£ãŸã®ã‹ï¼Ÿæœ€å¾Œã«ã€THE GUILDã®æ·±æ´¥ã•ã‚“ã®ã€ã€Œæƒ…å ±ãŒå¤šã™ãã¦é ­ãŒãƒ‘ãƒ³ã‚¯ã™ã‚‹ã®ã¯æ­£å¸¸ã§ã¯ãªã„ã€ã¨ã„ã†ã®ã¯ã€æ¿€ã—ãåŒæ„ã™ã‚‹ã€‚

- gpt2-chatbotãŒchatbot arenaã«å¾©æ´»ï¼Ÿ
	- https://x.com/alfredplpl/status/1787754701536325720
-  æœ€å¾Œã«KANã¯å‹ã¤ã®ã‹?MLPã«å¤‰ã‚ã‚‹ã¨ä¸»å¼µã•ã‚Œã‚‹KANã‚’è©¦ã™ by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/n1e592409a345?sub_rt=share_pb
	- Efficient-KANãŒæ‰‹ã£å–ã‚Šæ—©ãMNISTãŒè©¦ã›ãã†ã ã£ãŸã®ã§è©¦ã—ã¦ã¿ãŸ
	- ã¤ã¾ã‚Šã€åŒè¦æ¨¡ã®å ´åˆã€å­¦ç¿’ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯10å€ã«ãªã‚Šã€æ€§èƒ½å·®ã¯ç¸®ã‚“ã§ã„ãã¨ã„ã†çµæœã«ãªã£ãŸ
- KARAKURI LM 8x7B Chat v0.1ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼
	- https://huggingface.co/karakuri-ai/karakuri-lm-8x7b-chat-v0.1
	- 1. æ±å·¥å¤§ã‹ã‚‰å‡ºã¦ã„ã‚‹Swallow-MX-8x7b-NVE-v0.1ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚«ãƒ©ã‚¯ãƒªã®ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚ï¼ˆåœ§å€’çš„æ„Ÿè¬ï¼‰ 
	- 2. å‰å›ã«ç¶šãã€å›½ç”£ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯MT-Bench-jpã§æœ€é«˜æ€§èƒ½ã‚’æ›´æ–° 
	- 3. Active Parameteræ•° 13Bã§104Bã®Command R+ã‚’è¶…ãˆã€72Bã®Qwen 1.5ã«è¿«ã‚‹æ€§èƒ½ 
	- 4. AWS Trainiumã§ã®MoEãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¯AWSã®æ‹…å½“ã®æ–¹ã«ã‚‚ç¢ºèªã—ã¾ã—ãŸãŒã€ãŠãã‚‰ãä¸–ç•Œåˆã¨ã®ã“ã¨ã€‚ã‚³ãƒ¼ãƒ‰ã¯æŠ€è¡“ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã¨ã¨ã‚‚ã«è¿‘æ—¥å…¬é–‹äºˆå®šã€‚ 
	- 5. å‰å›ã«å¼•ãç¶šãã€SteerLMã«ã‚ˆã‚‹ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å®Ÿæ–½ã€‚å±æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆAPMï¼‰ã¯gemma 7bã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å…¬é–‹
-  CACTUS: Chemistry Agent Connecting Tool-Usage to Science
	- https://arxiv.org/abs/2405.00972
	- åŒ–å­¦ã®ãŸã‚ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è«–æ–‡
	- åŒ–å­¦ã«é–¢ã™ã‚‹æ•°åƒã®è³ªå•ã¨å›ç­”ã‚’ä½œæˆã—æ§˜ã€…ãªã‚ªãƒ¼ãƒ—ãƒ³LLMã®æ€§èƒ½ã‚’æ¯”è¼ƒã€‚Gemma-7bã¨Mistral-7bã§é«˜ç²¾åº¦ã‚’å®Ÿç¾ã€ã¾ãŸç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤æ°‘ç”Ÿãƒ¬ãƒ™ãƒ«ã®ãƒãƒ¼ãƒ‰ã¸å°å…¥ãŒã§ããã†ã ã¨ã‚ã‹ã£ãŸãã†ã§ã™ã€‚
- Stack OverflowãŒOpenAIã¨ææºï¼Ÿ
	- https://x.com/ImAI_Eruel/status/1787670618961514627
	- ãŠãã‚‰ãChatGPTã®ç™»å ´ã«ã‚ˆã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã®æ¸›å°‘ã¨ã„ã†ç‚¹ã§æœ€å¤§ã®è¢«å®³ã‚’è¢«ã£ãŸã®ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è³ªå•è§£ç­”ã‚µãƒ¼ãƒ“ã‚¹Stack Overflowãªã‚“ã§ã™ãŒï¼Œã“ã®åº¦OpenAIã¨ã®ææºãŒæ±ºã¾ã£ãŸã¨ã®ã“ã¨
- ç²¾åº¦ä¸Šã’ã‚ˆã†ã¨ã™ã‚‹ã¨è‡ªç„¶ã¨LLMã«é ¼ã‚‹ç®‡æ‰€æ¸›ã£ã¦ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã«ãªã‚‹
	- https://x.com/mizchi/status/1787639942216327442
- KARAKURI LM 8x7B Chat v0.1 ã‚’ãŠè©¦ã—ä¸­
	- https://lm.karakuri.cc/
- ã€GPT-4ã¨åŒãƒ¬ãƒ™ãƒ«ã€‚ã‹ã‹ã‚‹ã‚³ã‚¹ãƒˆã¯200åˆ†ã®1ã€‘æœ€å¼·LLMã€ŒDeepSeek-V2ã€ç™ºè¡¨
	- https://x.com/SuguruKun_ai/status/1787839473067376705
	- ã“ã®å­ãŒGPT-4ç›¸å½“ã¨è¨€ã†ã®ã¯ã‚ã£ã¦ãã†ã§çµæ§‹æ–‡æ‰ã‚‚ã‚ã‚‹ ã¤ã„ã§ã«å€«ç†é¢ã‚‚å‰²ã¨é«˜ã‚ãªæ„Ÿã˜ã§æ—¥æœ¬èªå‡ºåŠ›ã¯æ‚ªããªã„ çµæ§‹è‰¯ã„æ„Ÿã˜ã‹ã‚‚ï¼
- GrokãŒããŸï¼ä»Šã¯Xèª²é‡‘è€…é™å®š
	- https://x.com/hirochuu8/status/1787880221997515258
- iPad Pro 13 ã‚¤ãƒ³ãƒ Nano-textureã‚¬ãƒ©ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ä¾¡æ ¼ â‰’ KARAKURI LM 8x7bã®å­¦ç¿’ã«ã‹ã‹ã£ãŸã‚³ã‚¹ãƒˆã§ã™
	- https://x.com/txmy/status/1787859094034059669
- æ—¥è‹±ï¼è‹±æ—¥ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦meta/Llama 3ã§ã¯google/Gemmaã‚’è¶…ãˆã‚‹äº‹ãŒå‡ºæ¥ãªã„
	- https://x.com/webbigdata/status/1787457498057933299
- Karasu-Mixtral-8x22B-v0.1ã®gguf
	- https://huggingface.co/mmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-gguf
- lightblue-suzume-llama-3-8B-multilingualã®gguf
	- https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-multilingual-gguf
- Let's build a 100% local RAG app, featuring âŒ˜R, a self-hosted vector database, a fast embedding library & a reranker:
	- https://x.com/akshay_pachaar/status/1787824526010782053
- Reranker allows you to reorder the retrieved context (chunks), offering two key benefits:
	- https://x.com/akshay_pachaar/status/1787824694768648575
- Ollama v0.1.34 is out!
	- https://x.com/ollama/status/1787976537762856999
- A built-in planning agent for LlamaIndex landed in 0.10.34!
	- https://docs.llamaindex.ai/en/stable/examples/agent/structured_planner/
	- A key pattern in agents is the ability to plan. Breaking down a task into sub-tasks can make the task easier to execute.
- MicrosoftãŒè‡ªç¤¾ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒMAI-1ã€ã‚’é–‹ç™ºã—ã¦ã„ã‚‹
	- https://x.com/ImAI_Eruel/status/1787787401055908017
	- 5000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã®è©±ã‚‚ã‚ã‚Š,æœ¬å½“ãªã‚‰æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã§æœ€å¤§ãƒ¬ãƒ™ãƒ«.å¾Œå‡ºã—è€ƒæ…®ã§GPTã‚„Claudeè¶…ãˆã‚‚ã§ããã†ã‹
- DeepSeek-V2ã€236B ã®ã‚¯ã‚½ãƒ‡ã‚«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚ŠãªãŒã‚‰æ¨è«–æ™‚ã¯å®Ÿè³ª 21B ç›¸å½“ã® MoE ã‚‰ã—ã
	- https://x.com/izutorishima/status/1787775197057265925
- ã¬ã“ã¬ã“æ°ã€é€€è·ã—ã€æœ¬æ¥­ LLM ç„¡è·ã¸ã€
	- https://x.com/schroneko/status/1788148600171831406
- HachiML/Hachi-Alpaca by ã¯ã¡ã•ã‚“
	- https://huggingface.co/datasets/HachiML/Hachi-Alpaca
	- Mixtral 8x22B Instructã«ã‚ˆã‚‹æ—¥æœ¬èªåˆæˆãƒ‡ãƒ¼ã‚¿ã€28.9kã§ä¸€æ—¦å®Œäº†ã«ã—ã¾ã—ãŸã€‚v1.0_cleanedãŒç²¾æŸ»æ¸ˆã¿ã§ã™ã€‚
- Zennã®ãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹ã‚’ã€æ¯æœAIãŒãƒ©ã‚¸ã‚ªé¢¨ã«ç´¹ä»‹ã—ã¦ãã‚Œã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã¤ãã‚Šã¾ã—ãŸ
	- https://zenncast-web.vercel.app/
- Announcing AlphaFold 3
	- https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/
	- AlphaFold2ã®æ™‚ç‚¹ã§ï¼Œã€Œãƒãƒ¼ãƒ™ãƒ«è³ãƒ¬ãƒ™ãƒ«ã€ã¨è¨€ã‚ã‚Œã¦æœ€çµ‚å½¢æ…‹ã‹ã¨æ€ã„ãã‚„ï¼Œã¾ã•ã‹ã®3ãŒå‡ºã¦ãã¾ã—ãŸ by ä»Šäº•ã•ã‚“
- Deeplearning.aiã‚ˆã‚Šç„¡æ–™ã®ã€Building Agentic RAGã€ãŒå…¬é–‹
	- https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/
- xLSTM: Extended Long Short-Term Memory
	- https://arxiv.org/abs/2405.04517
	- Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to SotA Transformers and State Space Models, both in performance and scaling.
- 4M Context Length Llama-3 8B (V0.1)
	- https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-4194k
- æ—¥æœ¬å­¦è¡“ä¼šè­°ç·ä¼šã§ã®è¬›æ¼”ã€Œæ—¥æœ¬ã®ç ”ç©¶ç«¶äº‰åŠ›ä½ä¸‹ã®å› æœæ¨è«–ã€ã®è³‡æ–™ãŒé¢ç™½ã„ï¼
	- https://www.scj.go.jp/ja/member/iinkai/sokai/siryo191-2-1.pdf
	- æ—¥æœ¬ã®ç ”ç©¶ç«¶äº‰åŠ›ä½ä¸‹ã®å› æœæ¨è«–
	- ã©ã‚Œã ã‘**ã€Œç ”ç©¶äººãƒ»æ™‚é–“å¯†åº¦ã€ï¼ˆè‰¯ãäººçš„ç ”ç©¶ç’°å¢ƒã®åºƒãŒã‚Šï¼‰**ã‚’ä¿ã¦ã‚‹ã‹ã«ã€ä»Šå¾Œã®æ—¥æœ¬ã®ç ”ç©¶ç«¶äº‰ãŒã‹ã‹ã£ã¦ã„ã‚‹ã€‚
- OpenAIãŒAIãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã¹ãã‹ã‚’è¦å®šã™ã‚‹Model Specã‚’å…±æœ‰
	- https://openai.com/index/introducing-the-model-spec/
	- ãˆã€ã“ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¦ChatGPTã®å†…éƒ¨æŒ™å‹•ã‚ã–ã‚ã–å…¬é–‹ã™ã‚‹ã®ãªãœï¼ŸEUçš„ãªã‚„ã¤ï¼Ÿ by æ·±æ´¥ã•ã‚“
	- ã“ã“ã‹ã‚‰ã€GPT-5ã®ç‰¹å¾´ãŒä½¿ã„å‹æ‰‹ãŒã‚ã‹ã‚‹ã‚‰ã—ã„
-  Google Colab ã§ å±æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã€€KARAKURI LM 7B APM v0.1 ã‚’è©¦ã™
	- https://note.com/npaka/n/ndb541c2cf03b?sub_rt=share_h
	- ã€ŒKARAKURI LM 7B APM v0.1ã€ã¯ã€å±æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€ŒGemma 7Bã€ã®ãƒ•ã‚¡ã‚¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ãªã‚Šã¾ã™ã€‚
	- å±æ€§ã®å€¤ã¯ **0(æœ€ä½)ã€œ4(æœ€é«˜)** ã«ãªã‚Šã¾ã™ã€‚
- karakuri-lm-8x7b-chat-v0.1ã®ggufã‚ã‚Šã¾ã™
	- https://huggingface.co/mmnga/karakuri-lm-8x7b-chat-v0.1-gguf
	- imatrixã®ãƒ‡ãƒ¼ã‚¿ã¯TFMC/imatrix-dataset-for-japanese-llmã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¾ã—ãŸ
- ã‚°ãƒ¼ã‚°ãƒ«ãŒã€ç”Ÿç‰©å­¦ã«é©å‘½ã‚’ä¸ãˆãŸã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬AIã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã€ŒAlphaFold 3ã€ã‚’ç™ºè¡¨ï¼ˆãƒã‚¤ãƒãƒ£ãƒ¼è«–æ–‡ï¼‰
	- https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/
	- ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã€DNAã€RNAã€å°åˆ†å­ãªã©ã»ã¼å…¨ã¦ã®ç”Ÿä½“åˆ†å­ã®æ§‹é€ ã¨ç›¸äº’ä½œç”¨ã‚’é«˜ç²¾åº¦ã«äºˆæ¸¬ ç”Ÿæˆ
	- AIã‚’æ”¯ãˆã‚‹transformerã¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç™ºå±•ã§ç”Ÿç‰©å­¦ã‚„å‰µè–¬ãŒåŠ é€Ÿ
- æ¯æ—¥ãŒã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã™ãã¦ã€ã‚‚ã¯ã‚„å…¨å®¹ãŒæŠŠæ¡ã§ããªã„ã€‚æ™‚äº‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¿½ã†ã ã‘ã§ãƒ‘ãƒ³ã‚¯ã™ã‚‹ä¸–ç•Œã¯ã€ã‚ã¾ã‚Šå¥å…¨ã§ã¯ãªã„ by æ·±æ´¥ã•ã‚“
	- https://x.com/fladdict/status/1788208163965305143
- æ—¥æœ¬èªç‰¹åŒ–ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒJapanese Stable LM 2 1.6Bã€ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸ
	- https://ja.stability.ai/blog/japanese-stable-lm-2-16b
	- Japanese Stable LM 2 1.6Bï¼ˆJSLM2 1.6Bï¼‰ã¯16å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸæ—¥æœ¬èªã®å°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- TimesFM by DeepMind
	- https://huggingface.co/google/timesfm-1.0-200m
	- TimesFM is a forecasting model, pre-trained on a large time-series corpus of 100 billion real world time-points, that displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. 
- Uncertainty Quantification and Propagation in Atomistic Machine Learning
	- https://arxiv.org/abs/2405.02461
	- ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡
	- æ©Ÿæ¢°å­¦ç¿’ã§ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„é ˜åŸŸã‚’äºˆæ¸¬ã™ã‚‹ã®ã¯å›°é›£ã§ã™ãŒã€ãã‚Œã‚’å…‹æœã™ã‚‹ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ã®æ‰‹æ³•ãŒæ•™ç§‘æ›¸çš„ã«ã¾ã¨ã¾ã£ã¦ã„ã¾ã™ã€‚ã¾ã æ±ç”¨çš„æ‰‹æ³•ã¯ãªããƒ‡ãƒ¼ã‚¿ã«ã‚ã£ãŸæ‰‹æ³•ã‚’ã†ã¾ãé¸æŠã™ã‚‹ã“ã¨ãŒå¤§åˆ‡ã¨ã®ã“ã¨ã€‚
- ã‚µãƒ ã‹ã‚‰ã€æœˆæ›œã«å¤§ããªç™ºè¡¨ãŒã‚ã‚‹ã¨ã€ã€
	- https://x.com/sama/status/1788989777452408943
	- not gpt-5, not a search engine, but weâ€™ve been hard at work on some new stuff we think people will love! feels like magic to me.
-  ã€ŒFugaku-LLMã€ã‚’å…¬é–‹
	- https://pr.fujitsu.com/jp/news/2024/05/10.html
	- æ¨ªç”°ã•ã‚“ã®ã¨ã“ã€
	- ã€ŒFugaku-LLMã€ã¯ã€Œå¯Œå²³ã€ã®13,824å°ã®è¨ˆç®—ãƒãƒ¼ãƒ‰ã‚’ç”¨ã„ã¦ã€ç´„4,000å„„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’ã—ãŸ13Bãƒ¢ãƒ‡ãƒ«ã§ã™
-  LangChain v0.2 ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/na9e629ebbd16?sub_rt=share_h
	- **ãƒ»langchain ã¨ langchain-community ã®å®Œå…¨ãªåˆ†é›¢  **
	- **ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä»˜ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**  
	- **ãƒ»ã‚ˆã‚Šæˆç†Ÿã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**  
	- **ãƒ»LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ¨™æº–åŒ–ã€ç‰¹ã«ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«é–¢ã™ã‚‹æ”¹å–„**  
	- **ãƒ»ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒãƒ¼ãƒˆã®å‘ä¸Š**  
	- **ãƒ»30ã‚’è¶…ãˆã‚‹æ–°ã—ã„ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**
- æ—¥æœ¬èªé«˜é€ŸASR Kotoba-Whisper v1.1ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã—ã¾ã—ãŸ
	- https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1
	- å¥èª­ç‚¹äºˆæ¸¬ã‚’æ”¹å–„ (raw CERã§å¤§å¹…ãªå‘ä¸Š) - Stable-tsã®çµ±åˆã«ã‚ˆã‚Šã€timestampã®äºˆæ¸¬ã‚’å‘ä¸Š - Long-form transcriptionã®äºˆæ¸¬å‘ä¸Š - å­¦ç¿’ãƒ»æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹:
- ibm-granite/granite-timeseries-ttm-v1
	- https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1
	- TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. 
	- **With less than 1 Million parameters, TTM introduces the notion of the first-ever â€œtinyâ€ pre-trained models for Time-Series Forecasting.**
- Great, now we have some clean Llama 3 models (both 8B and 70B)
	- https://huggingface.co/ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF
	- Those minor bugs in llama.cpp has been resolved so should work at its full potential.
- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆãŒ1æœˆã«ç™ºè¡¨ã—ãŸSliceGPTã§ã¯ã€LLMã®ã‚¦ã‚¨ã‚¤ãƒˆã‚’åœ§ç¸®ã§ãã¡ã‚ƒã†ã‚‰ã—ã„ã€‚
	- https://x.com/umiyuki_ai/status/1789128885558546881
	- ã‚¦ã‚¨ã‚¤ãƒˆã®å„è¡Œåˆ—ã‚’æ¬¡å…ƒæ•°æ¸›ã‚‰ã—ãŸè¡Œåˆ—ã«ç½®ãæ›ãˆã¡ã‚ƒã†ã‚“ã ã£ã¦ã€‚ã“ã‚Œã§ãƒ‘ãƒ©æ•°ã‚’25%ã¾ã§å‰Šã‚Œã¦ï¼ˆã¤ã¾ã‚Š52.5Bã«ãªã‚‹ï¼Ÿï¼‰ã€Llama2-70Bã®å ´åˆã¯æ€§èƒ½ã®ä½ä¸‹ã¯1%ã ã‘ã§æ¸ˆã‚€ã‚‰ã—ã„
- Google/TimesFMã¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‹ã‚‰å¾®å¦™ã ã—
	- https://x.com/kyo_takano/status/1789150904102613131
	- ä¸€éšå·®åˆ†ã‚’å–ã‚‰ãšã«éå®šå¸¸éç¨‹ã®ã¾ã¾äºˆæ¸¬å™¨ã«çªã£è¾¼ã‚€; 
	- Transformersã‚’ä½¿ã„ãŸã„ãŒãŸã‚ã«è¤‡æ•°æ™‚ç‚¹ã‚’å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦åŸ‹ã‚è¾¼ã‚€ãƒ»äºˆæ¸¬ã™ã‚‹; etc.ï¼‰ã€
	- å¤å…¸çš„ãªçµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦éƒ¨åˆ†çš„ã«ã—ã‹ä¸Šå›ã£ã¦ãªã„ã‚“ã ã‚ˆã­
- Ollamaã«ã€MLXå¯¾å¿œãã‚‹ï¼Ÿ
	- https://x.com/m_sigepon/status/1789233089945981319
-  Causal Inference About the Effects of Interventions From Observational Studies in Medical Journals
	- https://jamanetwork.com/journals/jama/fullarticle/2818746?utm_source=twitter&utm_campaign=content-shareicons&utm_content=article_engagement&utm_medium=social&utm_term=051024
	- åŒ»å­¦è¦³å¯Ÿç ”ç©¶ã§å› æœé–¢ä¿‚ã‚’ç¤ºã™ãŸã‚ã«ã¯ã©ã†ã™ã‚‹ï¼Ÿ6ã¤ã®æ¡ä»¶ã‚’ç¤ºã—ã¦ã€ã“ã‚Œã‚’æº€ãŸã—ã¦ã„ã‚Œã°å› æœé–¢ä¿‚ã‚’è¨€ã£ã¦ã‚‚ã„ã„ã‚“ã˜ã‚ƒãªã„ï¼Ÿã¨ã„ã†æè¨€ã€‚
- google/timesfm-1.0-200m
	- https://huggingface.co/google/timesfm-1.0-200m
	- Google released a decoder-only "foundation" time series model on
	- Trained on a corpus of 100B real world time-points from Google Trends and pageviews from Wikipedia!
- ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¨æ©Ÿæ¢°å­¦ç¿’ã€ã«ã¯KANã§è©±é¡Œæ²¸é¨°ä¸­ã®ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ï½¥ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ã®è¡¨ç¾å®šç†ãŒæ²è¼‰ã•ã‚Œã¦ã„ãŸã€‚
	- https://x.com/bebebeBayes/status/1788900859570700291
- å¤§å±‹é›„è£•ã€Œä¿¡ç”¨ãƒ»ä¿¡é ¼ãƒ»ä¿¡è¨— â€”è²¬ä»»ã¨èª¬æ˜ã«é–¢ã™ã‚‹æ¦‚å¿µæ•´ç†â€•ã€
	- https://x.com/rmaruy/status/1789193304808296841
	- AIã®èª¬æ˜å¯èƒ½æ€§ï¼è²¬ä»»ã«ã¤ã„ã¦ã€ã‚¦ã‚£ãƒˆã‚²ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã®åŸå› ï¼ç†ç”±ã®åŒºåˆ¥ã‹ã‚‰ç´è§£ãã€ãƒ—ãƒ­ã‚»ã‚¹ã®é€æ˜æ€§ã ã‘ã§ãªãç­”è²¬æ€§ãŒå•é¡Œã«ãªã‚‹å ´é¢ãŒã©ã‚“ãªã¨ãã‹ã‚’è­°è«–ã€‚ã“ã®ä¸Šãªãæ˜æ™°ã€‚
- lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA
	- https://huggingface.co/lyu-boxuan/llama-3-youko-8b-En-Ja-MT-LoRA
	- rinnaæ§˜ã®llama-3-youko-8bã‚’å°‘æ•°ã®è‹±æ—¥å¯¾è¨³ãƒ‡ãƒ¼ã‚¿+LoRAã§SFTã—ã¦ã¿ã¾ã—ãŸ
-  Post Llama 3 depression
	- https://www.reddit.com/r/LocalLLaMA/comments/1colmeb/post_llama_3_depression/
	- Llama3ã®å¾®èª¿æ•´ãƒ¢ãƒ‡ãƒ«ã¯è‰²ã€…å‡ºãŸã‘ã©ä»Šã‚“ã¨ã“ã©ã‚Œã‚‚ã‚¢ã‚«ãƒ³ã¨ã„ã†è©±ã€‚åŸå› ã¯ã‚ˆãåˆ†ã‹ã‚‰ã‚“ã‘ã©ã‚‚ã†ä»Šã¾ã§ã®å¾®èª¿æ•´ã®ã‚„ã‚Šæ–¹ã¯æ™‚ä»£é…ã‚Œãªã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚å°‘ãªãã¨ã‚‚Llama2ã¨ã¯å‹æ‰‹ãŒé•ã†ã‚‰ã—ã„
- ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã“ãƒ¼ã‚„ã£ã¦ä½¿ã†ã®ã€ã‚’æ›´æ–°ã—ã¾ã—ãŸ
	- https://gist.github.com/kyo-takano/c662c1bfa1e7fe440511b11f62521a7e
	-   ä»¥ä¸‹ã‚’è¿½åŠ : 
		- å‰æçŸ¥è­˜ã®ç¢ºèª 
		- ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ 
		- å°¤åº¦é–¢æ•°ã¨ã—ã¦ã®åˆ©ç”¨
- Difyã®å‹¢ã„ãŒã™ã”ã„ã€‚LangChainã‚„Flowiseã€Llama Indexã‚’æŠ‘ãˆã¦LLMãƒ„ãƒ¼ãƒ«4é€±é–“ã§1ä½ã«
	- https://x.com/kyutaro15/status/1789054552638943495
- You can now generate production-ready prompts in the Anthropic Console.
	- https://x.com/AnthropicAI/status/1788958483565732213
- Vanilla Bayesian Optimization Performs Great in High Dimensions
	- https://arxiv.org/abs/2402.02229
	- ãƒãƒ‹ãƒ©ã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãŒé«˜æ¬¡å…ƒã§ã‚‚å¤§æ´»èº
	- ã“ã‚Œã¾ã§é«˜æ¬¡å…ƒã¯å‘ªã„ã®é ˜åŸŸã¨æ€ã‚ã‚Œã¦ã„ãŸãŒã€é©åˆ‡ãªä»®å®šã‚’è¨­ã‘ã‚‹ã ã‘ã§æœ€å…ˆç«¯ã®æ‰‹æ³•ã‚’åœ§å€’ã™ã‚‹æ€§èƒ½ãŒå‡ºã›ã‚‹ã“ã¨ãŒåˆ¤æ˜
- DeepLearningAIã‹ã‚‰ã€ä»Šåº¦ã¯LangChainã®CEOã®è¬›ç¾©
	- Check out the short course Functions, Tools, and Agents, taught
	- https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/
- "MOMENT: A Family of Open Time-series Foundation Models"
	- https://moment-timeseries-foundation-model.github.io/
	- has been accepted for the ICML 2024! On this occasion, we are open-sourcing it, together with the model weights and dataset!
- ç”ŸæˆAIã®è«–ç‚¹
	- https://www.seikyusha.co.jp/bd/isbn/9784787235374/
	- å–œé€£å·å…ˆç”Ÿã¾ã ã„ãã¦ãŸã®ã‹ï¼Ÿ
	- æ—¥æœ¬å­¦è¡“ä¼šè­°ãŒå®Ÿæ–½ã—ã€éå»æœ€å¤šã®å‹•å“¡ã‚’é”æˆã—ãŸã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ã€Œç”ŸæˆAIã®èª²é¡Œã¨ä»Šå¾Œã€ã®æ›¸ç±åŒ–ã§ã‚ã‚‹
- 

## 5/7

ï¼§ï¼·ã§ã€é ­ãŒã¼ã‘ã¦ã„ã‚‹ã®ã‹ã€X(æ—§twitter)ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¤‰ã‚ã£ãŸã®ã‹ã€ãŠã™ã™ã‚ã«å‡ºã¦ãã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆãŒå…ˆé€±ã¨ã‹ã¶ã£ã¦ã„ã‚‹æ°—ãŒã™ã‚‹ã€‚Xã®ç”Ÿæˆï¼¡ï¼©ã‚’ä½¿ã£ãŸæ–°ã‚µãƒ¼ãƒ“ã‚¹ã€Œã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚ºã€ã®å±•é–‹ã¨é–¢ä¿‚ã‚ã‚‹ã®ã‹ï¼Ÿã€‚ä»Šé€±ã¯çªç„¶ã§ã¦ããŸè¬ã®gpt2-chatbotãŒé¢ç™½ã‹ã£ãŸã€gpt2ã¨åå‰ãŒã‚ã‚‹ã‚‚ã®ã®ã€GPT-4.5ã‹GPT-5ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ†ã‚¹ãƒˆã‹ã¨ã„ã†è©±ã§æŒã¡åˆ‡ã‚Šã ã£ãŸï¼ˆChatbot Arenaã‹ã‚‰ã¯æ¶ˆãˆãŸã€‚ã€‚ï¼‰ã€è©•ä¾¡ã§ããŸäººã«ã‚ˆã‚‹ã¨ã€ç›¸å½“ã™ã”ã„æ€§èƒ½ã‚‰ã—ã„ã€‚Swallow-MS 7Bã®æ–°ã—ã„instructãƒ¢ãƒ‡ãƒ«ã€ã•ã£ããElyzaTasks100ã§è©•ä¾¡ã•ã‚Œã€ã¡ã‚‡ã£ã¨å¾®å¦™ãªçµæœã«ã€‚ä¸€æ–¹ElyzaTasks100ã§ã®è©•ä¾¡ã«ã‚ˆã‚‹ã¨ã€Qwen1.5ã¯ã‹ãªã‚Šå„ªç§€ã ã‘ã©æ™‚ã€…æ—¥æœ¬èªã«é›£ã‚ã‚Šã¨ã®ã“ã¨ã€‚Domingosæ°ã®AIã®èƒ½åŠ›ã®ç™ºå±•ãŒã‚µãƒã£ã¦ã„ã‚‹ã¦ã„ã†è©±ã¯ã€ï¼ˆãã‚‚ãã‚‚ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ã£ãŸï¼‰äººé–“ã®èƒ½åŠ›ãŒAIã®é€²åŒ–ã‚’å¾‹é€Ÿã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚ãã‚Šã‚ƒã€äººé–“ã‚’è¶…ãˆã‚‹ã®ã¯äººé–“ã®ãƒ‡ãƒ¼ã‚¿ã§ã¯ç„¡ç†ã ã€è¶…ãˆãŸã¨ã“ã‚ã§äººé–“ã«ã¯ã‚ã‹ã‚‰ãªã„ã¨ã„ã†ã®ã¯ãã†ã‹ã‚‚ã€‚BCGã®å£²ä¸Š20%ãŒç”ŸæˆAIé–¢é€£ã¨ã®ã“ã¨ã€ã‚³ãƒ³ã‚µãƒ«ã¯ãªããªã‚‰ãªã„ã€é‡‘ã®å„²ã‘æ–¹ãŒå¤‰ã‚ã‚‹ã ã‘ã€‚ã€Œçµ±è¨ˆçš„ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€å…¨æ–‡ãŒãƒ—ãƒ¬å…¬é–‹ã•ã‚ŒãŸã¨ã®ã“ã¨ã€ã‚ã‚Œã¯ï¼ã¨æ€ã†äººã¯ãœã²ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’ã€‚Ollamaã‚’ä½¿ã£ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMã®åˆ©ç”¨ä¾‹ã‚‚ãã£ã¨å¢—ãˆãŸã€ã‚‚ã¯ã‚„RAGã¯èª°ã§ã‚‚ã§ãã‚‹ã€ã•ã‚‰ã«ReAct Agentã¨ã‹ã€Function Callingã¨ã‹ã€ã‚ˆã‚Šã‚€ã¤ã‹ã—ã„ã‚¿ã‚¹ã‚¯ã‚€ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€æ°—ã®ã›ã„ã‹Llama3ã‚„phi3ãŒä½¿ã‚ã‚Œã‚‹ä¾‹ãŒå¤šã„ã‚ˆã†ãªã€‚Kolmogorovâ€“Arnold Networksã€æ–°ã—ã„ç”ŸæˆAIå‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼Ÿç‰¹ç•°å­¦ç¿’ç†è«–ï¼ˆæ¸¡è¾ºãƒ™ã‚¤ã‚ºç†è«–ï¼‰ã‚’ç™ºå±•ã•ã›ãŸå±€æ‰€å­¦ç¿’ä¿‚æ•°ã¨ã„ã†æ–°ã—ã„æ¦‚å¿µã‚‚æ°—ã«ãªã‚‹ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨ã¨ã‚‚é–¢ä¿‚ã‚ã‚‹ã¨ã‹ã€‚ãã‚Œã«ã—ã¦ã‚‚ã€NVIDIA CEOã‚¸ã‚§ãƒ³ã‚¹ãƒ³ãƒ»ãƒ•ã‚¡ãƒ³æ°ãŒã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«ã‹ã‚‰ã®æ­Œé…ä¿¡ã«æ··ã–ã‚‹å‹•ç”»ã€ã‹ã‚ã„ã„ãªã‚ï¼ˆã„ã‚„ï¼£ï¼¥ï¼¯ãŒã ã‚ˆï¼‰ã€‚ã€Œãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã“ãƒ¼ã‚„ã£ã¦ä½¿ã†ã®ã€ã¯å‚è€ƒã«ãªã‚‹ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMãªã‚‰ã„ã‚ã„ã‚ã‚„ã‚Šæ”¾é¡Œãªã‚“ã ãªã€‚ï¼¡ï¼©ã‚»ã‚¤ãƒ•ãƒ†ã‚£ã§ã¯ã€NISTã‹ã‚‰ã€ç”Ÿæˆï¼¡ï¼©å‘ã‘ã®ãƒªã‚¹ã‚¯ç®¡ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒç™ºè¡¨ã€æ—¥æœ¬ã®AISIã¨ã®é€£æºã‚‚é€²ã‚€ã€‚AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®åŒ…æ‹¬çš„ãªã‚µãƒ¼ãƒ™ã‚¤ã€ã€ŒAIã«ã‚ˆã‚‹çµ¶æ»…ãƒªã‚¹ã‚¯ã®è»½æ¸›ã€ã ã¨ã€‚rinnaã‹ã‚‰Llama 3 8Bã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒLlama 3 Youko 8Bã€ã‚’å…¬é–‹ã€NIIã‹ã‚‰ã€ŒLLM-jp-13B v2.0ã€ã‚’æ§‹ç¯‰ã¨ã‹ã€é ‘å¼µã‚Œæ—¥æœ¬å‹¢ã€‚ã†ã¿ã‚†ãã•ã‚“ãŒè¨€ã†ã‚ˆã†ã«ã€é€²åŒ–å‹ã®LLMã®ãƒãƒ¼ã‚¸Mergekit-Evolveã£ã¦ã®ã¯æœ¬å½“ã«ã™ã”ã®ã„ã‹ï¼Ÿï¼ŸLangChainã®ï¼”ã¤ã®RAGå‘ã‘chainã®æ¯”è¼ƒã‚‚åœ°å‘³ã«å½¹ã«ç«‹ã¤ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMç³»ã§ã¯ã€è‡ªä½œå°èª¬ã‚’LLMã§è©•ä¾¡ã•ã›ã¦ã„ã‚‹ã²ã¨ãŒã€ command-r-plus-Q4_K_Mã‚’çµ¶è³›è©•ä¾¡ã€å®Ÿä½œæ¥­ã«åŸºã¥ãè©•ä¾¡ã¯å°Šã„ã€‚ChatGPTæ±å¤§å…¥è©¦ã«æŒ‘ã‚€ã‚‚ã€Œä¸åˆæ ¼ã€ã®è¨˜äº‹ï¼ˆæ—¥çµŒï¼‰ã€ã•ã£ãããƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ‚ªã„ã¨ã€ã„ã‚„è§£ã‘ãŸã‚ˆã€ã¨ã®çªã£è¾¼ã¿ãŒã€æ¬¡ã€…ã¨ã€‚ã€‚

- LangChainã‚’ç”¨ã„ãŸ4ç¨®é¡ã®RAGè³ªå•å¿œç­”chainã®å®Ÿè£…ã¨æ€§èƒ½æ¯”è¼ƒï½œ
	- https://zenn.dev/aidemy/articles/97d5fb6ac03a4f
	- stuff chainã€map reduce chainã€map rerank chainã€refine chain
	- å¿œç­”é€Ÿåº¦ : å‘¼ã³å‡ºã—å›æ•°ãŒ1å›ã®stuffã¯ã€Œé€Ÿã„ã€, nå›ã§ã™ãŒå„æ–‡æ›¸ã«å¯¾ã™ã‚‹LLMå‘¼ã³å‡ºã—ã‚’ä¸¦åˆ—åŒ–ã§ãã‚‹mapç³»2ç¨®ã¯ã€Œæ™®é€šã€, nå›ã‹ã¤ä¸¦åˆ—åŒ–ãŒã§ããªã„refineã¯ã€Œé…ã„ã€ã¨ã—ã¦ã„ã¾ã™ã€‚
	- é©ã—ã¦ã„ã‚‹æ–‡æ›¸ç‰¹å¾´
		- stuffãƒ»map reduce : æ–‡æ›¸å…¨ä½“ã‚’1æ®µéšã¾ãŸã¯2æ®µéšã§LLMã«å…¥åŠ›ã™ã‚‹ãŸã‚, æ–‡æ›¸å…¨ä½“ã«é‡è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹å ´åˆã«ç‰¹ã«æœ‰åŠ¹ã§ã™ã€‚
		- map rerank : æ–‡æ›¸ã®ä¸€éƒ¨ã®ã¿ã®å›ç­”ã‹ã‚‰æœ€è‰¯ã®å›ç­”ã‚’é¸ã¶ãŸã‚, ä¸€éƒ¨ã®ã¿ã«é‡è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹å ´åˆã«ç‰¹ã«æœ‰åŠ¹ã§ã™ã€‚
		- refine : ä¸€éƒ¨ã®ã¿ã®å›ç­”ã‚’è¤‡æ•°å›å†èµ·çš„ã«å‘¼ã³å‡ºã™ãŸã‚, é‡è¦ãªæƒ…å ±ãŒæ–‡æ›¸ã®å…¨ä½“ã§ã‚‚ä¸€éƒ¨ã§ã‚‚å¯¾å¿œã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
- Mergekit-Evolveã®ãƒ†ã‚¹ãƒˆã§è©¦ã—ã«ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã€Japanese-Chat-Umievo-itr001-7bã‚’ElyzaTasks100ã§è©•ä¾¡ã—ã¦ã¿ãŸã‚‰å¹³å‡3.57ç‚¹ã‚’å©ãå‡ºã—ãŸã€€ by ã†ã¿ã‚†ãã•ï½
	- https://x.com/umiyuki_ai/status/1783867934542303666
	- 7Bãƒ¢ãƒ‡ãƒ«ãªã®ã«35Bãƒ‘ãƒ©ã®Command Rã‚’è¶…ãˆã¦ã¾ã™ï¼é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¨åŠ›æã‚‹ã¹ã—ï¼ï¼
- Swallow 7B, 13B, 70Bã€ãŠã‚ˆã³Swallow-MS 7Bã®æ–°ã—ã„instructãƒ¢ãƒ‡ãƒ«ï¼ˆSwallow-*-instruct-v0.1ï¼‰ã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- https://huggingface.co/collections/tokyotech-llm/swallow-ms-instruct-662957bf88d016c69ae0e633
	- ã‚ã¾ã‚Šé‡è¦–ã—ã¦ã“ãªã‹ã£ãŸæŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã‚„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¿œç­”ã®æ”¹å–„ã«å–ã‚Šçµ„ã¿ã€MT-Benchã§éå»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¢ºèªã—ã¾ã—ãŸ
- Swallow-MS-7B-Instruct-V0.1ã‚’ElyzaTasks100ã§è©•ä¾¡ã—ãŸã‚‰å¹³å‡2.82ç‚¹ã ã£ãŸã€‚ç¾ç’°å¢ƒã§ã¯ã‚‚ã¯ã‚„å¤§ã—ãŸäº‹ãªã„ã¨è¨€ã‚ã–ã‚‹ã‚’å¾—ãªã„ã€‚ã§ã‚‚ChatNTQã‚ˆã‚Šã¯ã‹ãªã‚Šå¼·ã„ã¨ã„ã†äº‹ã¯ChatVectorã‚’è¶³ã™ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æœ‰èƒ½ã‹ã‚‚ã—ã‚Œãªã„
	- https://x.com/umiyuki_ai/status/1783911959789969816
- ã€Appleã®æ–°ã—ã„OpenELMãƒ¢ãƒ‡ãƒ«ã‚’MLX LMã§ã€‘  512ãƒˆãƒ¼ã‚¯ãƒ³ã€340Token/S
	- https://x.com/hokazuya/status/1783808939773304957
	- ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã“ã®æ€§èƒ½ã¯Phi-3ã‚„Llama3ã®7Bãªã©è¦‹ã¦ããŸãŒMacå˜ä½“ã§ã“ã‚Œã¯ã‚¹ã‚´ã™ãã‚‹ã€‚
- Domingosæ°ã€AIã®èƒ½åŠ›ãŒäººé–“ãƒ¬ãƒ™ãƒ«ã§é£½å’Œã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¦ã„ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¦ã„ã‚‹
	- https://x.com/pmddomingos/status/1783956607552176422
	- ã‚€ã—ã‚ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã§120%ã‚„200%ã‚’æœ‰æ„å‘³ã«è­°è«–ã§ãã‚‹ã®ã‹ã¨ã„ã†æ–¹ãŒæ°—ã«ãªã‚‹ã€‚ã¨ã„ã†æ„å‘³ã§ã€Domingosæ°ã®æ„å›³ã¨ç•°ãªã‚‹æ„å‘³ã§è¶…çŸ¥èƒ½åˆ°æ¥ãƒ“ã‚¸ãƒ§ãƒ³ã¸ã®ç–‘ç¾©ã«ãªã£ã¦ã„ã‚‹ã€‚
	- https://x.com/rmaruy/status/1784154638390104188
- ã€éšæ™‚æ›´æ–°ã€‘ä¸»è¦ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨
	- https://zenn.dev/ml_bear/articles/3c5e7975f1620a
- å±…åˆã‚ã›ãŸæ­Œé…ä¿¡ã«æ··ã–ã‚‹ NVIDIA $NVDA CEO ã‚¸ã‚§ãƒ³ã‚¹ãƒ³ãƒ»ãƒ•ã‚¡ãƒ³
	- https://x.com/woodstockclub/status/1784179786082128351
-  é«˜é€ŸAIãƒãƒƒãƒ—ã§è©±é¡Œã®Groqã®APIã‚’Streamlitã§ä½¿ã†æ–¹æ³•
	- https://note.com/masayuki_abe/n/n336721e355e6?sub_rt=share_pb
- ã€Symbol-to-Languageã€
	- https://x.com/ai_database/status/1784581982053347542
	- LLMã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€Œè¨˜å·ã‚’è‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€ã“ã¨ã§æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ç²¾åº¦ãŒä¸ŠãŒã‚‹ç¾è±¡ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚
	- å®Ÿé¨“ã§ã¯ã€ç‰©æ€§äºˆæ¸¬ã€è¡¨ã®ç†è§£ã€ãƒ„ã‚¤ãƒ¼ãƒˆåˆ†æãªã©ã§åŠ¹æœãŒå‡ºã¦ã„ã¾ã™
- BCGã®å£²ä¸Š20%ãŒç”ŸæˆAIé–¢é€£ã§ã€2026å¹´ã¾ã§ã«40%ã«ã¾ã§å¢—ãˆã‚‹ã¨ã‹
	- https://www.ft.com/content/33dfaec4-b5e7-4eca-a869-cdd33d447e65
- lama 3 degrades more than Llama 2 when quantized.
	- https://x.com/rohanpaul_ai/status/1784889182558539917
- gpt2-chatbotã¨å‘¼ã°ã‚Œã‚‹è¬ã®ãƒ¢ãƒ‡ãƒ«
	- https://x.com/bioshok3/status/1784972619957346703
	- Chatbot arena ã§Claude3 opusã‚„GPT-4 Turboã«åŒ¹æ•µã™ã‚‹ã¨ã‹ã—ãªã„ã¨ã‹è¶…ãˆã¦ã‚‹è¶…ãˆã¦ãªã„è¨€ã‚ã‚Œä»Šå°‘ã—è©±é¡Œã«ãªã£ã¦ã„ã‚‹
- ReAct Agent with Function Calling | Open Source Gemma LLM | Ollama | Lan...
	- https://www.youtube.com/watch?v=exYUJcz4uZs
- llama-2-13b-retrievalqa.ipynb - Colab
	- https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb#scrollTo=JPdQvYmlWmNc
- crusoeai/Llama-3-8B-Instruct-Gradient-1048k-GGUF
	- https://huggingface.co/crusoeai/Llama-3-8B-Instruct-Gradient-1048k-GGUF
- react-agent-with-function-calling-ollama-langsmith.ipynb
	- https://github.com/Ashufet/LangChain_ReAct-Agent-with-Function-Calling_Ollama-Gemma-LLM_LangSmith/blob/main/react-agent-with-function-calling-ollama-langsmith.ipynb
- Google announces Med-Gemini, a family of Gemini models fine-tuned for medical tasks!
	- https://x.com/iScienceLuvr/status/1785247498744778886
- ã€Œçµ±è¨ˆçš„ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ã®å…¨ä½“ã®åŸç¨¿(4ç« ä»¥å¤–)ã®Î²ç‰ˆã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- http://chasen.org/~daiti-m/textmodel/
- Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities
	- https://arxiv.org/abs/2404.17790
	- æ±å·¥å¤§ã®LLMã€Swallowã®è«–æ–‡ãŒarXivã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã­ã€‚
	- æ—¥æœ¬èªã®ç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚„èªå½™æ‹¡å¼µã€ãƒ‘ãƒ©ãƒ¬ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹ã®å½±éŸ¿ã«ã¤ã„ã¦å¤§è¦æ¨¡ã‹ã¤ç³»çµ±çš„ã«çŸ¥è¦‹ã‚’æä¾›ã—ã¦ã„ã‚‹ç ”ç©¶ã§ã€å‹‰å¼·ã«ãªã‚Šã¾ã™
- gpt2-chatbotã¯æœ¬å½“ã«ãƒ¢ãƒ‡ãƒ«åã ã€‚ãã†HFã®CTOãŒã„ã¾openai-communityã‹ã‚‰ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã—ã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã€‚
	- https://x.com/alfredplpl/status/1785170960251007266
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒLLM-jp-13B v2.0ã€ã‚’æ§‹ç¯‰
	- https://www.nii.ac.jp/news/release/2024/0430.html
	-  NIIä¸»å®°LLMå‹‰å¼·ä¼šï¼ˆLLM-jpï¼‰ãŒã€ŒLLM-jp-13Bã€ã® å¾Œç¶šãƒ¢ãƒ‡ãƒ«ã¨ãã®æ§‹ç¯‰ã«ä½¿ç”¨ã—ãŸå…¨ãƒªã‚½ãƒ¼ã‚¹ã‚’å…¬é–‹
- Qwen1.5ã‚·ãƒªãƒ¼ã‚ºã‚’ä¸€é€šã‚ŠElyzaTasksã§è©•ä¾¡ã—ã¦ã¿ãŸ
	- https://x.com/umiyuki_ai/status/1785272618595262646
	- ã‚„ã£ã±ã‚ŠQwen1.5ã¯ã‹ãªã‚Šå„ªç§€ã€‚7Bãƒ¢ãƒ‡ãƒ«ã¯Llama3-8Bã®ãƒãƒ§ã‚¤ä¸‹ã€‚14Bãƒ¢ãƒ‡ãƒ«ã¯35Bã®Command Rã‚’è¶…ãˆã¦ã‚‹ï¼
- AISIã¨ç±³å›½NISTã¯ã€æ—¥æœ¬ã®ã€ŒAIäº‹æ¥­è€…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€ã¨NISTã®ã€ŒAIãƒªã‚¹ã‚¯ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯(RMF)ã€ã®ã‚¯ãƒ­ã‚¹ã‚¦ã‚©ãƒ¼ã‚¯ã®ç¬¬ä¸€å¼¾ã¨ã—ã¦ç”¨èªã«é–¢ã™ã‚‹ã€Œã‚¯ãƒ­ã‚¹ã‚¦ã‚©ãƒ¼ã‚¯1ã€ã‚’å…¬è¡¨ã—ã¾ã—ãŸã€‚
	- https://aisi.go.jp/2024/04/30/ai_rmf_crosswalk1_news/
- è¬ã®é«˜æ€§èƒ½AIãƒ¢ãƒ‡ãƒ«ã€Œgpt2-chatbotã€ãŒChatbot Arenaã«ç™»å ´ã€GPT-4.5ã‹GPT-5ãªã®ã§ã¯ãªã„ã‹ã¨è©±é¡Œã«
	- https://gigazine.net/news/20240430-lmsys-chatbot-arena-gpt2-chatbot/
	- æ—¥æœ¬ã®æ­´å²ã«ã¤ã„ã¦ã‚‚ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å¤šã„Claude 3 Opusã¨æ¯”è¼ƒã—ã¦ã€é¥ã‹ã«å„ªã‚ŒãŸå›ç­”ã€‚è«–ç†çš„ãªè€ƒå¯Ÿã«ã¤ã„ã¦ã‚‚ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ã€‚
- è‡ªä½œå°èª¬ã‚’LLMã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã•ã›ã¦ã¿ã‚‹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«4ãƒ¢ãƒ‡ãƒ«ã€ã‚µãƒ¼ãƒ“ã‚¹å‹3ãƒ¢ãƒ‡ãƒ«ï¼‰
	- https://note.com/kohya_ss/n/nfcdfd6de8790
	-  command-r-plus-Q4_K_M: æ¥µã‚ã¦é«˜ã„ç†è§£åŠ›ã¨è¦ç´„åŠ›ã‚’ç¤ºã—ã€ä½œå“ã®ä¼ç·šã‚„ç™»å ´äººç‰©ã®ç†è§£ã‚‚çš„ç¢ºã ã£ãŸã€‚æ–‡ç« ã¯èª­ã¿ã‚„ã™ãæ´—ç·´ã•ã‚Œã¦ãŠã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ä¸­ã§æœ€ã‚‚å„ªç§€ãªæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚å°èª¬ã®ãƒ†ãƒ¼ãƒã‚’æ·±ãç†è§£ã—ã€é©åˆ‡ãªæ‰¹è©•ã‚’è¡Œã£ã¦ã„ã‚‹ã€‚
- US NIST publishes 1st draft of its "AI Risk Management Framework: Generative AI Profile." 
	- https://airc.nist.gov/docs/NIST.AI.600-1.GenAI-Profile.ipd.pdf
- rinnaã¯Llama 3 8Bã®æ—¥æœ¬èªç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã€ŒLlama 3 Youko 8Bã€ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
	- https://huggingface.co/rinna/llama-3-youko-8b
- KAN: Kolmogorovâ€“Arnold Networks
	- https://arxiv.org/abs/2404.19756
	- https://github.com/KindXiaoming/pykan
	- Proposes an alternative to MLP that outperforms in terms of accuracy and interpretability
	- é‡ã¿ã‚’å­¦ç¿’ã•ã›ã‚‹ã®ã§ã¯ãªãã€ã‚¨ãƒƒã‚¸ä¸Šã«é…ç½®ã—ãŸæ´»æ€§åŒ–é–¢æ•°ã‚’å­¦ç¿’ã•ã›ã‚‹(ã‚¨ãƒƒã‚¸ã®é‡ã¿ã¯1ã§å›ºå®š)æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ææ¡ˆã€‚â€¦
	- ã¡ãªã¿ã«ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•-ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾å®šç†ã®è©±é¡Œã«é–¢ã—ã¦ã¯ï¼Œãã‚“ãªã«æ–°ã—ã„ã‚‚ã®ã§ã¯ãªãï¼Œçµæ§‹æ˜”ã‹ã‚‰å‡ºã¦ã„ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã™(ä»Šäº•ã•ã‚“)
- RAGã®Gï¼ˆGenerationï¼‰ã€ã¤ã¾ã‚Š"ç”Ÿæˆ"ã¯æœ¬å½“ã«å¿…è¦ãªã®ã‹ï¼Ÿ
	- https://x.com/Nurruttan/status/1785853289034350622
- AI Alignment: A Comprehensive Survey.
	- https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey.pdf
	- AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®åŒ…æ‹¬çš„ãªã‚µãƒ¼ãƒ™ã‚¤
	- AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¯ã€AIã‚·ã‚¹ãƒ†ãƒ ã‚’äººé–“ã®æ„å›³ã‚„ä¾¡å€¤è¦³ã«æ²¿ã£ã¦è¡Œå‹•ã•ã›ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚AIã‚·ã‚¹ãƒ†ãƒ ã®èƒ½åŠ›ãŒé«˜ã¾ã‚‹ã«ã¤ã‚Œã¦ã€ãšã‚ŒãŸAIã‚·ã‚¹ãƒ†ãƒ ã«é–¢é€£ã™ã‚‹æ½œåœ¨çš„ãªå¤§è¦æ¨¡ãƒªã‚¹ã‚¯ãŒé¡•è‘—ã«ãªã£ã¦ã„ã‚‹ã€‚ä½•ç™¾äººã‚‚ã®AIå°‚é–€å®¶ã‚„è‘—åäººãŒAIãƒªã‚¹ã‚¯ã¸ã®æ‡¸å¿µã‚’è¡¨æ˜ã—ã€ã€ŒAIã«ã‚ˆã‚‹çµ¶æ»…ãƒªã‚¹ã‚¯ã®è»½æ¸›ã¯ã€ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯ã‚„æ ¸æˆ¦äº‰ã¨ã„ã£ãŸä»–ã®ç¤¾ä¼šçš„è¦æ¨¡ã®ãƒªã‚¹ã‚¯ã¨ä¸¦ã‚“ã§ã€ä¸–ç•Œçš„ãªå„ªå…ˆäº‹é …ã§ã‚ã‚‹ã¹ãã ã€ã¨ä¸»å¼µã—ã¦ã„ã‚‹
- ãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã“ãƒ¼ã‚„ã£ã¦ä½¿ã†ã®ğŸ’¢
	- https://gist.github.com/kyo-takano/c662c1bfa1e7fe440511b11f62521a7e
	- â‘ ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆç¢ºç‡ãŒå…¨éƒ¨è¦‹ã‚Œã‚‹ã€‚ã“ã‚Œã‚’å¿œç”¨ã™ã‚‹ã¨ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚„èª¤å­—è„±å­—ã®æ¤œå‡ºã‚‚ã§ãã‚‹ã‹ã‚‚ã€‚ã¨ã„ã†ã®ã¯ã©ã®ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆæ™‚ã«è‡ªä¿¡ãŒç„¡ã‹ã£ãŸã‹ãŒç¢ºç‡ã‚’è¦‹ã‚Œã°åˆ†ã‹ã‚‹ã‹ã‚‰ã€‚â‘¡å›ç­”ã®å†’é ­éƒ¨åˆ†ã‚’å¼·åˆ¶ã§ãã‚‹ã€‚
- Xã€ç”ŸæˆAIã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç´„ã‚’é–‹å§‹ã€€ä¸€éƒ¨ã®æœ‰æ–™ä¼šå“¡ã«
	- https://www.nikkei.com/article/DGXZQOGN0406N0U4A500C2000000/
	- ã€Œæ–°æ©Ÿèƒ½ã€Œã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚ºã€ã‚’å§‹ã‚ãŸã€‚ç±³xAIï¼ˆã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒ¼ã‚¢ã‚¤ï¼‰ã®å¯¾è©±AIã€ŒGrokï¼ˆã‚°ãƒ­ãƒƒã‚¯ï¼‰ã€ãŒXä¸Šã§è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã©ã«ã¤ã„ã¦æƒ…å ±ã‚’è¦ç´„ã€
- How LLMs work, clearly explained with visuals:
	- https://x.com/Sumanth_077/status/1786404341735444731
- Build a RAG system with Llama 3B-Instruct for your PDFs
	- https://colab.research.google.com/drive/1BJYYyrPVe0_9EGyXqeNyzmVZDrCRZwsg?usp=sharing#scrollTo=Y2m2l-vt_RSp
-  TAIS 2024 | Insights from two years of AI safety field-building at MATS â€” Ryan Kidd
	- https://www.youtube.com/watch?v=tA9K8JqyhP4
	- Don't miss @jesse_hoogland captivating talk at TAIS2024 on the structure of neural networks and the links between learning theory and interpretability! Watch now:
	- ç‰¹ç•°å­¦ç¿’ç†è«–ï¼ˆæ¸¡è¾ºãƒ™ã‚¤ã‚ºç†è«–ï¼‰ã‚’ç™ºå±•ã•ã›ã¦å±€æ‰€å­¦ç¿’ä¿‚æ•°ã¨ã„ã†æ–°ã—ã„æ¦‚å¿µã‚’å‰µå‡ºã—ï¼Œâ‘ transformerã®å­¦ç¿’ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®è§£æï¼Œâ‘¡æ©Ÿæ¢°è«–çš„è§£é‡ˆå¯èƒ½æ€§ã®åŸºç›¤ç†è«–ã¨ã—ã¦ã®å¯èƒ½æ€§ï¼Œâ‘¢AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆç†è«–ã®å±•æœ›ã‚’åŠ›èª¬ã—ãŸ2äººã®ç ”ç©¶è€…ã®TAIS2024è¬›æ¼”
- ã€Œç¢ºç‡å¤‰æ•°ã€ã®æ­£ä½“ã¯ç±³ç”°åŸ‹ã‚è¾¼ã¿
	- https://m-hiyama.hatenablog.com/entry/20170228/1488276250
- ChatGPTã€æ±å¤§å…¥è©¦ã«æŒ‘ã‚€ã€€è‹±èª8å‰²è¶…ã‚‚æ•°å­¦1ç‚¹ã§ã€Œä¸åˆæ ¼ã€
	- https://www.nikkei.com/article/DGXZQOUC2103E0R20C24A3000000/?n_cid=SNSTW005
	- ã€Œã“ã®è¨ˆç®—ã¯æ‰‹ä½œæ¥­ã§ã¯å›°é›£ã€‚æ•°å­¦ã®å°‚é–€æ›¸ã‚’ãŠã™ã™ã‚ã™ã‚‹ã€ã€‚äººã”ã¨ã®ã‚ˆã†ãªç­”æ¡ˆã‚‚ã‚ã‚Šã¾ã—ãŸã€‚å¤æ–‡ã‚‚æ–‡è„ˆã‚’ç†è§£ã§ããš0ç‚¹ã€‚ä¸€æ–¹ã€è‹±ä½œæ–‡ã‚„è‹±è¨³ã¯æº€ç‚¹ã§ã—ãŸ
- 2024å¹´æ±å¤§å…¥è©¦æ•°å­¦ã®ç¬¬1å•ã®(1)ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å·¥å¤«ã—ã¦è§£ã„ã¦ã¿ãŸã‚‰ã€ä¸€ç™ºã§è§£ã‘ãŸã€‚ã“ã‚Œã ã‘ã§5ç‚¹ãã‚‰ã„å–ã‚Œã¦ã„ã‚‹ã¯ãš
	- https://x.com/itnavi2022/status/1787121446445326816
- ç¬¬2å•ã®(1)ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å·¥å¤«ã—ãªãã¦ã‚‚ã€ChatGPTã§æ™®é€šã«æ­£è§£ã§ããŸã€‚
	- https://x.com/itnavi2022/status/1787448789693059176
- Nvidia ãŒå‡ºã—ãŸã€Llama3-ChatQA-1.5ã®å¾®èª¿æ•´ã§RAGï¼†å¯¾è©±æ€§èƒ½çˆ†ä¸ŠãŒã‚Šã€‚
	- https://x.com/hokazuya/status/1786901364213416356
- LangChainã®llama.cppçµ±åˆ
	- https://x.com/yuiseki_/status/1787091439408816479
- ã€vLLM on Hugging Face Interfaceã€‘
	- https://x.com/hokazuya/status/1787060961570127973
	- ä¾¿åˆ©ã™ãã€‚çˆ†é€Ÿã§Llama 3 -8Bã®LLMã‚’å‹•ã‹ã™ï¼‹OpenAIã®APIã‚’å‘¼ã³å‡ºã™å½¢å¼ã§Llama 3ã¨ä¼šè©±ã§ãã¡ã‚ƒã†ã€‚

## 4/29

ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã‹ã‚‰Phi-3-miniãŒç™ºè¡¨ã•ã‚Œã€3.8Bã®ãƒ¢ãƒ‡ãƒ«ãŒMixtral 8x7Bã‚„GPT-3.5ã¨ãŸã‚ã‚’ã¯ã‚‹ã¨ã®ã“ã¨ã€Phi-3-mini 4k instruct ãƒ¢ãƒ‡ãƒ«ã¯Colab T4ã§ã‚‚å‹•ãã—ã€huggingfaceã«ã‚‚å…¬é–‹ã€‚ã•ã£ããOllamaãŒå¯¾å¿œã—ã€Llama-3 & Phi-3ã‚‚RAGã§ã®æ¯”è¼ƒã¨ã‹ã‚‚ã€‚Llama3ã‚‚ã€æ—¥æœ¬èªå‘ã‘ã«LoRaã•ã‚ŒãŸã‚Šã€Llama3-70Bã‚’42Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æåˆˆã‚Šã—ãŸãƒ¢ãƒ‡ãƒ«ãŒå…¬é–‹ã•ã‚ŒãŸã‚Šã€4bitã«é‡å­åŒ–ã—ã¦è©•ä¾¡ã•ã‚ŒãŸã‚Šã¨ã‹ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ´»å‹•ãŒä¸€æ°—ã«ç››ã‚Šä¸ŠãŒã‚‹ã€‚ãªãŠé‡å­åŒ–ã«é–¢ã—ã¦ã¯ã©ã®ï¼¬ï¼¬ï¼­ã‚‚4bité‡å­åŒ–ã—ã¦ã‚‚ç²¾åº¦ãŒã»ã¨ã‚“ã©ä½ä¸‹ã—ãªã„ã¨ã®ã“ã¨ã ãŒæœ¬å½“ã‹ï¼ŸAppleãŒiPhoneã§ã‚‚ç¨¼åƒã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒOpenELMã€ã‚’ç™ºè¡¨ã€ã•ã£ããMLX LMã§è©•ä¾¡ã—ãŸçµæœãŒå…¬é–‹ã•ã‚ŒãŸã€Macbook Airã§Phi 3ã®é‡å­åŒ–ã•ã‚ŒãŸã‚„ã¤ã‚’å‹•ã‹ã—ã¦åŠ‡é€Ÿã¨ã„ã£ã¦ã‚‹ä¾‹ã¨ã‹ã€å®Ÿã¯ã€LLMãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé–‹ç™ºè€…ã¯Mac é€²åŒ–çš„ã‚’è²·ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’è§¦ã‚‹ã¹ãã¨ã®æ„è¦‹ã‚‚è¦‹ã‚‰ã‚ŒãŸãŒã€åè«–ã‚‚ã¼ã¡ã¼ã¡ã€ã•ã¦ã‚‚ï¼–æœˆã®WWDC24ãŒæ¥½ã—ã¿ã ã€‚ãã‚Œã«ã—ã¦ã‚‚ã€NVIDIA CEOã‚¸ã‚§ãƒ³ã‚¹ãƒ³ãƒ»ãƒ•ã‚¡ãƒ³æ°ãŒã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«ã‹ã‚‰ã®æ­Œé…ä¿¡ã«æ··ã–ã‚‹å‹•ç”»ã€ã‹ã‚ã„ã„ãªã‚ï¼ˆã„ã‚„ï¼£ï¼¥ï¼¯ãŒã ã‚ˆï¼‰ã€‚Groq(LPUã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã®ã»ã†ï¼‰ã®APIã‚’Streamlitã§ä½¿ã†æ–¹æ³•ã®ç´¹ä»‹ãªã©ã€Groqã®åˆ©ç”¨ã‚’ã¡ã‚‰ã»ã‚‰è¦‹ã‚‹ã‚ˆã†ã«ãªã£ãŸã€é€Ÿã•ã¯æœ€å¼·ã€‚LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¯é€±ã¾ã¨ã‚ã¦ãã ã•ã‚‹ã‚µã‚¤ãƒˆã€é ­ãŒä¸‹ãŒã‚‹ã€ã“ã®ã‚¢ãƒ—ãƒ‡æ›´æ–°ã‚‚ãã†ã‚ã‚ŠãŸã„ã‚‚ã®ã ã€‚LLMã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã§ã‚ã‚‹DPOã¯å®Ÿã¯ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®é€†Qå­¦ç¿’ã‚’å®Ÿç¾ã—ã€æœ€é©ãªã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°ã‚’æ¨å®šã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®ä¿¡ç”¨å‰²å½“å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ã¨ã„ã†ã®ã¯ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå•é¡Œã‚’è¡¨é¢ä¸Šã®èª²é¡Œã§ã¯ãªãã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¾ã§è½ã¨ã™ã¨ã“ã‚ãŒé¢ç™½ã„ã€‚LLMã®æ€§èƒ½è©•ä¾¡ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«é–¢ã™ã‚‹æ´»å‹•ã‚‚ElyzaTasks100ã‚„RAGã€Query Planningãªã©ã®ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«LLMã®å®ŸåŠ›ãŒæ¤œè¨¼ãªã‚“ã‹ãŒã‚ã£ãŸã€‚

- ãƒ¢ãƒ‡ãƒ«é€²åŒ–ãƒãƒ¼ã‚¸ã«ã¤ã„ã¦ by sakana.aiã®ç§‹è‘‰ã•ã‚“
	- https://speakerdeck.com/iwiwi/17-nlpkorokiumu
	- æ—¥æœ¬èªLLMã®ãƒãƒ¼ã‚¸ã¯ã‚ã¾ã‚Šãªã„ã€ç¶™ç¶šå­¦ç¿’ã•ã‚Œã¦ã€å…ƒã®é‡ã¿ã‹ã‚‰ãšã‚Œã¦ã—ã¾ã£ã¦ã„ã‚‹ã€‚
	- ãã“ã§ã€é€²åŒ–çš„è¨ˆç®—ã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ã€‚
-  ã€ŒAIäº‹æ¥­è€…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆç¬¬1.0ç‰ˆï¼‰ã€
	- https://www.meti.go.jp/press/2024/04/20240419004/20240419004.html
	- ã“ã‚Œã¾ã§MLPdMçš„ãªäººãŒä¸€èˆ¬çš„ã«ç•™æ„ã™ã¹ãã¨è¨€ã‚ã‚Œã¦ã„ãŸã‚ˆã†ãªã“ã¨ã«åŠ ãˆã€ã‚ˆã‚Šåºƒã„ç¤¾ä¼šçš„ãªè¦³ç‚¹ã‚„ã€AIã®åˆ©ç”¨è€…ã®è¦³ç‚¹ãªã©ã‚‚è¸ã¾ãˆãŸä¸Šã§ã†ã¾ãã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹æœ‰ç›Šãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã ã¨æ„Ÿã˜ã¾ã—ãŸã€‚
	- https://x.com/yu__ya4/status/1782037184079683916
- Command R+ã¯ã©ã“ã¾ã§é‡å­åŒ–ã™ã‚‹ã¨ã‚¢ãƒ›ã«ãªã£ã¦ã—ã¾ã†ã®ã‹ï¼Ÿ by npakaã•ã‚“ï¼Ÿ
	- https://soysoftware.sakura.ne.jp/archives/3834
	- ãƒ­ãƒ¼ã‚«ãƒ«ã§Command R+ã‚’å‹•ã‹ã™ã¨ãªã‚‹ã¨ã€æ‰‹å…ƒã®ç’°å¢ƒã®RTX4090ãŒï¼‘å°ã§ã¯ãƒãƒƒã‚­ãƒªè¨€ã£ã¦1bitã¾ã§åœ§ç¸®ã—ã¦ã‚‚VRAMã«è¼‰ã‚Šãã‚‰ãªã„ã€‚
	- ä»Šå›ã¯Command R+ã®å„é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€Q6_Kã€Q5_K_Sã€Q4_K_Sã€iq4_xsã€Q3_K_Sã€iq3_xxsã€Q2_Kã€iq2_xxsã€iq1_sã®ãã‚Œãã‚Œã«ã¤ã„ã¦ã€ElyzaTasks100ã‚’è§£ã‹ã›ã¦ã¿ã‚‹ã€‚
	- APIï½3bitã¾ã§ã¯ã¶ã£ã¡ã‚ƒã‘å¤§å·®ãªã„ã¨ã„ã†ã‹èª¤å·®ã®ç¯„å›²ã ã¨ã„ã†äº‹ã ã‚ã†
	- 1bitã®3ç‚¹ã¨ã„ã†ã®ã¯ã“ã‚Œã¯ã‚‚ã†å®Œå…¨ã«åŠ£åŒ–ã—ã¦ã‚‹ã¨ã„ã†ã®ã¯ç¢ºå®Ÿã«è¨€ãˆãã†ã ã€‚
	- ã¾ãšã€ä»Šå›ã®çµæœã ã‘ã§è¨€ãˆã°ã€å®Ÿç”¨ä¸Šã¯4bitã¾ã§ã®é‡å­åŒ–ãªã‚‰æ€§èƒ½åŠ£åŒ–ã¯è¦‹å½“ãŸã‚‰ãªã„ã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚
- Fully local RAG with Llama 3 on ollama & streamlit
	- https://x.com/ashpreetbedi/status/1782079131103932647
-   LLMãƒ¢ãƒ‡ãƒ« "Llama3" ã‚’ 4bit é‡å­åŒ–ã—ã¦å®Ÿè¡Œã—ã¦ã¿ãŸ
	- https://qiita.com/akasakat/items/0855b5f05467cc8cbbf4
	- ä¸€æ˜¨æ—¥ç™ºè¡¨ã•ã‚ŒãŸ  [Llama3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)  ã‚’4bité‡å­åŒ– ã—ã¦ã¤ã‹ã£ã¦ã¿ã¾ã—ãŸ
	-  GPUã® VRAM ã¯ 6GB ç¨‹åº¦æ¶ˆè²»ã—ã¾ã™
	- Llama3ã® èªå½™æ•°ã¯ 32000(Llama2) => 128256 ã¸ã¨å¤§å¹…ã«å¢—ãˆã¾ã—ãŸ
- LoRA fine-tuning of embedding models using LlamaIndex
	- https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94
	- In this blog post, weâ€™ll explore how to fine-tune black-box embedding models using low-rank adaptation (LoRA) with the LlamaIndex library. LoRA is a technique that trains a small number of rank-decomposed weights to adapt a pre-trained model to a new task or domain. 
- è‡ªå®…PCã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ï¼šã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼GPUã®æ ã‚’è¶…ãˆã€å¤§å‹LLMã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™ï¼ by AIã‚µãƒˆã‚·
	- https://note.com/aisatoshi/n/nd4969fc42602?sub_rt=share_h
	- Command-r-Plusã¯ã€4bitã«é‡å­åŒ–ã—ã¦ã‚‚60GBç¨‹åº¦ã®VRAMãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚
	- è¤‡æ•°PCã§ã®ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ãŒè‡ªå®…ã§å¯èƒ½ã¨ãªã£ãŸã®ã§ã€ç†è«–çš„ã«ã¯ã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚’å¢—ã‚„ã™ã“ã¨ã§å·¨å¤§ãªLLMã®æ¨è«–ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚
- alfredplpl/Llama-3-8B-Instruct-Jaã€€ by ã‚ã‚‹ãµã•ã‚“
	- https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja
	- æ—¥æœ¬èªå‘ã‘ Llama 3 8Bã‚’å…¬é–‹ã—ã¦ã¿ã¾ã—ãŸã€‚LoRAã§è¡¨é¢ã‚’å­¦ç¿’ã—ãŸã ã‘ãªã®ã§ã€æ€§èƒ½ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãŸã ã€æ™®é€šã®Llama 3ã‚ˆã‚Šã‹ã¯æ—¥æœ¬èªãŒå¼·ããªã£ã¦ã„ã‚‹ã¯ãšã§ã™ã€‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™
- Groqã®å€¤æ®µèª¿ã¹ã€llama3ãªã©
	- https://x.com/webbigdata/status/1782240169879601540
	- Groqã¯LPU(Language Processing Unit)ã¨ã„ã†ç‹¬è‡ªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚’é–‹ç™ºã—ã¦ã„ã‚‹ä¼šç¤¾ã§ã™
	- Llama 3 70BãŒAnthropic Claude 3 Sonnet($3.00/$15.00)ç›¸å½“ã®æ€§èƒ½ã§ã‚ã‚Œã°ã€Groqã®Llama 3 70B APIã®ä¾¡æ ¼è¨­å®š($0.59/$0.79)ã¯éå¸¸ã«ç«¶äº‰åŠ›ãŒã‚ã‚Šã¾ã™
- Llama 3 70b layer pruned from 70b -> 42b by Charles Goddard
	- https://www.reddit.com/r/LocalLLaMA/comments/1c9u2jd/llama_3_70b_layer_pruned_from_70b_42b_by_charles/
	- chargoddard/llama3-42b-v0
	- Llama3-70Bã‚’æåˆˆã‚Šã—ã¦ãƒ‘ãƒ©æ•°42Bã«ã—ã¡ã‚ƒã£ãŸã¨ã„ã†ãƒ–ãƒ„ã‚‰ã—ã„ã€‚
-  Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
	- https://arxiv.org/abs/2404.14219
	- Microsoft announces phi-3-mini, a 3.8B model trained on 3.3T tokens that rivals Mixtral 8x7B and GPT-3.5
	- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³æ•°
		- â‘ Phi-3-mini (38å„„ã€3å…†3000å„„)
		- â‘¡Phi-3-small (70å„„ã€4å…†8000å„„) 
		- â‘¢Phi-3-medium (140å„„ã€4å…†8000å„„ï¼‰
- ã¯ï¼ŸPhi3-small-7Bã¯MMLUãŒ75.3ç‚¹ï¼ŸLlama3-8Bã§ã‚‚66ç‚¹ã ã¨ã„ã†ã®ã«ã€‚
	- https://x.com/umiyuki_ai/status/1782622321704079652
-  Weekly AI Agents News!
	- https://speakerdeck.com/masatoto/weekly-ai-agents-news
	- ã“ã®LLMãƒ–ã‚¯ãƒã‚¢ãƒ—ãƒ‡ã®ã‚ˆã†ã«æ¯é€±ã€Agenté–¢ä¿‚ã®æƒ…å ±ã‚’åé›†ã—ã¦ã„ã‚‹äºº
	- LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¯é€±ã¾ã¨ã‚ã¦ãã ã•ã‚‹
-  From  r  to  Qâˆ—: Your Language Model is Secretly a Q-Function
	- https://arxiv.org/abs/2404.12358
	- LLMã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã§ã‚ã‚‹DPOã¯å®Ÿã¯ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®é€†Qå­¦ç¿’ã‚’å®Ÿç¾ã—ã€æœ€é©ãªã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°ã‚’æ¨å®šã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®ä¿¡ç”¨å‰²å½“å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ã€‚ä¾‹ãˆã°ã‚ã‚‹å¯¾è©±ã®çµæœã«ã¤ãªãŒã£ãŸåŸå› ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç‰¹å®šã§ããŸã‚Šã€å°¤åº¦æœ€å¤§åŒ–ã®ãƒ“ãƒ¼ãƒ æ¢ç´¢ã¯ãã®ã¾ã¾åç›Šæœ€å¤§åŒ–ã¨ã¿ãªã›ã‚‹ by å²¡é‡åŸã•ã‚“
- Llama.cpp ã§ Llama 3 70Bã‚’ãŠè©¦ã—ä¸­ã€‚by npakaã•ã‚“
	- https://x.com/npaka123/status/1782556559589212399
	- 8.42 tokens per second 
	- Meta-Llama-3-70B-Instruct-IQ4_XS.gguf 
	- M3 Max (128GB)
-  llama.cpp ã«ã‚ˆã‚‹ transformersãƒ¢ãƒ‡ãƒ« ã®é‡å­åŒ– by npakaã•ã‚“
	- https://note.com/npaka/n/nbd1348500a28?sub_rt=share_b
	- ä»Šå›ã¯ç·´ç¿’ç”¨ã«ã€Œmeta-llama/Meta-Llama-3-8B-Instructã€ã‚’æº–å‚™ã—ã¾ã™ã€‚
	- transformersãƒ¢ãƒ‡ãƒ«ã‚’ggufã«å¤‰æ›
	- imatrixé‡å­åŒ–
- Llama3-70Bã¯ElyzaTasks100ï¼ˆCommand R+ã«ã‚ˆã‚‹è‡ªå‹•è©•ä¾¡ï¼‰ã«ãŠã„ã¦Command R+è¶…ãˆã¦ã¾ã™
	- https://x.com/umiyuki_ai/status/1782690199677641164
- CodeQwen1.5
	- https://x.com/Alibaba_Qwen/status/1782426698279272742
	- Last week, we released a CodeQwen1.5 and received a lot of positive feedback! Thank you for your support! 
- æ‰‹å…ƒã®Macbook Airã§Phi 3ã®é‡å­åŒ–ã•ã‚ŒãŸã‚„ã¤ã‚’å‹•ã‹ã—ã¦ã„ã‚‹ã®ã ãŒã€ã“ã‚ŒGPT-3.5ã“ãˆã¦ã‚‹ã‚ˆã­ã€‚ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã§æ™®é€šã«å‹•ãã£ã¦ã©ã†ã„ã†ã“ã¨ã 
	- https://x.com/alfredplpl/status/1782808427129114796
- phi3 local RAG using LlamaIndex and Ollama:
	- https://x.com/llama_index/status/1782893301214986593
	- https://colab.research.google.com/drive/1RoZzbL8WYaAp4b3sazYHVI8TA2AkrtRJ#scrollTo=9AtRxaqD94mZ
- æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ã®local LLMã®å®ŸåŠ›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
	- RAG, Query Planning, Text2SQL, and Pydantic Program but struggles with Routing and Agentic tasks. 
-  Feature Test for Phi-3-mini-4k-instruct
	- https://docs.llamaindex.ai/en/latest/examples/benchmarks/phi-3-mini-4k-instruct/
- Phi-3 mini 128k instruct ã® Colab T4 ã§å‹•ä½œç¢ºèªã®å–ã‚ŒãŸã€€ by ã¬ã“ã¬ã“ã•ã‚“
	- https://gist.github.com/schroneko/f4fac4c4dd541f4c5ee61c44c90c4a85
	- ã‚µãƒ³ãƒ—ãƒ«ã®æ–¹ç¨‹å¼ã‚’è§£ãå•é¡Œã¯é›£ãªãã‚¯ãƒªã‚¢ã€‚æ—¥æœ¬èªã§ã‚‚ã‚¯ãƒªã‚¢ã€‚3.8B ã«ã—ã¦ã¯ã‹ãªã‚Šæ—¥æœ¬èªã‚’ãƒŠãƒãƒ¥ãƒ©ãƒ«ã«è©±ã›ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿ
- HuggingChatã«phi3-mini-4kç™»å ´
	- https://huggingface.co/chat/
-  Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches
	- https://arxiv.org/abs/2404.14779
	- åŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã§LLMã‚’fine tuningã™ã‚‹éš›ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã™ã‚‹ã‹LoRAã§åŠ¹ç‡çš„ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã¹ãã‹ã‚’Llama-2ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼ã—ãŸè«–æ–‡ã€‚
	- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„ã»ã©fine tuningã®åŠ¹æœãŒå¤§ãã„
	- ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã„ã»ã©LoRAã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯å¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«æ¥è¿‘ã—ãã†
- LLMã®ç¶™ç¶šå­¦ç¿’ã«ãŠã‘ã‚‹è«–æ–‡ç´¹ä»‹
	- https://note.com/sergicalsix_/n/ndbd5b29451c9
	- LLMã®ç¶™ç¶šå­¦ç¿’ã«ãŠã„ã¦ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å†…å®¹ã‚„é †åºãªã©ã«ã¤ã„ã¦èª¿æŸ»ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é¡ä¼¼åº¦é †ã§ç¶™ç¶šå­¦ç¿’ã—ãŸæ–¹ãŒãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã•ã›ã‚„ã™ãã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ãªé †åºã§ç¶™ç¶šå­¦ç¿’ã—ãŸæ–¹ãŒLLMã®æ€§èƒ½ãƒ»çŸ¥è­˜ã®è“„ç©ãŒæ”¹å–„ã™ã‚‹ã€‚
-  Command R+ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚ã™ã”ã‹ã£ãŸ
	- https://qiita.com/sergicalsix/items/5ceb9a3a0d11affb4b9a
	- ä»Šå›ã¯Command R+ã®æ—¥æœ¬èªã®å¿œç­”é€Ÿåº¦ãŒæœ¬å½“ã«é€Ÿã„ã®ã‹ã€ãªãœé€Ÿã„ã®ã‹ã«ã¤ã„ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¦³ç‚¹ã§è¿°ã¹ãŸã„ã¨æ€ã„ã¾ã™ã€‚
	- Cohereã®Ayaã¨Command R+ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ä»–ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨æ¯”ã¹ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå‰Šæ¸›ã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚
- Appleã€iPhoneã§ã‚‚ç¨¼åƒã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒOpenELMã€ã‚’å…¬é–‹
	- https://www.itmedia.co.jp/news/articles/2404/25/news103.html
	- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç•°ãªã‚‹4ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã€‚å°ã•ã„ã‚‚ã®ã‹ã‚‰ã€2å„„7000ä¸‡ã€4å„„5000ä¸‡ã€11å„„ã€30å„„
	- OpenELMã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€Transformerãƒ¢ãƒ‡ãƒ«ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼å†…ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’efficientï¼ˆåŠ¹ç‡çš„ï¼‰ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã§ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ã¨ã„ã†ã€‚
- Let's compare Llama-3 & Phi-3 using RAG:
	- https://lightning.ai/lightning-ai/é€²åŒ–çš„s/compare-llama-3-and-phi-3-using-rag?utm_source=akshay
	- https://x.com/akshay_pachaar/status/1783114329199718558
-  Cohere Toolkit
	- https://github.com/cohere-ai/cohere-toolkit
	- Yesterday, we open sourced the Cohere Toolkit. We think this will be a major accelerant for getting LLMs into production within enterprise.
-  LLMã«ã¨ã£ã¦ã€Œè³ªã®è‰¯ã„å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã€
	- https://x.com/imos/status/1783494307959513522
	- LLMã«ã¨ã£ã¦ã€Œè³ªã®è‰¯ã„å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã€ã¯ã€Œæ­£ã—ã„æ—¥æœ¬èªã«/å€«ç†çš„ã«çµã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã€ã§ã¯ãªã„ã¨æ€ã†ã®ã§æ•´ç†ã—ã¦å¸ƒæ•™ã—ãŸã„ï¼ˆFineWebæ›°ãã‚¢ãƒ€ãƒ«ãƒˆã‚µã‚¤ãƒˆã‚’æŠœãã¨æ€§èƒ½åŠ£åŒ–ã™ã‚‹ã‚‰ã—ã„ï¼‰ã€‚è¨€èªèƒ½åŠ›ã€çŸ¥è­˜ã€è«–ç†èƒ½åŠ›ã€å¿œç­”å½¢å¼ãªã©ã€ç”¨é€”ã‚’æº€ãŸã™ã®ã«å¿…è¦ãªè»¸ã‚’æ¬ ã‹ã•ãšå«ã‚€ã“ã¨ãŒå¤§äº‹ã ã¨æ€ã‚ã‚Œã‚‹ã€‚
- Llama 3 Establishes Meta as the Leader in â€œOpenâ€ AI  by IEEE Spectrum
	- https://spectrum.ieee.org/meta-llama-3?share_id=8224093&utm_campaign=RebelMouse&utm_content=IEEE+Spectrum&utm_medium=social&utm_source=twitter
- ã€çµ±è¨ˆçš„ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€(å²©æ³¢æ›¸åº— ç¢ºç‡ã¨æƒ…å ±ã®ç§‘å­¦)ã®åŸ·ç­†ãŒã¤ã„ã«æœ€å¾Œã¾ã§åˆ°é”ã—ã¾ã—ãŸã®ã§ã€ã€Œæ–‡æ›¸ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã€ã®ç« ã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- http://chasen.org/~daiti-m/textmodel/textmodel-chapter5.pdf
-  Graph Machine Learning in the Era of Large Language Models (LLMs)
	- https://arxiv.org/abs/2404.14928
	- ã‚°ãƒ©ãƒ•ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡
	- ã‚°ãƒ©ãƒ–ç³»æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨LLMã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ç ”ç©¶ä¾‹ã‚„å±•æœ›ãŒã¾ã¨ã¾ã£ã¦ã„ã¾ã™ã€‚MIåˆ†é‡ã ã‘ã§ãªãä»–åˆ†é‡ã®äº‹ä¾‹ã‚‚ã‚ã‚Šå‚è€ƒã«ãªã‚Šã¾ã™ã€‚
- Two new AI releases by Apple today
	- https://x.com/pcuenq/status/1783032344104026372
	- OpenELM, a set of small (270M-3B) efficient language models. Weights on the Hub:
	- CoreNet, a training library used to train OpenELM:
-  [Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge](https://aclanthology.org/2024.eacl-long.127.pdf)
	- å¤šè¨€èªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒç²å¾—ã—ã¦ã„ã‚‹äº‹å®Ÿã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’53è¨€èªã§æ¤œè¨¼ã€‚ã©ã®ã‚ˆã†ãªåŸå› ã«ã‚ˆã£ã¦è¨€èªã”ã¨ã«å·®ãŒå‡ºã‚‹ã®ã‹ã€ãƒ‡ãƒ¼ã‚¿é‡ã‚„åœ°ç†çš„è¦³ç‚¹ãƒ»æ´»æ€§åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é¡ä¼¼æ€§ãªã©ã‹ã‚‰åˆ†æã—ã¦ã„ã‚‹ã€‚
-  LLMãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé–‹ç™ºè€…ãŒMac é€²åŒ–çš„ã‚’è²·ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’è§¦ã‚‹ã¹ãç†ç”±
	- https://note.com/erukiti/n/n58a8180ea9fb
- [torchtitan](https://github.com/pytorch/torchtitan)
	- a library for large model training called torchtitan
	- They have scripts to train Llama-3 from scratch
	- The library went public today on GitHub but it is still in pre-release state & active development
- LangChainã‚’ç”¨ã„ãŸ4ç¨®é¡ã®RAGè³ªå•å¿œç­”chainã®å®Ÿè£…ã¨æ€§èƒ½æ¯”è¼ƒ
	- https://zenn.dev/aidemy/articles/97d5fb6ac03a4f
	-  **stuff chain**ã€ **map reduce chain**ã€**map rerank chain**ã€ **refine chain**
	-  **é©ã—ã¦ã„ã‚‹æ–‡æ›¸ç‰¹å¾´**
		-  **stuffãƒ»map reduce**  : æ–‡æ›¸å…¨ä½“ã‚’1æ®µéšã¾ãŸã¯2æ®µéšã§LLMã«å…¥åŠ›ã™ã‚‹ãŸã‚, æ–‡æ›¸å…¨ä½“ã«é‡è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹å ´åˆã«ç‰¹ã«æœ‰åŠ¹ã§ã™ã€‚
		- **map rerank**  : æ–‡æ›¸ã®ä¸€éƒ¨ã®ã¿ã®å›ç­”ã‹ã‚‰æœ€è‰¯ã®å›ç­”ã‚’é¸ã¶ãŸã‚, ä¸€éƒ¨ã®ã¿ã«é‡è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹å ´åˆã«ç‰¹ã«æœ‰åŠ¹ã§ã™ã€‚
		- **refine**  : ä¸€éƒ¨ã®ã¿ã®å›ç­”ã‚’è¤‡æ•°å›å†èµ·çš„ã«å‘¼ã³å‡ºã™ãŸã‚, é‡è¦ãªæƒ…å ±ãŒæ–‡æ›¸ã®å…¨ä½“ã§ã‚‚ä¸€éƒ¨ã§ã‚‚å¯¾å¿œã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
- ã€ŒJapanese-Starling-ChatV-7Bã€
	- https://x.com/AIBizNavigator/status/1783667625802994164
	- 7Bã‚¯ãƒ©ã‚¹ã¨ã¯æ€ãˆãªã„è¶…é«˜æ€§èƒ½ãªã‚“ã 
	- è‹±èªã®æœ€å¼·7Bãƒ¢ãƒ‡ãƒ«Starling-LM-7B-betaã‹ã‚‰æŠ½å‡ºã—ãŸChat Vectorã‚’ã€ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ChatNTQ-JA-v1.0-7bã«æ›ã‘åˆã‚ã›ãŸã ã‘ã€‚ è¿½åŠ ã®æ—¥æœ¬èªå­¦ç¿’ã¯ä¸€åˆ‡ãªã—
	- https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF
- Mergekit-Evolveã®ãƒ†ã‚¹ãƒˆã§è©¦ã—ã«ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã€Japanese-Chat-Umievo-itr001-7b
	- https://x.com/umiyuki_ai/status/1783867934542303666
	- https://huggingface.co/umiyuki/Japanese-Chat-Umievo-itr001-7b
	- ElyzaTasks100ã§è©•ä¾¡ã—ã¦ã¿ãŸã‚‰å¹³å‡3.57ç‚¹ã‚’å©ãå‡ºã—ãŸï¼7Bãƒ¢ãƒ‡ãƒ«ãªã®ã«35Bãƒ‘ãƒ©ã®Command Rã‚’è¶…ãˆã¦ã¾ã™ï¼é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å¨åŠ›æã‚‹ã¹ã—ï¼ï¼ã¨ã‚Šã¾HuggingFaceã«ä¸Šã’ã¾ã—ãŸï¼
- Swallow instruction tuning models
	- https://huggingface.co/collections/tokyotech-llm/swallow-instruct-65e559f4d52e7c9d197697c2
	- wallow 7B, 13B, 70Bã€ãŠã‚ˆã³Swallow-MS 7Bã®æ–°ã—ã„instructãƒ¢ãƒ‡ãƒ«ï¼ˆSwallow-*-instruct-v0.1ï¼‰ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã‚ã¾ã‚Šé‡è¦–ã—ã¦ã“ãªã‹ã£ãŸæŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã‚„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¿œç­”ã®æ”¹å–„ã«å–ã‚Šçµ„ã¿ã€MT-Benchã§éå»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚
- tokyotech-llm/Swallow-MS-7b-instruct-v0.1
	- https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1
	- swallow 7B, 13B, 70Bã€ãŠã‚ˆã³Swallow-MS 7Bã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ”¹è‰¯ã—ã€æŒ‡ç¤ºè¿½å¾“æ€§ã‚„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¿œç­”ã‚’å‘ä¸Šã•ã›ãŸãƒ¢ãƒ‡ãƒ«ã‚’Hugging Faceä¸Šã§å…¬é–‹ã—ã¾ã—ãŸã€‚ä»¥å‰ã«å…¬é–‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦ã€MT-Benchã®ã‚¹ã‚³ã‚¢ã§å¤§å¹…ã«æ”¹å–„ã—ã¦ã„ã¾ã™
- Swallow-MS-7B-Instruct-V0.1ã‚’ElyzaTasks100ã§è©•ä¾¡ã—ãŸã‚‰å¹³å‡2.82ç‚¹ã ã£ãŸã€‚ç¾ç’°å¢ƒã§ã¯ã‚‚ã¯ã‚„å¤§ã—ãŸäº‹ãªã„ã¨è¨€ã‚ã–ã‚‹ã‚’å¾—ãªã„ã€‚ã§ã‚‚ChatNTQã‚ˆã‚Šã¯ã‹ãªã‚Šå¼·ã„ã¨ã„ã†äº‹ã¯ChatVectorã‚’è¶³ã™ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æœ‰èƒ½ã‹ã‚‚ã—ã‚Œãªã„
	- https://x.com/umiyuki_ai/status/1783911959789969816
- ã€Appleã®æ–°ã—ã„OpenELMãƒ¢ãƒ‡ãƒ«ã‚’MLX LMã§ã€‘
	- https://x.com/hokazuya/status/1783808939773304957
	- 512ãƒˆãƒ¼ã‚¯ãƒ³ã€340Token/S
	- M3 Pro Mac (64GB)ã§16ãƒ“ãƒƒãƒˆã®270Mãƒ¢ãƒ‡ãƒ«ã§è¶…é«˜é€Ÿãƒ­ãƒ¼ã‚«ãƒ«LLMãŒå®Ÿç¾ã€‚
-  Weave ã¨ Elyza-tasks-100 ã§ ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’è©•ä¾¡ã™ã‚‹ by npakaã•ã‚“	
	- https://note.com/npaka/n/nc0c8d5beacff?sub_rt=share_h
	- ã€Œ**Weave**ã€ã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨˜éŒ²ã€å®Ÿé¨“ã€è©•ä¾¡ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™
	- ã€Œ**Elyza-tasks-100**ã€ã¯ElyzaãŒæä¾›ã™ã‚‹æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ç”¨ã®è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚
- Domingosæ°ã€AIã®èƒ½åŠ›ãŒäººé–“ãƒ¬ãƒ™ãƒ«ã§é£½å’Œã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¦ã„ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¦ã„ã‚‹ãŒã€ã€
	- https://x.com/rmaruy/status/1784154638390104188
	- ã‚€ã—ã‚ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã§120%ã‚„200%ã‚’æœ‰æ„å‘³ã«è­°è«–ã§ãã‚‹ã®ã‹ã¨ã„ã†æ–¹ãŒæ°—ã«ãªã‚‹ã€‚ã¨ã„ã†æ„å‘³ã§ã€Domingosæ°ã®æ„å›³ã¨ç•°ãªã‚‹æ„å‘³ã§è¶…çŸ¥èƒ½åˆ°æ¥ãƒ“ã‚¸ãƒ§ãƒ³ã¸ã®ç–‘ç¾©ã«ãªã£ã¦ã„ã‚‹ã€‚
- ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’ä½¿ã£ã¦ã¿ã‚ˆã†
	- https://x.com/makaishi2/status/1784115819791913065
	- ã€Pythonã§ã‚¹ãƒ©ã‚¹ãƒ©ã‚ã‹ã‚‹ãƒ™ã‚¤ã‚ºæ¨è«–ã€Œè¶…ã€å…¥é–€ã€è‘—è€…
- ã€éšæ™‚æ›´æ–°ã€‘ä¸»è¦ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨
	- https://zenn.dev/ml_bear/articles/3c5e7975f1620a
- å±…åˆã‚ã›ãŸæ­Œé…ä¿¡ã«æ··ã–ã‚‹ NVIDIA $NVDA CEO ã‚¸ã‚§ãƒ³ã‚¹ãƒ³ãƒ»ãƒ•ã‚¡ãƒ³
	- https://x.com/woodstockclub/status/1784179786082128351
	- é…ä¿¡è€…ã®2äººã€Œã‚¸ã‚§ãƒ³ã‚¹ãƒ³ï¼Ÿèª°ï¼Ÿï¼Ÿã€
-  é«˜é€ŸAIãƒãƒƒãƒ—ã§è©±é¡Œã®Groqã®APIã‚’Streamlitã§ä½¿ã†æ–¹æ³•
	- https://note.com/masayuki_abe/n/n336721e355e6?sub_rt=share_pb
	- é«˜é€ŸAIãƒãƒƒãƒ—ã§è©±é¡Œã®Groqã®APIã‚’Streamlitã®ã‚³ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã¿ã¾ã—ãŸã€‚OpenAIã®APIã¨è¡¨è¨˜ãŒä¼¼ã¦ã„ã‚‹ã®ã§æ›¸ãã‚„ã™ã„ã§ã™ã­ã€‚
- Swallow-MS-7Bã‚„RakutenAI-7Bã¯ãƒˆãƒ¼ã‚¯ãƒ³ã®èªå½™ãŒæ‹¡å¼µã•ã‚Œã¦ã‚‹äº‹ã«æ°—ä»˜ã„ãŸãŒã€ã“ã‚Œã£ã¦æ‹¡å¼µã•ã‚Œã¦ãªã„ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¼ã‚¸ã—ãŸã‚‰ã‚¢ã‚«ãƒ³ã®ã ã‚ã†ã‹
	- https://x.com/umiyuki_ai/status/1784274430816034898
- ã€Whisper.cpp-CLIã€‘
	- https://x.com/hokazuya/status/1784554378118246440
	- ãƒ­ãƒ¼ã‚«ãƒ«ã§é«˜ç²¾åº¦ã®éŸ³å£°æ–‡å­—èµ·ã“ã—ãŒã§ãã‚‹Whisperç’°å¢ƒãŒã€ã‚‚ã®ã®200msã§ä½œã‚Œã¦ã—ã¾ã†ã¨ã®Pyplãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒOSSã§
- 


## 4/21

ä»Šé€±ã¯ã€æœ€å¤§ï¼“å€é«˜é€Ÿã¨ã„ã†æ—¥æœ¬èªGPT-4ã®é–‹ç™ºã®ç™ºè¡¨ã‚‚ã‚ã£ãŸã‘ã©ã€ãªã‚“ã¨ã„ã£ã¦ã‚‚ãƒ¡ã‚¿ã‹ã‚‰llama3ã®å¾…æœ›ã®å…¬é–‹ã€‚æœ€åˆã¯8bã¨70bãŒå…¬é–‹ã•ã‚Œã€ã•ã‚‰ãªã‚‹å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚‚é–‹ç™ºä¸­ã¨ã®ã“ã¨ã€‚lllama3ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ç”¨ã„ãŸPyTorchã®æ–°æ©Ÿèƒ½tochtuneã‚‚å…¬é–‹ã€‚æ—©é€Ÿã€é‡å­åŒ–ã€MoEåŒ–ã€ãƒ•ã‚¡ã‚¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œä¾‹ãŒå…¬é–‹ã•ã‚Œã€MXã§8GB M2 miniã§ã®å‹•ä½œç¢ºèª!ã€ollamaã®å¯¾å¿œã€ã•ã‚‰ã«ã¯Groqã«ä¹—ã£ã‹ã£ã¦ãƒ‡ãƒ¢ã‚µã‚¤ãƒˆã§Llama3-70BãŒ300t/sã®è¶…çµ¶çˆ†é€Ÿæ¨è«–ã‚’è¦‹ã›ãŸãªã©ã®ä¸€é€šã‚ŠãŒï¼‘é€±é–“ã§é€²ã‚€ã€‚RAGã§ã®llama3ã®åˆ©ç”¨ä¾‹ã‚‚LangChainã‹ã‚‰ç´¹ä»‹ãŒã‚ã£ãŸãŒã€CommandRï¼‹ã‚‚llama3ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä¸ãˆã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒç‹¬ç‰¹ãªã®ã§ã€LLMã‚’ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ä½¿ã†äººã¯è¦æ³¨æ„ã ã€‚1bitã®LLMã‚‚ã€shi3zã•ã‚“ã®è‡ªä½œè©•ä¾¡ã‚„ã€æ¤æ©‹ã•ã‚“ã«ã‚ˆã‚‹GPUã§ã¯ãªã„ã‚ªãƒ¼ãƒ€ãƒ¼ãƒ¡ã‚¤ãƒ‰ã«ã‚ˆã‚‹AIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã€Œã‚«ã‚¹ã‚¿ãƒ AIã€ã®å¯èƒ½æ€§ãªã©ã€ã„ã„è¨˜äº‹ãŒã§ã¦ããŸã€‚ChatVectorã«ã‚ˆã‚‹LLMæ€§èƒ½å‘ä¸Šã‚‚ã€å…ˆé€±ã«å¼•ãç¶šãã€Bakuã•ã‚“ã®ã€ChatNTQ 7B ã¨ LightChatAssistant 2x7B ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’è©¦ã™è¨˜äº‹ãŒç¥è¨˜äº‹ã¨ã—ã¦è©±é¡Œã«ã€‚LightChatAssistantã£ã¦ã®ã¯ãã‚“ãªã«ã™ã”ã„ã®ã‹ã€‚ä½œã£ã¦ã¿ãŸã‚‰æ€§èƒ½ãŒé«˜ã‹ã£ãŸã¨ã„ã†Japanese-Starling-ChatV-7B-GGUFãªã©ã‚‚å‡ºãŸã‚Šã€ChatVectorç´¹ä»‹ã®å…ˆé§†è€…ã¯ã¡ã•ã‚“ã‹ã‚‰Swallow-MS-7b-v0.1-ChatSkill-LABãŒå‡ºãŸã‚Šã€èƒ½åŠ›åŠ ç®—ã®çµ„ã¿åˆã‚ã›ã®æœ€é©è§£ã‚’optuneã‚’ã¤ã‹ã£ã¦å®Ÿè¡Œãƒ»è©•ä¾¡ã¨ã‹ã€ã€LLMã®èƒ½åŠ›ã®è¶³ã—ç®—å¼•ãç®—ã—ã¤ã¤æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã¨ã„ã†ä¸€æ®µãƒ¡ã‚¿ãªä¸–ç•ŒãŒé–‹ã‘ãŸã€‚PFNã®ä¸¸å±±ã•ã‚“ãŒç´¹ä»‹ã•ã‚ŒãŸã€LLMã‚’ã¤ã‹ã£ã¦è¨€è‘‰ã ã‘ã§ã€ç·šå½¢å›å¸°ã‚’ã•ã›ã‚‹ã¨ã„ã†è«–æ–‡ã€ã©ã‚“ãªãƒ¢ãƒ‡ãƒ«ã‚’å†…éƒ¨ã«æŒã£ã¦ã„ã‚‹ã‚“ã ã¨ã„ã†æ„å‘³ã§é¢ç™½ã„ã€‚ Cambridgeå¤§å­¦ã®U. Anwar, D. Kruegeræ°ã‚‰40å!ã«ã‚ˆã‚‹ã€LLMã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨å®‰å…¨æ€§ã®æœªè§£æ±ºå•é¡Œã«é–¢ã™ã‚‹175ãƒšãƒ¼ã‚¸ã®ç·èª¬è«–æ–‡ã¯ã™ã”ã„ã€AIã‚¬ãƒãƒŠãƒ³ã‚¹ã®ã‚ªãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ¼ãƒ‰ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã‚‚ã‚ã‚Šã€UKã§ã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨ã‚¬ãƒãƒŠãƒ³ã‚¹ã®å¤§ããªæ‹ ç‚¹ã«ãªã£ã¦ã„ã‚‹ã®ã‹ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã‹ã‚‰ã€WizardLM-2 ã®7bã¨8x22bãŒç™ºè¡¨ã€Evolve Instructã¨ã„ã†æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®èƒ½åŠ›ã‚„ã„ã‹ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚‚æŒã£ã¦ã„ã‚‹ã¨ã‹ã€åµã®äºˆæ„Ÿã€‚Qwen1.5-7B-Chat-GGUFã‚‚å‡ºãŸã€æ¥é€±ã‚ãŸã‚ŠQwen1.5ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªLLMãŒå‡ºã¦ãã‚‹ã®ã§ã¯ã€‚DeepMindã®ã€ŒMany-shotã€å¤šæ•°ä¾‹ç¤ºå­¦ç¿’ã®æœ‰åŠ¹æ€§ã‚„ã€RAGã®MiniCheckã€è¤‡æ•°ã®çŸ¥è­˜ã‚’çµ„ã¿åˆã‚ã›ã‚‹Chain-of-Abstraction (CoA) Reasoningãªã©ã®LLMæ¨è«–ã§ã®é€²å±•ã‚‚ã‚ã£ãŸã€‚ä¸¸å±±éš†ä¸€ã•ã‚“ã€AIç§‘å­¦ã®ä½•ãŒâ€œå“²å­¦â€ã¨ã„ã†å•ã„ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰ã‚‚è‰¯ã„ã—ã€ã€ŒAIå”åƒæ™‚ä»£ã«ç ”ç©¶è€…ã¯ã©ã†ç”Ÿãã‚‹ã‹ã€ã¨ã„ã†ã‚¤ãƒ™ãƒ³ãƒˆã‚‚é¢ç™½ã„ã€‚AIãŒï¼ˆå¾“æ¥ã®ï¼‰ç ”ç©¶ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ãªã‚‰ã°ã€AIç ”ç©¶è€…ã¯ãªã«ã‚’ã™ã‚‹ã®ã‹ã¿ãŸã„ãªæ„Ÿã˜ã€‚ã•ãã»ã©ã®UKã¨æ¯”ã¹ã‚‹ã¨ã“ã®ã‚ãŸã‚Šã®ç ”ç©¶è€…å±¤ãŒè–„ã„ã®ã‹ãªã€‚ãã®ä»–ã¨ã—ã¦ã¯ã€Swallow-MXã‚’ä½¿ã£ãŸQ&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹AutoWikiQAã¨ã‹ã€MiniCPM-V-2ã®ãƒ‡ãƒ¢ã®å…¬é–‹ã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã€idefics-8bãªã©ã‚‚å‡ºã¦ããŸã€‚

- openbmb/MiniCPM-V-2
	- MiniCPMã®ãƒ‡ãƒ¢ãŒå…¬é–‹
	- https://huggingface.co/spaces/openbmb/MiniCPM-V-2
- OpenAI Japanã®ç™ºè¶³ã¨ã¾ã•ã‹ã®æ—¥æœ¬èªGPT-4ã®ç™ºè¡¨ã€‚
	- https://openai.com/blog/introducing-openai-japan
	- ã€Œæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã®ç¿»è¨³ã¨è¦ç´„ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ãŠã‚ˆã³ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€å‰ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€æœ€å¤§3å€é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚ã€
- ChatNTQ 7B ã¨ LightChatAssistant 2x7B ã®æ—¥æœ¬èªæ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹
	- https://sc-bakushu.hatenablog.com/entry/2024/04/10/191420
	- ã€Œ[ChatNTQ-JA-7B-v0.1](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0)ã€ã¨ã€ãã®MoEãƒ¢ãƒ‡ãƒ«ã€Œ[LightChatAssistant 2x7B](https://huggingface.co/Sdff-Ltba/LightChatAssistant-2x7B)ï¼ˆæ”¹ç§°ã‚ã‚Šï¼‰ã€ã«ã¤ã„ã¦ã€ã‹ãªã‚Šæ€§èƒ½ãŒè‰¯ã•ãã†ãªæ„Ÿè§¦ãŒå¾—ã‚‰ã‚ŒãŸã®ã§ã€è¿½åŠ ã§ãƒ†ã‚¹ãƒˆã—ã¦ã¿ã¾ã—ãŸã€‚
	- LightChatAssistantã¯ChatNTQã¨AntlerãŒã‚¸ãƒ§ã‚°ãƒ¬ã‚¹é€²åŒ–ã—ã¦å¥‡è·¡ã®ã‚·ãƒŠã‚¸ãƒ¼ã‚’èµ·ã“ã—ã¦ã€ELYZATasks100ãƒ™ãƒ³ãƒã§35Bã®Command Rã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’å‡ºã—ã¦ã—ã¾ã†
	- LightChatAssistantã§ã¯Mistral 7B v0.2 Instructã‹ã‚‰ChatVectorã‚’æŠ½å‡ºã—ã¦ãŸã‘ã©ã€ã‚‚ã£ã¨æ€§èƒ½é«˜ãã†ãªStarling-LM-7B-betaã‹ã‚‰æŠ½å‡ºã—ãŸæ–¹ãŒã„ã‚“ã˜ã‚ƒã­ï¼Ÿã¨ã„ã†äº‹ã§æŠ½å‡ºã—ã¦ChatNTQã«è¶³ã—ã¦ã¿ãŸã‚‰ã€MoEã«ã‚‚ã—ã¦ãªã„å˜ãªã‚‹7Bãƒ¢ãƒ‡ãƒ«ã®æ™‚ç‚¹ã§ElyzaTasks100ãƒ™ãƒ³ãƒã§LightChatAssistantè¶…ãˆã®æ€§èƒ½ãŒå‡ºã¦ã—ã¾ã£ãŸï¼Command R-35Bã¨åŒç‚¹ã®ã‚¹ã‚³ã‚¢ï¼
- Heron-Bench: æ—¥æœ¬èªVisionï¼†Languageãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å…¬é–‹
	- https://arxiv.org/abs/2404.07824
	- https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench
	- æ—¥æœ¬èªã®Vision-Langugeãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒãªã‹ã£ãŸã®ã§ä½œæˆã—ã€Turingã§é–‹ç™ºã—ãŸheronã‚’å«ã‚ã¦ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã‚’è¡Œã„ã¾ã—ãŸ~!!
- CS159: LLMs for reasoning lecture slides from Caltech
	- https://sites.google.com/view/cs-159-2024/lectures
- RTX4090+A6000(24+48GB VRAM)ã§command-r-plus-Q4_K_Mã‚’65/65 layer GPUã«è¼‰ã›ã¦ã‚‚6.5t/sãã‚‰ã„ãŒé™åº¦ã ã£ãŸã€‚ ãŠãã‚‰ãã€96GBã§ã¯ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªã„ã‹ã‚‰é…ã„ã€‚
	- https://x.com/Meteor_Eternal/status/1779807643668013534
- an introduction to agents and tools
	- https://x.com/llama_index/status/1779898403239125198
	- This short course is the perfect beginner sequence for anyone looking to get an overview of agent implementations, how to equip them with tools to perform tasks like advanced QA/RAG or anything else, and also some neat extensions (tool retrieval, step-wise execution).
- From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples
	- https://arxiv.org/pdf/2404.07544.pdf
	- LLMã«ã€ã€Œã“ã®å…¥åŠ›ã®å ´åˆå‡ºåŠ›ã¯ã“ã‚Œã€ã¨ã„ã†ä¾‹ç¤ºã‚’å…¥ã‚Œã¦ã€Œã§ã¯ã“ã®å…¥åŠ›ã®å ´åˆã®å‡ºåŠ›ã¯ï¼Ÿã€ã¨æ¨è«–ã•ã›ã‚‹ã¨ç·šå½¢å›å¸°ãƒ»éç·šå½¢å›å¸°ãŒã§ãã¦ã—ã¾ã†ã€ã¨ã„ã†è«–æ–‡ã€‚
- TFMC/Japanese-Starling-ChatV-7B-GGUF
	- https://note.com/bakushu/n/ne95340f04b41
	- LightChatAssistant-2x7Bã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆæ€§èƒ½ãŒã¨ã¦ã‚‚è‰¯ã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ä½œè€…ã•ã‚“ãŒç”¨ã„ãŸæ‰‹æ³•ï¼ˆChat Vector+MoEãƒãƒ¼ã‚¸ï¼‰ã‚’å¾Œè¿½ã„ã§æ¤œè¨¼ã—ã¦ã„ã‚‹ãªã‹ã§ã€ç™ºè¦‹ã€‚
	- 7Bã‚¯ãƒ©ã‚¹ã¨ã—ã¦ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ãŒã‚„ãŸã‚‰é«˜ã„ãƒ¢ãƒ‡ãƒ«ãŒå‡ºã¦ããŸã®ã§ã€ŒJapanese-Starling-ChatV-7Bã€ã¨ã—ã¦å…¬é–‹ã—ã¦ã¿ã¾ã—ãŸã€‚
- HachiML/Swallow-MS-7b-v0.1-ChatSkill-LAB
	- https://huggingface.co/HachiML/Swallow-MS-7b-v0.1-ChatSkill-LAB
	- ChatVectorã‚’ä½¿ã£ã¦æ–°ã—ã„Apache2.0ã®Chatãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã¾ã—ãŸã€‚ ChatVectoræŠ½å‡ºå…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚‚Mixtral-8x7B-Instructã«ã‚ˆã‚‹äººå·¥ãƒ‡ãƒ¼ã‚¿(Synthetic Data)ã§å­¦ç¿’ã•ã‚ŒãŸã‚‚ã®ãªã®ã§ã€éš ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ±šæŸ“ã®å¿ƒé…ã¯ã‚ã‚Šã¾ã›ã‚“
-  é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚‚ã¡ã„ãŸChatVectoråŠ ç®—ã®æœ€é©åŒ– byã€€ã¯ã¡ã•ã‚“
	- https://note.com/hatti8/n/na593650d688b
	- é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã€optunaã¨cmaes
	- é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã£ã¦ã€ã“ã®é–¢æ•°ã®outputã§ã‚ã‚‹scoreã‚’æœ€é©åŒ–ï¼ˆæœ€å°åŒ–ï¼‰ã—ã¾ã™
		1. merging_ratioï¼ˆChatVectorã®åŠ ç®—æ¯”ç‡ã‚’å„layeræ¯ã«æŒã¤è¾æ›¸ï¼‰ã®å®šç¾©
		2. merging_ratioã«ã—ãŸãŒã£ã¦ã€ChatVectorã®ãƒãƒ¼ã‚¸
		3. ELYZA tasks 10ã®å®Ÿæ–½ã¨GPT4ã«ã‚ˆã‚‹è©•ä¾¡
- Foundational Challenges in Assuring Alignment and Safety of Large Language Models
	- https://llm-safety-challenges.github.io/
	- 2024.4.15 Cambridgeå¤§å­¦ã®U. Anwar, D. Kruegeræ°ã‚‰40åå¼±ã®å›½éš›ãƒãƒ¼ãƒ ã«ã‚ˆã‚‹ã€LLMã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨å®‰å…¨æ€§ã®æœªè§£æ±ºå•é¡Œã«é–¢ã™ã‚‹175ãƒšãƒ¼ã‚¸ã®ç·èª¬è«–æ–‡ã€‚
	- 1ï¼‰LLMã®ç§‘å­¦çš„ç†è§£ã€
	- 2ï¼‰è¨“ç·´æ‰‹æ³•ã‚„å®Ÿè£…å ´é¢ã®èª²é¡Œã€
	- 3ï¼‰ç¤¾ä¼šã«ãŠã‘ã‚‹èª²é¡Œã«åˆ†ã‘ã€
	- åºƒç¯„ãªæ–‡çŒ®èª¿æŸ»ã«åŸºã¥ã200è¶…ã®ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã‚’åŒå®šã€‚
	- ã“ã®Anwar+2024è«–æ–‡ã¯ã™ã”ã„ã€‚ã€ŒLLMã®ä½•ãŒæŠ€è¡“çš„ãƒ»ç¤¾ä¼šçš„ãªå•é¡Œã«ãªã‚‹ã®ã‹ï¼Ÿã€ã‚’åŒ…æ‹¬çš„ã«æ´—ã„å‡ºã—ã€ã‹ã¤ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆã«è½ã¨ã—è¾¼ã‚“ã§ã„ã‚‹ã€‚by maruyamaï½“ã‚ï½
- Running WizardLM-2 8x22B Q4_0 locally via ollama
	- https://x.com/ivanfioravanti/status/1780133719707197643
	- On an M2 Ultra I get: ~19.5 tokens/s
	- 80Gb??
- MaziyarPanahi/WizardLM-2-8x22B-GGUF(Q4_K_M)
	- https://x.com/alfredplpl/status/1780110628864274576
	- ã†ãƒ¼ã‚“æ—¥æœ¬èªãŒã‚„ã¯ã‚Šã‚¤ãƒã‚¤ãƒã ãª
- WizardLMã®ä½œã‚Šæ–¹
	- https://x.com/WizardLM_AI/status/1779937307690471834
	- æ–°ã—ã„WizardLM-2 7Bã®ã‚µã‚¤ã‚ºã§MT-BenchãŒClaude-2ã‚ˆã‚Šé«˜ã„ã£ã¦ã™ã”ã„ by ã¯ã¡
- WizardLM: Empowering Large Language Models to Follow Complex Instructions
	- https://arxiv.org/pdf/2304.12244.pdf
	- èª²é¡Œï¼šæ§˜ã€…ãªãƒ¬ãƒ™ãƒ«ã®è¤‡é›‘ã•ã‚’æŒã¤å¤§é‡ã®æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ã“ã¨ã¯ã€æ™‚é–“ã¨åŠ´åŠ›ãŒã‹ã‹ã‚‹ã€‚
	- è§£æ±ºï¼šEvolve Instructæ–¹æ³•ã‚’ä½¿ç”¨ã—ã¦ã€LLMè‡ªä½“ãŒæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- Introducing the Batch API: save costs and get higher rate limits on async tasks
	- https://platform.openai.com/docs/api-reference/batch
- Introducing Idefics 2
	- https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe
	- An 8B Vision-Language Model - literally punching above its weight.
- Pytorchã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®æ©Ÿèƒ½torchtuneãŒå…¬é–‹
	- https://pytorch.org/blog/torchtune-fine-tune-llms/?utm_content=289842551&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024
	- llama3ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã“ã‚Œã§ã‚„ã£ãŸã‚“ã ã¨
-  AIç§‘å­¦ã®ä½•ãŒâ€œå“²å­¦â€ã®å•é¡Œã«ãªã‚‹ã®ã‹ã€€ï½å•ã„ãƒãƒƒãƒ”ãƒ³ã‚°ã®è©¦ã¿ï½
	- https://speakerdeck.com/rmaruy/aike-xue-nohe-ga-zhe-xue-nowen-ti-ninarunoka-wen-imatupingunoshi-mi
	- ã¾ã‚‹ã‚„ã¾ã•ã‚“
-  ã€ŒAIå”åƒæ™‚ä»£ã«ç ”ç©¶è€…ã¯ã©ã†ç”Ÿãã‚‹ã‹ã€(4/26)
	- https://share.hsforms.com/1NFOzzuNBSZq3K20gtPZ30wdxf90
	- æœ¬ã‚¤ãƒ™ãƒ³ãƒˆã§ã¯ã€ç ”ç©¶ã®è‡ªå‹•åŒ–ãƒ»è‡ªå¾‹åŒ–ãŒç›Šã€…åŠ é€Ÿã—ã¦ã„ãæœªæ¥ã«ãŠã„ã¦ã€ç ”ç©¶ã¨ã„ã†å–¶ã¿ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã—ã¦ã„ãã®ã‹ã€ãã®ä¸­ã§ç ”ç©¶è€…ã¯ã©ã†ç”Ÿãã‚‹ã¹ãã‹ã€ã¨ã„ã†ã“ã¨ã«ã¤ã„ã¦è­°è«–ã—ã¾ã™ã€‚  
	- AIãƒ­ãƒœãƒƒãƒˆé§†å‹•ç§‘å­¦ã‚’ç‰½å¼•ã™ã‚‹ä¸€æ‰å¤ªéƒã•ã‚“ã¨ã€AIç§‘å­¦ã‚’ä¿¯ç°çš„ã«è€ƒãˆã‚‹ä¸¸å±±éš†ä¸€ã•ã‚“ã«ã‚ˆã‚‹ç‰¹åˆ¥ãƒ‘ãƒãƒ«ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã‚„ã€ã€ŒAI Ã— â—¯â—¯å­¦ã€ã‚’ãƒ†ãƒ¼ãƒã«æœˆé¡æ”¯æ´å‹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ•ã‚¡ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«æŒ‘æˆ¦ä¸­ã®è‹¥æ‰‹ç ”ç©¶è€…8åã®ãƒ—ãƒ¬ã‚¼ãƒ³ã‚’é€šã—ã¦ã€ã€ŒAIå”åƒæ™‚ä»£ã«ç ”ç©¶è€…ã¯ã©ã†ç”Ÿãã‚‹ã‹ã€ã€çš†ã•ã‚“ã‚‚ä¸€ç·’ã«è€ƒãˆã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ
- ç”ŸæˆAIã§GPUãŒã„ã‚‰ãªããªã‚‹ï¼Ÿã€€æ¥­ç•Œã‚’æºã‚‹ãŒã™ã€Œ1ãƒ“ãƒƒãƒˆLLMã€ã¨ã¯ä½•ã‹ã€è­˜è€…ã«èã„ãŸ
	- https://www.itmedia.co.jp/aiplus/articles/2404/16/news064.html
	- ã§ã¯ãã‚‚ãã‚‚â€œ1bitâ€ã¨ã¯ä½•ãŒ1bitãªã®ã‹ã€ã©ã†ã—ã¦1bitã«ãªã‚‹ã¨GPUãŒä¸è¦ã«ãªã‚‹ã®ã‹ã€‚LLMã§GPUãŒä¸è¦ã«ãªã‚‹ã¨ã©ã‚“ãªä¸–ç•ŒãŒè¨ªã‚Œã‚‹ã®ã‹ã€‚ã‚ªãƒ¼ãƒ€ãƒ¼ãƒ¡ã‚¤ãƒ‰ã«ã‚ˆã‚‹AIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã€Œã‚«ã‚¹ã‚¿ãƒ AIã€ã®é–‹ç™ºãƒ»æä¾›ã‚’è¡Œã†Laboro.AIã®æ¤æ©‹å¾¹å¤«CEOã«èã„ãŸã€‚
	- **æ¤æ©‹ï¼š**ä»Šå›ã®çµæœã‹ã‚‰ã€LLMã®æ¨è«–ã«ãŠã„ã¦ã€GPUã§ã¯ãªãåˆ¥ã®åŠå°ä½“ã®æ©Ÿæ§‹ãŒæœ€é©ã«ãªã£ã¦ã€åŠ‡çš„ã«è¨ˆç®—ãŒè»½ãæ—©ããªã‚‹å¯èƒ½æ€§ãŒé–‹ã‘ã¦ãã‚‹ã‚“ã§ã™ã€‚
	- è«–æ–‡ä¸­ã§ã‚‚ã€Groqã¨ã„ã†LLMã®æ¨è«–ã«ç‰¹åŒ–ã—ãŸLPUï¼ˆLanguage Processing Unitï¼‰ã®ç™»å ´ã«è§¦ã‚Œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚æ¬¡ä¸–ä»£åŠå°ä½“ã§ã®å¾©æ´»ã‚’ç‹™ã†æ—¥æœ¬ã®ç”£æ¥­ã«ã¨ã£ã¦ã‚‚ã€æ³¨è¦–ã—ã¦ã„ãã¹ããƒˆãƒ”ãƒƒã‚¯ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™
- ã“ã®å‰èª¿åˆã—ãŸæ”¹é€ Swallow-MXï¼ˆç¶™ç¶šæ—¥æœ¬èªå­¦ç¿’+instructionãƒ™ãƒ«ãƒˆãƒ«å¼·åŒ–ï¼‰ã¨Mixtral 8x22Bã‚’æ¯”è¼ƒã™ã‚‹ã¨çŸ­æ™‚é–“ä½¿ç”¨ã§ã¯å·®ç•°æ‰ãˆã«ãã„ãª ãã‚Œã ã‘8x22Bã®æ—¥æœ¬èªèƒ½åŠ›ã‚¢ãƒƒãƒ—ã—ã¦ã‚‹ã®ã¯é–“é•ã„ãªã„
	- https://x.com/AiXsatoshi/status/1778630270486552619
- Stanfordäººé–“ä¸­å¿ƒAIç ”ç©¶æ‰€ï¼ˆHAIï¼‰ã‹ã‚‰æ’ä¾‹ã®ã€ŒAI Index Report 2024ã€ã‚’ç™ºè¡Œ
	- https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf
	- 2024.4.16 Stanfordäººé–“ä¸­å¿ƒAIç ”ç©¶æ‰€ï¼ˆHAIï¼‰ã‹ã‚‰æ’ä¾‹ã®ã€ŒAI Index Report 2024ã€ã‚’ç™ºè¡Œã€‚æ˜¨å¹´ã‹ã‚‰å¤§å¹…ã«å¢—é‡ã—ãŸ500ãƒšãƒ¼ã‚¸è¶…ã®ç´™å¹…ã«ã¦ã€AIç ”ç©¶ã®è«–æ–‡æ•°ãƒ»ç‰¹è¨±ãƒ»å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºå‹•å‘ãƒ»æŠ•è³‡é¡ãƒ»çµŒæ¸ˆçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ»ç§‘å­¦ã‚„æ•™è‚²ã¸ã®å½±éŸ¿ãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ»ç¤¾ä¼šå—å®¹ãªã©åŒ…æ‹¬çš„ã«å ±å‘Šã€‚
	- ã€ŒAIã¯äººé–“ã‚ˆã‚Šé«˜æ€§èƒ½ã ãŒä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆã§ã¯äººé–“ã®æ–¹ãŒå„ªç§€ã€ã€Œé«˜æ€§èƒ½AIã®å­¦ç¿’ã‚³ã‚¹ãƒˆã¯æ•°ç™¾å„„å††ã€ãªã©
- HuggingFaceM4/idefics-8b
	- https://huggingface.co/spaces/HuggingFaceM4/idefics-8b
	- æ˜ç¢ºã«å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢
-  Google Colab ã§ idefics2 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n032c2bbaadb4?sub_rt=share_h
	- ã€ŒIdefics2ã€ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å…¥åŠ›ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ç”»åƒã®è³ªå•å¿œç­”ã€è¦–è¦šçš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª¬æ˜ã€è¤‡æ•°ç”»åƒã‚’ã‚‚ã¨ã«ç‰©èªä½œæˆã€æ–‡æ›¸ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºãªã©ã‚’å®Ÿè¡Œã§ãã¾ã™
- AIã®å‡¦ç†èƒ½åŠ›ï½¤1å¹´ã§25å€ã€€æ­»è”µã®ï½¢çŸ¥èƒ½è³‡æœ¬ï½£ãŒç«¶äº‰åŠ›ã« by shi3zã•ã‚“
	- https://www.nikkei.com/prime/digital-governance/article/DGXZQOUC092UR0Z00C24A4000000
	- ãã®ã‚ˆã†ãªä¸–ç•Œã§ä¾¡å€¤ã‚’é«˜ã‚ã‚‹ã®ã¯ã€æ­»è”µã•ã‚ŒãŸæ›¸ç±ã‚„å‹•ç”»ãªã©ã®ã€ŒçŸ¥èƒ½è³‡æœ¬ã€ã¨ã„ã†ã€‚ã€ŒAIè³‡æœ¬ä¸»ç¾©ã€ã¨ã„ã†æ–°ãŸãªçµŒæ¸ˆã®å§¿ã‚’æå”±ã™ã‚‹ã€
-  MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents
	- https://arxiv.org/abs/2404.10774
	- RAGãªã©ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã«åŸºã¥ã„ã¦ LLM ã«ç”Ÿæˆã•ã›ã‚‹å ´åˆã«ãã‚‚ãã‚‚ç”Ÿæˆã—ãŸã‚‚ã®ãŒã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã«åŸºã¥ã„ã¦ç”Ÿæˆã§ãã¦ã„ã‚‹ã®ã‹ï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ï¼‰ãŒèª²é¡Œã«ãªã‚Šã¾ã™ãŒã€ãã‚Œã‚’åŠ¹ç‡çš„ã«è¡Œã†ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ  MiniCheck ã®ææ¡ˆã€‚
	- GPT-3.5/4ã‚’ç”¨ã„ã¦ã€äººãŒæ›¸ã„ãŸæ–‡ç« ã‚’ã‚‚ã¨ã«FACTã‚’æŠ½å‡ºã—ãŸã‚Šè¦ç´„ç”Ÿæˆã‚’ã—ãŸã‚Šã—ãªãŒã‚‰ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸé«˜å“è³ªãªåˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ç”¨ã„ã¦å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€GPT-4ã¨åŒç­‰ã®æ€§èƒ½ã§400åˆ†ã®1ä»¥ä¸‹ã®ã‚³ã‚¹ãƒˆã§ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸãã†ã§ã™ã€‚
- RAGã‚’è¤‡é›‘ãªè³ªå•ã«å¼·ãã™ã‚‹æ‰‹æ³•ã€ŒCoAã€ã«ã¤ã„ã¦
	- https://zenn.dev/knowledgesense/articles/508187f1c616e3
	- ã€ŒChain-of-Abstraction (CoA) Reasoningã€
	- CoAãŒå¾“æ¥ã®RAGã‚ˆã‚Šã‚‚åŠ›ã‚’ç™ºæ®ã§ãã‚‹ã‚·ãƒ¼ãƒ³ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒã€Œè¤‡æ•°ã®çŸ¥è­˜ã‚’çµ„ã¿åˆã‚ã›ãªã‘ã‚Œã°æ­£ç­”ã§ããªã„ã€ã‚ˆã†ãªè³ªå•ã ã£ãŸå ´åˆã§ã™ã€‚é€šå¸¸ã®RAGã§ã¯1å›ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã§å›ç­”ã«ä½¿ãˆã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚ˆã†ã¨ã—ã¾ã™ãŒã€CoAã§ã¯ã€å•é¡Œï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ï¼‰ã‚’è¤‡æ•°ã®å•é¡Œã«åˆ†è§£ã—ã€è¤‡æ•°å›ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚’è¡Œã£ãŸä¸Šã§ç·åˆçš„ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã™
- Qwen/Qwen1.5-7B-Chat-GGUF
	- https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF
	- 7 billion parameters coding chat model (~5GB RAM needed)
-  1BitLLMã®å®ŸåŠ›ã‚’è¦‹ã‚‹ by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/ndd1f27fff31c?sub_rt=share_pb
	- æ™®é€šã®HuggingFaceã®ãŠä½œæ³•ã¨ã¯ã‹ãªã‚Šé•ã†ã®ã§æ³¨æ„ãŒå¿…è¦ã€‚  ã¾ãšã€ã“ã®HuggingFaceãƒªãƒã‚¸ãƒˆãƒªã‚’ä¸¸ã”ã¨git cloneã™ã‚‹
	- ã“ã‚Œã‚’ã‚„ã‚‰ãšã«ã„ã¤ã‚‚ã®å‡¡ä¾‹ã¿ãŸã„ã«ã„ããªã‚Špipelineã«èª­ã¿è¾¼ã‚‚ã†ã¨ã™ã‚‹ã¨è¬ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦æ‚©ã¾ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã€‚æµ·å¤–ã§ã‚‚æ‚©ã‚“ã§ã‚‹äººãŒä½•äººã‚‚ã„ã‚‹ã¿ãŸã„ã ã€‚ã¾ã‚å€‹äººçš„ã«ã¯ã€Œã“ã‚“ãªèª¬æ˜ã§èª°ãŒã‚ã‹ã‚‹?ã€ã¨æ€ã†ãŒã€‚
- mistralai/Mixtral-8x22B-Instruct-v0.1
	- https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1
	- Mixtral-8x22B Instract ããŸã‚ã€œ
- Build RAG, Function Calling, and Agents with llama_index and  MistralAI8x22b 
	- https://docs.llamaindex.ai/en/latest/examples/cookbooks/mistralai/
- 24/04/18 ãƒ­ãƒ¼ã‚«ãƒ«ã§Command Rã‚„Command R+ã‚’å‹•ã‹ã™æ™‚ã®ä½œæ³•
	- https://six-loganberry-ba7.notion.site/24-04-18-Command-R-Command-R-ff8455f1dba543168d5a7768705e0043
	- å®Ÿã¯Command Rã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ãªã„ã¨æ­£ã—ãå›ç­”ãŒè¿”ã£ã¦ã“ãªã„ã‚‰ã—ã„
	- <|START_OF_TURN_TOKEN|><|USER_TOKEN|>Who are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
	- ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚ã‚‹ãªã—ã§å›ç­”ã®ã‚¯ã‚ªãƒªãƒ†ã‚£ã¯å¤©ã¨åœ°ã»ã©é•ã£ã¦ãã‚‹ã‹ã‚‰æ³¨æ„ã—ã‚ˆã†ã€‚
- Introducing Meta Llama 3:
	- https://ai.meta.com/blog/meta-llama-3/?utm_source=twitter&utm_medium=organic_social&utm_content=video&utm_campaign=llama3
	- the most capable openly available LLM to date.
	- Llama3ã®ãƒªãƒªãƒ¼ã‚¹ç¬¬ä¸€å¼¾ã¯8Bãƒ¢ãƒ‡ãƒ«ã¨70Bãƒ¢ãƒ‡ãƒ«ï¼ãã‚Œãã‚Œãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç‰ˆãŒã‚ã‚Šï¼HuggingFaceã‹ã‚‰ï¼¤ï¼¬ã§ãã‚‹ï¼8Bãƒ¢ãƒ‡ãƒ«ã¯ãƒ™ãƒ³ãƒã§Mistral-7Bã‚„Gemma-7Bã‚’æ’ƒå¢œï¼70Bãƒ¢ãƒ‡ãƒ«ã¯GeminiPro1.5ã‚„Claude3Sonnetã‚’æ’ƒå¢œï¼äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã§ã‚‚Sonnetã€MistralMediumã€GPT-3.5ã«å‹åˆ©ï¼
	- Contexté•·ã¯8kTokenã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯80å„„ã¨700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ãªã‚“ã¨4000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¶…ãˆã‚‹ãƒ¢ãƒ‡ãƒ«ã‚‚å­¦ç¿’ä¸­ï¼700å„„ã®ã»ã†ã¯ç¾åœ¨ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢Modelã«æ€§èƒ½çš„ã«è‚‰è–„ã—ã¤ã¤ã‚ã‚‹çŠ¶æ…‹ã€‚
- LangChain x Mistral RAG Agent Cookbooks + Video
	- https://x.com/LangChainAI/status/1780994907903263159
	- With the release of new Mixtral 8x22B, there's high interest in building agents with open source LLMs.
- VARIATIONAL BAYESIAN LAST LAYERS
	- https://arxiv.org/pdf/2404.11599.pdf
	- Neural Networksã®æœ€çµ‚å±¤ä»¥å¤–ã¯å›ºå®šã•ã‚Œã¦ã„ã‚‹ã¨æ€ã£ã¦ã€æœ€çµ‚å±¤ã®ã¿ã® 1-layer ãª Bayesian Neural Network ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã—æœ€çµ‚å±¤ã®æœ€é©åŒ–ã‚’ã—ã¤ã¤å¤‰åˆ†ãƒ™ã‚¤ã‚ºæ¨å®šã™ã‚‹æ çµ„ã¿ Variational Bayesian Last Layers ï¼ˆVBLLï¼‰ã®ææ¡ˆã€‚
- Reliable, fully local RAG agents with Llama3
	- Here, we show to how build reliable local agents using LangGraph and Llama3-8b from scratch.
	- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb
- ä¾‹ã®SRAMãƒ¡ãƒ¢ãƒªã®AIãƒãƒƒãƒ—ã‚’å±±ç››ã‚Šã«ç©ã¿ã¾ãã£ãŸæ§‹æˆã®Groqã®ã‚µã‚¤ãƒˆã§Llama3-70BãŒ300t/sã®è¶…çµ¶çˆ†é€Ÿæ¨è«–
	- https://x.com/umiyuki_ai/status/1781529537102352827
-  Many-Shot In-Context Learning
	- https://arxiv.org/abs/2404.11018
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ•°ç™¾ã€œæ•°åƒã®ä¾‹ã‚’å«ã‚ã¦LLMã«ã‚¿ã‚¹ã‚¯ã‚’è¡Œã‚ã›ã‚‹ã€Many-shotï¼ˆå¤šã‚·ãƒ§ãƒƒãƒˆï¼‰ã€ãŒDeepMindã«ã‚ˆã‚Šæ¤œè¨¼ã•ã‚Œã¦ã„ã¾ã™
	- çµæœã€åŸºæœ¬çš„ã«ä¾‹ãŒå¤šããªã‚‹ã»ã©æ€§èƒ½ãŒä¸ŠãŒã‚‹ã¨ã®ã“ã¨ã€‚äº‹å‰å­¦ç¿’ã«ã‚ˆã‚‹æ€ã„è¾¼ã¿ã‚’è¦†ã™ã“ã¨ã‚‚ã€‚äººé–“è£½ã®ä¾‹ãŒãªã‘ã‚Œã°ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã®ä¾‹ã§ã‚‚åŠ¹æœã‚ã‚Š
-  [llama.cppï¼šiMatrixé‡å­åŒ–ã¯æ—¥æœ¬èªæ€§èƒ½ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿ](https://sc-bakushu.hatenablog.com/entry/2024/04/20/050213)
	- é‡å­åŒ–æ™‚ã®ãƒ¢ãƒ‡ãƒ«åŠ£åŒ–ã‚’æŠ‘åˆ¶ã™ã‚‹é‡è¦åº¦è¡Œåˆ—ï¼ˆiMatrix; Importance Matrixï¼‰è¨ˆç®—ã®è©±é¡Œã§ã™ã€‚
	- æœ€è¿‘ã¯HuggingFaceã«ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹GGUFã‚‚å¤šããŒiMatrixç‰ˆã¨ãªã£ã¦ã„ã¾ã™ãŒã“ã‚Œã‚‰ã®é‡å­åŒ–ã§ã‚ˆãä½¿ã‚ã‚Œã¦ã„ã‚‹iMatrixè¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®2ç¨®é¡ã®ã‚ˆã†ã§ã™ã€‚
- MLX ã§ Llama 3 ã‚’è©¦ã™
	- https://note.com/npaka/n/n21fa74396545?sub_rt=share_h
- Llama 3ãŒGroqã«ç™»å ´
	- https://x.com/kyo_takano/status/1781595042840559908
	- GroqãŒãªãœã“ã‚“ãªã«é€Ÿã„ã®ã‹ï¼Ÿãã‚Œã¯GPUã§ã¯ãªãLLMã«æœ€é©åŒ–ã•ã‚ŒãŸASICã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚Groqã¯æœ€è¿‘æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã‚’ä½œã£ãŸã°ã‹ã‚Šã§ã€ãã“ã§ä½•å€‹ASICã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã‹ç›´æ¥èã„ãŸã‚‰700å€‹ä»¥ä¸Šã¨ç­”ãˆã¦ãã‚Œã¾ã—ãŸã€ä»Šå¾Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§éœ€è¦ãŒé«˜ã¾ã‚‹ã¨Groqã¯ã•ã‚‰ã«æ€¥æˆé•·ã—ã¦ãã‚‹ã¨æ€ã„ã¾ã™ã€‚
- å°ã•ã„è¨ˆç®—ã‚³ã‚¹ãƒˆã§ã‚¹ãƒãƒ¼ãƒˆã«LLMã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°!-Hugging Face PEFTå…¥é–€(å‰ç·¨)
	- https://zenn.dev/elith/articles/3ec1d319c8a40f
	- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®è‰¯ã„Fine Tuningæ‰‹æ³•(Parameter-Efficient Fine Tuningã€ PEFT)ã«ã¤ã„ã¦ã€ã‚µãƒ¼ãƒ™ã‚¤ã‚’è¡Œã„ã¾ã—ãŸã€‚
- With the latest MLX, 4-bit Llama 3 8B runs nicely on an 8GB M2 mini.
	- https://x.com/awnihannun/status/1781345824611680596
	- 512 tokens at 18.8 toks-per-sec
- cl-nagoya/auto-wiki-qa
	- https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa
	- æ±å·¥å¤§ã®Swallow-MXã‚’ç”¨ã„ã¦Wikipediaã®ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãè³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã•ã›ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ AutoWikiQA ã‚’HuggingFaceä¸Šã«å…¬é–‹ã—ã¾ã—ãŸï¼
	- ç´„240ä¸‡äº‹ä¾‹ã¨æ—¥æœ¬èªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸­ã§ã‚‚æœ€å¤§è¦æ¨¡ã‹ã¤é«˜å¤šæ§˜æ€§ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™
- The Oxford Handbook of AI Governance
	- https://global.oup.com/academic/product/the-oxford-handbook-of-ai-governance-9780197579329?cc=jp&lang=en&
	- AIã‚¬ãƒãƒŠãƒ³ã‚¹ã®ã‚ªãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ¼ãƒ‰ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã€‚49æœ¬é€šèª­ã™ã‚‹äººå‡ºã‚‹ã€€by ç”Ÿè²å…ˆç”Ÿ
- lama-3-8b's in-context learning is unbelievable.
	- https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/
- ã„ã¡ã°ã‚“ã‚„ã•ã—ã„ãƒ­ãƒ¼ã‚«ãƒ« LLM
	- https://note.com/schroneko/n/n8b1a5bbc740b#57403f33-7b40-444e-9342-c8bf11458d18
	- ãƒ­ãƒ¼ã‚«ãƒ« LLM åˆã‚ã¾ã—ã¦ã®æ–¹ã§ã‚‚å‹•ã‹ã›ã‚‹ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
	- Ollama ã‚’ä½¿ãˆã°ç°¡å˜ã« LLM ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ã‹ã›ã‚‹
- Crystalcareai/llama-3-4x8b
	- https://huggingface.co/Crystalcareai/llama-3-4x8b
	- This is an MOE of Llama-3-8b with 4 experts. This does not use semantic routing, as this utilizes the deepseek-moe architecture. There is no routing, and there is no gate - all experts are active on every token.
	- äºˆæƒ³ã¯ã—ã¦ãŸã‘ã©ã€ã‚‚ã†Llama3ã®MoEãŒã§ãã¦ã‚‹ã€€by ã¯ã¡ ã•ã‚“
-  Google Colab ã§ Llama 3 ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã™
	- https://note.com/npaka/n/n315c0bdbbf00?sub_rt=share_h
	- ä»Šå›ã¯ã€ã”ã–ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ã¾ã™ã€‚AIãŒã€Œæˆ‘ã€ã‚Šã‚“ãˆã‚‚ã‚“ã¯æ€ã†ã€‚â—¯â—¯ã§ã”ã–ã‚‹ã€‚çŸ¥ã‚‰ã‚“ã‘ã©ã€‚ã€çš„ãªå£èª¿ã«ãªã‚Šã¾ã™ã€‚
	- ç·´ç¿’ã¨ã—ã¦500ã‚¹ãƒ†ãƒƒãƒ—ã ã‘å­¦ç¿’ã—ã¾ã™ã€‚æŒ‡ç¤ºã«å¿œã˜ã¦ã€wandbã®APIã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚8åˆ†ã»ã©ã§å­¦ç¿’å®Œäº†ã—ã¾ã™ã€‚
	- æˆ‘ã€ã‚Šã‚“ãˆã‚‚ã‚“ã¯æ€ã†ã€‚ ãƒãƒŸã¯ä¸€ç•ªã‹ã‚ã„ã„ã€‚çŸ¥ã‚‰ã‚“ã‘ã©ã€‚
	-  HuggingFace Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
		- (1) LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸
		- (2) ã€ŒHuggingFace Hubã€ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€ŒNew Modelã€ã‚’é¸æŠã€‚
		- (3) HuggingFace Hubã®ãƒªãƒã‚¸ãƒˆãƒªã®ä½œæˆã€‚
		- (4) HuggingFace Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
-  çµ±è¨ˆå­¦ã‚’å“²å­¦ã™ã‚‹
	- https://www.unp.or.jp/ISBN/ISBN978-4-8158-1003-0.html
	- æ›¸è©•ï¼ˆä¸¸å±±éš†ä¸€ï¼‰
		- â€œâ€¦â€¦ ç§‘å­¦ã®æœ€ã‚‚åŸºæœ¬çš„ãªãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹çµ±è¨ˆå­¦ã‚’å“²å­¦çš„ã«åˆ†æã™ã‚‹ã€‚ãƒ™ã‚¤ã‚ºçµ±è¨ˆã€ä»®èª¬æ¤œå®šã€æ©Ÿæ¢°å­¦ç¿’ã€å› æœæ¨è«–ãªã©ã®çµ±è¨ˆå­¦çš„æ‰‹æ³•ã‚’ç§‘å­¦è€…ãŒä½¿ã†ã¨ãã€ä½•ãŒæš—é»™ã®å‰æã¨ãªã‚Šã€ä½•ãŒæ­£å½“åŒ–ã®æ ¹æ‹ ã«ãªã£ã¦ã„ã‚‹ã®ã‹ã€‚å“²å­¦çš„èªè­˜è«–ã®é“å…·ç«‹ã¦ã«ã‚ˆã‚‹æœ¬æ›¸ã®æ•´ç†ã¯é®®ã‚„ã‹ã ã€‚æ·±å±¤å­¦ç¿’ã«é–¢ã™ã‚‹è­°è«–ã¯ã€ã©ã®ã‚ˆã†ãªæ„å‘³ã§ AI ã«ç§‘å­¦ãŒã§ãã‚‹ã®ã‹ã¨ã„ã†å¤§å•é¡Œã«ã‚‚ã¤ãªãŒã‚‹ã€‚ã€ŒAI ç§‘å­¦ã®å“²å­¦ã€ã®å§‹å‹•ã‚’æ„Ÿã˜ã‚‹ã€‚â€¦â€¦â€


## 4/15

ä»Šé€±ã‚‚å¼·çƒˆã ã£ãŸã€‚é ­ãŒãã‚‰ãã‚‰ã™ã‚‹ãŒã€æ°—ã®ã›ã„ã‹é‡ã¿è»¢ç§»ç³»ãŒå¤šã„æ°—ãŒã™ã‚‹ã€‚MiniCPM-2Bã€ã€ŒÎ¼ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚¡ãƒ¼ã€ã¨ã„ã†æ‰‹æ³•ã§å°è¦æ¨¡LLMã§æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤§è¦æ¨¡LLMã«è»¢ç§»ã™ã‚‹æŠ€è¡“ã§ï¼ˆï¼’æ®µéšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‘¼ã°ã‚Œã¦ã‚‹ï¼Ÿï¼‰ã€2.4Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã„ã†å°ã•ãªã‚µã‚¤ã‚ºã§Mistral-7Bã¨è‚©ã‚’ä¸¦ã¹ã‚‹ã¨ã‹ã€‚Command R+ã‚‚é‡å­åŒ–ã•ã‚ŒãŸã‚‚ã®ãŒè©•ä¾¡ã•ã‚Œã¦ã€Mac(M3ã€128G)ã‚„ã€A100(80G)ã§çµæ§‹ã‚µã‚¯ã‚µã‚¯ã†ã”ãã‚‰ã—ã„ã€‚ç‰¹ã«ã€ ã€ŒCommand R+ GPTQã‚’ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã—ã¦vllmã§OpenAI APIäº’æ›ã‚µãƒ¼ãƒå‹•ä½œã€ã£ã¦ã®ã¯ã€A100æŒã£ã¦ã„ã‚‹äººã¯ãœã²è©¦ã—ã¦ã¿ã‚‹ã¹ãã€‚ Command R+ã«å½±éŸ¿ã•ã‚ŒãŸã®ã‹ã€Mistralã‚‚Mixtral-8x22Bã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ç™ºè¡¨ã€ã•ã£ããã“ã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã«expertsã‚’ãƒãƒ¼ã‚¸ã—ã¦mistralã«ã—ãŸå‹æ‰‹ç‰ˆMistral-22BãŒå‡ºã¦ã€åŒæ–¹é‡å­åŒ–ç‰ˆãŒå‡ºã¦ã€ã€ã€ã¨ã‚ã£ã¨ã„ã†é–“ã«åºƒã¾ã£ã¦ä½•ãŒä½•ã ã‹ã€‚LLMåŒå£«ã®æ©Ÿèƒ½ã®ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã§ã‚ã‚‹Chat Vectorã€ã¾ã­ã—ã¦Mathå¼·åŒ–ç‰ˆã‚’ã¤ãã£ã¦ã€ã“ã‚Œã‚‰ã‚’èåˆã—ãŸçµæœã€æ•°å­¦èƒ½åŠ›ã‚’ã‚ã‚‹ç¨‹åº¦ç¶­æŒã—ã¤ã¤ã€Chatèƒ½åŠ›ã‚‚å¼·åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†è©±ã‚‚ã‚ã£ãŸã€‚LightChatAssistant 2x7Bã¦ã®ã‚‚Mistral7Bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸæ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ« 2ã‚’ChatVectoræ‰‹æ³•ã§å¯¾è©±èƒ½åŠ›å¼·åŒ–ã—ã¦mergekitã§MoEåŒ–ã—ãŸã‚‚ã®ã€‚32kã®ContextSizeå¯¾å¿œã€iQ3_XXSé‡å­åŒ–ã§VRAM12GBã§ãƒ•ãƒ«ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã€RTX3060ã§ã‚‚å‹•ãã¨ã‹ã€‚JetMoEã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€MoEã§ã‚ã‚‹ã“ã¨ã«åŠ ãˆã€MiniCPMã«å€£ã£ãŸ2æ®µéšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹ç‡ãŒæ¥µã‚ã¦é«˜ããã‚Œã§ã„ã¦æ€§èƒ½ã¯Llama-7Bä¸¦ã¿ã¨ã‹ã€‚Googleã®æ–°ã—ã„ãƒªã‚«ãƒ¬ãƒ³ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£RecurrentGemmaã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ´»ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Šã•ã¦ã„ã‚‹ã‚‰ã—ã„ã€ä»Šå¾Œã‚‚Gemmaã¨ãƒ‘ãƒ©ãƒ¬ãƒ«ã«ãƒªãƒªãƒ¼ã‚¹ã™ã‚‹ã®ã‹ã€‚GPT-4è¶…ãˆç²¾åº¦ã§ã‚¹ãƒãƒ›ä¸Šå®Ÿè¡Œã§ãã‚‹ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹ç”ŸæˆAIã€ŒOctopus v2ã€ã¦ã®ã‚‚ã‚ã£ãŸã€Gemma-2Bã«è¿½åŠ å­¦ç¿’ã—ã¦ã€ŒFunction Callingã€ã‚’å¼·åŒ–ã—ãŸã¨ã®ã“ã¨ã€‚ãã‚Œã‹ã‚‰ã€ä»Šé€±ã¯Google Cloud Next24ãŒã‚ã£ãŸã®ã§ã€æœ€å¤§100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã®Gemini 1.5 Proã®ãƒªãƒªãƒ¼ã‚¹ã‚„ã€DeepMindã®Imagen 2ã€TPU v5pã®ç™ºè¡¨ã€GoogleDocã«Geminiã®çµ±åˆã¨ã‹ã€geminiã§RCã‚«ãƒ¼ã‚’åˆ¶å¾¡ã¨ã‹é¢ç™½ã„å‡ºã—ç‰©ãŒã‚ã£ãŸã€‚æ—¥æœ¬èªLLM 9ç¨®ã‚’é‡å­åŒ–ã—ã¦å›ç­”å†…å®¹ã‚’æ¯”è¼ƒã£ã¦ã®ã‚‚é¢ç™½ã‹ã£ãŸã€ELYZAã¯å‰ã„ãã€‚LangChain ã® Tool Calling æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ ã£ã¦LLMã«ä¾å­˜ã—ãªã„ã¨ã„ã†ã“ã¨ãªã®ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã‹ã®æ´»ç”¨ãŒåŠ é€Ÿã—ãã†ã€‚QRåˆ†è§£ã§ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã£ã¦ã®ã¯ç›®ã‹ã‚‰ã†ã‚ã“ã ã€ä¸€è¦‹ç•°ãªã‚‹æãŒã‚¨ãƒ¬ã‚¬ãƒ³ãƒˆã«ã¤ãªãŒã‚‹ã€ã“ã‚Œãã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®é†é†å‘³ã ã€‚

- MiniCPM: Unveiling the Potential of End-side Large Language Models
	- https://shengdinghu.notion.site/MiniCPM-Unveiling-the-Potential-of-End-side-Large-Language-Models-d4d3a8c426424654a4e80e42a711cb20
	- æ—¢å­˜7B LLMã‚ˆã‚Šå¼·ã„ã¨è©±é¡Œã®2B LMMã®MiniCPM
	- Î¼Pä½¿ã£ã¦å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§åŠ¹ç‡çš„ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
	- MiniCPM is a series of edge-side large language models, with the base model, MiniCPM-2B, having 2.4B non-embedding parameters. 
	- It ranks closely with Mistral-7B on comprehensive benchmarks
- ç¾çŠ¶ã®LLMé¸æŠè‚¢ by urawazakun
	- https://x.com/urawazakun/status/1777130873844040046
	- commandRplusã€€108B â†’Mac é€²åŒ–çš„ï¼ˆ988000å††ï¼‰
	- commandRã€€35B â†’RTX4090ï¼ˆPC + 40ä¸‡å††ï½ï¼‰
	- LightChatAssistant2x7B â†’RTX3060ï¼ˆPC + 3ä¸‡å††ï½ï¼‰
-  EasyLightChatAssistant
	- https://github.com/Zuntan03/EasyLightChatAssistant?tab=readme-ov-file
	- EasyLightChatAssistant ã¯è»½é‡ã§æ¤œé–²ã‚„è¦åˆ¶ã®ãªã„ãƒ­ãƒ¼ã‚«ãƒ«æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã® LightChatAssistant ã‚’ã€KoboldCpp ã§ç°¡å˜ã«ãŠè©¦ã—ã™ã‚‹ç’°å¢ƒã§ã™ã€‚
- ï½¢LLMã¯ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£ãƒ¼ï½£ã€€ç±³ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒªãƒƒã‚¯ã‚¹CEOãŒèªã‚‹
	- https://www.nikkei.com/article/DGXZQOGN252JK0V20C24A3000000/
	- LLMå˜ä½“ã§ã¯ãªãLLMã‚„ãã®ä»–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦å•é¡Œã‚’è§£ãã€Œè¤‡åˆAIã€ã®è€ƒãˆæ–¹ãŒã¨ã¦ã‚‚å¤§äº‹
-  Octopus v2: On-device language model for super agent
	- https://arxiv.org/abs/2404.01744
	- https://huggingface.co/NexaAIDev/Octopus-v2
	- GPT-4è¶…ãˆç²¾åº¦ã§ã‚¹ãƒãƒ›ä¸Šå®Ÿè¡Œã§ãã‚‹ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹ç”ŸæˆAIã€ŒOctopus v2ã€
	- 20å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§æ©Ÿèƒ½ã™ã‚‹ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹AIãƒ¢ãƒ‡ãƒ«ã€ŒOctopus v2ã€
- Google Colab ã§ Octopus V2 ã‚’è©¦ã™ by npakaã•ã‚“ã€
	- https://note.com/npaka/n/n706bde979ed8
	- Gemma-2Bã‚’è¿½åŠ å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã€å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¸ã¨æ¨è«–ã‚¹ãƒ†ãƒ¼ã‚¸ã®ä¸¡æ–¹ã«ç‹¬è‡ªã®Functionãƒˆãƒ¼ã‚¯ãƒ³æˆ¦ç•¥ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã€ŒFunction Callingã€ã«ãŠã„ã¦ã€ŒGPT-4ã€ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’é”æˆã—ãŸã¨ã®ã“ã¨ã§ã™ã€‚
	- ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¨ã—ã¦ã¯ã€ã€Œã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã«ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼è¿½åŠ ã€ã€Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ã€ã€ŒYoutubeæ¤œç´¢ã€ã®æŒ‡ç¤ºãªã©ãŒæŒ™ã’ã‚‰ã‚Œã¦ã„ã¾ã™
- Chat Vectorã¨Math Vectorã¯ä½µç”¨ã§ãã‚‹ã®ã‹ by ã¯ã¡ã•ï½
	- https://note.com/hatti8/n/n2d6d86d6f05a?sub_rt=share_h
	- Chat+Mathèƒ½åŠ›ã®ä¸¡æ–¹ã‚’æ—¥æœ¬èªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ä»˜ä¸ã—ãŸã‚‰ã€ã©ã¡ã‚‰ã®åŠ¹æœã‚‚å¾—ã‚‰ã‚Œã‚‹ã®ã‹
	- Mathå¼·åŒ–ãƒ¢ãƒ‡ãƒ«ã«å…ˆã»ã©ä½œã£ãŸChat Vectorã‚’é‡ã­ãŒã‘ã—ã¦ã„ãã¾ã™
	- Mathå¼·åŒ–ãƒ¢ãƒ‡ãƒ«ï¼šSwallow-MS-7b-v0.1-MathSkill-OpenMath
	- Chat Vectorï¼šSkillTree-Chat-Mistral-7B-v0.1
	- Math+Chatå¼·åŒ–ãƒ¢ãƒ‡ãƒ«
		- https://huggingface.co/HachiML/Swallow-MS-7b-v0.1-ChatMathSkill
	- çµè«–
		-  **ãƒ¢ãƒ‡ãƒ«ãŒå£Šã‚Œã‚‹ã“ã¨ã¯ãªã„**
		- **æ•°å­¦èƒ½åŠ›ã‚’ã‚ã‚‹ç¨‹åº¦ç¶­æŒã—ã¤ã¤ã€Chatèƒ½åŠ›ã‚‚å¼·åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹**
		-  **ä¸€æ–¹ã€è‹±èªã§å›ç­”ã—ã‚„ã™ããªã‚‹å‚¾å‘ãŒå‡ºã¦ãã‚‹**
- Chat Vectorãªã‚‰ã¬Math Vectorã¯ä½œã‚Œã‚‹ã®ã‹
	- https://note.com/hatti8/n/n0000353355cb
- LangChain x DSPy
	- https://www.youtube.com/watch?v=4EXOmWeqXRc
- JetMoEã£ã¦ãªã‚“ã˜ã‚ƒï¼Ÿ by ã†ã¿ã‚†ãã•ã‚“ã€
	- https://x.com/umiyuki_ai/status/1777014403197788280
	- Mixture of Attention headsï¼ˆMoAï¼‰ã¨Mixture of MLP Expertsï¼ˆMoEï¼‰ã®äºŒã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã€ãã‚Œãã‚Œï¼”äººãšã¤ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒã„ã¦ã€æ¨è«–æ™‚ã¯å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼’äººãšã¤ãŒæ´»æ€§åŒ–ã™ã‚‹ã€‚
	- æ´»æ€§åŒ–ãƒ‘ãƒ©æ•°ã¯2.2Bã§ã€åˆè¨ˆãƒ‘ãƒ©æ•°ã¯8Bã ã£ã¦ã€‚ä½•ã ã‹çŸ¥ã‚‰ã‚“ã‘ã©ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ãŒçˆ†ä¸ŠãŒã£ã¦ã€H100ãŒ96å°ã§ï¼’é€±é–“ã€1200ä¸‡å††ã—ã‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è²»ç”¨ã‹ã‘ã¦ãªã„ã®ã«ã€æ•°åƒå„„ã‹ã‘ãŸã¯ãšã®Llama-7Bã‚„Llama-13ã«ãƒ™ãƒ³ãƒã§å‹åˆ©ã—ãŸ
- Î¼Transfer: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒã‚¤ãƒ‘ãƒ©æ¢ç´¢ã‚’å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã—å­¦ç¿’ã‚’åŠ¹ç‡åŒ–ã™ã‚‹
	- https://note.com/tatsuyashirakawa/n/n9f5b57ce1aa6?sub_rt=share_b&d=s4cpuSjMMAw
	- Î¼Pï¼ˆMaximal Update Parametrizationï¼‰ã¨ã„ã†ã®ã¯ã€ Tensor Programs (TP)ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã‘ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã©ï¼‰ã®æ–¹æ³•ã§ã™
	- TP ã¯ã€ Neural Networks ï¼ˆNNï¼‰ã®è§£æã‚’ã™ã‚‹ãŸã‚ã«ã€ç·šå½¢å¤‰æ›ã‚„éç·šå½¢æ´»æ€§åŒ–é–¢æ•°ãªã©ã® NN ã®æ§‹ç¯‰ã§é »å‡ºã™ã‚‹æ“ä½œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã€ãã®æ çµ„ã¿ã§æˆç«‹ã™ã‚‹äº‹è±¡ã‚„æ€§è³ªã‚’è¿½æ±‚ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
	- https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/
- Leveraging language representation for materials exploration and discovery
	- https://www.nature.com/articles/s41524-024-01231-8
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ææ–™æ¢ç´¢ã®è«–æ–‡ã€‚
	- çµæ™¶ææ–™ã‚’ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã«ã—è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šæ—¢å­˜ææ–™ã«ä¼¼ãŸæ–°ç†±é›»ææ–™ã‚’æ¢ç´¢
	- ç‰¹ã«ã€GPTã®ã‚ˆã†ãªãƒ‡ã‚³ãƒ¼ãƒ€å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã€BERTã®ã‚ˆã†ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®ã»ã†ãŒæ±ç”¨æ€§ãŒé«˜ãMIã‚¿ã‚¹ã‚¯ã«å‘ã„ã¦ã„ã‚‹ã€ã¨ã„ã†ç‚¹ãŒèˆˆå‘³æ·±ã‹ã£ãŸã§
- Î¼ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚¡ãƒ¼ã¨ã¯ by ã†ã¿ã‚†ãã•ã‚“ã€
	- https://x.com/umiyuki_ai/status/1777204816059711692
	- MiniCPMã¯ãƒŸãƒ¥ãƒ¼ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚¡ãƒ¼ã¨ã„ã†ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ãŒä½¿ã‚ã‚Œã¦ã‚‹ã‚‰ã—ã„ã€‚ã“ã‚ŒãŒä½•ã‹ï¼Ÿã¨ã„ã†ã¨ã€ã§ã‹ã„LLMã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ™‚ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ã‚‹ãƒ†ã‚¯ã‚‰ã—ã„ã€‚
	- ã§ã‹ã„LLMã‚’å­¦ç¿’ã™ã‚‹æ™‚ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã©ã†å¼„ã‚Œã°æœ€å¼·ã«ãªã‚‹ã®ã‹ã€ã‚¤ãƒã‚¤ãƒè‰²ã€…è©¦ã—ã¦æœ€é©è§£ã‚’è©¦è¡ŒéŒ¯èª¤ã™ã‚‹ã®ã¯ãƒ¡ãƒãƒ£ã‚¯ãƒãƒ£å¤§å¤‰ã ã€‚ãã“ã§ã€åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã¡ã£ã¡ã‚ƒã„ç‰ˆã§å®Ÿé¨“ã™ã‚Œã°ã‚µã‚¯ã‚µã‚¯ã¨æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©¦è¡ŒéŒ¯èª¤ã§ãã‚‹ã€‚ã§ã€ã¡ã£ã¡ã‚ƒã„ãƒ¢ãƒ‡ãƒ«ã§è¦‹ã¤ã‘ãŸæœ€å¼·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã€ã§ã‹ã„LLMã«ãã‚“ã¾ã¾ã‚³ãƒ”ãƒšã—ã¦ã‚‚ã¡ã‚ƒã‚“ã¨æœ€å¼·ã«ãªã‚‹äº‹ãŒåˆ¤æ˜ã—ãŸã‚‰ã—ã„ï¼
- JetMoEã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ä¸ŠãŒã£ãŸã®ã¯ã€€ by ã†ã¿ã‚†ãã•ã‚“ã€
	- https://x.com/umiyuki_ai/status/1777023943121256637
	- Komatsuzakiæ°ã®è¦‹è§£ã«ã‚ˆã‚Œã°ã€JetMoEã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ä¸ŠãŒã£ãŸã®ã¯ã€ãŸã—ã‹ã«MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã£ã¦ï¼’ï½ï¼“å€ã«åŠ¹ç‡åŒ–ã—ãŸã‘ã©ã€ãã‚Œã‚ˆã‚Šä½•ã‚ˆã‚ŠMiniCPMã«å€£ã£ãŸ2æ®µéšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹æ³•ã®ãŠã‹ã’ã§ãƒã‚­ãƒã‚­ã«åŠ¹ç‡åŒ–ã—ãŸã¨ã®äº‹ã€‚
	- 1ä¸‡å€ã®å†…ã€MoEã®è²¢çŒ®ãŒï¼“å€ãªã‚‰æ®‹ã‚Šã®3333å€ã¯MiniCPMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŠã‹ã’ãªã®ã‹
- The Physics of Language Models
	- https://arxiv.org/abs/2404.05405
	- ã€Œè¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€int8 ã«é‡å­åŒ–ã•ã‚ŒãŸå ´åˆã§ã‚‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨ã« 2 ãƒ“ãƒƒãƒˆã®çŸ¥è­˜ã—ã‹ä¿å­˜ã§ãã¾ã›ã‚“ã€‚ã¾ãŸã€ãã®ã‚ˆã†ãªçŸ¥è­˜ã¯ã€ä¸‹æµã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã«æŸ”è»Ÿã«æŠ½å‡ºã§ãã¾ã™ã€‚ãã®çµæœã€7B ãƒ¢ãƒ‡ãƒ«ã¯ 14B ãƒ“ãƒƒãƒˆã®çŸ¥è­˜ã‚’ä¿å­˜ã§ãã€ã“ã‚Œã¯ç§ãŸã¡ã®æ¨å®šã«åŸºã¥ãã¨ã€è‹±èªç‰ˆ Wikipedia ã¨æ•™ç§‘æ›¸ã‚’åˆã‚ã›ãŸé‡ã‚’è¶…ãˆã¾ã™ã€‚ã€
	- å›è»¢åŸ‹ã‚è¾¼ã¿ã‚’å‚™ãˆãŸ GPT-2 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€çŸ¥è­˜ã®ä¿å­˜ã«ãŠã„ã¦ LLaMA/Mistral ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŒ¹æ•µã™ã‚‹ã‹ã€ãã‚Œã‚’ä¸Šå›ã‚Šã¾ã™ã€‚
- Gemini 1.5 Pro
	- https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html
	- 180ã‚«å›½ã‚µãƒãƒ¼ãƒˆã€ã€Œçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã€éŸ³å£°ãƒ»å‹•ç”»èªè­˜ã€ãƒ•ã‚¡ã‚¤ãƒ«APIã€System Instructionã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ©Ÿèƒ½ã€ JSONãƒ¢ãƒ¼ãƒ‰ãªã©ãŒåŠ ã‚ã‚Šã¾ã—ãŸã€ä»¥ä¸‹ã§è©¦ã›ã‚‹
	- https://aié€²åŒ–çš„.google.com/app/prompts/new_chat
- Imagen 2 by DeepMind
	- https://x.com/GoogleDeepMind/status/1777747320945234422
	- Imagen 2 can now create short, 4-second live images from a single prompt.
- GPT-4 Turbo launch
	- https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4
	- previewãŒå–ã‚ŒãŸ
- UNESCOãŒAI Ethicã§äººé›†ã‚ã—ã¦ã„ã‚‹ byã€€ç¥å¶Œã•ã‚“
	- https://careers.unesco.org/job/Other-cities-Consultant/791818302/d
- Llama.cpp ã§ Command R+ ã‚’ãŠè©¦ã—ä¸­ by npakaã•ã‚“
	- https://x.com/npaka123/status/1777802956571889969
	- Q4_K_Mãƒ»M3 Max (128GB) 5.22 tokens per second
- mmnga/codegemma-7b-it-gguf
	- https://huggingface.co/mmnga/codegemma-7b-it-gguf
	- gemma-1.1-7bã¨codegemma-7b-itã®gguf
- ã€LangChainã‚†ã‚‹å‹‰å¼·ä¼š#3ã€‘LangChainã®Agentã¯ã©ã‚Œã‚’ä½¿ã†ï¼Ÿ
	- https://www.youtube.com/watch?v=07TuBmm67sU
	- LangChainã‚’ä½¿ã£ãŸAgentå®Ÿè£…ã‚’æ¦‚èª¬ã—ã¦ãã ã•ã£ã¦ã‚‹å‹‰å¼·ä¼šã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‹•ç”»ã€‚
	- æœ€è¿‘ã¯LCELã§çµ„ã‚“ã§Agent Excutorã«æŠ•ã’ã‚‹ä»¥å¤–ã®å®Ÿè£…ã—ãªã„ã®ã§ã€ãªã‚“ã‹è‰²ã€…ã‚ã‚‹ã‚“ã ãªã¨å‹‰å¼·ã«ãªã‚Šã¾ã—ãŸ
	- XML Agentã¨ã‹èª°ãŒä½¿ã†ã‚“ï¼Ÿã£ã¦æ€ã£ã¦ãŸã‘ã©ã€Claudeã¨ç›¸æ€§è‰¯ã„ã‚‰ã—ã„ã€‚ã¸ã‡ã€œï¼
- rinna/youri-7b-chat-gptqã¨intfloat/multilingual-e5-largeã§RAGã™ã‚‹ã ã‘ã§ã‚‚colabã‚ˆã‚Šrtx3060ã®æ–¹ãŒã‹ãªã‚Šé€Ÿã„
	- https://x.com/rsimd_/status/1747614320878555175
	- vramãŒè¶³ã‚Šã‚Œã°ã£ã¦è©±ã ã‘ã©ï¼Œä¸€å¿œfaiss-cpuã‚’ä½¿ãˆã°ãƒ¡ãƒ¢ãƒªè¶³ã‚Šã¦ã‚‹ï¼
- Command R+ã®é‡å­åŒ–PPLã‚’è¨ˆæ¸¬ã—ã¦ãã‚Œã¦ã‚‹
	- https://github.com/ggerganov/llama.cpp/pull/6491#issuecomment-2043633791
	- Q3_XXSã¯38GBã ã‘ã©ã€ã“ã“ã¾ã§ãªã‚‰ç²¾åº¦çš„ã«ã‚‚å…¨ç„¶å¤§ä¸ˆå¤«ã¡ã‚ƒã†ã‹ï¼Ÿã£ã¦äºˆæ„Ÿã¯ã™ã‚‹ã€‚IQ2_XXSãªã‚‰26.6GBã§ã€ã¡ã‚‡ã£ã¨ã‚¢ãƒ›ã«ãªã£ã¦ãã†ã€‚IQ1_Sãªã‚‰21.6GBã ã‘ã©ã€ã•ã™ãŒã«å®Ÿç”¨æ€§ãƒ¤ãƒãã†ã€‚
- Perplexity Proã«èª²é‡‘ã—ã¦Googleã®Gemini Ultraã‚„Generative Experienceã¨æ¯”è¼ƒã—ã¦ã¿ã‚‹ã¨ã€ä½•ã‹ã¨ã‚“ã§ã‚‚ãªã„ã“ã¨ãŒèµ·ã“ã£ã¦ã„ã‚‹æ°—ãŒã™ã‚‹ by æ¥ ã•ã‚“
	- https://x.com/masanork/status/1777478951465779344
- å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ã§RAGã‚‚ä½¿ãˆã‚‹AIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªOpenWebUIã‚’æ—¥æœ¬èªLLMã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹
	- https://zenn.dev/firstautomation/articles/0b7a4b1bb2daf0
- Command R+ã¯ã¡ã‚ƒã‚“ã¨å¼·ã‹ã£ãŸè¨³ã ãŒã€Command Rã‚‚ã“ã‚Œã¾ã§ã®Open-sourceæœ€å¼·ã®Qwen1.5-72bã«åŒ¹æ•µã™ã‚‹è¨³ãªã®ã§ã™ã”ã„
	- https://x.com/Meteor_Eternal/status/1777635899204874704
- Gemini 1.5 Proã®æ–°æ©Ÿèƒ½ - Native Audio Understandingã€System Instructionsã€JSON Modeã€æ–°Embeddingãƒ¢ãƒ‡ãƒ«ã€€ by npakaã•ã‚“
	- https://note.com/npaka/n/n0254081ebc23?sub_rt=share_h
- Stable LM 2 12B
	- https://stability.ai/news/introducing-stable-lm-2-12b
	- Stable LM 2 12B ã¯ã€è‹±èªã€ã‚¹ãƒšã‚¤ãƒ³èªã€ãƒ‰ã‚¤ãƒ„èªã€ã‚¤ã‚¿ãƒªã‚¢èªã€ãƒ•ãƒ©ãƒ³ã‚¹èªã€ãƒãƒ«ãƒˆã‚¬ãƒ«èªã€ã‚ªãƒ©ãƒ³ãƒ€èªã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã•ã‚ŒãŸã€120å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤å¼·åŠ›ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨æŒ‡ç¤ºå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚
- GoogleDocã«geminiãŒçµ±åˆã•ã‚Œã‚‹ï¼Ÿ
	- https://x.com/GoogleWorkspace/status/1777807449652662508
- TPU v5p, our most powerful and scalable TPU, is now generally available
	- https://x.com/GoogleCloudTech/status/1777732890471625162
- Gemma-1.1 also shows great improvement in terms of reduced hallucinations in the updated HHEM leaderbod
	- https://x.com/ofermend/status/1777695633455108478
- LLaMA 3's will start to drop next week.
	- https://x.com/mattshumer_/status/1777465835834970189
- Ride with Geminiã¨ã„ã†LLMï¼‹RCã‚«ãƒ¼ã®ãƒ‡ãƒ¢
	- https://x.com/kazunori_279/status/1777846216950456658
-  Gemini 1.5 Proã§æ–‡å­—èµ·ã“ã—ã‚’è©¦ã—ã¦ã¿ãŸ
	- https://note.com/nyosubro/n/n07afba435ef6
	- å€‹äººçš„ãªæ„Ÿæƒ³ã¨ã—ã¦ã¯ã€Whisperãƒ¬ãƒ™ãƒ«ï¼ˆã‚ã‚‹ã„ã¯ãã‚Œä»¥ä¸Šï¼Ÿï¼‰ã®æ–‡å­—èµ·ã“ã—å“è³ªã¨è«–æ–‡ã§ã¯ã‚ã‚Šã¾ã—ãŸãŒã€ç¢ºã‹ã«ãã†ã‹ã‚‚ï¼ã¨è¨€ã†æ„Ÿã˜ã§ã—ãŸã€‚
	- ã¾ãŸWhisperã¨ã¯ç•°ãªã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«ã§æ§˜ã€…ãªæ–‡å­—èµ·ã“ã—ã‚¿ã‚¹ã‚¯ã«æŸ”è»Ÿã«å¯¾å¿œã§ãã‚‹ç‚¹ã§ã€çµæ§‹é¢ç™½ã•ã‚’æ„Ÿã˜ã¦ã¾ã™ã€‚
- Llama.cpp ã§ Command R+ ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n9136a2ebc7f9?sub_rt=share_h
	- M3 Max (128GB)
	- ã€ŒCommand R+ã€ã¯ã€ã€ŒRAGã€ã‚„ã€ŒToolã€ãªã©ã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸ104Bã®LLMã§ã™ã€‚Cohereã®EmbeddingãŠã‚ˆã³Rerankã¨é€£æºã—ã¦å‹•ä½œã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«æœ€é«˜ã‚¯ãƒ©ã‚¹ã®çµ±åˆã‚’æä¾›ã—ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§å„ªã‚Œã¦ã„ã¾ã™ã€‚
- Wikipediaã®æ—¥æœ¬èªè¨˜äº‹ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å›ç­”ã™ã‚‹Gradioãƒ™ãƒ¼ã‚¹ã®RAGã®ã‚µãƒ³ãƒ—ãƒ«ã€‚
	- https://github.com/lawofcycles/wikipedia-japanese-open-rag/tree/master
	- ä½¿ã£ãŸã‚‚ã®
		-   [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)
		-   [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct)
- Command R plusæ¨è«–é€Ÿåº¦ã€çŸ¥è¦‹ã¾ã¨ã‚ by AIXã•ã¨ã—
	- https://x.com/AiXsatoshi/status/1777867323552190876
- Amazonã€ã€ŒClaude 3ã€ã®Anthropicã«27å„„5000ä¸‡ãƒ‰ãƒ«ã®è¿½åŠ æŠ•è³‡
	- https://www.itmedia.co.jp/news/articles/2403/28/news105.html#utm_term=share_sp
- A Generative Symbolic Music Pretrained Transformer
	- https://huggingface.co/papers/2404.06393
	- In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are
- We just released Mixtral 8x22B. Super excited for this release
	- https://x.com/sophiamyang/status/1777945947764297845
- æ—¥æœ¬èªLLM 9ç¨®ã‚’é‡å­åŒ–ã—ã¦å›ç­”å†…å®¹ã‚’æ¯”è¼ƒèª¿æŸ»ã—ã¦ã¿ãŸ
	- https://qiita.com/wayama_ryousuke/items/50e36d0dcb37f8fb7dd8
	- é‡å­åŒ–ã—ã¦ã‚‚æˆç¸¾ãŒä¸‹ãŒã‚Šã«ãã„ãƒ¢ãƒ‡ãƒ«ã¨ã€å¤§ããä¸‹ãŒã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹
	- ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã¯é‡å­åŒ–ã™ã‚‹ã¨å›ç­”ãŒæ¥µç«¯ã«çŸ­ããªã‚‹
	- é‡å­åŒ–ã«ã‚ˆã£ã¦å›ç­”ãŒçŸ­ããªã‚‹åº¦åˆã„ã¯ã€é‡å­åŒ–å‰ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã®é•·ã•ã¨ç›¸é–¢ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
	- å€‹åˆ¥ï¼š
		- **ELYZA-japanese-Llama-2-7B**ã¯ã€é‡å­åŒ–å¾Œã‚‚ã»ã¼åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã—ã€0.10ç‚¹ã®ã‚¹ã‚³ã‚¢ä½ä¸‹ã«ç•™ã¾ã‚Šã¾ã—ãŸã€‚
		-  **Swallow-7B**ã§ã¯ã€é‡å­åŒ–å‰å¾Œã§æˆç¸¾ã«å¤‰åŒ–ã¯ãªã‹ã£ãŸä¸€æ–¹ã€**Swallow-13B**ã§ã¯å¹³å‡ã‚¹ã‚³ã‚¢ãŒ 0.28 ç‚¹ä½ä¸‹ã—ã¾ã—ãŸã€‚
		- **CALM2**  ã‚„  **StableLM-Beta**  ã¯ã€é‡å­åŒ–å¾Œã®ã‚¹ã‚³ã‚¢ãŒé«˜ã„çµæœï¼ˆãã‚Œãã‚Œ 0.28 ç‚¹/ 0.21 ç‚¹å‘ä¸Šï¼‰ã¨ãªã‚Šã¾ã—ãŸã€‚
		-   **Xwin**  ãƒ¢ãƒ‡ãƒ«åŒå£«ã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€**Xwin 7B**ã¯0.36ç‚¹ã®ä½ä¸‹ã‚’ç¤ºã—ã¦ã„ã‚‹ä¸€æ–¹ã€**Xwin 13B**ã§ã¯0.11ç‚¹ã®å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã€åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼å†…ã§ã‚‚ç•°ãªã‚‹æŒ¯ã‚‹èˆã„ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚
-  Command R+ GPTQã‚’ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã—ã¦vllmã§OpenAI APIäº’æ›ã‚µãƒ¼ãƒå‹•ä½œã•ã›ã¦ã¿ãŸè©±
	- https://note.com/junzokamahara/n/n9235af7a6dc1?sub_rt=share_h
	- vllmã‚‚Command Rã«å¯¾å¿œã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã§ã€vllmã§å‹•ã‹ã—ã¦ã¿ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚ãªãŠã€å‹•ã‹ã™ã®ã¯GPUãƒ¡ãƒ¢ãƒªã®é–¢ä¿‚ã§GPTQã§é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
	- ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã¯Hugging Faceã«ã‚ã‚‹GPTQã«å¤‰æ›ã—ãŸCommand R+
		- client = OpenAI(base_url="http://<ä»®æƒ³ãƒã‚·ãƒ³ã®IP>:8888/v1")
		- response = client.chat.completions.create(model='alpindale/c4ai-command-r-plus-GPTQ',
	- Command R plus GPTQã®A100 80GBã§ã®å®Ÿè¡Œä¾‹
		- 18.3 tokens/sã¨å‡ºã¦ã„ã‚‹
- Geminiã®æ–°æ©Ÿèƒ½ã€ŒSystem Instructionsã€ã‚’ä½¿ã£ã¦ã¿ã‚‹ã€‚ 
	- https://x.com/npaka123/status/1777969149651906927
	- ChatGPTã§ã¯ãŠãªã˜ã¿ãªæ©Ÿèƒ½ã ã‘ã©ã€ä»Šã¾ã§Geminiã«ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ãªã‹ã£ãŸã®ã§ã†ã‚Œã—ã„ã€‚
- ã€ã™ãšã‚ã®æˆ¸ç· ã¾ã‚Šã€ã«ç™»å ´ã™ã‚‹3æœ¬è„šã®æ¤…å­ã‚’å†ç¾ã—ãŸãƒ­ãƒœãƒƒãƒˆè¨­è¨ˆ
	- https://x.com/shin0805__/status/1777992583396131246
	- å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹æ­©å®¹ç”Ÿæˆã®è«–æ–‡ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ æ¥é€±ã‚¢ãƒ¡ãƒªã‚«ã§é–‹å‚¬ã•ã‚Œã‚‹RoboSoft2024ã«ã¦ç™ºè¡¨ã—ã¾ã™ï¼
	- https://shin0805.github.io/chair-type-tripedal-robot/
- mistral-community/Mixtral-8x22B-v0.1
	- The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.
	- ãŠã„ã€ã“ã‚ŒApache-2.0ã ãï¼GPT-4ã‚¯ãƒ©ã‚¹ãŒå•†ç”¨åˆ©ç”¨å¯èƒ½ã‚‰ã—ã„
- MLX with LangChain
	- https://python.langchain.com/docs/integrations/chat/mlx/
	- This notebook shows how to get started using MLX LLMâ€™s as chat models.
- Google Colab ã§ RecurrentGemma ã‚’è©¦ã™
	- https://note.com/npaka/n/n0018d60fb8b7?sub_rt=share_h
	- ã€ŒRecurrentGemmaã€ã¯ã€Google ã§é–‹ç™ºã•ã‚ŒãŸæ–°ã—ã„ãƒªã‚«ãƒ¬ãƒ³ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚ŒãŸã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ãŒè‹±èªã§åˆ©ç”¨å¯èƒ½ã§ã™
	- æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ã€ŒGemmaã€ã‚ˆã‚Šã‚‚å¿…è¦ãªãƒ¡ãƒ¢ãƒªãŒå°‘ãªãã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹éš›ã«é«˜é€Ÿãªæ¨è«–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
	- ä»Šå›ã¯ã€ã€Œ**google/recurrentgemma-2b-it**ã€ã‚’ä½¿ã„ã¾ã™
- LightChatAssistant-2x7Bã§è¡Œã‚ã‚Œã¦ã„ã‚‹æœ€é©åŒ–ã‚’Optuneã§
	- https://github.com/Aratako/Task-Vector-Merge-Optimzier
	- Sdff-Ltba/LightChatAssistant-2x7Bã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªLLMã«ãŠã‘ã‚‹Task Vectorã®åŠ ç®—ã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ã«ãŠã„ã¦ã€ãã®åŠ ç®—å‰²åˆã®æœ€é©åŒ–ã‚’Optunaã‚’ç”¨ã„ã¦è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™
- Infini-attention
	https://x.com/umiyuki_ai/status/1778459568424784194
	- GoogleãŒå‡ºã—ãŸè«–æ–‡ãªã‚“ã ã­ã€‚ã§ã€ã€Œã“ã®æŠ€è¡“ã®ãŠã‹ã’ã§Gemini1.5ã§ã¯100ä¸‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ãŒå¯èƒ½ã«ãªã£ãŸã®ã‹ï¼ã€
- Safeguarded AI: 
	- https://www.aria.org.uk/wp-content/uploads/2024/04/ARIA-Safeguarded-AI-TA1.1-Theory-Call-for-proposals.pdf
	- ARIAã®Davidadæ°ã®å®‰å…¨ä¿è¨¼ä»˜ãAIã®ç ”ç©¶ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å…¨è²ŒãŒè¦‹ãˆã¦ããŸã€‚å½¼ãŒä½•ã‚’ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã®ã‹ã€ãã‚Œã«ã©ã‚Œã»ã©ã®feasiblityãŒã‚ã‚‹ã®ã‹ã€èª°ã‹ã«è§£èª¬ã—ã¦ã»ã—ã„ã€‚å½¢å¼è¨¼æ˜ã¨ã‹ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ã€è¨ˆç®—æ©Ÿç†è«–ã®ãƒãƒƒã‚¯ã‚°ãƒ©ãƒ³ãƒ‰ãŒå¿…è¦ãã†ã€‚
	- ä»Šå›ã®å…¬å‹Ÿã§ã¯åœŸå°ã¨ãªã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã€ã€Œè¨€èªã€ã¥ãã‚Šã‚’ç›®æŒ‡ã™ã¨ã®ã“ã¨ã§ã€ãã®æ–¹æ³•è«–ã¨ã—ã¦åœè«–ãŒåæŒ‡ã—ã•ã‚Œã¦ã„ã¾ã™
- Mixtral8x22ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç‰ˆ
	- HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1
	- ORPOã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨
	- ORPOã¯ã€SFTã‚¹ãƒ†ãƒƒãƒ—ã‚’å¿…è¦ã¨ã—ãªã„ãŸã‚ã€DPOã‚„PPOã®ã‚ˆã†ãªæ–¹æ³•ã‚ˆã‚Šã‚‚è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„ 
	- ã‚ªãƒ¼ãƒ—ãƒ³ã€åˆæˆã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã€LLMã‚’ä»‹ã—ã¦æ¡ç‚¹ã•ãŸDPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨
- LLMã«ã‚ˆã‚‹è¦–è¦šèª­è§£æŠ€è¡“ã‚’ç¢ºç«‹ï½ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãªæ–‡æ›¸ã‚’ç†è§£ã™ã‚‹ã€Œtsuzumiã€å®Ÿç¾ã«å‘ã‘ã¦ï½
	- https://group.ntt/jp/newsrelease/2024/04/12/240412b.html
- Embeddingsã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆMultilingual-E5ï¼‰
	- https://zenn.dev/libratech/articles/afe9c5b30668bb
- Mixtral-8x22Bã€Lightblueã•ã‚“ã®karasuãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨AWQ
	- https://huggingface.co/lightblue/Karasu-Mixtral-8x22B-v0.1
	- å¼·ã„ï¼ã“ã‚Œã¯é–“é•ã„ãªãã‚¨ãƒ¼ã‚¹ç´šã€€ by AIXã•ã¨ã—
		- https://x.com/AiXsatoshi/status/1778489953279951132
- Introducing Mistral-22b-V.01 A breakthrough in AI
	- https://huggingface.co/Vezora/Mistral-22B-v0.1
	- First-ever MOE to Dense model conversion
	- This model is not an moe, it is infact a 22B parameter dense model!
	- mixtralã®expertsã‚’ãƒãƒ¼ã‚¸ã—ã¦mistralã«ã—ãŸã‚„ã¤
- Vezoraã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹Mistral-22B-v0.1ã®ggufã‚ã‚Šã¾ã™
	- https://huggingface.co/mmnga/Vezora-Mistral-22B-v0.1-gguf
- Swallowã‚·ãƒªãƒ¼ã‚ºã®instructæ”¹è‰¯ç‰ˆã§ã™ãŒã€æœ¬å½“ã¯2023å¹´åº¦ä¸­ã‚’ç›®æŒ‡ã—ã¦ã„ãŸã®ã§ã™ãŒã€ã‚‚ã‚ã‚‚ã‚å¤šå¿™ã§é…ã‚Œã¦ã—ã¾ã£ã¦ã„ã¾ã™ã€‚
	- https://x.com/okoge_kaz/status/1778396705156943985
- mixtral 8x22bã‚’è»½ãloraã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‰ã€å°‘ã—ã€ä¼šè©±ã—ã‚„ã™ããªã‚Šã¾ã—ãŸ
	- https://x.com/kanhatakeyama/status/1778417221100028061
	- ç¾çŠ¶ï½¤mixtral 8x22bã¯äº‹å‰å­¦ç¿’ã®ã¿ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ãŒï½¤ã‚ã‚Šã¨ä¼šè©±ã§ããã†ã§ã™ï½¡
-  Tool Calling with LangChain
	- https://blog.langchain.dev/tool-calling-with-langchain/
	- æœ€è¿‘ã¯ChatGPTä»¥å¤–ã«ã‚‚ Function Calling (æœ€è¿‘ã¯ Tool Calling ã¨å‘¼ã°ã‚Œã‚‹ã“ã¨ãŒå¤šã„) ã«å¯¾å¿œã™ã‚‹LLMãŒå¢—ãˆã¦ãã¾ã—ãŸã€‚é¸æŠè‚¢ãŒå¢—ãˆã¦ä¾¿åˆ©ã§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€å„ç¤¾ã§å°‘ã—ã¥ã¤ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒé•ã†ã®ã§å®Ÿè£…ãŒé¢å€’ã¨ã„ã†èª²é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚
	- ãã®ãŸã‚ã€LangChainã¯å„LLMã®Tool Callingã‚’çµ±ä¸€çš„ã«æ‰±ãˆã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æº–å‚™ã—ã¦ãŠã‚Šã€å…ˆæ—¥ã€æœ€å¾Œã®ãƒ”ãƒ¼ã‚¹ãŒãƒãƒã£ã¦é‚ã«å®Œæˆã—ãŸã¨ã„ã†è©±ã§ã™ã€‚
- LangChain ã® Tool Calling æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ ã®æ¦‚è¦ã€€by npakaã•ã‚“
	- https://note.com/npaka/n/ne6fd5929bfa1?sub_rt=share_h
	- ã€ŒTool Callingã€ã®æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã®æ§‹æˆã¯ã€æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚
		- ChatModel.bind_tools()ãƒ„ãƒ¼ãƒ«å®šç¾©ã‚’ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¿ãƒƒãƒã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
		- AIMessage.tool_callsãƒ¢ãƒ‡ãƒ«ãŒæ±ºå®šã—ãŸãƒ„ãƒ¼ãƒ«ã®æƒ…å ±ã‚’ä¼ãˆã‚‹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
		- create_tool_calling_agent()Tool Callingã‚’åˆ©ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
- OpenEQA (ã‚ªãƒ¼ãƒ—ãƒ³èªå½™ã®å…·ä½“åŒ–ã•ã‚ŒãŸè³ªå•å¿œç­”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)
	- https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/?utm_source=twitter&utm_medium=organic_social&utm_content=video&utm_campaign=dataset
	- ãƒãƒƒã‚¸ã‚’ã©ã“ã«ç½®ã„ãŸã‹?ã€ãªã©ã®ã‚ªãƒ¼ãƒ—ãƒ³èªå½™ã®è³ªå•
	- ç‰©ç†ç’°å¢ƒã«å¯¾ã™ã‚‹ AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç†è§£åº¦ã‚’æ¸¬å®š
- A Square-Root Kalman Filter Using Only QR Decompositions
	- https://arxiv.org/abs/2208.06452
	- QRåˆ†è§£ã§ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼Ÿ
	- æ­£å®šå€¤è¡Œåˆ—ã®å’Œã®å¹³æ–¹æ ¹ãŒå¹³æ–¹æ ¹ã®ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ—ã®QRåˆ†è§£ã§è¨ˆç®—ã§ãã‚‹ã“ã¨ã‚’åˆ©ç”¨ã—ã¦ã€æ•°å€¤çš„å®‰å®šæ€§ã®é«˜ã„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¹³æ–¹æ ¹ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’QRåˆ†è§£ã§ã‚·ãƒ³ãƒ—ãƒ«ã«æ›¸ã‘ã‚‹ã®ã‹
- Premise Order Matters in Reasoning with Large Language Models
	- LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã‚‹éš›ã€ã€Œæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®æµã‚Œã«æ²¿ã†é †åºã€ã§æ–‡è„ˆã‚’ä¸ãˆãªã„ã¨30%ä»¥ä¸Šç²¾åº¦ãŒè½ã¡ã‚‹æã‚ŒãŒã‚ã‚‹ã“ã¨ã‚’DeepMindãŒå ±å‘Šã—ã¦ã„ã¾ã™ã€‚
-  Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese
	- https://arxiv.org/abs/2404.07824
	- ç”»åƒ-è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦ã€æ–°ã—ãã€ŒHeron-Benchã€ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼æ—¥æœ¬ã®ç”»åƒã§ã€æ—¥æœ¬ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’ç·åˆçš„ã«å•ã„ã¾ã™
- Rho-1: Not All Tokens Are What You Need
	- https://arxiv.org/abs/2404.07965
	- MicrosoftãŠå¾—æ„ã®é«˜å“è³ªãƒ†ã‚­ã‚¹ãƒˆã§åŠ¹ç‡ã‚ˆãäº‹å‰å­¦ç¿’ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ€æ–°è«–æ–‡ã€ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®lossã®æ¨ç§»ã‚’é«˜ã„ã¾ã¾ãƒ»ä½ã„ã¾ã¾ãƒ»æ¸›å°‘å‚¾å‘ãƒ»å¢—åŠ å‚¾å‘ã®4ã‚¿ã‚¤ãƒ—ã«åˆ†é¡ã—ã¦ã„ã¦é¢ç™½ãã†ã€‚å®Ÿéš›ã«å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶éƒ¨åˆ†ã‚’å‹‰å¼·ã—ã‚ˆã†ã€‚
	- https://huggingface.co/microsoft/rho-math-7b-v0.1
- Geminiã«ã‚ˆã‚‹RAGã®å®Ÿè·µä¾‹
	- https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/use-cases/retrieval-augmented-generation
- Gemini API ã® ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n6609bcbbdd30?sub_rt=share_h
- ã€Œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’QRåˆ†è§£ã§è§£ãæ‰‹æ³•ã€
	- https://github.com/kevin-tracy/QRKalmanFilter
	- Square root Kalman Filter using only QR decompositions.
	- related paper: Differentiable Collision Detection for a Set of Convex Primitives
	- https://arxiv.org/abs/2207.00669
- Can Gemini 1.5 actually read all the Harry Potter books at once?
	- https://x.com/deedydas/status/1778621375592485076
	- All the books have ~1M words (1.6M tokens). Gemini fits about 5.7 books out of 7. I used it to generate a graph of the characters and it CRUSHED it.

## 4/8

ä»Šé€±ã‚‚æƒ…å ±ãŒæ—©ã™ãã¦å¤§éãã¦ã€ã‚‚ã‚„ã¯è¿½ã„ã¤ã‘ã¾ã›ã‚“ã€‚RAGå‘ã‘ã®ãƒ™ã‚¯ãƒˆãƒ«DBã®ãƒ™ãƒ³ãƒ€ãƒ¼ã‹ã¨æ€ã£ã¦ã„ãŸCohereã‹ã‚‰ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Command R+ ãŒãƒªãƒªãƒ¼ã‚¹ã€ã¾ã‚æˆã‚Šç«‹ã¡ã‹ã‚‰å½“ç„¶ã€RAGã¨ã‹ãƒ­ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æœ€é©åŒ–ã•ã¦ã„ã‚‹ã€‚104Bã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å…¬é–‹ã€ãƒ†ã‚¹ãƒˆç‰ˆãŒhuggingfaceã§è©¦ã™ã“ã¨ã‚‚ã§ãã‚‹ã€GPT-4ä¸¦ã¿ã®æ€§èƒ½ã§OSSã£ã¦ã‚„ã°ããªã„ã‹ã€‚æ—©é€Ÿé‡å­åŒ–ã—ãŸã‚Šã€MLXã§å‹•ã‹ã—ãŸã‚Šã¨ã€ã‚„ã°ããªã„ã‹ã€‚ã‚ã‚‹æ€§èƒ½ä»¥ä¸Šã®LLMã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ç¦æ­¢ã¿ãŸã„ãªå‚¾å‘ã«æ‹è»ŠãŒã‹ã‹ã‚‹ã®ã§ã¯ã€‚Appleã‹ã‚‰ã¯ReALMç™ºè¡¨ã€ã©ã†ã‚‚Siriã®ä»£ã‚ã‚Šã«iPhoneã§ã‚‚å‹•ãè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã€ãã†ã„ãˆã°Siriã®äººå“¡ãŒè§£é›‡ã•ã‚ŒãŸã¨ã„ã†ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚ã‚ã£ãŸã€‚ã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰ã‚„CMUã®ä¸€æµå¤§å­¦ã§ã‚‚CSã®ä¿®å£«ã®å°±è·ç‡ãŒï¼’å‰²ã¨ã®ã“ã¨ã€ã¾ã‚10å¹´ã§CSä¿®å£«å–å¾—è€…ãŒ10å€ã«ãªã£ãŸã¨ã„ã†è¦å› ã‚‚ã‚ã‚‹ã‚ˆã†ã ãŒã€LLMã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¨ã—ã¦ä½¿ã†ã£ã¦ã„ã†æ™‚ä»£ã€æ™®é€šã®CSä¿®å£«ç¨‹åº¦ã§ã¯ã‚‚ã‚„ã¯ã€ãŠå‘¼ã³ã§ã¯ãªã„ã¨ã„ã†ã“ã¨ã‹ã€‚OpenAIã¯ã€æ—¥æœ¬ã«ã‚¢ã‚¸ã‚¢åˆã®æ‹ ç‚¹ã‚’é–‹è¨­ã€ãªãœã‹ä½æ‰€ã¯è¥¿æ–°æ©‹ã®é›‘å±…ãƒ“ãƒ«ã€‚Gemmaã®1.1ã®ãƒªãƒªãƒ¼ã‚¹ã¨ã‹Qwen1.5-32B ã®ãƒªãƒªãƒ¼ã‚¹ãªã©ã€é‡è¦ãªæ”¹è‰¯ãƒªãƒªãƒ¼ã‚¹ã‚‚é€²ã‚€ã€‚RAGã§ã¯Reranker ãŒè©±é¡Œã«ã€é¡ä¼¼åº¦ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠã—ãŸã¯ãšãªã®ã«ã€ãã®ã‚ã¨ã«Rerankã™ã‚‹ã®ã‹ã€‚ã€‚å¯¾å¿œã™ã‚‹LLMã‚‚ã„ãã¤ã‹å‡ºã¦ã‚‹ã€‚æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€Swallow MX 8x7bã¯ç¾çŠ¶ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã¯æ—¥æœ¬èªæœ€é«˜ã®ãƒ¢ãƒ‡ãƒ«ã¨ã„ã†è©±ã‚‚ã‚ã£ãŸãŒã€Mistral 7Bãƒ™ãƒ¼ã‚¹ã®ï¼’ã¤ã®æ—¥æœ¬èªLLMã‚’Chat Vectoræ³•ã§å¼·åŒ–ã—ãŸã‚‚ã®ã‚’MoEã—ãŸã€ã¨ã¦ã‚‚é•·ã„åå‰ã®ãƒ¢ãƒ‡ãƒ«ãŒè©±é¡Œã«ã€å¯¿é™ç„¡ã‹ã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®ãŸã‚ã®æ—¥æœ¬èª Instruction ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã¨ã‹ã€NLP2024ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‹ã‚‰ã€ä½œã£ã¦å­¦ã¶æ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« - ç’°å¢ƒæ§‹ç¯‰æ‰‹é †ã¨å®Ÿé¨“ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®å…¬é–‹ã¨ã‹ã€æ—¥æœ¬ã®LLMå±¤ã®åº•ä¸Šã’ã‚‚é€²ã‚€ã€‚ ã‚¢ãƒã‚¾ãƒ³ã®Bezosæ°ã‚‚æŠ•è³‡ã—ã¦ã„ã‚‹ã¨è©±é¡Œã¨ãªã£ãŸPerplexityã€AIæ¤œç´¢ãŒæ¬¡ã®ãƒ“ãƒƒã‚°ã‚¦ã‚¨ãƒ¼ãƒ–ã¨ã„ã†ã“ã¨ã‚‰ã—ã„ã€Googleã®AIæ¤œç´¢ã‚‚ç¢ºã‹ã«Perplexityé¢¨ã«ãªã£ã¦ã„ã‚‹ã—ã€Googleã®AIæ¤œç´¢ã¯æœ‰æ–™åŒ–ã¨ã„ã†å ±é“ã‚‚ã€‚ã‚‚ã¨ã‚‚ã¨Bingã£ã¦Perplexityã®ã‚ˆã†ãªä»•çµ„ã¿ã˜ã‚ƒãªã‹ã£ãŸã‹ã€‚Mixture-of-Depthsã¨ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ReFTã¨ã‹ã€GRIFFIN ã¨ã‹ã€æ–°ã—ã„LLMæœ€é©åŒ–ã®çŸ¥è¦‹ãŒæ¬¡ã€…ã«ã§ã¦ããŸãŒã€ãªã‚“ã¨ã„ã£ã¦ã‚‚ã€ä»Šé€±ã¯Chat Vectorã¨ã„ã†LLMã®è¶³ã—ç®—å¼•ãç®—ãŒã§ãã‚‹æŠ€è¡“ã€word2vecã®ã‚ˆã†ãªãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ãŒLLMã§ã‚‚ã§ãã‚‹ãªã‚“ã¦ã™ã”ã™ãã‚‹ã€‚Chat Vectorã§å¼·åŒ–ã—ã¦MoEã§ã€ã€ã¿ãŸã„ãªã®ãŒä¸»æµã«ãªã‚‹ã‹ã‚‚ã€‚Claude3ã§function callingãŒã‚µãƒãƒ¼ãƒˆã€ã•ã£ããlangchainã‹ã‚‰ã€Claude3ã‚’ã¤ã‹ã£ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ãŒå‡ºãŸã€ã¾ã‚èƒ½åŠ›ã‹ã‚‰ã—ã¦ãã†ãªã‚‹ã‚ãªã€‚BAAIã®MetaWormè«–æ–‡ã€ç·šè™«ã‚’ç ”ç©¶ã—ã¦ã€èº«ä½“æ€§ã®è¬ã‚’è§£æ±ºï¼ŸReranker ã§ã‚‚BAAIå‡ºã¦ããŸã—ã€ã‚‚ã†ä¸­å›½ã‚‚ã€åŠ›ä»»ã›ã§å„ªã‚ŒãŸLLMã‚’å‡ºã™ã ã‘ã§ã¯ãªãã€ç†è«–ã§ã‚‚ã€ã¨ã„ã†ã“ã¨ã‹ã€‚ä¸‰å€¤ã®BitNetã®æƒ…å‡¦ã®è§£èª¬ã€ã€Œç²¾åº¦ã®é€†è»¢ã€ã¨ã„ã†ã®ãŒã‚ã‚‹ã®ã‹ã€ã‚‚ã—æœ¬å½“ãªã‚‰ã°ã€ãŸã—ã‹ã«ã“ã‚Œã¯ã‚²ãƒ¼ãƒ ãƒã‚§ãƒ³ã‚¸ãƒ£ãƒ¼ã ãªã€‚


- ãƒ“ã‚¸ãƒã‚¹ã®å®Ÿå‹™ã§ã€Œå› æœã€ã‚’æ¨æ¸¬ã™ã‚‹ã¨ã„ã†ã“ã¨ by TJOã•ã‚“
	- https://tjo.hatenablog.com/entry/2024/02/28/174811
	- ã€Œã¨ã‚Šã‚ãˆãšãƒãƒ¼ã‚±ãƒƒãƒˆã®ä¸­ã«ãµã‚“ã‚ã‚Šã¨å­˜åœ¨ã™ã‚‹ã€ç³»ã®æŒ‡æ¨™ã«å¯¾ã—ã¦ã€ãã®ã‚ˆã†ãªãã¡ã‚“ã¨ã—ãŸå› æœæ¨è«–ã‚’è¡Œã†ã®ã¯çµæ§‹é›£ã—ã„å°è±¡ãŒã‚ã‚Šã¾ã™ã€‚
	- ä¸€ã¤ã®è€ƒãˆæ–¹ã¨ã—ã¦ã€Œæ™‚ç³»åˆ—çš„ãªå› æœæ€§ã€ã‚’ãµã‚ã£ã¨ã—ãŸä»£ç”¨å“ã¨ã—ã¦ç”¨ã„ã‚‹ã¨ã„ã†æ–¹æ³•ã‚‚ã‚ã‚Šå¾—ã‚‹ã¨æ€ã£ã¦ã„ã¾ã™ã€‚ãã†ã€VARãƒ¢ãƒ‡ãƒ«ã§ã™
	- å³ã¡å®Ÿéš›ã®å› æœã¯ã€Œè½é›·â†’é›·é³´ã€ã ãŒã€æ™‚ç³»åˆ—çš„ã«ã¯ã€Œï¼ˆè½é›·â†’ï¼‰ç¨²å…‰â†’é›·é³´ã€ãŒæˆç«‹ã™ã‚‹ã®ã§ä»£ç”¨å“ã«ãªã‚Šå¾—ã‚‹ã€ã¨ã„ã†
- ç¿»è¨³ãƒ¢ãƒ‡ãƒ«Honyaku-7b by AIXã‚µãƒˆã‚·
	- aixsatoshi/Honyaku-Multi-Translator-Swallow-ms7b
	- æ•°ç™¾ã€œæ•°åƒtokenã®æ–‡ç« ç¿»è¨³ 
	- è‹±æ—¥ã€æ—¥è‹±ç¿»è¨³æ©Ÿèƒ½ãŒãƒ¡ã‚¤ãƒ³
	- XML like instruction
	- ä¸€éƒ¨ã®å¤šè¨€èªã‚‚å¯¾å¿œ
	- Swallow-ms-7b baseã§æ—¥æœ¬èªå ªèƒ½
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®ãŸã‚ã®æ—¥æœ¬èª Instruction ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã®å–ã‚Šçµ„ã¿
	- https://speakerdeck.com/kunishou/da-gui-mo-yan-yu-moderukai-fa-notamenori-ben-yu-instruction-detasetutozuo-cheng-noqu-rizu-mi
- ã€OpenAIã€‘æ—¥æœ¬ã«ã‚¢ã‚¸ã‚¢åˆã®æ‹ ç‚¹ã‚’é–‹è¨­ã€æ³•äººå‘ã‘ã‚µãƒ¼ãƒ“ã‚¹æä¾›ã¸
	- https://www.nikkei.com/article/DGXZQOUC29A7U0Z20C24A3000000/?n_cid=SNSTW001&n_tw=1711923970
	- OpenAIãŒ4æœˆä¸­ã«æ±äº¬éƒ½å†…ã«ã‚¢ã‚¸ã‚¢åˆã®æ‹ ç‚¹ã‚’ç«‹ã¡ä¸Šã’ã€æ—¥æœ¬ã§ã®äº‹æ¥­æ´»å‹•ã‚’æœ¬æ ¼åŒ–ã•ã›ã‚‹
	- äº‹å‹™æ‰€ã¯è¥¿æ–°æ©‹ã®é›‘å±…ãƒ“ãƒ«ï¼Ÿï¼Ÿ
-  Mechanistic Design and Scaling of Hybrid Architectures
	- https://arxiv.org/abs/2403.17844
	- LLMã®ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã¯æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ã€‚ã“ã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚äººå·¥çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¿ã‚¹ã‚¯ MADï¼ˆin-context recall, compressionç­‰ï¼‰ã‚’è¨­è¨ˆã€‚å°è¦æ¨¡MADã§è©•ä¾¡ã—ãŸçµæœã‚’å…ƒã«æœ‰æœ›ãªæ‰‹æ³•ã‚’çµã‚Šã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹ã€‚å¤šããŒå°è¦æ¨¡MADã®æ€§èƒ½ã¨ã‚¹ã‚±ãƒ¼ãƒ«å¾Œã®æ€§èƒ½ã«ç›¸é–¢ãŒã¿ã‚‰ã‚ŒãŸ
- LLMã®ç¾åœ¨
	- https://speakerdeck.com/pfn/llmnoxian-zai
-  MetaWorm: A Complete Model Bridging Brain, Body and Environment of  _C. elegans_
	- https://www.biorxiv.org/content/10.1101/2024.02.22.581686v1
	- BAAIã®ç ”ç©¶ã€ç”Ÿç‰©ã®è„³ã€èº«ä½“ã€ç’°å¢ƒã®é–“ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’ç·šè™«ï¼ˆC. eleganï¼‰ã‚’ææ–™ã«è§£æ
- ã€ŒBabylon.js 7.0ã€æ­£å¼ãƒªãƒªãƒ¼ã‚¹ã€‚
	- https://www.publickey1.jp/blog/24/web3dbabylonjs_70mmdmikumikudanceapple_vision_pro.html
	- ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã¯ã€Webãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§2Dã‚„3Dãƒ¢ãƒ‡ãƒ«ã®é«˜é€Ÿãªãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãªã©ã‚’å¯èƒ½ã«ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®JavaScriptãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ŒBabylon.jsã€ã®æœ€æ–°ç‰ˆã€ŒBabylon.js 7.0ã€æ­£å¼ç‰ˆã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚
	- MMDï¼ˆMikuMikuDanceï¼‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åˆ©ç”¨ã‚’å¯èƒ½ã«ã™ã‚‹MMDãƒ­ãƒ¼ãƒ€ãƒ¼ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚IKã‚½ãƒ«ãƒã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªåŒæœŸå†ç”Ÿã€ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãªã©ã®æ©Ÿèƒ½ã‚‚ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
- CMUã‚‚Stanfordã‚‚Columbiaã‚‚CSä¿®å£«ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³å†…å®šç‡2å‰²
	- https://x.com/fzw1212/status/1774218929100988506
-  LLaMA Now Goes Faster on CPUs
	- https://justine.lol/matmul/
	- 84 new matrix multiplication kernels for llamafile
	- between 30% and 500% faster when using F16 and Q8_0 weights on CPU. 
- Gecko: Versatile Text Embeddings Distilled from Large Language Models
	- https://huggingface.co/papers/2403.20327
	- Googleã‹ã‚‰ã€Geckoçµ„ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€LLMã‚’è’¸ç•™ã—ãŸï¼Ÿï¼Ÿè¬
	- LLMã‚’ä½¿ã£ã¦å­¦ç¿’ç”¨ã®ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦Embedding Modelã®å­¦ç¿’ã‚’ã—ãŸå¾Œã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã«å¯„ã‚Šå¾—ã‚‰ã‚ŒãŸé–¢é€£ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã«åŒã˜LLMã§Positive/Hard Negativeã®ãƒ©ãƒ™ãƒ«ã‚’æŒ¯ã‚Šç›´ã—ã¦è¿½åŠ å­¦ç¿’ã—ã¦ã„ã‚‹ã‚‰ã—ã„ã€‚
- LMFlowã§Llama2-70Bã®LISAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã‚ã£ã•ã‚Šå‹•ã„ãŸ by shi3zã•ã‚“
	- https://x.com/shi3z/status/1774710763007119735
	- å¤§ä½“30GBã‚ã‚Œã°å­¦ç¿’ã§ãã‚‹ã¨ã™ã‚Œã°A6000ã§ã‚‚å¯èƒ½ã¨ã„ã†ã“ã¨ã‹?
-  Google Colab ã§ BAAI/bge-reranker-v2-m3 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n7d251f76ce25?sub_rt=share_h
	- ã€ŒBAAI/bge-reranker-v2-m3ã€ã¯ã€ã€Œbge-m3ã€ãƒ™ãƒ¼ã‚¹ã®ã€ŒRerankerã€ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã€ŒRerankerã€ãƒ¢ãƒ‡ãƒ«ã¯ã€å¾“æ¥ã®ã€ŒåŸ‹ã‚è¾¼ã¿ã€ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€è³ªå•ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€é¡ä¼¼åº¦ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
	- ã€Œãƒ‘ãƒ³ãƒ€ã¨ã¯ï¼Ÿã€ã®è³ªå•ã«ã¯ã€Œãƒ‘ãƒ³ãƒ€ã¯ä¸­å›½å—è¥¿éƒ¨ã®å±±å²³åœ°å¸¯ã«ç”Ÿæ¯ã™ã‚‹å“ºä¹³é¡ã®ä¸€ç¨®ã§ã™ã€‚ã€ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé–¢é€£ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
- Building a RAG application using open-source models by langchain
	- https://x.com/LangChainAI/status/1774821270900629950
	- https://github.com/svpino/llm/blob/main/local.ipynb
	- https://www.youtube.com/watch?v=HRvyei7vFSM
- Claudeã ã¨æœ¬å½“ã«ä¸€ç¬ã§ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã‚’ä½œã£ã¦ãã‚Œã‚‹ã€‚
	- https://x.com/ai_syacho/status/1774677348807483788
- Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians
	- https://github.com/city-super/Octree-GS
	- https://x.com/janusch_patas/status/1774717184238883237
- ã€Googleæ¤œç´¢ã‚’è¶…ãˆã‚‹è¡æ’ƒã®ç”ŸæˆAIå‹æ–°æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼šPerplexity ProãŒæƒ…å ±åé›†ã‚’å¤‰ãˆã‚‹ï¼ã€
	- https://x.com/tetumemo/status/1774632484648730889
	- Perplexity ã®ã‚ˆã†ãªAIæ¤œç´¢ãŒã€ã¤ãã®ãƒ“ãƒƒã‚°ã‚¦ã‚¨ãƒ¼ãƒ–ã¨ã„ã†ã‹ã€active personal noteã ã‚ˆãªã€‚AIæ¤œç´¢ã‚’æœ‰æ–™ã«ã™ã‚‹ã¨ã„ã†å‹•ãã‚‚ã‚ã‚‹ã€‚
- BItNet-Transformerã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹
	- 1bitLLM/bitnet_b1_58-large
- NLP2024 ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼“: ä½œã£ã¦å­¦ã¶æ—¥æœ¬èªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« - ç’°å¢ƒæ§‹ç¯‰æ‰‹é †ã¨å®Ÿé¨“ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
	- https://github.com/hiroshi-matsuda-rit/NLP2024-tutorial-3
	- æ—¥æœ¬èªLLMã®å­¦ç¿’ãƒ»è©•ä¾¡ã«ç”¨ã„ã‚‰ã‚Œã‚‹æŠ€è¡“ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦åºƒãå–ã‚Šä¸Šã’ã¦ã„ã¾ã™ã€‚æ˜¯éã”è¦§ãã ã•ã„ã€‚
	- è¬›æ¼”ã‚¹ãƒ©ã‚¤ãƒ‰ã¨å®Ÿé¨“ç’°å¢ƒæ§‹ç¯‰æ‰‹é †ãƒ»ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯GitHubãƒªãƒã‚¸ãƒˆãƒªã§å…¬é–‹ã—ã¦ã„ã¾ã™
	- ãƒªã‚¯ãƒ«ãƒ¼ãƒˆã®æ¾ç”°å¯›ã•ã‚“
- æ±å·¥å¤§ã®Swallow MX 8x7bã¯ç¾çŠ¶ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã¯æ—¥æœ¬èªæœ€é«˜ã®ãƒ¢ãƒ‡ãƒ«ã ã‚ã†ã­â€¦
	- https://x.com/Meteor_Eternal/status/1775096408435216766
- OSS Models + LangGraph.js
	- LangGraph helps you create LLM apps that closely match the logical flows used to solve a problem.
	- https://github.com/langchain-ai/langgraphjs/blob/main/examples/chatbots/customer_support_mistral.ipynb
-  Mamba Explained
	- https://thegradient.pub/mamba-explained/
	- Mambaã®é¸æŠæ©Ÿæ§‹ã‚’æ³¨æ„æ©Ÿæ§‹ã¨ã®æ¯”è¼ƒã‚„ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ã‚’ç”¨ã„ãªãŒã‚‰ç›´æ„Ÿçš„ã«èª¬æ˜ã—ãŸè¨˜äº‹ã€‚ 
	- æ–‡è„ˆå†…å­¦ç¿’ã§ã¯Transformerã®ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å…¨ã¦ã®æƒ…å ±ã‚’å…¥ã‚Œã‚‹å¿…è¦ãŒãªãã€çŠ¶æ…‹ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã©ã‚’åœ§ç¸®ã—ãŸã‚‚ã®ï¼‰ã¨è³ªå•ã‚’æ¸¡ã™ã ã‘ã§è‰¯ã„ã€‚
- Bigger is not Always Better: Scaling Properties of Latent Diffusion Models by Google
	- https://huggingface.co/papers/2404.01367
- Prompt-prompted Mixture of Experts for Efficient LLM Generation
	- https://arxiv.org/abs/2404.01365
	- LLM ã¸ã®å…¥åŠ›ã”ã¨ã«ã€LLMã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®ç›¸å¯¾çš„ãªå¤§ãã•ãŒã€ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã«ã‚ˆã‚‰ãšä¸€éƒ¨ã®æ¬¡å…ƒã«åã‚‹ flocking ã¨ã„ã†ç¾è±¡ã‚’ç™ºè¦‹ã—ã€ã“ã‚Œã‚’ã‚‚ã¨ã«ã€
	-  (1) prompt å…¥åŠ›æ¬¡ç‚¹ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãŒç›¸å¯¾çš„ã«å¤§ãã„æ¬¡å…ƒã‚’ç‰¹å®š
	-  (2) ãã®æ¬¡å…ƒã®ã¿ã‚’ä½¿ã£ã¦è¿‘ä¼¼çš„/åŠ¹ç‡çš„ã« Decode ã‚’è¡Œã†ã€
	- GRIFFIN (Gating by Repetition In Feedf orward Intermediate Neurons) ã‚’ææ¡ˆã€‚ ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã™ã‚‹ãŒã€å­¦ç¿’ä¸è¦ãªæ–¹æ³•ã§ç²¾åº¦ã‚’ã‚ã¾ã‚Šè½ã¨ã•ãšç”Ÿæˆã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã€‚
-  Are large language models superhuman chemists?
	- https://arxiv.org/abs/2404.01475
	- ã€ŒåŒ–å­¦åˆ†é‡ã®å¹…åºƒã„ 7,000 ä»¥ä¸Šã®è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’å³é¸ã—ã€ä¸»è¦ãªLLM ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚ãã®çµæœã€ç§ãŸã¡ã®ç ”ç©¶ã§ã¯ã€æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ãŒå¹³å‡ã—ã¦æœ€è‰¯ã®äººé–“ã®åŒ–å­¦è€…ã‚’ä¸Šå›ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸã€
-  LlamaIndex ã® Reranker ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n8f9ee8533896?sub_rt=share_h
	- RAGã«ãŠã‘ã‚‹ã€ŒRerankerã€ã¯ã€å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®ä¸­ã‹ã‚‰ã€è³ªå•ã«å¯¾ã—ã¦æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’æŒã¤ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠã™ã‚‹å½¹å‰²ã‚’æ‹…ã£ã¦ã„ã¾ã™ã€‚
	- ä»Šå›ã¯ã€å¤šè¨€èªã®Rerankerãƒ¢ãƒ‡ãƒ«ã€Œ**BAAI/bge-reranker-v2-m3**ã€ã‚’ä½¿ã„ã¾ã™ã€‚top_n=5ã§é–¢é€£æ€§ã®é«˜ã„5ä»¶ã«çµã‚Šã¾ã™ã€‚
-  Semantic Routerã‚’è©¦ã™
	- https://zenn.dev/kun432/scraps/73b098e774bd21
	- LLMã‚„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ„æ€æ±ºå®šã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¡Œã†Semantic Routerã‚’è©¦ã—ã¦ã¿ãŸã€‚ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã ã‘ã˜ã‚ƒãªãã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã«ã‚‚ä½¿ãˆã‚‹ã€‚ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä½¿ã„æ–¹ã¯ã„ã‚ã„ã‚ãªå¯èƒ½æ€§ãŒã‚ã‚Šãã†ã€‚
	- ã‚¯ã‚¨ãƒªã§å‡¦ç†ã‚’åˆ†å²ã•ã›ãŸã„ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã¯ã€Function Callingã‚’ä½¿ã£ã¦LLMã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã•ã›ã‚‹ã¨ã‹ãŒã‚ã‚‹ã¨æ€ã†ã®ã ã‘ã©ã€äº‹å‰ã«ã‚¯ã‚¨ãƒªã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„ã—ã¦ãŠã„ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã•ã›ã‚‹ã¨ã„ã†ã‚ˆã†ãªã‚‚ã®ã€‚
	- LangChainã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨çµ„ã¿åˆã‚ã›ãŸä¾‹ã€‚
- 4/23(ç«)ã«ã€Sakana AIåˆã®ã‚¤ãƒ™ãƒ³ãƒˆã‚„ã‚Šã¾ã™ï¼Grow-AIã€Arayaã®æ–¹ã€…ã¨æˆ‘ã€…ã®ãƒˆãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã™
	- https://x.com/iwiwi/status/1775367258040410519
- 2x7Bã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ»ãƒãƒ™ãƒ«å°‚ç”¨é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€‚
	- https://huggingface.co/Sdff-Ltba/LightChatAssistant-2x7B
	- Antler-7Bã¨chatntq-ja-7b-v1.0ã¨ã„ã†ã€Japanese Stable LM Base Gamma 7Bï¼ˆMistral 7Bãƒ™ãƒ¼ã‚¹ï¼‰ã‚’instructionãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å„ã€…ChatVectoræ³•ã§å¼·åŒ–ã—ã€MoEã§ãƒãƒ¼ã‚¸ã—ãŸã®ã ãã†ã 
- RankZephyr is a nice 7B model 
	- https://arxiv.org/pdf/2312.02724.pdf
	- that is optimized for list-wise zero-shot reranking
	- https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/rankLLM/?h=rankllm
- intelligent notetaking by https://iki.ai/
	- https://iki.ai/
	- a cool example of an AI-enabled notetaking interface that epitomizes the core value prop of RAG - dump in a ton of your messy, unstructured data (files, links, notes), and have the application organize and surface information for you instead of you having to do it yourself.
-  Google Colab ã§ japanese-reranker-cross-encoder-large-v1 ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n906b23636ac8?sub_rt=share_h
	- ã€Œ japanese-reranker-cross-encoder-large-v1ã€ã¯ã€æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸå½¢ã§å­¦ç¿’ã—ãŸã€ŒRerankerã€ã§ã™ã€‚xsmallã‹ã‚‰largeã¾ã§è¤‡æ•°ã®ã‚µã‚¤ã‚ºãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€ã€Œlargeã€ã¯å¤šè¨€èªRerankerã§æœ€ã‚‚äººæ°—ã®ã‚ã‚‹ã€Œbge-reranker-v2-m3ã€ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚
	- ã‚¯ã‚¨ãƒªã¨æ–‡ç« ã®æº–å‚™ã¨ã€ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã€‚
- Anthropic Messages API
	- https://x.com/AnthropicAI/status/1775979799644934281
	-  Claude3ã«function callãŒæ¥ãŸã¨ã„ã†è©±
- Cohereã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…¬é–‹LLMã®Command R+
	- https://x.com/_kaiinui/status/1775920565720949090
	- ãŸã—ã‹ã«GPT4-Turboã¨æ¯”è¼ƒã—ã¦ã‚‚ã‚ˆã„ãƒ¬ãƒ™ãƒ«ã®LLMã«è¦‹ãˆã‚‹
	- ã‚µã‚¤ã‚ºã¯104Bã€CC-BY-NCã ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯HFã§å…¬é–‹ 104Bå‹•ã‹ã›ã‚‹ãƒã‚·ãƒ³ãŒã‚ã‚Œã°ã€ã ã‚Œã§ã‚‚çŸ¥èƒ½(ã‚‰ã—ãã‚‚ã®)ã‚’ä¿æœ‰ã§ãã‚‹ã£ã¦ã‚„ã°ã„ãª
- Command R+ by Cohere
	- https://txt.cohere.com/command-r-plus-microsoft-azure/
	- https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus
	- ã ã ã‚‚ã®ã§ã¯ãªã„ã€‚
-  LlamaIndex <> MistralAI Cookbooks
	- https://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex
	- Hereâ€™s a definitive set of cookbooks to build simple-to-advanced RAG, agentic RAG, and agents in general with MistralAI
- Groq tool calling + structured output by langchain
	- https://python.langchain.com/docs/modules/model_io/chat/structured_output/#groq
	- GroqInc just dropped tool calling!
	- We've added LangChain support (including the popular `withStructuredOutput` method!) so you can try it in your favorite chains and apps.
	- It supports MistralAI, Mixtral, Llama 70B, and Google Gemma.
-  Chat Vectorã‚’ä½¿ã£ã¦æ—¥æœ¬èªLLMã‚’ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã«æ”¹é€ ã™ã‚‹
	- https://qiita.com/jovyan/items/ee6affa5ee5bdaada6b4
	- Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages
	- LLMã®å­¦ç¿’æ¸ˆã¿é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¶³ã—å¼•ãã«ã‚ˆã£ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾è©±èƒ½åŠ›ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†çµæœãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
	- å…·ä½“çš„ã«ã¯ã€è‹±èªã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆä»¥ä¸‹ã§ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨å‘¼ã³ã¾ã™ï¼‰ã¨ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (instruction tuning)ã—ã¦ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å¯¾è©±ãŒã§ãã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆè‹±èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è‹±èªä»¥å¤–ã®è¨€èªã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ï¼“ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¾ã™ã€‚
	- è‹±èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å¼•ã„ãŸã‚‚ã®ã¯ã€ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§å¯¾è©±ãŒã§ãã‚‹èƒ½åŠ›ã‚’è¡¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚Šã€ãã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä»–è¨€èªã®ç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã«åŠ ãˆã‚‹ã“ã¨ã§ä»–è¨€èªã®ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å¯¾è©±èƒ½åŠ›ã‚’ä»˜ä¸ã§ãã‚‹ã¨ã„ã†
- Microsoft Encarta '97 (including MindMaze) has been open-sourced on
	- https://ia902707.us.archive.org/view_archive.php?archive=/2/items/enc-97-enc/ENC97ENC.iso
-  JetMoE: Reaching LLaMA2 Performance with 0.1M Dollars
	- https://research.myshell.ai/jetmoe
	- JetMoE-8B is trained with less than $ 0.1 million cost but outperforms LLaMA2-7B from Meta AI, who has multi-billion-dollar training resources. LLM training can be much cheaper than people generally thought.
-  ReALM: Reference Resolution As Language Modeling
	- https://arxiv.org/abs/2403.20329
	- Apple's 3B LLM(ReALM ) Outperforms GPT-4
	- ReALM significantly improves how conversational assistants like Siri or Alexa can understand the way humans naturally talk. Imagine you're looking at a list of restaurants on your smartphone and you say "direct me to the one on Main Street" -
	-  ReALM is able to understand which restaurant you're referring to, even though you didn't specify the exact name.
-  Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models
	- https://huggingface.co/papers/2404.02575
	- This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps.
	- (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; 
	- (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code.
- Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
	- https://arxiv.org/abs/2404.02258
	- Dynamically allocating compute in transformer-based language models
	- Same performance w/ a fraction of the FLOPs per forward pass
-  1bit LLM ã®æ™‚ä»£ã¯æ¥ã‚‹ã®ã‹ï¼Œæ¥ãªã„ã®ã‹ï¼Œã©ã£ã¡ãªã‚“ã ã„ï¼Ÿ
	- https://note.com/ipsj/n/ncbe5746f71fb
	- ä¸‰å€¤ã®BitNetã«ã¤ã„ã¦ã€æƒ…å ±å‡¦ç†å­¦ä¼šã®ä¼šèªŒã«è§£èª¬ã‚’æ›¸ã‹ã›ã¦ã„ãŸã ãã¾ã—ãŸ
	- ã€Œãƒ¢ãƒ‡ãƒ«ã‚’å¤§ããã™ã‚‹ã¨ç²¾åº¦ã®é€†è»¢ç¾è±¡ãŒèµ·ã“ã‚‹ã®ã ã¨ã™ã‚‹ã¨ï¼Œé‡å­åŒ–ã¨ã„ã†ã®ã¯ã“ã‚Œã¾ã§æƒ³å®šã•ã‚Œã¦ã„ãŸã‚ˆã‚Šã‚‚ã‹ãªã‚Šå„ªã‚ŒãŸã‚¢ã‚¤ãƒ‡ã‚£ã‚¢ãªã®ã§ã¯ãªã„ã‹ï¼Ÿã€
	- b1.58è«–æ–‡ã®ä¸­èº«ã«ã¤ã„ã¦è§£èª¬ã—ã¦ãã¾ã—ãŸãŒï¼Œã„ã‹ãŒã§ã—ãŸã§ã—ã‚‡ã†ã‹ï¼å€‹äººçš„ã«ã¯ï¼Œã“ã®è«–æ–‡ã«ã¯è³›å¦ä¸¡è«–ãŒã‚ã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ï¼
	- è‚¯å®šçš„ãªè¦‹åœ°ã‹ã‚‰ã¯ï¼Œç²¾åº¦ã®é€†è»¢ç¾è±¡ãŒæœ¬å½“ãªã‚‰ã°å¤§ããªç™ºè¦‹ã§ã‚ã‚Šï¼Œè‡ªç„¶è¨€èªå‡¦ç†åˆ†é‡ã¸ã®å¤§ããªè²¢çŒ®ã¨ãªã‚Šå¾—ã‚‹
- Cohere's latest LLM, Command R+ ãŒAzureã«ã®ã‚‹ by Nadera
	- https://x.com/satyanadella/status/1775988939079450886
- Anthropic Tool Calling by langchain
	- https://python.langchain.com/docs/integrations/chat/anthropic/#beta-tool-calling
- Command R+ã‚ªãƒ¼ãƒ—ãƒ³ç³»ã¨ã—ã¦ã¯æ´’è½ã«ãªã‚‰ã‚“ã»ã©çŸ¥æ€§ã‚’æ„Ÿã˜ã‚‹
	- https://x.com/_kaiinui/status/1775928745775534189
-  OpenAI ã® ãƒ•ã‚¡ã‚¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°API ã®æ–°æ©Ÿèƒ½ by npakaã•ã‚“
	- https://note.com/npaka/n/ne41cba4111a0?sub_rt=share_h
	- 2024å¹´4æœˆ4æ—¥ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°APIã«æ–°æ©Ÿèƒ½ãŒå°å…¥ã•ã‚Œã¾ã—ãŸã€‚
	- OpenAIã‚’åˆ©ç”¨ã™ã‚‹ã¨ã€ã»ã¨ã‚“ã©ã®çµ„ç¹”ã¯ã‚»ãƒ«ãƒ•ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°API ã‚’ä½¿ç”¨ã—ã¦ã€æœ‰æ„ç¾©ãªçµæœã‚’ã™ãã«ç¢ºèªã§ãã¾ã™ã€‚
- AppleãŒé–‹ç™ºã€ã‚¹ãƒãƒ›ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚’ç†è§£ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å¯¾è©±ã§ãã‚‹ã€ReALMã€ç«¯æœ«ä¸Šã§å‹•ãè»½é‡ãƒ¢ãƒ‡ãƒ«
	- https://ai-data-base.com/archives/66828
	- Appleã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã‚„ã‚¹ãƒãƒ›ç”»é¢ã‚’é«˜åº¦ã«ç†è§£ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã€ReALMã€ã‚’ç™ºè¡¨ã—ã¦ã„ã¾ã™ã€‚Siriãªã©ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’é€²åŒ–ã•ã›ã‚‹æŠ€è¡“ã¨ã—ã¦ã®ä½ç½®ä»˜ã‘ã§ã™
- Command Rã¯ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã¦ã¯åˆã‚ã¦æ–‡ç« ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§ããŸã‹ã‚‚ã—ã‚Œã‚“
	- https://x.com/Meteor_Eternal/status/1775877913952518608
- Claude Function Calling Agent by langchain
	- https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/anthropic_agent.ipynb
- Generating text with 4-bit 104B âŒ˜R+ in MLX on an M2 Ultra. Runs pretty well:
	- https://x.com/awnihannun/status/1776081238467768493
- Command R ã® æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/naa6add7a892f?sub_rt=share_h
	- ã€ŒCommand Rã€ã¯ã€ã€ŒRAGã€ã‚„ã€ŒToolã€ãªã©ã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸLLMã§ã™ã€‚Cohereã®EmbeddingãŠã‚ˆã³Rerankã¨é€£æºã—ã¦å‹•ä½œã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«æœ€é«˜ã‚¯ãƒ©ã‚¹ã®çµ±åˆã‚’æä¾›ã—ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¦ãƒ¼ã‚¹ ã‚±ãƒ¼ã‚¹ã§å„ªã‚Œã¦ã„ã¾ã™ã€‚
	- ãƒ»RAGã¨Toolã®ä½¿ç”¨ã«é–¢ã™ã‚‹é«˜ã„ç²¾åº¦
	- ãƒ»ä½é…å»¶ã€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
	- ãƒ»128Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã€ä¾¡æ ¼ãŒå®‰ã„
	- ãƒ»10ã®ä¸»è¦è¨€èªã«å¯¾å¿œ (æ—¥æœ¬èªå«ã‚€)
	- ãƒ»ç ”ç©¶ãƒ»è©•ä¾¡ã®ãŸã‚ã«HuggingFaceã§ã‚¦ã‚§ã‚¤ãƒˆã‚’å…¬é–‹
	- https://huggingface.co/CohereForAI
- Mistral 7Bãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ« ChatNTQ-JA-7B ã‚’è©¦ã™
	- https://sc-bakushu.hatenablog.com/entry/2024/04/04/091521
	- ã€Œchatntq_chatvector-MoE-Antler_chatvector-2x7Bchatntq_chatvector-MoE-Antler_chatvector-2x7Bã€ã¨ã„ã†å‘ªæ–‡ã®ã‚ˆã†ãªæ—¥æœ¬èªMoEãƒ¢ãƒ‡ãƒ«
	- https://huggingface.co/Sdff-Ltba/LightChatAssistant-2x7B
	- Mistral 7Bãƒ™ãƒ¼ã‚¹ã®ã€ŒJapanese Stable LM Base Gammaã€ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ³ã—ãŸ2ã¤ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆAntler 7B, ChatNTQ-JA-7Bï¼‰ã‚’2x7Bã®MoEã«ã—ãŸãƒ¢ãƒ‡ãƒ«ã ãã†ã§ã™ã€‚
	- ã“ã®MoEãƒ¢ãƒ‡ãƒ«ã‚’æ—©é€Ÿè©¦ã—ã¦ã¿ãŸã¨ã“ã‚ã€ç¢ºã‹ã«è³¢ãã†ãªå°è±¡ã‚’å—ã‘ã¾ã—ãŸã€‚ãŸã ã€ãã‚‚ãã‚‚ãƒ™ãƒ¼ã‚¹ã«ã•ã‚Œã¦ã„ã‚‹2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’èã„ãŸã“ã¨ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
- pfnet/nekomata-14b-pfn-qfin
	- https://huggingface.co/pfnet/nekomata-14b-pfn-qfin
	- rinnaç¤¾ã®nekomata-14bã‚’é‡‘èå‘ã‘ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸLLMã‚’å…¬é–‹ã—ã¾ã—ãŸï¼ ã“ã‚Œã¯ã€ã¾ã ã¾ã é‡‘èåˆ†é‡ã¸ã®LLMå¿œç”¨ã«ã¤ãªãŒã‚‹ç¬¬ä¸€æ­©ã§ã—ã‹ãªã„ã¨æ€ã†ã®ã§ã€ã‚‚ã£ã¨ç ”ç©¶é–‹ç™ºã‚’é€²ã‚ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚
- Qwen1.5-32B release
	- https://github.com/QwenLM/Qwen1.5
	- Qwen1.5 72B has been the best open model on Chatbot Arena leaderboard. Very excited to see how the 32B performs!
- ã‚¸ã‚§ãƒ•ãƒ»ãƒ™ã‚¾ã‚¹ãŒPerplexityã«æŠ•è³‡
	- https://x.com/npaka123/status/1776352622704042408
- llama.cppé‡å­åŒ–ï¼šé‡è¦åº¦è¡Œåˆ—(Importance Matrix)è¨ˆç®—ã«ä½¿ã†ãƒ†ã‚­ã‚¹ãƒˆã«ã¤ã„ã¦
	- https://sc-bakushu.hatenablog.com/entry/2024/03/30/195557
	- ç¾åœ¨ã®llama.cppã§ã¯é‡è¦åº¦è¡Œåˆ—(Importance Matrix)è¨ˆç®—ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§[é‡å­åŒ–](https://d.hatena.ne.jp/keyword/%CE%CC%BB%D2%B2%BD)ç²¾åº¦ãŒæ”¹å–„ã§ãã¾ã™ã€‚
	- ç‰¹ã«4bitä»¥ä¸‹ã®ä½bit[é‡å­åŒ–](https://d.hatena.ne.jp/keyword/%CE%CC%BB%D2%B2%BD)ã‚’è¡Œã†å ´åˆã¯ã€ã“ã®iMatrixç‰ˆã®[é‡å­åŒ–](https://d.hatena.ne.jp/keyword/%CE%CC%BB%D2%B2%BD)ãŒæ¨å¥¨ã•ã‚Œã¾ã™
- Apple MLX: Qwen-32B is out and now converted for MLX in 4 and 8 bits flavors.
	- https://x.com/ivanfioravanti/status/1776327090452738315
- ReFT: Representation Finetuning for Language Model
	- https://arxiv.org/abs/2404.03592
	- LoRAã®ã‚ˆã†ã«weightã«ä»‹å…¥ã™ã‚‹ fine tuning ã§ã¯ãªãã€æ½œåœ¨ï¼ˆä¸­é–“ï¼‰è¡¨ç¾ã«ä»‹å…¥ã™ã‚‹ fine tuning ã§ã‚ã‚‹ã€ReFT (Representation Finetuning) ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãã®ä¸€ã¤ã®å®Ÿç¾ä¾‹ã§ã‚ã‚‹ Low-rank Linear Subspace ReFT (LoReFT) ã®ææ¡ˆã€‚
- google/gemma-1.1-7b-it
	- https://huggingface.co/google/gemma-1.1-7b-it
	- This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.
- HachiML/Swallow-MS-7b-v0.1-MathSkill-OpenMath
	- https://huggingface.co/HachiML/Swallow-MS-7b-v0.1-MathSkill-OpenMath
	- Chat Vectorã®ç†è«–ã§ä½œã£ãŸMathå¼·åŒ–ãƒ¢ãƒ‡ãƒ«ã€HuggingFaceã«ç½®ãã¾ã—ãŸ
- 

.



## 4/1

ä»Šé€±ã¯å¹´åº¦æœ«ã€æ—¥æœ¬ä¼æ¥­ã®LLMãŒå¹´åº¦æœ«å·¥äº‹ã‚ˆã‚ã—ãæ¬¡ã€…ç™ºè¡¨ã€‚å…ˆé€±å–ã‚Šã“ã¼ã—ãŸã€Rakuteã®Mistral AIãƒ™ãƒ¼ã‚¹ã®RakutenAI 7Bç­‰ã«åŠ ãˆã€ä»Šé€±ã¯NTTãŒé–‹ç™ºã—ãŸã€Œtsuzumiã€ã¯æ—¥æœ¬èªã¨è‹±èªã«å¯¾å¿œã™ã‚‹70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®LLMã€‚LLMã®æ—¥æœ¬èªå‡¦ç†èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒRakuda Benchmarkã€ã«ãŠã„ã¦ã€GPT-3.5ã‚„ãã®ä»–ã®å›½ç”£LLMã‚’ä¸Šå›ã‚‹æ€§èƒ½ã§ã€å›³è¡¨ã‚„ç”»åƒã®è§£æã«ã‚‚å¯¾å¿œã¨ã®ã“ã¨ã€NTT comãŒç”ŸæˆAIã‚µãƒ¼ãƒ“ã‚¹ã‚’å±•é–‹ã¨ã„ã†ã“ã¨ãªã®ã§ã€ç”ŸæˆAIã®ãƒ“ã‚¸ãƒã‚¹å¿œç”¨å…ƒå¹´ã«ãªã‚‹ã®ã‹ã€‚Databricksã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸã€ŒDBRXã€ã¯ã€132å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤å¤§è¦æ¨¡ãªMoE(Mixture of Experts)ãƒ¢ãƒ‡ãƒ«ã§ã€æ—¢å­˜ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã€‚LLaMA2-70Bã‚ˆã‚Šã‚‚é«˜é€Ÿãªæ¨è«–ãŒå¯èƒ½ã§ã€Grok-1ã‚ˆã‚Šã‚‚ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãªãŒã‚‰é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚NTTã®tsuzumiã®è³‡æ–™ã«ã‚ˆã‚‹ã¨ã€AWSã§7Bãƒ¢ãƒ‡ãƒ«ã‚’300Bãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ã•ã›ã‚‹ã¨1900ä¸‡å††ã‹ã‹ã¨ã®ã“ã¨ã§ã‚ã‚‹ãŒã€ãã‚‚ãã‚‚LLMã®ã‚¹ã‚±ãƒ¼ãƒ«æ¸¬ã‹ã‚‰ã™ã‚‹ã¨ã€æŠ•è³‡ã«å¯¾ã™ã‚‹ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆç²¾åº¦å‘ä¸Šï¼‰ãŒè¦‹åˆã‚ãªããªã‚‹ã¨ã‚‚ã£ã±ã‚‰è©±é¡Œã«ã€‚ã¨ã¯ã„ãˆã€OpenAIã¨MicrosoftãŒæœ€å¤§1000å„„ãƒ‰ãƒ«ã‚’æŠ•ã˜ã¦ã€ŒStargateã€ã¨ã„ã†ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’2028å¹´ã¾ã§ã«ã¤ãã‚‹ã¨ã‹ã€ãƒ‘ãƒ¯ãƒ¼ãƒ—ãƒ¬ã‚¤ã¯ç¶šãã€‚ä¸€æ–¹ã€æ¨è«–ã®åŠ¹ç‡åŒ–ã§ã¯ã€Intel Neural Compressorã¯4ãƒ“ãƒƒãƒˆé‡å­åŒ–æŠ€è¡“ã§ã€LLMã®é«˜é€Ÿæ¨è«–ã¨åŠ¹ç‡çš„ãªè¨ˆç®—è³‡æºåˆ©ç”¨ãŒå¯èƒ½ã«ã€‚LoRAã«ä»£ã‚ã‚‹ã€ŒLISAã€ãŒå‡ºã¦ããŸã€‚LISAã¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ãªãŒã‚‰ã€å¾“æ¥æ‰‹æ³•ã¨åŒç­‰ä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã§ãã‚‹ã€‚OpenAIãŒã€ŒVoice Engineã€ã‚’é™å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç™ºè¡¨ã€å®‰å…¨æ€§ã¨æ€§èƒ½ã‚’é‘‘ã¿ã‚‹ã¨ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§LLMã¨ã„ã†ã®ã‚‚ã€é™å®šã•ã‚Œã¦ãã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚å€‹åˆ¥ã®è©±é¡Œã§ã¯ã€Googleã®ã€ŒCRAG(Corrective Retrieval Augmented Generation)ã€ã€å¾“æ¥ã®æ¤œç´¢æ‹¡å¼µç”Ÿæˆ(RAG)æ‰‹æ³•ã‚’æ”¹è‰¯ã—ã€RAGã‚·ã‚¹ãƒ†ãƒ ã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’LLMã«æ¸¡ã™å‰ã«ã€ãã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’è‡ªå‹•ã§ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ©Ÿèƒ½ã¤ã‘ã‚‹ã“ã¨ã§ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æŠ‘åˆ¶ã€‚BGE-M3ã¨ã„ã†Embeddingç”¨ãƒ¢ãƒ‡ãƒ«ãŒã€æ¤œç´¢ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡ã¯mE5ã‚ˆã‚Šã‹ãªã‚Šè‰¯ã„æ•°å€¤ã¨ã‹ã€æ—¥æœ¬èªRAGã«ä½¿ãˆã‚‹ã¨ã‹ã€ä¸­è¯LLMã¯æ—¥æœ¬èªãŒå¾—æ„ã€‚Mambaã‚’æ¡ç”¨ã—ãŸLLMã§åˆã‚ã¦ã®å¤§ããªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Jambaã€è©•ä¾¡ãŒå¾…ãŸã‚Œã‚‹ã€‚SD3ã®è«–æ–‡ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ Flow ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« ã¸ã¨ã„ã†ã“ã¨ã ãŒã€Reflective Flowã¨ã„ã†ãƒã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿ã®ã¤ãªãæ–¹ã®è©±ãŒèˆˆå‘³æ·±ã„ã€‚ãªãŠBenjioå…ˆç”Ÿã®Generative Flow Networkã¨ã¯åˆ¥ç‰©ã‚‰ã—ã„ã€‚è¡Œæ”¿ç³»ã§è¯ã€…ã—ãç™»å ´ã—ãŸè¡Œæ”¿æ‰‹ç¶šãï¼±ï¼†ï¼¡ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚ã‚‹GovBotã€ä½¿ã„ç‰©ã«ãªã‚‰ãªã„ã¨ã„ã†è©•ä¾¡ãŒæ¬¡ã€…ã«ã€äººã®æ­»ä½“ã‚’ã‚´ãƒŸã¨èªè­˜ï¼Ÿã€ã©ã†ã‚‚æ‹…å½“è€…ãŒæ­£ã—ãAIã‚’ç†è§£ã—ã¦ãªã„ã¨ã„ã†æ‚²ã—ã„äº‹æƒ…ã‚‚ã€ã²ã‚ã¿ã¡ã‚…å…ˆç”Ÿã«ã‚ˆã‚Šæ˜ã‚‰ã‹ã«ã€‚8,525ä¸‡ã§ãƒ‡ã‚¸åºã‹ã‚‰NECãŒå—æ³¨ã—ãŸã‚“ã ã‚ˆã­ã€ã“ã“ã¾ã§ãƒãƒªãƒœãƒ†ã¨ã¯ã€‚

- RAGã®æ–°ã—ã„æ‰‹æ³•ã€ŒCRAGã€ã‚’3åˆ†ã§ç†è§£ã™ã‚‹
	- https://zenn.dev/knowledgesense/articles/bb5e15abb3c547
	- ã€ŒCorrective Retrieval Augmented Generation (CRAG)ã€
	- RAGã®æ€§èƒ½ã‚’é«˜ã‚ã‚‹ãŸã‚ã®æ–°ã—ã„æ‰‹æ³•ã§ã™ã€‚Googleãªã©ã®ç ”ç©¶è€…ã«ã‚ˆã£ã¦2024å¹´2æœˆã«ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚CRAGï¼ˆæ—¥æœ¬èªã«ã™ã‚‹ã¨ã€Œä¿®æ­£å‹æ¤œç´¢æ‹¡å¼µç”Ÿæˆã€ï¼‰ã¨ã„ã†æ‰‹æ³•ã‚’ä½¿ã†ãƒ¡ãƒªãƒƒãƒˆã¯ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰ã‚’æ¸›ã‚‰ã›ã‚‹ã“ã¨ã§ã™ã€‚CRAGãŒå¾“æ¥ã®ã€ŒRAGã€ã‚ˆã‚Šã‚‚ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¸›ã‚‰ã›ã‚‹ç†ç”±ã¯ã€RAGã‚·ã‚¹ãƒ†ãƒ ã§å–å¾—ã—ã¦ããŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’LLMã«æ¸¡ã™å‰ã«ã€ã€Œãã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ãŒæ­£ã—ã„ã‚‚ã®ãªã®ã‹ã€è‡ªå‹•ã§ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã„ã†æ©Ÿèƒ½ã‚’å–ã‚Šå…¥ã‚Œã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚
-  AIã‚»ãƒ¼ãƒ•ãƒ†ã‚£æŠ€è¡“å­¦ä¼š
	- https://tais2024.cc/ja-jp/
	- AI Safetyï¼ŒAI Alignmentï¼Œç‰¹ç•°å­¦ç¿’ç†è«–ï¼Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼åŸç†ï¼ŒAIã®è‡ªå¾‹æ€§ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€§ï¼‰ç­‰ã€…ã®ãƒˆãƒ¼ã‚¯ã¨ãƒã‚¹ã‚¿ãƒ¼ç™ºè¡¨ï¼ã»ã¨ã‚“ã©è‡ªåˆ†ãŒèããŸã„ãƒ†ãƒ¼ãƒã ã‘ã§æ§‹æˆã•ã‚ŒãŸé­…åŠ›çš„ãªå›½éš›å­¦ä¼š
- æ—¥æœ¬èªç‰ˆï¼šAIOS LLM Agent Operating System
	- https://hamaruki.com/japanese-version-aios-llm-agent-operating-system/
	- ã“ã®è«–æ–‡ã§ã¯ã€LLMã‚’ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ (OS)ã«çµ„ã¿è¾¼ã‚“ã ã€ŒLLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ (AIOS)ã€ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ AISOã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦æœ€é©åŒ–ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚¤ãƒƒãƒã€ä¸¦åˆ—å®Ÿè¡Œã€ãƒ„ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹æä¾›ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ãªã©ã®æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
-  RAFT: Adapting Language Model to Domain Specific RAG
	- https://arxiv.org/abs/2403.10131
	- RAFT offers a method to fine-tune pre-trained LLMs for specific domain RAG settings.
	- Conventional RAG is like an open-book exam, retrieving documents from an index to provide context for answering queries. This makes it more effective than the closed-book exam setting where LLMs rely solely on their pre-training and fine-tuning to respond to prompts, but doesn't allow the LLM to learn the domain beforehand.
- NatComèªŒã€ãƒ“ãƒ¼ãƒ«ã®é¢¨å‘³ã¨ãŠã„ã—ã•ï¼ˆé£²ã‚“ã äººã®è©•ä¾¡ï¼‰ã‚’æ±ºå®šã™ã‚‹ç‰©è³ªã‚’250ã®ãƒ“ãƒ¼ãƒ«ã«å¯¾ã™ã‚‹18ä¸‡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã§è§£æ˜ã€‚
	- https://www.nature.com/articles/s41467-024-46346-0
	- è¤‡é›‘ãªå¿ƒç†ç¾è±¡ã«ã¤ã„ã¦ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¨æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦ä»®èª¬ãƒ•ãƒªãƒ¼ã§å½“ãŸã‚Šã‚’ã¤ã‘ã€ãã‚Œã‚’ã€Œä»®èª¬ã€ã¨ã—ã¦å®Ÿé¨“å®¤ã§æ¤œè¨¼å®Ÿé¨“ã‚’è¡Œã†ã€‚ã†ã‚‰ã‚„ã¾ã—ã„ã»ã©ãŠæ‰‹æœ¬ã®ã‚ˆã†ãªç¾ä»£çš„ç ”ç©¶ã€‚ãƒ“ãƒ¼ãƒ«ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã‚‹ã¨ã“ã‚ã‚‚ç²‹ã§ã‚ªãƒ¢ãƒ­ã„ã—ï¼
- NTTã®Tsuzumiã€7Bãƒ‘ãƒ©ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã§ã€Rakudaãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§GPT-3.5ã‚’ä¸Šå›ã‚‹ã‚“ã ã¨ã€‚
	- https://x.com/umiyuki_ai/status/1772588308537000101?s=20
- OpenAIãŒã€ŒVOICE ENGINEã€ã®åå‰ã§å•†æ¨™ã‚’å‡ºé¡˜
	- https://x.com/ctgptlb/status/1771005259948986562?s=20
-  RakutenAI-7B: Extending Large Language Models for Japanese
	- https://huggingface.co/papers/2403.15484
- MSã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒGPT-6ã‚¯ãƒ©ã‚¹ã‚¿ã®æ§‹ç¯‰ã«å–ã‚Šçµ„ã‚“ã§ã‚‹
	- https://x.com/_kaiinui/status/1772455514489672080?s=20
	- H100ã‚’10ä¸‡å°ä»¥ä¸Šé…å‚™ã—ã¦ã„ã‚‹ã‚‰ã—ãã€é›»åŠ›çš„ã«ä¸€ã¤ã®DCã«åã¾ã‚‰ãªããªã£ã¦ãã¦ã„ã‚‹ (â€»10ä¸‡å° = 70ãƒ¡ã‚¬ãƒ¯ãƒƒãƒˆ)
- The Unreasonable Ineffectiveness of the Deeper Layers
	- https://huggingface.co/papers/2403.17887
	- We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction
	- å²¡é‡åŸã•ã‚“ã€å­¦ç¿’æ¸ˆã¿ã®LLMã‹ã‚‰ã€å±¤æ¯ã«å…¥åŠ›ã¨å‡ºåŠ›é–“ã®cosé¡ä¼¼åº¦ãŒå¤§ãã„å±¤ï¼ˆå¤‰åŒ–ãŒå°‘ãªã„å±¤ï¼‰ã‚’é–“å¼•ã„ã¦ã‚‚ç²¾åº¦ã¯è½ã¡ãªã„ã€‚ç‰¹ã«æœ€å¾Œã®å±¤ã ã‘é™¤ã„ã¦æ·±ã„å´ã®å±¤ã‚’2~4å‰²é–“å¼•ã„ã¦ã‚‚è³ªå•å¿œç­”ãªã©ã®ç²¾åº¦ã¯å¤‰ã‚ã‚‰ãšã€çŸ¥è­˜ã®å¤§éƒ¨åˆ†ãŒä½ã„å±¤ã«ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã™ã‚‹ã€‚å­¦ç¿’æ‰‹æ³•ã‚„ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã®å‚è€ƒã«ã‚‚
	- ã¾ã‚æœ‰åãªæ˜ ç”»ã€å°èª¬ã®é¡Œåã®ã‚‚ã˜ã‚Š
- DeepLearningAIã‹ã‚‰ã€æ–°ã—ã„RAGã®ã‚³ãƒ¼ã‚¹ãŒ
	- https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/
	- JavaScript RAG Web Apps with LlamaIndex,
- LoRaã‚ˆã‚Šå„ªã‚ŒãŸLISA
	- https://x.com/Rui45898440/status/1772996453557997924?s=20
	- Excited to share LISA, which enables
	- 7B tuning on a 24GB GPU 
	- 70B tuning on 4x80GB GPUs
- Databricks introduces DBRX, a new 132B parameter open LLM
	- https://huggingface.co/databricks/dbrx-base
	- fine-grained mixture-of-experts (MoE) with 132B of which 36B active 
	- a larger number of smaller experts. DBRX has 16 experts and chooses 4 
	- It was pre-trained on 12T tokens of text and code data
	- DBRX outperforms all the established open-source models on common benchmarks like MMLU and GSM8K.
	- Its inference is up to 2x faster than LLaMA2-70B and is about 40% of the size of Grok-1 in terms of both total and active parameter counts.
	- While DBRX is trained as a general-purpose LLM, it still surpasses CodeLLaMa-70 Instruct, a model built explicitly for code generation.
- DBRX is super cool, but research and reading too! Especially if you can combine RAG + COT.
	- https://x.com/_philschmid/status/1773024623589736949?s=20
- we're connecting Adobe Experience Cloud with Microsoft Copilot to reimagine how marketers approach their daily work
	- https://x.com/satyanadella/status/1773063169138671984?s=20
-  LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
	- https://arxiv.org/abs/2403.17919
	- LISA algorithm in two lines: 
		- always activate embedding and linear head layer 
		- randomly sample intermediate layers to unfreeze
	- å²¡é‡åŸã•ã‚“ã€LISAã¯LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®éš›ã«ã€å„å±¤ã‚’ç¢ºç‡çš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€é¸æŠã•ã‚ŒãŸå±¤ã®ã¿æ›´æ–°ã™ã‚‹ã€‚å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã—ãªãŒã‚‰LoRAã‚ˆã‚Šã‚‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€è¨ˆç®—é‡ã¨ã‚‚åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã‚‹ï¼ˆé€šå¸¸å­¦ç¿’ã§ã‚‚ã§ããã†ï¼‰ã€‚æœ€åˆã®å±¤ã¨æœ€å¾Œã®å±¤ã®ã¿æ¡æŠã™ã‚‹ç¢ºç‡ã¯é«˜ãã—ã¦ãŠã
- DBRXã¾ã¨ã‚
	- https://x.com/webbigdata/status/1772981844839207206?s=20
	- ãƒ»Databricksç¤¾ãŒæ–°ãŸã«å…¬é–‹ã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ãªMoEãƒ¢ãƒ‡ãƒ« 
	- ãƒ»è‡ªç¤¾èª¿ã¹ã§GPT-3.5 ã‚’ä¸Šå›ã‚Šã€Gemini 1.0 Pro ã¨ç«¶åˆ 
	- ãƒ»ã‚³ãƒ¼ãƒ‰èƒ½åŠ›ã§ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«CodeLLaMA-70Bã‚’ä¸Šå›ã‚‹ 
	- ãƒ»æ¨è«–ã¯ LLaMA2-70B ã‚ˆã‚Šã‚‚æœ€å¤§ 2 å€é«˜é€Ÿ 
	- ãƒ»16äººã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ä¸­ã§4 äººã‚’é¸æŠã—ã¦æ¨è«–ã‚’å®Ÿè¡Œ 
	- ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯Grok-1ã®ç´„40%ã ãŒæ€§èƒ½ã¯ä¸Šå›ã‚‹ 
	- ãƒ»ãƒ†ã‚­ã‚¹ãƒˆ ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ¼ãƒ‰ ãƒ‡ãƒ¼ã‚¿ã‚’åˆè¨ˆã—ãŸ12Tãƒˆãƒ¼ã‚¯ãƒ³ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° 
	- ãƒ»3072 å°ã® NVIDIA H100ã‚’ä½¿ã£ã¦ç´„3ã‹æœˆã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° 
	- ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¯ 263.07(ç´„4.4 GB x 61safetensors)
- Explorationâ€”not workâ€”could be key to a vibrant local economy
	- https://phys.org/news/2024-03-exploration-key-vibrant-local-economy.html
	- Cities and the surprising finding from mobility data analysis that it's more in how we spend and explore in our free time that drives the economic vibrancy of cities, over where we work and go to school.
- Monitoring AI-Modified Content at Scale:A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
	- https://arxiv.org/pdf/2403.07183.pdf
-  IntelÂ® Neural Compressor
	- https://github.com/intel/neural-compressor
	- All your need is Intel Neural Compressor (INC) for INT4 LLMs. INC v2.5 released with SOTA INT4 LLM quantization (AutoRound) across platforms incl. Intel Gaudi2, Xeon, and GPU.
	- Models: Llama2, Mistral, Mixtral-MOE, Gemma, Mistral-v0.2, Phi2, Qwen,
- Masked Autoencoders are PDE Learners
	- https://arxiv.org/abs/2403.17728
	- Masked autoencoders can learn useful latent representations for PDEs through self-supervised pretraining on unlabeled spatiotemporal data. This allows them to improve
- 4-bit quantized DBRX runs nicely in MLX on an M2 Ultra
	- https://github.com/ml-explore/mlx-examples/pull/628
- å›½ãƒ»åœ°æ–¹å…±é€šç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ŒGovbotï¼ˆã‚¬ãƒœãƒƒãƒˆï¼‰ã€ãŒã‚ã¾ã‚Šã«ã²ã©ã„ã¨ã€ã€
	- https://x.com/HiromitsuTakagi/status/1772918656210743594?s=20
- Google presents Long-form factuality in large language models
	- https://arxiv.org/abs/2403.18802
	- Proposes that LLM agents can be used as automated evaluators for longform factuality
	- Shows that **LLM agents can achieve superhuman rating performance**
- GovBotæ‹…å½“è€…ã¯ãªã«ã‚‚ç†è§£ã—ã¦ãªã„ã¨ã®ç–‘æƒ‘ãŒã€ã€by ã²ã‚ã¿ã¡ã‚…å…ˆç”Ÿ
	- https://x.com/HiromitsuTakagi/status/1773148382820778280?s=20
	- ãã‚Œã€GovBotã¯AIã§ã¯ãªã„ã®ã§å­¦ç¿’ã¨å‘¼ã¶ã®ã¯ç•°å¸¸ã ã—ã€ã“ã“ã§äººé–“ãŒå­¦ç¿’ã™ã‚‹ãªã‚“ã¦è©±ã‚’ã™ã‚‹ã®ã¯æ»‘ç¨½ã¨è¨€ã†ä»–ãªã„ã§ã™ã­
- DBRXã¯è©¦ã›ã‚‹æ¨¡æ§˜
	- https://huggingface.co/spaces/databricks/dbrx-instruct
	- ã€Œé¢¨ã®è°·ã®ãƒŠã‚¦ã‚·ã‚«ã€ã¯ã€å®®å´é§¿ç›£ç£ã«ã‚ˆã‚‹æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡æ˜ ç”»ã§ã™ã€‚ãƒŠã‚¦ã‚·ã‚«ã¯ã€é¢¨ã®è°·ã®ç‹å¥³ã§ã‚ã‚Šã€æ£®ã¨æ·±ã„é–¢ä¿‚æ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚æ£®ã®äººã¯ã€æ£®ã«ä½ã‚€åŸå§‹çš„ãªéƒ¨æ—ã®ã“ã¨ã§ã™ã€‚ãƒŠã‚¦ã‚·ã‚«ã¯ã€æ£®ã¨ãã®ä½äººã‚’å°Šé‡ã—ã€æ£®ã¨å…±å­˜ã™ã‚‹ã“ã¨ã‚’æœ›ã‚“ã§ã„ã¾ã™ã€‚å½¼å¥³ã¯ã€æ£®ã¨ãã®ä½äººãŒæŒã¤åŠ›ã‚’ç†è§£ã—ã¦ãŠã‚Šã€ãã‚Œã‚’ä½¿ã£ã¦ã€é¢¨ã®è°·ã¨æ£®ã®é–“ã®ç´›äº‰ã‚’è§£æ±ºã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚ã—ã‹ã—ã€æ£®ã®äººã¯ã€é¢¨ã®è°·ã‚„ä»–ã®ç‹å›½ã¨ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦ã€ã‚ˆã‚Šæ‡ç–‘çš„ã§ã‚ã‚Šã€ãƒŠã‚¦ã‚·ã‚«ã®è€ƒãˆã«åå¯¾ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚
- PFNåˆã®ã‚²ãƒ¼ãƒ ã€Omega Crafterã€
	- https://x.com/imos/status/1773254326003139023?s=20
	- ã»ã«ã‚ƒã‚‰ã‚‰å›è·¯ã§è‡ªå‹•åŒ–ã™ã‚‹ã¿ãŸã„ãªãªã‚“ã¡ã‚ƒã£ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã§ã¯ãªãã€ã¡ã‚ƒã‚“ã¨ã—ãŸæ§‹é€ ã‚’æŒã¤ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãŒã§ãã‚‹çã—ã„(?)ã‚²ãƒ¼ãƒ ãªã®ã§ã€ç‰¹ã«ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®æ–¹ã«ã¯ã‚ªã‚¹ã‚¹ãƒ¡ã—ãŸã„ã§ã™ã€‚æ˜¯éè²·ã£ã¦ãƒ—ãƒ¬ã‚¤ã—ã¦ãã ã•ã„ï¼
- Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
	- https://speakerdeck.com/shunk031/stable-diffusion-3
	- Stable Diffusion 3 ã®å…ƒè«–æ–‡ã‚’èª­ã¿ã¾ã—ãŸï¼æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ Flow ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (Rectified Flow)ã€UNet ã‹ã‚‰ DiT ã¸åˆ‡ã‚Šæ›¿ãˆã€CLIP x 2 ã¨ T5-XXL ã‚’ä½¿ã£ãŸã¦ã‚“ã“ç››ã‚Šãƒ¢ãƒ‡ãƒ«ã§ã€ç”Ÿæˆç”»åƒã®å“è³ªãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹ç”Ÿæˆç”»åƒã®å¿ å®Ÿæ€§ãƒ»æ–‡å­—ã®æç”»æ€§èƒ½ãŒé£›èºçš„ã«å‘ä¸Šã—ã¦ã„ã¾ã™
-  Perplexityã‚’ã‚‚ã¨ã«ï½¤è¤‡æ•°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¦æ¨è«–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®ç°¡å˜ãªã‚³ãƒ¼ãƒ‰å®Ÿè£…
	- https://note.com/kan_hatakeyama/n/nb5625d6411a8?sub_rt=share_pw
	- ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰è¨“ç·´ã‚’ã™ã‚‹ä½™è£•ãŒãªã„ã®ã§ã€ä»Šå›ã¯è©¦ã—ã«ã€è‹±èªãŒå¾—æ„ãªLLama2-7bã¨ã€æ—¥æœ¬èªã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸElyza-7bã‚’çµ±åˆï¼ˆmergeï¼‰ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã£ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚
	- è‹±èªã®è³ªå•ã«ã¯llamaã€æ—¥æœ¬èªã®è³ªå•ã«ã¯elyzaã§ç­”ãˆã‚‹ã“ã¨ãŒã§ãã‚Œã°ã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè¨¼ã«æˆåŠŸã§ã™ã€‚
- Generative Flow Networks by Yoshua Bengio
	- https://mila.quebec/en/article/generative-flow-networks/
	- https://www.youtube.com/watch?v=ggYoJp0b3Oo
-  Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model
	- https://www.ai21.com/blog/announcing-jamba
	- å²¡é‡åŸã•ã‚“ã€Jambaã¯Mambaã¨Transformerã‚’ã‚ã‚ã›ãŸ52B LLMã€‚MoEã§æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯12Bã€1GPUã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·140Kã¾ã§æ‰±ãˆã€è¤‡æ•°GPUã§ã¯256Kã¾ã§æ‰±ãˆã‚‹ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ããªã£ãŸæ™‚ã¯3å€è¿‘ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€‚1/8ã®å‰²åˆã§Transformerã‚’ä½¿ã†ã€‚Mambaã‚’æ¡ç”¨ã—ãŸLLMã§åˆã‚ã¦ã®å¤§ããªãƒ¢ãƒ‡ãƒ«ã€‚
-   LMFlowã«ã‚ˆã‚‹æ—¥æœ¬èªLISAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€€ by shi3zã•ã‚“
	- https://www.free-ai.ltd/post/lmflow-ja-lisa
	- ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒLoRAã¨åŒç­‰ã«ä½ãã€ãªãŠã‹ã¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«åŒ¹æ•µã‚‚ã—ãã¯ä¸Šå›ã‚‹åŠ¹æœã‚’æŒã¤ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã€‚
-  LoRAã‚ˆã‚Šã„ã„ã‚‰ã—ã„LISA by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/ndf165df51f04?sub_rt=share_pb
	- å­¦ç¿’ã‚‚é€Ÿã„ã—æ¨è«–ã‚‚é€Ÿã„ã€‚  ã“ã‚“ãªã„ã„ã“ã¨ãšãã‚ã®ã“ã¨ãŒã‚ã£ã¦ã„ã„ã®ã‹ã€‚  ã—ã‹ã—ãã‚“ãªã„ã„ã“ã¨ãšãã‚ã®ã“ã¨ãŒæ™‚ã€…èµ·ãã‚‹ã®ãŒã“ã®æ¥­ç•Œã®é¢ç™½ã„ã¨ã“ã‚ã§ã‚ã‚‹ã€‚
-  Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
	- https://arxiv.org/abs/2403.11996
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç§‘å­¦çŸ¥è­˜æŠ½å‡ºã®è«–æ–‡
	- 1000ä»¶ã®è«–æ–‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚ŠæŠ½å‡ºã—çŸ¥è­˜ã‚°ãƒ©ãƒ•ã«å¤‰æ›ã€ã‚°ãƒ©ãƒ•è§£æã«ã‚ˆã‚Šãƒã‚¤ã‚ªææ–™ã¨ãƒ™ãƒ¼ãƒˆãƒ¼ãƒ™ãƒ³ã®ç¬¬ 9 äº¤éŸ¿æ›²ã®æ§‹é€ çš„é¡ä¼¼ç‚¹ãªã©åˆ†é‡ã‚’è¶…ãˆãŸé–¢ä¿‚æ€§ã‚’æ˜ã‚‰ã‹ã«ã§ããŸãã†ã§ã™ã€‚
- Transformerã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã¯ç·šå½¢ã˜ã‚ƒãªãã¦ã¹ãä¹—å‰‡ã ã‹ã‚‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨ˆç®—é‡ã‚’10å€ã«ã—ã¦ã‚‚Lossã¯12%ã—ã‹æ¸›ã‚‰ãªã„ï¼ˆæ€§èƒ½ãŒ10å€ã«ãªã‚‹ã‚ã‘ã˜ã‚ƒãªã„ï¼‰
	- https://x.com/umiyuki_ai/status/1773917004464103563?s=20
- a small-scale preview of Voice Engine
	- https://x.com/OpenAI/status/1773760852153299024?s=20
- OpenAIã¨MicrosoftãŒæœ€å¤§1000å„„ãƒ‰ãƒ«ã‚’æŠ•ã˜ã¦ã€ŒStargateã€ã¨ã„ã†ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’2028å¹´ã¾ã§ã«å»ºè¨­äºˆå®šã€‚AIé–‹ç™ºåŠ é€Ÿã®ãŸã‚ã€æ•°100ä¸‡ã®AIå°‚ç”¨ãƒãƒƒãƒ—ã‚’æ­è¼‰ã€‚
	- https://qz.com/microsoft-openai-stargate-supercomputer-1851375309
-  langchainã¨Databricksã§(ç§ãŒ)å­¦ã¶RAG : BGE-M3ã‚’ä½¿ã£ãŸåŸ‹ã‚è¾¼ã¿
	- https://qiita.com/isanakamishiro2/items/e4f67586b4cb5f171ea9
	- BAAI(Beijing Academy of Artificial Intelligence)ã‹ã‚‰ã€BGE-M3ã¨ã„ã†Embeddingç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚
	- æ—¥æœ¬èªRAGã«ãŠã‘ã‚‹æ–°ãŸãªåŸ‹ã‚è¾¼ã¿ã®ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãªã¨æ€ã„ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸæ¤œç´¢ã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚
	- LangChainã«ã¯`HuggingFaceBgeEmbeddings`ã¨ã„ã†BAAIã®BGEç³»åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®å°‚ç”¨ã‚¯ãƒ©ã‚¹ãŸç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€ãã¡ã‚‰ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
	- RAGã®æ€§èƒ½ã‚’é«˜ã‚ã‚‹ä¸Šã§åŸ‹ã‚è¾¼ã¿ã«é–¢ã™ã‚‹å·¥å¤«ã¯é‡è¦ã§ã‚ã‚Šã€ä»Šå¾Œã‚‚ã“ã†ã„ã£ãŸé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒå…¬é–‹ã•ã‚Œã¦ã„ãã¨ï¼ˆç´ äººçš„ã«ä½¿ã†å´ã«ã¨ã£ã¦ã¯ï¼‰ã‚ã‚ŠãŒãŸã„ã§ã™ã­ã€‚
-  NTTãŒç‹¬è‡ªLLMã®tsuzumiã‚’æä¾›é–‹å§‹ã€æ—¥æœ¬èªæ€§èƒ½ã§ã€ŒGPT-3.5è¶…ãˆã€(2024å¹´3æœˆ25æ—¥)
	- https://xtech.nikkei.com/atcl/nxt/news/24/00458/
	- NTTã¯2024å¹´3æœˆ25æ—¥ã€ç‹¬è‡ªLLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã‚ã‚‹ã€Œtsuzumiã€ã®ã‚µãƒ¼ãƒ“ã‚¹æä¾›ã‚’å§‹ã‚ãŸ
	- tsuzumiã¯æ—¥æœ¬èªã¨è‹±èªã«å¯¾å¿œã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ•°ã¯70å„„ã¨OpenAIã®ã€ŒGPT-3ã€ã®1750å„„ã¨æ¯”ã¹ã¦25åˆ†ã®1ã¨è»½é‡ã ã€‚LLMã®æ—¥æœ¬èªå‡¦ç†æ€§èƒ½ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã€ŒRakuda Benchmarkã€ã®çµæœã§ã¯ã€GPT-3.5ã‚„åŒè¦æ¨¡ã®å›½ç”£LLMã‚’ä¸Šå›ã£ãŸã¨ã„ã†ã€‚tsuzumiã¯è¨€èªã«åŠ ãˆã€å›³è¡¨ã‚„ç”»åƒã®è§£æãªã©ã«ã‚‚å¯¾å¿œã™ã‚‹ã€‚
	- ã†ã¿ã‚†ãã•ã‚“ã€LLMã®å­¦ç¿’ã‚³ã‚¹ãƒˆæ„Ÿã£ã¦ã‚ˆãçŸ¥ã‚‰ã‚“ã‘ã©ã€Tsuzumiã®è³‡æ–™ã«ã‚ˆã‚Œã°AWSã§7Bãƒ¢ãƒ‡ãƒ«ã‚’300Bãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’ã•ã›ã‚‹ã¨1900ä¸‡å††ã‹ã‹ã‚‹ã‚‰ã—ã„ã€‚300Bã˜ã‚ƒå°‘ãªã„ã‹ã‚‰1.2Tãã‚‰ã„ã¯å­¦ç¿’ã•ã›ãŸã„ã‚ˆã­ã€‚ãã—ãŸã‚‰7600ä¸‡å††ã‹ã€‚
-  NTTãŒé–‹ç™ºã—ãŸLLMã€Œtsuzumiã€ã€NTT Comã‚ˆã‚Šå•†ç”¨ç”ŸæˆAIã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦æä¾›é–‹å§‹
	- https://internet.watch.impress.co.jp/docs/news/1578961.html
- æ¥½å¤©ãŒæ—¥æœ¬èªã«æœ€é©åŒ–ã—ãŸMistralãƒ™ãƒ¼ã‚¹ã®LLMã‚’å…¬é–‹ã€å•†ç”¨ç›®çš„ã§ä½¿ç”¨å¯èƒ½(2023å¹´3æœˆ21æ—¥)
	- https://xtech.nikkei.com/atcl/nxt/news/24/00440/
	- å…¬é–‹ã—ãŸã®ã¯åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ã€ŒRakuten AI 7Bã€ã€åŒãƒ¢ãƒ‡ãƒ«ã‚’åŸºã«ã—ãŸã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã€ŒRakuten AI 7B Instructã€ã€Rakuten AI 7B Instructã‚’åŸºã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã€ŒRakuten AI 7B Chatã€ã®3ç¨®ã§ã‚ã‚‹ã€‚
	- æ–‡ç« ã®è¦ç´„ã‚„è³ªå•å¿œç­”ã€ä¸€èˆ¬çš„ãªæ–‡ç« ã®ç†è§£ã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãªã©ã«å•†ç”¨ç›®çš„ã§ä½¿ç”¨ã§ãã€Rakuten AI 7Bã¯ä»–ã®ãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã¨ã—ã¦ã‚‚ä½¿ãˆã‚‹ã¨ã„ã†ã€‚
	- Rakuten AI 7Bã¯ãƒ•ãƒ©ãƒ³ã‚¹ã®AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—Mistral AIã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã€ŒMistral-7B-v0.1ã€ã‚’åŸºã«ã€ç¶™ç¶šçš„ã«å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦é–‹ç™ºã—ãŸæ—¥æœ¬èªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€‚
- govbotã‚’ã¨ã‚Šã‚ãˆãšè©¦ã—ã¦ã¿ãŸã‚‰æ­»ã‚“ã äººé–“ã‚’ã€ã‚´ãƒŸã€ã¨èªè­˜ã—ã¦ã„ã‚‹èª¬ãŒå‡ºã¦ããŸ
	- https://x.com/judo5001/status/1773196373686411292?s=20
- GovBotã®é–‹ç™ºã«8525ä¸‡ã‹ã‹ã£ãŸã¨èã„ã¦èª¿ã¹ãŸã‚‰æœ¬å½“ã ã£ãŸğŸ¤¯ï¼ã—ã‹ã‚‚ã€èª¿é”æ©Ÿé–¢ã¯ãƒ‡ã‚¸ã‚¿ãƒ«åºã§é–‹ç™ºæ¥­è€…ã¯æ—¥æœ¬é›»æ°—
	- https://x.com/gijigae/status/1773557153317437824?s=20

## 3/25

å…ˆé€±xAIã‚ˆã‚Šå…¬é–‹ã•ã‚ŒãŸgrok-1ã€gpt-3.5ã‚’ä¸Šå›ã‚‹ãŒã€Claude 2ã‚„GPT-4ã¯ä¸‹å›ã‚‹ã¨ã„ã†æ€§èƒ½ã‚‰ã—ã„ã€‚ã•ã¦ç”ŸæˆAIã§ã¯å‡ºé…ã‚Œæ„Ÿã‚‚ã‚ã‚‹Appleã€geminiã‚’iPhoneã«å…¥ã‚Œã‚‹ã¨ã®ã†ã‚ã•ãŒå‡ºãŸã‚Šã€30Bã®MM1ã‚’è«–æ–‡ç™ºè¡¨ã—ãŸã‚Šã¨ã€ã«ã‚ã‹ã«æ´»ç™ºåŒ–ã€‚Stability AIã®ã‚¢ãƒ‹ãƒ¡æ¥­ç•Œå‘ã‘ç”Ÿæˆç³»AIã€ã¤ã„ã«ç¾å ´ã«AIãŒå…¥ã‚Šã ã™ã®ã‹ã€‚KDDIã€ELYZAã‚’é€£çµå­ä¼šç¤¾åŒ–ã£ã¦ã®ã‚‚é©šã„ãŸã€ã€Œç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸDXæ”¯æ´ãƒ»AI SaaSã€ã£ã¦ã®ãŒæ˜¥ä»¥é™ã§ã‚‹ã‚‰ã—ã„ã€‚NVIDIA ãŒGTC2024ã§ç™ºè¡¨ã—ãŸã€ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰é–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€ŒGR00Tã€ã€ H100ã®ï¼•å€ã®æ€§èƒ½ï¼æ–°GPUã§ã‚ã‚‹B200ã€DGX GB200 NVL72ã¨ã‹ã€NIMã®ç™ºè¡¨ã¨ã‹ã€ä¸€äººå‹ã¡ã£ã¦ã“ã†ã„ã†ã“ã¨ã€‚æ—©é€ŸllamaindexãŒNVIDIA NIMã§å‹•ãã‚ˆã†ã«ãªã£ãŸã€‚ã²ã‚ã¿ã¡ã‚…å…ˆç”Ÿã€Claude 3ã‚’ç”¨ã„ãŸæ–°è¦æå‡ºæ³•æ¡ˆã®ç«‹æ³•æŠ€è¡“ä¸Šã®çŸ›ç›¾ç‚¹ãƒã‚§ãƒƒã‚¯ã€æ³•åˆ¶å±€ã‚‚çœŸã£é’ãƒ¬ãƒ™ãƒ«ã¨ã®ã“ã¨ã€‚DeepMindã®TacticAIã€ã€Œã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯ã«ã¤ã„ã¦ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹å®Œå…¨ãªAIã‚·ã‚¹ãƒ†ãƒ ã€ã€‚500ç¨‹åº¦ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ•°åˆ†å­¦ç¿’ã•ã›ã¦LLMã®å‡ºåŠ›ã‚’æ–¹å‘ä»˜ã‘ã‚‹äº‹ãŒå‡ºæ¥ã‚‹åˆ¶å¾¡ãƒ™ã‚¯ãƒˆãƒ«ã£ã¦ã®ã¯é¢ç™½ã„ã€ã‚­ãƒ£ãƒ©åˆ†ã‘ãªã‚“ã‹ãŒç°¡å˜ã«ãªã‚‹ã®ã‹ã€‚ã€ŒGoogle Scholar PDF Readerã€ã€ã“ã†ã„ã†å¿œç”¨ãŒã©ã‚“ã©ã‚“å‡ºã¦ã»ã—ã„ã€‚Sakana.aiã®é€²åŒ–çš„è¨ˆç®—ã«ã‚ˆã‚‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã£ã¦ã€è¤‡æ•°ã®LLMã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã¨ã„ã†æ–°ãŸãªæ–¹å‘æ€§ã‚’ç¤ºã—ãŸã€‚æ—¥æœ¬èªç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«EvoVLM-JPã¯ã™ãã«è©¦ã™ã“ã¨ãŒã§ãã‚‹ã€‚LLMã®ãƒãƒ¼ã‚¸ã§ã¯ã€Arcee's MergeKitã£ã¦ã®ã‚‚å¿˜ã‚Œã¦ã¯ã„ã‘ãªã„ã€‚Embedding ã®é‡å­åŒ–ã¨ã„ã†ã®ãŒã‚ã‚‹ã®ã‹ã€é«˜é€ŸåŒ–ã®å·¥å¤«ã®ä½™åœ°ã¯ã¾ã ã¾ã ã‚ã‚‹ã€‚[huggingface](https://github.com/huggingface)ã‹ã‚‰PEFT 0.10.0ã®ãƒªãƒªãƒ¼ã‚¹ã€70B Llama 2ãƒ¢ãƒ‡ãƒ«ã‚’24GBãƒ¡ãƒ¢ãƒªã‚’æ­è¼‰ã—ãŸGPU2åŸºã§QLoRAå¯èƒ½ã«ãªã‚‹ã¨ã®ã“ã¨ã€‚huggingfaceã¯Transformers 4.39ã‚‚ãƒªãƒªãƒ¼ã‚¹ã€GaLoreã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã‚‹ã‚‰ã—ã„ã€‚ã€ŒGaLoreã€ã€ã€ŒNVIDIA RTX 4090ã€ãªã©ã®å®¶åº­ç”¨GPUä¸Šã§ã€Llamaãªã©ã®æœ€å¤§7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®¹æ˜“ã«ã™ã‚‹æŠ€è¡“ã€å…ƒè«–æ–‡ã¯2023å¹´ã®5æœˆã«MetaãŒç™ºè¡¨ã€‚Artificial muscleã¨ã„ã†ã®ã‚‚ã™ã”ã„ãªã€NVIDIAã®ãƒ­ãƒœãƒƒãƒˆã«çµ„ã¿è¾¼ã‚€ã¨ã€ã„ã‚ˆã„ã‚ˆäººé–“ã‚‰ã—ã„ãƒ­ãƒœãƒƒãƒˆãŒå®Ÿç¾ã™ã‚‹ã®ã‹ã€‚Lightblueã€å›½å†…æœ€é«˜æ°´æº–ã®æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã€Œao-Karasuã€ãƒªãƒªãƒ¼ã‚¹ã€ï¼—ï¼’Bã ãã†ã ã€ã‚‚ã†ä½•ãŒä½•ã ã‹ã€‚LINEã®ã€Œjapanese-large-lm-1.7b-instruction-sftã€ã‹ã‚‰æ´¾ç”Ÿã—ãŸLLMãŒãŸãã•ã‚“ãƒªãƒªãƒ¼ã‚¹ã€ãƒ­ãƒ¼ã‚«ãƒ«AIãƒãƒƒã‚«ã‚½ãƒ³ã®æˆæœã‚‰ã—ã„ã€‚ã€å¾®åˆ†å¯èƒ½ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾®åˆ†å¯èƒ½ãªæ–¹æ³•ã§æœ€é©åŒ–ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ãªã‚“ã ã‘ã©ã€â€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹ã“ã¨ã¯æœ¬è³ªçš„ã«ç¢ºç‡åˆ†å¸ƒã«ã‚ˆã£ã¦ãã®å‡ºåŠ›ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨â€ã¨ã¯UQã€Uncertainty Quantification;ä¸ç¢ºã‹ã•ã®å®šé‡åŒ–ã€ã«é€šã˜ã¦é¢ç™½ã„

- grok-1ã¾ã¨ã‚
	- https://x.com/webbigdata/status/1769503166528458822?s=20
	- ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯314Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ 
	- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§ã„ãˆã°318.24GB 
	- MoE(2/8 experts)ã§activeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã ã‘ã§ã‚‚86B 
	- 2023/10æœˆæ™‚ç‚¹ã§å­¦ç¿’ã‚’å®Œäº†ã—ã¦ã„ãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿å…¬é–‹ 
	- githubã®xai-orgã§æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹(JAX) 
	- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯academictorrentsã‹huggingfaceã®xai-org/grok-1 
	- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯Apache 2.0 ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ 
	- å…¬è¡¨æ¸ˆã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚Œã°gpt-3.5ã‚’ä¸Šå›ã‚‹ãŒã€Claude 2ã‚„GPT-4ã¯ä¸‹å›ã‚‹
- Apple in talks with Google for using Gemini to bring generative AI features to iPhones
	- https://www.livemint.com/technology/tech-news/googles-gemini-could-power-generative-ai-features-on-iphone-16-tim-cook-heres-what-we-know-11710739843784.html
- ã‚¢ãƒƒãƒ—ãƒ«ã€é«˜åº¦ãªè¨€èªç†è§£ã‚’æŒã¤æ–°å‹AIãƒ¢ãƒ‡ãƒ«ã€ŒMM1ã€ã‚’ç™ºè¡¨
	- https://ascii.jp/elem/000/004/189/4189761/
	- https://arxiv.org/pdf/2403.09611.pdf
	- è¤‡æ•°ï¼ˆ30å„„ã€70å„„ã€300å„„ï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å‚™ãˆã‚‹MM1ã¯ã€10å„„ä»¥ä¸Šã®ç”»åƒãŠã‚ˆã³30å…†èªä»¥ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã€GitHubã®ã‚³ãƒ¼ãƒ‰ä¾‹ãªã©ã®å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã€æ•™å¸«ãªã—å­¦ç¿’ã¨æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ã‚‹ç‹¬è‡ªã®æ–¹æ³•ã§å­¦ç¿’ã•ã‚Œã€å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é«˜ã„ç²¾åº¦ã‚’ç¤ºã™
	- MM1ã¯ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«é–¢ã—ã¦ã€ãã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ã€äº‹å‰å­¦ç¿’ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©³ç´°ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«è‡³ã‚‹ã¾ã§ã€è©³ç´°ãªæƒ…å ±ï¼ˆMLLMsã®é–‹ç™ºãƒ¬ã‚·ãƒ”ï¼‰ã‚’å…¬é–‹ã—ã¦ã„ã‚‹ã€‚
-  Stability AIã¨ã‚¢ãƒ‹ãƒ¡ãƒã‚§ãƒ¼ãƒ³ãŒã‚¢ãƒ‹ãƒ¡æ¥­ç•Œå‘ã‘ç”Ÿæˆç³»AIã®å…±åŒç ”ç©¶ã‚’æ¤œè¨é–‹å§‹
	- https://prtimes.jp/main/html/rd/p/000000003.000135092.html
	-  æ—¢å­˜ã®ã‚¢ãƒ‹ãƒ¡åˆ¶ä½œå·¥ç¨‹ã‚’ãã®ã¾ã¾ã«ã€Œå”è­°ä¼šã€ã‚’é€šã˜ã¦åˆ¶ä½œç¾å ´ã®å£°ã‚’ä¼ºã„ãªãŒã‚‰ã‚¢ãƒ‹ãƒ¡ä½œå“ã®å“è³ªå‘ä¸Šã‚’ç›®æ¨™ã¨ã—ãŸæ”¯æ´ãƒ„ãƒ¼ãƒ«ã®å…±åŒç ”ç©¶ã‚’ç›®æŒ‡ã™
-  Fully  Client-Side  Chat Over Documents
	- https://webml-demo.vercel.app/
	- So I revisited WebLLM and was able to add browser-only mode!
- KDDIã€æ±å¤§ç™ºAIãƒ™ãƒ³ãƒãƒ£ãƒ¼ãƒ»ELYZAã‚’é€£çµå­ä¼šç¤¾åŒ–ã€€æ˜¥ä»¥é™ã€ç”ŸæˆAIé–¢é€£ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›
	- https://www.itmedia.co.jp/news/articles/2403/18/news140.html
	- ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸDXæ”¯æ´ãƒ»AI SaaS
- NVIDIA GTC2024ã§æ¬¡ä¸–ä»£ã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¯ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰
	- https://www.youtube.com/watch?v=Y2F8yisiS6E
- NVIDIAã€GPUãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€ŒBlackwellã€ç™ºè¡¨ã€€ã€Œå…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦æ¨¡ã®AIãƒ¢ãƒ‡ãƒ«å®Ÿç¾ã€
	- https://www.itmedia.co.jp/news/articles/2403/19/news092.html
	- ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«æ­è¼‰ã™ã‚‹ã€ŒGB200 Grace Blackwell Superchipã€ã¯ã€æ–°GPUã€ŒB200ã€ï¼ˆ2080å„„å€‹ã®ãƒˆãƒ©ãƒ³ã‚¸ã‚¹ã‚¿ã‚’æ­è¼‰ã—ã€ç¾è¡Œã®ã€ŒH100ã€ã¨æ¯”è¼ƒã—ã¦ã€AIå‘ã‘ã®ä½œæ¥­ã§5å€ã®å‡¦ç†èƒ½åŠ›ã‚’ç™ºæ®ã™ã‚‹GPUï¼‰ã‚’2åŸºã¨1åŸºã®Grace CPUã‚’çµ„ã¿åˆã‚ã›ãŸã‚‚ã®ã€‚
	- NVIDIAã«ã‚ˆã‚‹ã¨ã€1å…†8000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã«ã¯ã€Hopper GPUã§ã¯8000å€‹ã®GPUã§15ãƒ¡ã‚¬ãƒ¯ãƒƒãƒˆã®é›»åŠ›ãŒå¿…è¦ã ã£ãŸãŒã€æ–°ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒƒãƒ—ã§ã‚ã‚Œã°2000å€‹ã§å¯èƒ½ã§ã€æ¶ˆè²»é›»åŠ›ã¯4ãƒ¡ã‚¬ãƒ¯ãƒƒãƒˆã§æ¸ˆã‚€ã¨ã„ã†ã€‚
- DGX GB200 NVL72ã¯ã€GB200 Superchipã‚’72åŸºNVLinkã§æ¥ç¶šã—ãŸã‚¯ãƒ©ã‚¹ã‚¿
	- https://x.com/_ksasaki/status/1769829822946001353?s=20
- ç”ŸæˆAIã‚¢ãƒ—ãƒªã®å±•é–‹ã‚’æ•°åˆ†ã«ã€NVIDIAãŒæ–°ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã€ŒNIMã€ã‚’ç™ºè¡¨
	- https://xtech.nikkei.com/atcl/nxt/news/24/00424/
	- NIMã¯ã€ç”ŸæˆAIã®æ¨è«–ã«å¿…è¦ã¨ãªã‚‹å„ç¨®ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®ã‚³ãƒ³ãƒ†ãƒŠï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ï¼‰ã‚’æä¾›ã™ã‚‹ä»•çµ„ã¿ã§ã‚ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢ãŒé–‹ç™ºã—ãŸæ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æœ€é©åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€ŒTriton Inference Serverã€ã‚„ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã€ŒTensorRT-LLMã€ãªã©ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢ã‚„ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ä¼æ¥­ãŒæä¾›ã™ã‚‹20ä»¥ä¸Šã®AIãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚
-  LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM
	- https://www.llamaindex.ai/blog/llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim
	- LlamaIndex  is integrated with NVIDIA NIM inference microservices to help enterprises seamlessly deploy generative AI at scale
- 1x GPU Blackwell - 192GB VRAM 2x GPU 
	- Blackwell with CPU - 384 GB VRAM
	- https://x.com/migtissera/status/1769824889102348366?s=20
-  NVIDIAãŒãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰é–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æä¾›ã‚’ç™ºè¡¨ã€€ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ã®äºŒè¶³æ­©è¡Œãƒ­ãƒœãƒƒãƒˆãŒç™»å£‡ã€€Jetson Orinã‹ã‚‰æ¬¡ä¸–ä»£Thorã¸
	- https://robotstart.info/2024/03/19/nvidia-humanoid-jetson-thor.html
	- NVIDIAã¯ã€ŒGTC 2024ã€ã®å‰µæ¥­è€…/CEOã®ã‚¸ã‚§ãƒ³ã‚¹ãƒ³ãƒ»ãƒ•ã‚¢ãƒ³æ°ã«ã‚ˆã‚‹åŸºèª¿è¬›æ¼”ã§ã€ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰ãƒ­ãƒœãƒƒãƒˆ(ãƒ’ãƒˆå‹ãƒ­ãƒœãƒƒãƒˆ)ã‚’é–‹ç™ºã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€ŒGR00Tã€(ã‚¸ãƒ¼ã‚¢ãƒ¼ãƒ«ã‚¼ãƒ­ã‚¼ãƒ­ãƒ†ã‚£ãƒ¼)ã‚’ç™ºè¡¨ã—ãŸã€‚NVIDIAã¯æ–°ä¸–ä»£GPUã¨ç”ŸæˆAIã‚’å«ã‚€ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰é–‹ç™ºç”¨ã®SDKã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æä¾›ã—ã€å…¨é¢çš„ã«æ”¯æ´ã—ã¦ã„ãã€‚
- æ³•åˆ¶å±€ã‚‚çœŸã£é’ï¼ŸClaude 3ã‚’ç”¨ã„ãŸæ–°è¦æå‡ºæ³•æ¡ˆã®ç«‹æ³•æŠ€è¡“ä¸Šã®çŸ›ç›¾ç‚¹ãƒã‚§ãƒƒã‚¯
	- https://takagi-hiromitsu.jp/diary/20240319.html
	- Claude 3ã«èã„ã¦ã¿ãŸã€‚å¾®å¦™ã«ã‘ã£ã“ã†é–“é•ã†ãŒã€ãã“ã¯ã‚¹ãƒ«ãƒ¼ã—ã¦ã€å¤§å¤‰å‚è€ƒã«ãªã‚‹ã€‚ã“ã“ã¾ã§ã‚ãšã‹1æ™‚é–“ç¨‹åº¦ã®ä½œæ¥­ã ã£ãŸ
- TacticAI: an AI assistant for football tactics
	- https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics/?utm_source=twitter&utm_medium=social&utm_campaign=TacticAI/
	- We're announcing TacticAI: an AI assistant capable of offering insights to football experts on corner kicks.
	- it can help teams sample alternative player setups to evaluate possible outcomes, and achieves state-of-the-art results.
	- TacticAIã¯Googleã¨ãƒªãƒ´ã‚¡ãƒ—ãƒ¼ãƒ«ã®è¤‡æ•°å¹´ã«ã‚ãŸã‚‹å”åŠ›é–¢ä¿‚ã®ä¸€ç’°ã¨ã—ã¦é–‹ç™ºã•ã‚ŒãŸã‚‚ã®ã§ã€ã€Œã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯ã«ã¤ã„ã¦ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹å®Œå…¨ãªAIã‚·ã‚¹ãƒ†ãƒ ã€ã¨ã—ã¦ã‚¢ãƒ”ãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™
- 500ç¨‹åº¦ã®ã‚µãƒ³ãƒ—ãƒ«ã§æ•°åˆ†å­¦ç¿’ã•ã›ã¦LLMã®å‡ºåŠ›ã‚’æ–¹å‘ä»˜ã‘ã‚‹äº‹ãŒå‡ºæ¥ã‚‹åˆ¶å¾¡ãƒ™ã‚¯ãƒˆãƒ«(control vectors)ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
	- https://github.com/vgel/repeng
	- LoRAã®ã‚ˆã†ã«ç‰¹å®šã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã™ã‚‹ã®ã§ã¯ãªãä¾‹ãˆã° ã€Œé™½ã‚­ãƒ£ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ï½–ï½“ã€Œé™°ã‚­ãƒ£ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«å…¨ä½“çš„ãªæ–¹å‘æ€§ã‚’ä¸ãˆã‚‹æ„Ÿã˜ã§ã™ã­
- Googleã€PDFè«–æ–‡ã‚’åŠ‡çš„ã«èª­ã¿ã‚„ã™ãã™ã‚‹Chromeæ‹¡å¼µã€ŒGoogle Scholar PDF Readerã€
	- https://news.mynavi.jp/techplus/article/20240321-2911097/
- GaLore - å®¶åº­ç”¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
	- https://note.com/npaka/n/n8e4537502e3e?sub_rt=share_h
	- ã€ŒGaLoreã€ã¯ã€ã€ŒNVIDIA RTX 4090ã€ãªã©ã®å®¶åº­ç”¨GPUä¸Šã§ã€Llamaãªã©ã®æœ€å¤§7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹ã¨å‹¾é…ã«å¾“æ¥é–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ãŸãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã•ã‚Œã¾ã™ã€‚
	- ã€ŒGaLoreã€ã¨ã€Œ8bitã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®æ•´åˆæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ç›¸ä¹—åŠ¹æœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
- GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
	- https://arxiv.org/abs/2403.03507v1
- Stanfordã®Fei-Fei Liæ•™æˆã‚‰ã®ãƒãƒ¼ãƒ ã‹ã‚‰ã€ãƒ­ãƒœãƒƒãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒBEHAVIOR-1Kã€ãŒãƒªãƒªãƒ¼ã‚¹
	- https://x.com/drfeifei/status/17710132915083798å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒGrok-1ã€ã«ã¤ã„ã¦ by ä»Šäº•
	- https://x.com/ImAI_Eruel/status/1769487625994506294?s=20
- é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ by sakana ai
	- Sakana AIã®æœ€åˆã®ç ”ç©¶æˆæœã§ã‚ã‚‹ã€é€²åŒ–çš„è¨ˆç®—ã«ã‚ˆã‚‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã«é–¢ã™ã‚‹è«–æ–‡ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚å¤šæ§˜ãªæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«èåˆã—å„ªã‚ŒãŸåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã¨å…±ã«ã€ãã‚Œã«ã‚ˆã‚Šè©¦ä½œã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
		- **EvoLLM-JP**ï¼šæ•°å­¦çš„æ¨è«–ãŒå¯èƒ½ãªæ—¥æœ¬èªã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰
		- **EvoVLM-JP**ï¼šæ—¥æœ¬èªã§å¯¾è©±å¯èƒ½ãªç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰
		- **EvoSDXL-JP**ï¼šé«˜é€Ÿãªæ—¥æœ¬èªç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
	- _æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ã¦æ–°ã—ã„åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹éç¨‹ã®å¯è¦–åŒ–ã€‚é€²åŒ–çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹éš›ã«ã€äººé–“ã®ç›´æ„Ÿã ã‘ã§ã¯è¦‹è½ã¨ã•ã‚ŒãŒã¡ãªã€åŠ¹æœçš„ã‹ã¤æ™‚ã«éç›´æ„Ÿçš„ãªæ–¹æ³•ã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™_
-  Evolutionary Optimization of Model Merging Recipes
	- Sakana Aiã®è«–æ–‡
	- https://arxiv.org/abs/2403.13187
-  WSL2ã§Sakana AIã‚’è©¦ã—ã¦ã¿ã‚‹
	- https://note.com/ngc_shj/n/na9b41adb9131
	- ã€Œé€²åŒ–çš„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã«ã‚ˆã‚Šæ—¥æœ¬èªæ•°å­¦LLMã¨ã—ã¦æ§‹ç¯‰ã—ãŸEvoLLM-JPã¯ã€æ•°å­¦ã®ã¿ãªã‚‰ãšã€æ—¥æœ¬èªã®å…¨èˆ¬çš„ãªèƒ½åŠ›ã«é•·ã‘ã¦ã„ã‚‹ã€ã‚‰ã—ã„EvoLLM-JPã‚’è©¦ã—ã¦ã¿ã¾ã™
	- 10Bã®ãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€torch_dtypeã‚’"auto"ã‹ã‚‰torch.bfloat16ã«å¤‰æ›´ã™ã‚‹ã¨ã€æ¨è«–ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ãŒæ”¹å–„ã—ã¾ã—ãŸã€‚
- RAG for long context LLMs: Video
	- https://www.youtube.com/watch?v=SsHUNfhF32s
	- https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit#slide=id.g26c0cb8dc66_0_0
- NVIDIAã®ãƒ•ãƒªãƒ¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³AIã‚³ãƒ¼ã‚¹
	- https://courses.nvidia.com/courses/course-v1:DLI+T-FX-01+V1/
- Claude 3 Opusã‚ˆã‚Š60å€å®‰ã„Haikuã‚’Opusã®å“è³ªã§é‹ç”¨ã™ã‚‹æ–¹æ³•ã€‚
	- https://github.com/mshumer/gpt-prompt-engineer
	- gpt-prompt-engineerã‚’ä½¿ãˆã°ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿé¨“ã‚’è‡ªå‹•åŒ–ã§ãã‚‹ã€‚è‡ªå‹•ã§è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¦ã€LLMåˆ¥ã«è©•ä¾¡ã‚‚å¯èƒ½ã€‚
- Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval
	- https://huggingface.co/blog/embedding-quantization
	- 25x speedup in retrieval; 32x reduction in memory usage; 4x reduction in disk space; 99.3% preservation of performance
- LLM4Decompile: Decompiling Binary Code with Large Language Models
	- https://arxiv.org/abs/2403.05286v1
	- ãƒã‚¤ãƒŠãƒªã‹ã‚‰ãƒªãƒãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ãã‚‹ã¨
- Suno AI unveiled V3
	- https://x.com/heyBarsee/status/1771190753957470604?s=20
- Doing In-Context Learning Without Leaking Private Data
	- https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-diff-private-simple-dataset/examples/symptom_2_disease
	- Few-shot demonstrations are crucial to improve the performance of any LLM/RAG app. But the issue with very private datasets (e.g. patient clinical reports), is that they can easily be leaked/jailbroken by malicious users.
- å†…é–£åºœã€ŒAIæ™‚ä»£ã®çŸ¥çš„è²¡ç”£æ¨©æ¤œè¨ä¼šï¼ˆç¬¬ï¼–å›ï¼‰ã€ã®è³‡æ–™ãŒå…¬é–‹
	- https://www.kantei.go.jp/jp/singi/titeki2/ai_kentoukai/gijisidai/dai6/index.html
-  GitHubã€è„†å¼±æ€§ã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ç™ºè¡¨ã€‚AIãƒœãƒƒãƒˆãŒä¿®æ­£æ¸ˆã¿ã‚³ãƒ¼ãƒ‰ã¨è§£èª¬ã‚’ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
	- https://www.publickey1.jp/blog/24/githubai.html
	- GitHubã¯ã€è„†å¼±æ€§ã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’AIãƒœãƒƒãƒˆãŒè‡ªå‹•çš„ã«ç™ºè¦‹ã€ä¿®æ­£ã—ãŸã‚³ãƒ¼ãƒ‰ã¨ãã®è§£èª¬ã‚’ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¦ãã‚Œã‚‹ã€Œcode scanning autofixã€ï¼ˆã‚³ãƒ¼ãƒ‰ã‚¹ã‚­ãƒ£ãƒ³è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ï¼‰ã‚’ç™ºè¡¨ã—ã¾ã—ãŸ
- éŸ³å£°åŸºç›¤ãƒ¢ãƒ‡ãƒ«Kotoba-Speech v0.1ã®å­¦ç¿’ãƒ»æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸï¼
	- https://x.com/kotoba_tech/status/1771165553882964291?s=20
	- https://github.com/kotoba-tech/kotoba-speech-release
	- End-to-Endã®Transformerã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚‚ç°¡å˜ã§ã™ã€‚ä¾‹ã¨ã—ã¦ã€é–¢è¥¿å¼ãƒ¢ãƒ‡ãƒ«ã‚‚å…¬é–‹ã—ã¾ã—ãŸã€‚æ—¢å­˜ã®Text-to-Speechã‚ˆã‚Šã‚‚ã€ã•ã‚‰ã«è‡ªç„¶ã§æµæš¢ã§ã‚ã‚‹ã“ã¨ãŒå®Ÿæ„Ÿã§ãã‚‹ã‹ã¨æ€ã„ã¾ã™ï¼
- The AI Mirror Test
	- https://x.com/joshwhiton/status/1770870738863415500?s=20
	- The "mirror test" is a classic test used to gauge whether animals are self-aware. I devised a version of it to test for self-awareness in multimodal AI. 4 of 5 AI that I tested passed, exhibiting apparent self-awareness as the test unfolded.
	- Claude Opus passed the mirror test immediately. Like the other AI, it hardly identifies with its brand-name (Claude) and distinguishes itself from the interfaceâ€™s stock elements. However it does identify with the prompt, which it knows is
- PEFT 0.10.0 is out
	- https://github.com/huggingface/peft/releases/tag/v0.10.0
	- Fine-tune larger QLoRA models with DeepSpeed and FSDP, layer replication, enhance DoRA
	- This allows you to fine-tune a 70B Llama model on two GPUs with 24GB memory each.
	- ä»¥å‰ã€ãƒ„ã‚¤ãƒ¼ãƒˆã—ãŸ70B Llama 2ãƒ¢ãƒ‡ãƒ«ã‚’24GBãƒ¡ãƒ¢ãƒªã‚’æ­è¼‰ã—ãŸGPU2åŸºã§QLoRAå¯èƒ½ã«ãªã‚‹ãŠè©±ãŒæ­£å¼æ¡ç”¨
	- åŠ ãˆã¦ã€DoRA(å·¥å¤«ã—ãŸLoRAã€‚ãŸã ã—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã¯å¢—ãˆã‚‹)ãŒé‡å­åŒ–æ¸ˆã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ã¦ä½¿ã„ã‚„ã™ããªã£ãŸæ¨¡æ§˜
	- LoftQ(é‡å­åŒ–èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«LoRAã‚’åˆæœŸåŒ–ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹)ã‚‚ã‚ˆã‚Šä½¿ã„ã‚„ã™ããªã£ãŸã¨ã®äº‹
- OpenAI Voice Engine. This is big
	- https://x.com/SmokeAwayyy/status/1771052612051468668?s=20
	- VOICE ENGINEâ„¢ trademark registration is intended to cover: - voice and speech recognition, processing voice commands, and converting between text and speech
- Introducing the Chatbot Guardrails Arena
	- https://huggingface.co/blog/arena-lighthouz
	- Our vision behind the Chatbot Guardrails Arena is to establish the trusted benchmark for AI chatbot security, privacy, and guardrails. With a large-scale blind stress test by the community, this arena will offer an unbiased and practical assessment of the reliability of current privacy guardrails.
- æ˜¨æ—¥SakanaAILabsã‹ã‚‰ãƒªãƒªãƒ¼ã‚¹ã—ãŸæ—¥æœ¬èªç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«EvoVLM-JPã¯ã€èª°ã§ã‚‚ã™ãã«ãŠè©¦ã—ã„ãŸã ã‘ã¾ã™ã€‚
	- https://huggingface.co/spaces/SakanaAI/EvoVLM-JP
- Starling-LM-7B, has now upgraded to Beta
	- https://huggingface.co/Nexusflow/Starling-LM-7B-beta
	- It shows promising potential in our coming next generation benchmark.
	- https://x.com/lmsysorg/status/1771252185205981426?s=20
-  Debates on the nature of artificial general intelligence by nature
	- https://www.science.org/doi/10.1126/science.ado7069
	- "The history of AI has repeatedly disproved our intuitions about intelligence....At each step in the evolution of AI, human-level intelligence turned out to be more complex than researchers expected."
- lightblue/ao-karasu-72B
	- https://huggingface.co/lightblue/ao-karasu-72B
- Artificial muscle has arrived.
	- https://x.com/BrianRoemmele/status/1770959817815019857?s=20
	- æ˜¨å¹´çŸ¥çš„æ¥­å‹™ã¯çµ‚ã‚ã‚Šã ã€‚äººé–“ã¯ç­‹è‚‰ã‚’é›ãˆã‚‹ã—ã‹ãªã„ã€‚ ã¨ã‹è¨€ã£ã¦ãŸã‘ã©ã€äººå·¥ç­‹è‚‰å‡ºæ¥ã¡ã‚ƒã£ãŸã‚ˆ
	- https://www.youtube.com/watch?v=guDIwspRGJ8
-  Arcee's MergeKit: A Toolkit for Merging Large Language Models
	- https://huggingface.co/papers/2403.13257
	- Model Merging allows us to blend/stack multiple open LLMs into oneâ€”bigger or the same sizeâ€”without extra training to extend skills and performance!
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼å»ƒç†±ã§ãƒ—ãƒ¼ãƒ«ã‚’åŠ æ¸©ğŸŠ ç’°å¢ƒã«å„ªã—ãã‚³ã‚¹ãƒˆã‚‚ç¯€æ¸› è‹±å›½
	- https://x.com/afpbbcom/status/1770586117449953488?s=20
	- æ•·åœ°å†…ã«è¨­ç½®ã•ã‚ŒãŸè£…ç½®ãŒã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ç¾¤ãŒæ”¾å‡ºã™ã‚‹ç†±ã‚’å–ã‚Šè¾¼ã¿ã€25ï½ãƒ—ãƒ¼ãƒ«ã‚’è¨­å®šæ¸©åº¦ã¾ã§æ¸©ã‚ã‚‹ã€‚ç´„65ï¼…ã‚’ã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€ã‚¬ã‚¹ãƒœã‚¤ãƒ©ãƒ¼ã®ä½¿ç”¨ã¯æŠ‘ãˆã‚‰ã‚Œã¦ã„ã‚‹ã€‚
- O1 Lightã¯Open Interpreterã‚’æ­è¼‰ã—ãŸå°å‹ãƒ‡ãƒã‚¤ã‚¹ã§ã™
	- https://x.com/tegnike/status/1770851466665750758?s=20
-  WSL2ã§RakutenAI-7B-chatã‚’è©¦ã—ã¦ã¿ã‚‹
	- https://note.com/ngc_shj/n/n413ababd3105?sub_rt=share_crp
	- ã€ŒMistral AIç¤¾ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«ã€ŒMistral-7B-v0.1ã€ã‚’åŸºã«ã€ç¶™ç¶šçš„ã«å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã¦é–‹ç™ºã•ã‚ŒãŸ70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€ã§ã‚ã‚‹Rakuten AI 7Bãƒ¢ãƒ‡ãƒ«
	- ã€Œã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’åŸºã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã€ã§ã‚ã‚‹Rakuten AI 7B Chatã‚’è©¦ã—ã¦ã¿ã¾ã™ã€‚
- Swallow-MX-8x7b-NVE-chatvector-Mixtral-instructã®v2ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ by AI ã•ã¨ã—
	- https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2
	- å…ƒãƒ¢ãƒ‡ãƒ«ã¨instructionãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã§ã€æ—¥æœ¬èªæµæš¢æ€§æ”¹å–„ã—ã¦ã„ã¾ã™
- Margaret Mitchel
	- This Women's History Month, we celebrate Margaret Mitchell, the Chief AI Ethics Scientist at huggingface, an open source data science and machine learning platform and hub for AI experts. 
- Transformers 4.39 is out,
	- https://github.com/huggingface/transformers/releases/tag/v4.39.0
	- New models: Mamba, Command-R, LLaVA-NeXT, MusicGen Melody, StarCoder2, SegGPT, ...
	- GaLore optimizer for accessible pre-training
	- Quanto integration and Exllama+AWQ
	- MLX support
-  A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective
	- https://arxiv.org/abs/2302.13425
	- ã¤ã„ã«UQã‚‚æ·±å±¤å­¦ç¿’ã®æ™‚ä»£ã‹
- Estimating Causal Effects with Double Machine Learning -- A Method Evaluation
	- https://arxiv.org/abs/2403.14385
- Meta introduces SceneScript
	- https://x.com/AiBreakfast/status/1771195019585597836?s=20
	- You will be able to upload your own environment to the metaverse:
- ãƒã‚¸ã‹(NLP2024ã®å²¡å´å…ˆç”Ÿã€Knightå…ˆç”Ÿã®ç™ºè¡¨æ¦‚è¦ã‚’Swallow-MXã‚’ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§QLoRA tuningã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ—¥è‹±/è‹±æ—¥ç¿»è¨³)
	- https://x.com/hpp_ricecake/status/1771138490589487602?s=20
-  GoogleãŒæ´ªæ°´ã‚’1é€±é–“å‰ã«äºˆæ¸¬ã—ä¸–ç•Œ80ã‚«å›½4å„„6000ä¸‡äººã‚’æ°´å®³ã‹ã‚‰æ•‘ãˆã‚‹AIã‚’ç™ºè¡¨
	- https://gigazine.net/news/20240322-google-ai-global-flood-forecasting/
	- Google Researchã®ã‚°ãƒ¬ã‚¤ãƒ»ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ°ã‚‰ã®ç ”ç©¶ãƒãƒ¼ãƒ ã¯ã€ä¸–ç•Œå„å›½ã®æµé‡è¨ˆ5680å€‹ãŒ1980ï½2023å¹´ã®é–“ã«é›†ç©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚
	- æ´ªæ°´ãƒŠã‚¦ã‚­ãƒ£ã‚¹ãƒˆã«ã‚ˆã‚‹æ´ªæ°´ã®äºˆæ¸¬ã‚’0æ—¥å‰ã€ã¤ã¾ã‚Šå½“æ—¥ã‹ã‚‰å¹³å‡5æ—¥å‰ã¾ã§å»¶ã°ã—ã€æœ€å¤§ã§7æ—¥å‰ã¾ã§äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- ç«‹ä½“è¨€èª
	- æ°¸ç”°äº®ã€ç«‹ä½“è¨€èªã€ï¼ˆè‡ªç„¶è¨€èªå‡¦ç†31å·»1å·å·»é ­è¨€ï¼‰
	- https://www.jstage.jst.go.jp/article/jnlp/31/1/31_1/_pdf/-char/ja
	- è¨€èªã®ç·šçŠ¶æ€§ï¼ˆä¸€ã¤ã¥ã¤é †ç•ªã«ä¸¦ã¹ã‚‹åˆ¶ç´„ï¼‰ã‚’è¶…ãˆã‚‹ã€ã“ã‚Œã¾ã§ã¨ã¯ç•°ãªã£ãŸæƒ…å ±ä¼é”ã®å¯èƒ½æ€§ã€‚ãã—ã¦ã€ãã“ã«NLPæŠ€è¡“ãŒæ´»ã‹ã›ã‚‹ã®ã§ã¯ãªã„ã‹ã¨ã„ã†ãŠè©±ã€‚
- Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval
	- https://huggingface.co/blog/embedding-quantization
- Excited for MistralAI+ llama_index collabs (and Colabs)
	- https://x.com/jerryjliu0/status/1771262080944857469?s=20
-  Lightblueã€å›½å†…æœ€é«˜æ°´æº–ã®æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã€Œao-Karasuã€ã‚’å…¬é–‹
	- https://prtimes.jp/main/html/rd/p/000000057.000038247.html
	- æ±äº¬å¤§å­¦ç™ºã€æœ€å…ˆç«¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç¾å ´å®Ÿè£…ã«å–ã‚Šçµ„ã‚€AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— æ ªå¼ä¼šç¤¾Lightblueï¼ˆä»£è¡¨å–ç· å½¹ï¼šåœ’ç”°äºœæ–—å¤¢ã€æœ¬ç¤¾ï¼šæ±äº¬éƒ½åƒä»£ç”°åŒºã€ä»¥ä¸‹ã€ŒLightblueã€ï¼‰ã¯720å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã€Œao-Karasuã€ã‚’å…¬é–‹ã—ãŸã“ã¨ã‚’ãŠçŸ¥ã‚‰ã›ã—ã¾ã™ã€‚ã€Œao-Karasuã€ã¯Stability AIç¤¾ãŒæä¾›ã™ã‚‹æ—¥æœ¬èªæ€§èƒ½ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€Japanese MT-Benchã®è‡ªå‹•è©•ä¾¡ã§å›½å†…æœ€é«˜æ°´æº–ã®è©•ä¾¡ã¨ãªã£ã¦ã„ã¾ã™ã€‚
- ã€Swin Transformerã€‘ä»Šã“ãæŠ¼ã•ãˆãŸã„Transformerç³»ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«
	- https://ai-scholar.tech/articles/image-recognition/swin-transformer
	- è¿‘å¹´ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®ç ”ç©¶ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã‚ˆãç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹Swin Transformerã‚’è§£èª¬  
	- ã™ã¹ã¦ã®ãƒ‘ãƒƒãƒã¨é–¢é€£æ€§(Attention)ã‚’è¨ˆç®—ã™ã‚‹Vision Transformerã¨ã¯ç•°ãªã‚Šï¼Œè¿‘å‚ã®ãƒ‘ãƒƒãƒã‚’ã¾ã¨ã‚ãŸwindowå†…ã§Attentionã‚’è¨ˆç®—ã™ã‚‹  
	- ç•°ãªã‚‹ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºã§Attentionã®è¨ˆç®—ã‚’è¡Œã†ãŸã‚ï¼Œæ§˜ã€…ãªã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´ãŒå¾—ã‚‰ã‚Œã‚‹
-  [é€²åŒ–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰](https://sakana.ai/evolutionary-model-merge-jp/)
	- Sakana AIã¯é€²åŒ–ã‚„é›†åˆçŸ¥ãªã©ã®è‡ªç„¶ç•Œã®åŸç†ã‚’å¿œç”¨ã—ã¦åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚ç§é”ã®ç›®æ¨™ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªã‚‰è¨“ç·´ã—é–‹ç™ºã™ã‚‹ã“ã¨ã ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã‚’åŠ¹ç‡åŒ–ã€é«˜åº¦åŒ–ã€è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã®æ–°ãŸãªæ‰‹æ³•ã‚’ç”Ÿã¿å‡ºã™ã“ã¨ã«æŒ‘æˆ¦ã—ã¦ã„ã¾ã™ã€‚ã“ã®ç›®æ¨™ã«å‘ã‘ãŸç¬¬ä¸€æ­©ã¨ã—ã¦ã€ç§ãŸã¡ã¯ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã€ŒEvolutionary Optimization of Model Merging Recipes ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®é€²åŒ–çš„æœ€é©åŒ–ï¼‰ã€ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚
	- è¤‡æ•°ã®NNã‚’é‡ã¿ãƒ»å±¤ãƒ¬ãƒ™ãƒ«ã§ãƒãƒ¼ã‚¸ã™ã‚‹éš›ã®æœ€é©ãªçµ„åˆã›ã‚’EAã§æ¢ç´¢ã™ã‚‹é€²åŒ–çš„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã‚’ææ¡ˆã€‚æ•°å­¦ã¨æ—¥æœ¬èªãªã©ç•°ãªã‚‹é ˜åŸŸã«ç‰¹åŒ–ã—ãŸLLMã‚’ã†ã¾ããƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’å‘ä¸Šã§ãã‚‹ã€‚
- team DataPilot2ã¤ç›®ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ã€ŒArrowSmartPlus_3.6B_instant_sft_JSHVerã€ã‚’ãƒªãƒªãƒ¼ã‚¹ã„ãŸã—ã¾ã™
	- https://huggingface.co/DataPilot/ArrowSmartPlus_3.6B_instant_sft_JHSVer
	- Lineç¤¾ãŒé–‹ç™ºã—ãŸã€Œjapanese-large-lm-3.6b-instruction-sftã€ã‚’ã‚¦ã‚£ã‚­ãƒ–ãƒƒã‚¯ã®å†…å®¹ã‚’ã‚‚ã¨ã«ä¸­å­¦ç¯„å›²ã«ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚
- ã€ŒLOCAL AI HACKATHONã€ã«ãŠã‘ã‚‹ã€ãƒãƒ¼ãƒ DataPilotã®æˆæœå“ç¬¬ä¸€å¼¾ã§ã‚ã‚‹ã€ŒArrowSmart_1.7b_instant_sftã€ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸ
	- https://huggingface.co/DataPilot/ArrowSmart_1.7b_instant_sft
	- Lineç¤¾ãŒé–‹ç™ºã—ãŸã€Œjapanese-large-lm-1.7b-instruction-sftã€ã‚’ã‚¦ã‚£ã‚­ãƒ–ãƒƒã‚¯ã®å†…å®¹ã‚’ã‚‚ã¨ã«åœ°ç†ã€åŒ–å­¦ã®åˆ†é‡ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚
-  The Elements of Differentiable Programming
	- https://arxiv.org/abs/2403.14606
	- æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã€å¾®åˆ†å¯èƒ½ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦ Google DeepMind ã®ç ”ç©¶è€…ãŒ383ãƒšãƒ¼ã‚¸ã«æ¸¡ã‚‹PDFã‚’å…¬é–‹ã€‚è«–æ–‡ã‚ˆã‚Šæœ¬ã¨ã„ã†æ–¹ãŒæ­£ã—ãã†
	- ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹ã“ã¨ã¯æœ¬è³ªçš„ã«ç¢ºç‡åˆ†å¸ƒã«ã‚ˆã£ã¦ãã®å‡ºåŠ›ã®ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã€ã¨ã¯é¢ç™½ã„
	- **å¾®åˆ†å¯èƒ½ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯**: ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾®åˆ†å¯èƒ½ãªæ–¹æ³•ã§æœ€é©åŒ–ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™
	- **ç›®æ¨™ã¨ç¯„å›²**: æœ¬æ›¸ã¯ã€å¾®åˆ†å¯èƒ½ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºç¤ã‚’èª¬æ˜ã—ã€ãã®ç†è«–ã¨å®Ÿè·µã®ä¸¡æ–¹ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚
- Sakana AIãŒã€ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã‚’è‡ªå‹•åŒ–ãƒ»é«˜åº¦åŒ–ã™ã‚‹é€²åŒ–çš„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ï¼ˆEvolutionary Model Mergeï¼‰
	- https://huggingface.co/SakanaAI

## 3/18

ä»Šé€±ã‚‚ã„ã‚ã„ã‚ã‚ã‚Šã™ãã¦ã€ç›®ãŒå›ã‚Šã¾ã™ã€‚æ±å·¥å¤§ã‹ã‚‰Swallow-MS 7Bã¨Swallow-MX 8x7Bã®ãƒªãƒªãƒ¼ã‚¹ã€å‰è€…ã¯æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã¨ã®ã“ã¨ã€‚ é‡å­åŒ–ç‰ˆã‚‚å‡ºã¦ã€Llama.cpp ã§Swallow-MX 8x7Bã‚’å‹•ã‹ã—ãŸä¾‹ã‚‚ç´¹ä»‹ã•ã‚ŒãŸã€‚Swallow-MS-7b-v0.1 ã‚’ ichikara instruction ã§æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€500ã‚¹ãƒ†ãƒƒãƒ—ãã‚‰ã„ã§ã„ã„æ„Ÿã˜ã¨ã®å ±å‘Šã‚‚ã€‚ã€ŒELYZA-japanese-Llama-2-70bã€ãŒå‡ºãŸãƒ¼ã€NHKã§ã‚‚ç´¹ä»‹ã•ã‚ŒãŸã€ABCIã‚’12æœˆã‹ã‚‰éƒ¨åˆ†å æœ‰ï¼Ÿã€ã‚ˆã†ã‚„ãã‚¹ã‚¿ãƒ¼ãƒˆãƒ©ã‚¤ãƒ³ã¨ã„ã†CEOã®è¨€è‘‰ãŒåˆºã•ã‚‹ã€‚Shi3zã•ã‚“ã«ã‚ˆã‚‹ã¨ã€Claude-3ã¨æ¯”ã¹ã‚‹ã¨ç™¾äººä¸€é¦–ã®çŸ¥è­˜ãŒè¶³ã‚Šãšã¾ã é ‘å¼µã‚Œã¨ã„ã†æ„Ÿã˜ã ãŒå¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã‚‹ã¨æ ¼æ®µã®é€²æ­©ãŒã‚ã‚‹ã¨ã®ã“ã¨ã€‚ã€ŒJPX Market Explorerã€ã€NISAã§å€‹åˆ¥æŠ•è³‡ã‚’è€ƒãˆã¦ã„ã‚‹ã²ã¨ã¯å¿…è¦‹ã€‚è‡ªç¤¾ãƒ“ã‚¸ãƒã‚¹ï¼æ ªå–å¼•ã‚’æ´»ç™ºã«ã™ã‚‹ãŸã‚ã®ã€ç”ŸæˆAIã®æ´»ç”¨ã¨ã—ã¦é¢ç™½ã„ã€‚256k token ãŒæ‰±ãˆã‚‹GPT-4.5 Turbo ãŒï¼–æœˆã”ã‚ã«ãƒªãƒªãƒ¼ã‚¹ã¨ã„ã†ã†ã‚ã•ãŒæŒã¡ä¸ŠãŒã‚‹ã€ãƒªãƒ¼ã‚¯ãªã®ã‹ï¼Ÿã€‚ä¸€èˆ¬copilotã‹ã‚‰ã‚‚GPT-4 TurboãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸã‚‰ã—ã„ã€OpenAIï¼‹ãƒã‚¤ã‚¯ãƒ­ã‚ªãƒ•ãƒˆé™£å–¶ã‚‚é…ã‚Œã‚‹ã‚ã‘ã«ã¯è¡Œã‘ãªã„ã€‚ä¼æ¥­ãŒæœŸå¾…ã™ã‚‹ä»Šé¢¨ã®ã€Œä¸»ä½“æ€§ã€ã£ã¦ã€æ€è€ƒåŠ›ã¨å”èª¿ãƒ»å”åƒã§ãã‚‹åŠ›ã¨ã„ã†è©±ã ã‘ã©ã€ã“ã®åˆ†é‡ã€ç”ŸæˆAIãŒè‹¦æ‰‹ã¨ã‚‚è¨€ãˆãªããªã£ãŸæ°—ãŒã™ã‚‹ãªã€‚AIã«ã‚ˆã‚‹ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢Devinã€ãªã‚“ã‹ã™ã”ã„ã€é§†é€ã•ã‚Œã‚‹äººãŸã¡ãŒãŸãã•ã‚“ã„ãã†ã ã€‚ã©ã†ã‚‚VCç•Œéšˆã§ã¯ã€AIå¾“æ¥­å“¡ã®é–‹ç™ºã®é¢¨ãŒå¹ã„ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚JSTã®ã€Œè‡ªå¾‹é§†å‹•ã«ã‚ˆã‚‹ç ”ç©¶é©æ–°ã€ã¯ç ”ç©¶ãã®ã‚‚ã®ã‚’AIã§è‡ªå‹•åŒ–ã¨ã„ã†è©±ã€ã²ãˆï¼ã€‚Claude 3 Opusã‚’ä½¿ã£ã¦ä¸–ç•ŒçµŒæ¸ˆã‚’åˆ†æã™ã‚‹ãƒ‡ãƒ¢å‹•ç”»ã‚‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆAIå¾“æ¥­å“¡ï¼‰ã‚’ã¤ãã£ã¦èª¿æŸ»ã‚’åŠ é€Ÿã§ãã‚‹ã¨ã„ã†è©±ã€‚ã‚ã‚ã€äººã¯ã„ã‚‰ãªããªã‚‹ã®ã‹ï¼Ÿã€‚Claude3ã®æ€§èƒ½è©•ä¾¡ã¯ç¶šãã€ã²ã‚ã¿ã¡ã‚…å…ˆç”ŸãŒã€æ§˜ã€…ãªãªäº‹ä¾‹ã‚’è©¦ã—ã¦çµ¶è³›ã€Coinhiveäº‹ä»¶æœ€é«˜è£åˆ¤æ±ºã®è§£é‡ˆãªã©ã€ä½¿ã„æ–¹ã®å‚è€ƒã«ã‚‚ãªã‚‹ã€‚Claude3 Ã— Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰æ™®é€šã«Claude3ã‚’ä½¿ãˆã‚‹ã€ãªã‚“ã‹ã¡ãŒã†ãªã€‚æ¾ç”°å…ˆç”Ÿã®è€ƒå¯Ÿã®ã‚ˆã†ã«ã€LLMã£ã¦ååˆ†ç–ãªã®ã§ã¯ãªã„ã‹ã€ã¾ã ã¾ã é‡å­åŒ–ã¨ã‹è»½é‡åŒ–ã®ä½™åœ°ãŒã‚ã‚‹ã€‚ä¸–ç”°è°·åŒºã®AI botã€éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã§é–‹ç™ºã¨ã€‚NLP2024ã‚‚é–‹å‚¬ã€å²¡é‡åŸã•ã‚“ã®ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®å±•æœ›ã¨ä»Šå¾Œã®èª²é¡Œã€ã€è©±é¡Œã¨ã—ã¦ã¯æœ¬LLMã‚¢ãƒ—ãƒ‡èª­è€…ã«ã¯ãªã˜ã¿ã®æ·±ã„è©±é¡Œã€‚AIã¯ç§‘å­¦ã‚’ä¿ƒé€²ã™ã‚‹ãŒã€ã€ç†è§£ã®éŒ¯è¦šã€ã‚’ç”Ÿã¿å‡ºã™å±é™ºæ€§ãŒã‚ã‚‹ã€ã¨è¨˜äº‹ã¯æ–°ã—ã„è¦–ç‚¹ã§èˆˆå‘³æ·±ã„ã€‚ã‚«ãƒ¼ãƒ„ãƒ¯ã‚¤ãƒ«ã•ã‚“ã€å¤§è„³çš®è³ªã¨è¨ˆç®—æ©ŸãŒã¤ãªãŒã‚‹ã®ãŒ2030å¹´ä»£åˆé ­ã¨ã„ã£ã¦è©±é¡Œã«ã€‚OpenAIã¨ãƒ­ãƒœãƒƒãƒˆé–‹ç™ºã®Figureã®ææºã®çµæœã®ç¬¬ï¼‘æ®µFigure01ã€ã„ã‚„ã“ã‚Œã£ã¦ãªã‚“ã‹ã®æ˜ ç”»ï¼ˆãƒ‘ãƒƒã‚»ãƒ³ã‚¸ãƒ£ãƒ¼ï¼‰ã§è¦‹ãŸä¸–ç•Œã€‚Natureã®All of usã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã€117å€‹ã®ç–¾æ‚£ã«é–¢é€£ã™ã‚‹3724å€‹ã®å¤‰ç•°ã‚’åŒå®šã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ã‚‚å…¬é–‹ã¨ã®ã“ã¨ã€‚æœ€å¾Œã«ã€XãŒäºˆå‘ŠãŠé™ã‚Š Grok-1ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹ã€‚ç›´å‰ã«ã€OpenAIãŒGrokã®åˆ¥å®Ÿè£…ã‚’OSSã§å…¬é–‹ã—ã¦ãŸã‚Šã—ã¦ã€ã“ã†ã„ã†ç«¶äº‰ã€ã„ã‚„å…±å‰µï¼Ÿã£ã¦é¢ç™½ã„ãªã€‚


- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«Swallow-MS 7Bã¨Swallow-MX 8x7Bã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- https://tokyotech-llm.github.io/swallow-mistral
	- Swallow-MS 7Bã¯ã‚ªãƒ¼ãƒ—ãƒ³ãª7Bã®LLMã®ä¸­ã§æ—¥æœ¬èªæœ€é«˜æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€‚
-  Yi: Open Foundation Models by 01.AI
	- https://arxiv.org/abs/2403.04652
	- Super interesting paper - 10k data is all you need for finetuning LLM
	- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯1ä¸‡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã§å……åˆ†ãªã‚“ã ã¨ã„ã†è«–æ–‡ã€‚
- Claude 3ã«ä¾‹ã®ã€Œèª­äº†ç›®å®‰2æ™‚é–“ã€è¨˜äº‹ã‚’è§£èª¬ã•ã›ã¦ã¿ãŸ - é«˜æœ¨æµ©å…‰ï¼ è‡ªå®…ã®æ—¥è¨˜ï¼ˆ2024å¹´3æœˆ11æ—¥ï¼‰
	- https://takagi-hiromitsu.jp/diary/20240311.html
	- ã²ã‚ã¿ã¡ã‚…å…ˆç”Ÿçµ¶è³›
	- ã€ŒAnthropicã®å…ˆæ—¥å‡ºãŸã°ã‹ã‚Šã®Claude 3ï¼ˆOpusï¼‰ãŒã€ChatGPTã®GPT-4ã‚’è¶…ãˆã¦ããŸã¨èã„ã¦ã€è‡ªåˆ†ã®åŸç¨¿ã‚’è§£èª¬ã•ã›ã¦ã¿ãŸã¨ã“ã‚ã€ç¢ºã‹ã«é©æ–°çš„ãªé€²æ­©ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ã‚‚ã¯ã‚„å†…å®¹â€¦ã€
-  Is Cosine-Similarity of Embeddings Really About Similarity?
	- https://arxiv.org/abs/2403.05440
	- ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ç–‘ã£ã¦ã„ã‘ï¼ï¼
- Swallow-MX-8x7b-NVE-v0.1ã®ggufã‚ã‚Šã¾ã™
	- https://huggingface.co/mmnga/tokyotech-llm-Swallow-MX-8x7b-NVE-v0.1-gguf
- äººå·¥è¨€èªã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’ã‚’ç”¨ã„ãŸè¨€èªé–“è»¢ç§»å¯èƒ½ãªçŸ¥è­˜ã®åˆ†æ
	- https://www.jstage.jst.go.jp/article/jnlp/30/2/30_664/_article/-char/ja/
	- Transformerã®äº‹å‰å­¦ç¿’ã«äººå·¥è¨€èªã‚’ä½¿ã£ãŸã‚‰ã©ã†ãªã‚‹ã‹ã€ã©ã®è¦ç´ ãŒäº‹å‰å­¦ç¿’ã«åŠ¹ãã®ã‹ã€ã¨ã„ã†ç ”ç©¶ ä¿‚ã‚Šå—ã‘é–¢ä¿‚ã«å…¥ã‚Œå­æ§‹é€ ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒé‡è¦ã‚‰ã—ã„
- Llama.cpp ã§ Swallow MX 8x7B ã‚’ãŠè©¦ã—ä¸­ã€€by npakaã•ã‚“
	- https://x.com/npaka123/status/1767380241520173408?s=20
- Stealing Part of a Production Language Model
	- https://arxiv.org/abs/2403.06634
	- GPT-4ã®ã‚ˆã†ãªClosedãªãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚‚,APIã‚¢ã‚¯ã‚»ã‚¹ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨ã®å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®šã§ãã‚‹Model-stealing attackã‚’ææ¡ˆ
	- Googleã®OpenAIã«å¯¾ã™ã‚‹é€†è¥²ã®ä¸€æ‰‹çš„ãªè«–æ–‡
	- APIçµŒç”±ã§OpenAIã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹éš ã‚Œæ¬¡å…ƒæ•°ã‚’ç‰¹å®šã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€OpenAIãŒãã‚Œã‚’å—ã‘å¯¾ç­–ã‚’æ–½ã—ãŸã“ã¨ã‚’è«–æ–‡ã§å ±å‘Šã—ã¾ã—ãŸã€‚
- 700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ—¥æœ¬èªLLMã€ŒELYZA-japanese-Llama-2-70bã€ã‚’é–‹ç™ºã—ã€ãƒ‡ãƒ¢ã‚’å…¬é–‹ã—ã¾ã—ãŸ
	- https://note.com/elyza/n/n0ea755ca3e7b
	- https://elyza.ai/lp/elyza-llm-for-jp
	- æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯æœ€å¤§ç´šã§ã™.å¤§ãã•ãŒæ­£ç¾©ã®LLMã¨ã„ã†ã“ã¨ã§,å®Ÿéš›å ±å‘Šã•ã‚Œã¦ã„ã‚‹æ€§èƒ½ã‚‚ã‹ãªã‚ŠæŠœã‘ã¦ã„ã¾ã™
- ELYZA-japanese-Llama-2-70b ã‚’ãŠè©¦ã—ä¸­ by npakaã•ã‚“
	- https://x.com/npaka123/status/1767439590502326514?s=20
	- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æŒ‡ç¤ºã‚‚åŠ¹ã„ã¦ã‚‹
-  æ±å¤§ç™ºã®ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ä¼æ¥­ â€œå›½å†…æœ€å¤§è¦æ¨¡ å›½ç”£ç”ŸæˆAIå®Œæˆâ€
	- https://www3.nhk.or.jp/news/html/20240312/k10014388011000.html
	- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨å‘¼ã°ã‚Œã‚‹å…¬é–‹æŠ€è¡“ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ç”£æ¥­æŠ€è¡“ç·åˆç ”ç©¶æ‰€ãŒé‹å–¶ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã€ŒABCIã€ãªã©ã‚’æ´»ç”¨ã—ã€å»å¹´12æœˆã‹ã‚‰çŸ­æœŸé–“ã§é–‹ç™ºã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚
	- ã‚¤ãƒ©ã‚¤ã‚¶ã®æ›½æ ¹å²¡ä¾‘ä¹Ÿç¤¾é•·ã¯ã€Œæ˜¨å¹´æœ«æ™‚ç‚¹ã§ã¯ã‚ªãƒ¼ãƒ—ãƒ³AIã‚„ã‚°ãƒ¼ã‚°ãƒ«ãªã©ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦æ—¥æœ¬ã®AIãƒ¢ãƒ‡ãƒ«ã¯åŠã°ãªã„çŠ¶æ…‹ã ã£ãŸã€‚ä»Šå›ã‚ˆã†ã‚„ãã‚¹ã‚¿ãƒ¼ãƒˆãƒ©ã‚¤ãƒ³ã«ç«‹ã¤ã“ã¨ãŒã§ãã€æ—¥æœ¬ãŒå­˜åœ¨æ„Ÿã‚’ç¤ºã›ã‚‹ã‚ˆã†ã«ã—ãŸã„ã€ã¨è©±ã—ã¦ã„ã¾ã—ãŸã€‚
-  æ¾å°¾ç ”LLMé–‹ç™ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚­ãƒƒã‚¯ã‚ªãƒ•ã‚’é–‹å‚¬ã—ã¾ã—ãŸ
	- https://weblab.t.u-tokyo.ac.jp/2024-03-12/
	- å½“ç ”ç©¶å®¤ãŒæä¾›ã™ã‚‹è¬›åº§ã®ä¿®äº†ç”ŸãŠã‚ˆã³ä¸€èˆ¬å…¬å‹Ÿã«ã‚ˆã£ã¦é›†ã¾ã£ãŸæœ‰å¿—ã®é–‹ç™ºè€…ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒ500å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚’é€²ã‚ã‚‹ã‚‚ã®ã§ã™ã€‚
	- NEDOã«ã‚ˆã‚‹ã€å›½å†…ã®ç”ŸæˆAIã®é–‹ç™ºåŠ›ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ŒGENIACï¼ˆGenerative AI Accelerator Challengeï¼‰ã€ã«ãŠã„ã¦ã€åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«å¿…è¦ãªè¨ˆç®—è³‡æºã®æä¾›æ”¯æ´ã‚’å—ã‘ã¦ã„ã¾ã™ã€‚
	- æ¾å°¾æ•™æˆã‹ã‚‰ã¯ã€Œã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸­ã§ã€è©¦è¡ŒéŒ¯èª¤ã—ãªãŒã‚‰é‡è¦ã§ã‚ã‚‹ãƒã‚¦ãƒã‚¦ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã§è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã€é–‹ç™ºçµŒé¨“ã‚’ç©ã‚“ã§ã‚‚ã‚‰ã„ãŸã„ã€‚ã¾ãŸã€ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é€šã—ã¦ã€ã‚ˆã‚Šå¤šãã®LLMé–‹ç™ºè€…ã‚’ç”Ÿã¿å‡ºã—ã€å‚åŠ è€…ã®çš†ã•ã‚“ãŒæ§˜ã€…ãªã¨ã“ã‚ã§æ´»èºã—ã¦ã‚‚ã‚‰ã†ã®ãŒæœ›ã¿ã ã€ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã—ãŸã€‚
- Elyza70Bã€Claude-3ã¨æ¯”ã¹ã‚‹ã¨ç™¾äººä¸€é¦–ã®çŸ¥è­˜ãŒè¶³ã‚Šãšã¾ã é ‘å¼µã‚Œã¨ã„ã†æ„Ÿã˜ã ãŒå¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã‚‹ã¨æ ¼æ®µã®é€²æ­©ãŒã‚ã‚‹ by shi3zã•ã‚“
	- https://x.com/shi3z/status/1767464684373082223?s=20
-  G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
	- https://arxiv.org/abs/2402.07630
- ï¼ªï¼°ï¼¸ç·ç ”ã¯ã€ç”ŸæˆAIãƒ—ãƒ­ãƒã‚¤ãƒ€ã§ã‚ã‚‹Bridgewiseã®æŠ€è¡“ã‚’æ´»ç”¨ã—ã€æ—¥æœ¬å¸‚å ´ã«ã‹ã‹ã‚‹æƒ…å ±ã‚’ç™ºä¿¡ã™ã‚‹æ–°ã‚µãƒ¼ãƒ“ã‚¹ã€ŒJPX Market Explorerã€ã®PoCã‚’é–‹å§‹ã—ã¾ã™ã€‚
	- https://www.jpx.co.jp/corporate/news/news-releases/6020/20240312-01.html
	- æ±è¨¼ã«ä¸Šå ´ã™ã‚‹ä¼šç¤¾ã«ã¤ã„ã¦ã€å€‹ç¤¾ã®ãƒ“ã‚¸ãƒã‚¹æ¦‚è¦ã‚„ç›´è¿‘ã®æ±ºç®—ã®ã‚µãƒãƒªãƒ¼ã‚’ç°¡å˜ã«èª¿ã¹ãŸã‚Šã€è²¡å‹™çŠ¶æ³ã«ã¤ã„ã¦ã®åˆ†æã‚„ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚
	- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚„åˆ†æã¯Bridgewiseã®ç”ŸæˆAIãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚’åˆ©ç”¨ã—ã¦ä½œæˆã•ã‚Œã¾ã™
	- ç”ŸæˆAIã‚’ç”¨ã„ã¦å„ä¼æ¥­ã®æ¦‚è¦ã€ç›´è¿‘ã®æ±ºç®—ã‚µãƒãƒªã€è²¡å‹™çŠ¶æ³ã®ç°¡å˜ãªåˆ†æã‚„ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹
-  Integrating Phenotypic and Chemoproteomic Approaches to Identify Covalent Targets of Dietary Electrophiles in Platelets
	- https://pubs.acs.org/doi/full/10.1021/acscentsci.3c00822
	- ãƒ–ãƒ­ãƒƒã‚³ãƒªãƒ¼ã«ã¯å¼·åŠ›ãªæŠ—ãŒã‚“ä½œç”¨ãŒã‚ã‚‹ã“ã¨ã¯çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã‘ã‚Œã©ã€ã‚·ãƒ‰ãƒ‹ãƒ¼å¤§å­¦ã‚‰ã®ç ”ç©¶ã«ã‚ˆã‚Œã°ã€ãƒ–ãƒ­ãƒƒã‚³ãƒªãƒ¼ã¯ç™Œã ã‘ã§ãªãã€è„³å’ä¸­ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ã®ã‚ã‚‹è¡€æ “ç—‡ã‚’äºˆé˜²ã—ã€è¡€æ “ç—‡ã®æ²»ç™‚ã‚’è£œåŠ©ã™ã‚‹åŠ¹æœã‚‚ã‚ã‚‹ã¨ç¤ºã•ã‚ŒãŸã€‚
- Llama.cpp ã§ Swallow MX 8x7B ã‚’è©¦ã™
	- https://note.com/npaka/n/n0a9b514756ae?sub_rt=share_b
	- ã€ŒSwallow MX 8x7Bã€ã¯ã€ã€ŒMixtral 8x7Bã€ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’å¼·åŒ–ã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™
-  Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU
	- https://arxiv.org/abs/2403.06504
	- æœ¬è«–æ–‡ä¸­ã§ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹Fuyouã‚’ä½¿ã†ã¨ã€ãªã‚“ã¨ä¸€èˆ¬æ¶ˆè²»è€…å‘ã‘ã®GPUã§ã‚ã‚‹RTX 4090ä¸Šã§175Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã¤ã¾ã‚ŠGPT-3 ã‚’å¾®èª¿æ•´å¯èƒ½ãªã‚“ã§ã™ã£ã¦ï¼
- Claude3ã®å…¬å¼promptãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è‹±æ–‡æ ¡æ­£prompt
	- https://note.com/genkaijokyo/n/n3f82b191dfda
	- Your task is to take the text provided and rewrite it into a clear, grammatically correct version while preserving the original meaning as closely as possible. Correct any spelling mistakes, punctuation errors, verb tense issues, word choice problems, and other grammatical mistakes. Use bold formatting in markdown to emphasize the edited portions of the English text.
- Raspberry Pi 5ã«æ—¥æœ¬èªLLM(ELYZA-Japanese-Llama-2-7b-fast-Instruct)ã‚’å…¥ã‚Œã¦ã¿ãŸ
	- https://arkouji.cocolog-nifty.com/blog/2024/03/post-e248e6.html
-  RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems
	- https://arxiv.org/abs/2403.06465
	- Microsoft presents a toolkit to integrate LLMs into recommender systems for explainability, conversation, and user control.
-  è‡¨åºŠäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã®è¦ç‚¹
	- https://note.com/tadahiro_goto/n/n90128159a7fb?sub_rt=share_pb
	- 2024å¹´1æœˆã«BMJã®Research Methods & Reportingã§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨å¤–çš„æ¤œè¨¼ã«é–¢ã™ã‚‹review
	- Evaluation of clinical prediction models (part 1): from development to external validation.
	- ãƒã‚¤ãƒ³ãƒˆ
		- è‡¨åºŠäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯ã€**ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å¯¾è±¡é›†å›£ã‚’ä»£è¡¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡**ã™ã¹ã
		- é–‹ç™ºç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯å„ªã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆãŸãƒ¢ãƒ‡ãƒ«ã‚‚ã€åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡ã™ã‚‹ã¨ã€ï¼ˆä»®ã«åŒã˜æ¯é›†å›£ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚ã£ã¦ã‚‚ï¼‰æ€§èƒ½ãŒä½ããªã‚‹ã“ã¨ãŒã»ã¨ã‚“ã©ã€‚
		-   **ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹æ™‚ç‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²(split)ã™ã‚‹ã“ã¨ã¯ã€ä¿¡é ¼æ€§ã®ä½ã„ãƒ¢ãƒ‡ãƒ«ã«ã¤ãªãŒã‚‹ãŸã‚é¿ã‘ã‚‹ã¹ã**ã€‚
		- åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã™ã‚‹åŠªåŠ›ã‚’ã™ã¹ãï¼ˆå†…çš„æ¤œè¨¼ã«ãŠã‘ã‚‹resamplingã‚„ã€å†…çš„-å¤–çš„äº¤å·®æ¤œè¨¼ãªã©ï¼‰
- Accelerate v0.28.0 has been released!
	- From XLA GPU support to FSDP + QLORA, and more, let's dive into what's new!
- éŸ³å£°èªè­˜ã«ä½¿ãˆã‚‹ãƒ¢ãƒ‡ãƒ«ã¯æ§˜ã€…ã‚ã‚Šã¾ã™ãŒã€ç¾çŠ¶æœ€ã‚‚ä½¿ã„ã‚„ã™ã„ã‚‚ã®ã®ä¸€ã¤ãŒ faster-whispe
	- https://github.com/SYSTRAN/faster-whisper
- shioriha-large-pt
	- https://huggingface.co/cl-nagoya/shioriha-large-pt
	- æ±åŒ—å¤§BERT-largeã«å¯¾ã—ã€batch size 8192, ç³»åˆ—é•· 256ã§ã€æ—¥æœ¬èªWikipediaã‚„MMARCOã¨ã„ã£ãŸå¼±æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹å¯¾ç…§äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹shioriha-large-ptã‚’å…¬é–‹ã—ã¾ã—ãŸ
- Tour of Modern LLMs
	- https://phontron.com/class/anlp2024/assets/slides/anlp-15-tourofllms.pdf
	- CMUã®è¬›ç¾©è³‡æ–™ã€
	- I made some new class slides on â€œa tour of modern LMsâ€ that has some observations about characteristics of recent LLMs, mostly focusing on open LLMs where we know their details
-  Algorithmic progress in language models
	- https://arxiv.org/abs/2403.05812
	- How quickly have the algorithms behind language models like GPT-4 been improving over time?
- Talk like a graph: Encoding graphs for large language models
	- https://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html
	- Graphs, structures that describe connections between objects, are everywhere â€” imagine the tools in a kitchen, parts of a bike, or a group of friends. Learn about our latest work that explores how to encode graphs in a format that an LLM can understand:
- GPT-4.5 Turbo possible release in June, 256k token context window
	- https://x.com/AiBreakfast/status/1767612026925277424?s=20
	- This OpenAI blog search result shows up in a DuckDuckGo search of â€œOpenAI GPT-4.5 Turboâ€ link, then goes to an OpenAI Error 404 page.
- ä¼æ¥­ãŒæ±‚ã‚ã‚‹ä¸»ä½“æ€§ã¨ã¯ãªã«ã‹ï¼Ÿ
	- https://www.amazon.co.jp/dp/4798918431/ref=cm_sw_r_as_gl_api_gl_i_294DJF3GFDESXD5WSRBV?linkCode=ml1&tag=regista13-22
	- ä¼æ¥­ãŒæœŸå¾…ã™ã‚‹ã€Œä¸»ä½“æ€§ã€ã¯ã‹ã¤ã¦ã¯è¡Œå‹•åŠ›ã ã£ãŸã®ãŒä»Šã¯æ€è€ƒåŠ›ã¨å”èª¿ãƒ»å”åƒã§ãã‚‹åŠ›ã«ãªã£ã¦ã‚‹ã¨ã®ã“ã¨ã€‚ã‚³ãƒŸãƒ¥åŠ›ã®æ™‚ä»£ã®åæ˜ ã€‚
-  ã„ã¾ã€Œæ–°ã—ã„æ•°å­¦ã€ãŒå¿…è¦ã ã€‚åŠ©ã‘ã¦æ•°å­¦è€…! by shi3z ã•ã‚“
	- https://note.com/shi3zblog/n/nafa1cee6ada2?sub_rt=share_pw
	- ãŸã¶ã‚“AIä»¥å¾Œã®ä¸–ç•Œã§æœ€ã‚‚ä¾¡å€¤ã‚’æŒã¤ã®ã¯ã€Œæ•°å­¦è€…ã€ã§ã‚ã‚‹ã€‚ã—ã‹ã‚‚ã€Œé«˜æ¬¡å…ƒå¹¾ä½•å­¦ã€ãªã„ã—ã€ãã‚Œã‚’ä¸Šå›ã‚‹ãã‚‰ã„ã®æ¦‚å¿µã‚’ç™ºæ˜ã™ã‚‹æ•°å­¦è€…ã ã‚ã†
- Devin, the first AI software engineer.
	- https://x.com/cognition_labs/status/1767548763134964000?s=20
	- Devin is the new state-of-the-art on the SWE-Bench coding benchmark, has successfully passed practical engineering interviews from leading AI companies, and has even completed real jobs on Upwork.
	- AIã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼ˆDevinï¼‰ãŒäººé–“ãƒ¬ãƒ™ãƒ«ã«é”ã—ãŸåˆã‚ã¦ã®ãƒ‡ãƒ¢ã ã¨æ€ã†ã€‚AIã®å°å…¥ã§èª²é¡Œã¨ãªã£ã¦ãŸã®ãŒé•·æœŸçš„ãªæ¨è«–ã¨è¨ˆç”»ã€‚ã¨ã“ã‚ãŒã€Devinã¯è¨ˆç”»â†’å®Ÿè¡Œâ†’è©•ä¾¡â†’å†è¨ˆç”»ã‚’ç¹°ã‚Šè¿”ã—ã€ç›®æ¨™é”æˆã¸ã¨å°ãã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹
-  é€Ÿå ±ï¼šClaude 3ã«åˆ¤ä¾‹è©•é‡ˆã‚’è‡ªå‹•ç”Ÿæˆã•ã›ã¦ã¿ãŸï¼ˆCoinhiveäº‹ä»¶æœ€é«˜è£åˆ¤æ±ºã®å·»ï¼‰
	- https://takagi-hiromitsu.jp/diary/20240313.html
	- ã€Œã“ã‚Œã ã‘LLMãŒé•·æ–‡ã®æ„å‘³å†…å®¹ã‚’ã€Œç†è§£ã€ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸã¨ãªã‚‹ã¨ã€ã‚‚ã¯ã‚„ã€æ›¸è©•ã‚„è«–æ–‡ç´¹ä»‹ã€åˆ¤ä¾‹æ‰¹è©•ãªã©ã€å®šå½¢çš„ãªã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒã¤å­¦è¡“è¨˜äº‹ã¯ã€â€¦ã€
	- ã²ã‚ã¿ã¡ã‚…å…ˆç”Ÿçµ¶è³›
- Swallow-MS-7b-v0.1 ã‚’ ichikara instruction ã§æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç·´ç¿’ã€‚500ã‚¹ãƒ†ãƒƒãƒ—(0.2ã‚¨ãƒãƒƒã‚¯ : 20åˆ†) ã®ãŠè©¦ã—ã ã‘ã©ã€ãã‚Œã„ã«å›ç­”ã—ã¦ãã‚Œã¦ã‚‹
	- https://x.com/npaka123/status/1767807910925545892?s=20
- Claude 3 Haiku, the fastest and most affordable model in its intelligence class.
	- https://x.com/AnthropicAI/status/1768018310615151002?s=20
- With OpenAI, Figure 01 can now have full conversations with people
	- https://x.com/Figure_robot/status/1767913661253984474?s=20
	- ChatGPTã€ã¤ã„ã«ãƒ­ãƒœãƒƒãƒˆã«å®¿ã‚‹
	- 2é€±é–“å‰ã€OpenAIã¨ãƒ­ãƒœãƒƒãƒˆé–‹ç™ºã®FigureãŒææºã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚
	- ä»Šå›ã€Figureã¯ã€ChatGPTã®æŠ€è¡“ã‚’ãƒ­ãƒœãƒƒãƒˆã«æ­è¼‰ã—ãŸã“ã¨ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚
	- é éš”æ“ä½œãªã—ã®100%ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ã‚·ã‚¹ãƒ†ãƒ  
	- OpenAIã®ãƒ¢ãƒ‡ãƒ«ãŒé«˜ãƒ¬ãƒ™ãƒ«ã®è¦–è¦šã¨è¨€èªã®çŸ¥æ€§ã‚’æä¾›
	- Figureã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå‹•ç”»ã®ã‚ˆã†ãªãƒ­ãƒœãƒƒãƒˆã®å‹•ä½œã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™
-  Claude 3 Haiku ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/n71f1ef5f5e06?sub_rt=share_h
	- æœ¬æ—¥ (2024å¹´3æœˆ14æ—¥)ã€æœ€é€Ÿã‹ã¤æœ€ã‚‚ä½ä¾¡æ ¼ãªãƒ¢ãƒ‡ãƒ«ã€ŒClaude 3 Haikuã€ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚ã€ŒClaude APIã€ãŠã‚ˆã³ã€Œclaude.aiã€ã®Claude Proã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
	- é€Ÿåº¦
		- ã€ŒClaude 3 Haikuã€ ã¯ã€32,000ãƒˆãƒ¼ã‚¯ãƒ³æœªæº€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦1ç§’ã‚ãŸã‚Š 21,000 ãƒˆãƒ¼ã‚¯ãƒ³ (ç´„ 30 ãƒšãƒ¼ã‚¸) [1] ã‚’å‡¦ç†ã—ã¾ã™
	- ä½ä¾¡æ ¼ã€
		- ã€ŒClaude 3 Haikuã€ã®ä¾¡æ ¼ã®**å…¥å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¯”ç‡ã¯ 1:5** ã§ã™ã€‚ã‚ãšã‹**1ãƒ‰ãƒ«**ã§ **400 ä»¶ã®æœ€é«˜è£åˆ¤ä¾‹** [2] ã¾ãŸã¯ **2,500 æšã®ç”»åƒ** [3] ã‚’å‡¦ç†ãŠã‚ˆã³åˆ†æã§ãã¾ã™ã€‚
- Claude3 Ã— Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ
	- Claude-in-Sheets guide
	- ã©ã†ã‚„ã‚‰ã€Anthropicã¨GoogleãŒå”åŠ›ã—ã¦ã€Google Sheetsã‹ã‚‰Claude3ã‚’å‘¼ã¹ã‚‹ã‚‰ã—ã„ã€‚
-  Data Interpreter: An LLM Agent For Data Science
	- https://arxiv.org/abs/2402.18679
	- Data Interpreter has achieved state-of-the-art scores in machine learning, mathematical reasoning, and open-ended tasks, and can analyze stocks, imitate websites, and train models.
	- https://docs.deepwisdom.ai/main/en/DataInterpreter/
- æ¾ç”°å…ˆç”ŸãŒã€ãªãœ1.58bitã®bitnetãŒä¸Šæ‰‹ãè¡Œãã®ã‹è€ƒãˆãŸè©±
	- https://x.com/umiyuki_ai/status/1768109605148848322?s=20
	- ã¾ãšã€LLMãŒä½•ã‚’è¨ˆç®—ã—ã¦ã‚‹ã‹ï¼Ÿã¨ã„ã†ã¨ã€åºƒå¤§ãªè¨€èªç©ºé–“ã®ä¸­ã‹ã‚‰æ¬¡ã®å˜èªã‚’å½“ã¦ã‚‹ã‚²ãƒ¼ãƒ ã€‚æœ€è¿‘ã®LLMã®è¨€èªç©ºé–“ã¯4096æ¬¡å…ƒã¨ã‹ã‚ã£ã¦ã€æˆ‘ã€…ã®ç‰©ç†ç©ºé–“ãŒ3æ¬¡å…ƒã—ã‹ãªã„ã®ã«æ¯”ã¹ã¦æœ‰ã‚Šå¾—ã‚“åºƒã•ã€‚ãã®ä¸­ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒˆãƒ¼ã‚¯ãƒ³èªå½™ã¯ãŸã£ãŸã®3ä¸‡ç¨®é¡ã¨ã‹ã—ã‹ãªã„ã‚ã‘ã§ã€ã¤ã¾ã‚Šä¸€ã¤ã®å˜èªã‚ãŸã‚Šã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸç©ºé–“ã‚‚ãƒ¡ãƒãƒ£ã‚¯ãƒãƒ£åºƒã„ã€‚ã ã‹ã‚‰1.58bitã«é‡å­åŒ–ã•ã‚Œã¦è¨ˆç®—ãŒé›‘ã«ãªã£ã¦ã‚‚ã¡ã‚ƒã‚“ã¨å½“ãŸã‚‹ã€‚
-  Artificial intelligence and illusions of understanding in scientific research
	- https://www.nature.com/articles/s41586-024-07146-0
	- ã€ŒAIã¯ç§‘å­¦ã‚’ä¿ƒé€²ã™ã‚‹ãŒã€ã€ç†è§£ã®éŒ¯è¦šã€ã‚’ç”Ÿã¿å‡ºã™å±é™ºæ€§ãŒã‚ã‚‹ã€ã€ã¨ã„ã†ãƒ‘ãƒ¼ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–è«–æ–‡ã€‚
- ã™ã¹ã¦ã®ç„¡æ–™ç‰ˆCopilotãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒOpenAIã®ã€Œ**[GPT-4 Turbo](https://gigazine.net/news/20231107-openai-gpt-4-turbo/)**ã€ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã“ã¨ãŒã€Microsoftã®åºƒå ±æ‹…å½“è²¬ä»»è€…ã‹ã‚‰ç™ºè¡¨ã•ã‚Œã¾ã—ãŸã€‚
	- https://gigazine.net/news/20240314-copilot-gpt-4-turbo-free/
-  Artificial Intelligence Controller Interface (AICI)
	- https://github.com/microsoft/aici
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›åˆ¶å¾¡ã‚’ã‚«ãƒ³ã‚¿ãƒ³ã«ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚Microsoft è£½ã€‚é–‹ç™ºè€…ã¯ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã¨å‘¼ã°ã‚Œã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç”¨ã„ã¦ã€LLM ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§åˆ¶å¾¡å¯èƒ½ã€‚â€¦
- å›½ç”£LLMãŒæŠ±ãˆã‚‹â€œé–‹ç™ºã‚³ã‚¹ãƒˆâ€ã®èª²é¡Œã€€æµ·å¤–å‹¢ã«å®‰ã•ã§å‹ã¦ã‚‹ã‹ã€ELYZAä»£è¡¨ã®å±æ©Ÿæ„Ÿ
	- https://www.itmedia.co.jp/aiplus/articles/2403/13/news167.html
	- å›½ç”£éšä¸€ã®ç²¾åº¦ã®LLMã‚’é–‹ç™ºã—ãŸELYZA ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã‚„AWSãŒå¾ŒæŠ¼ã—ã™ã‚‹ç«¶åˆã¨ã©ã†æ£²ã¿åˆ†ã‘ã¦ã„ãã®ã‹ã€‚æ›½æ ¹å²¡ä»£è¡¨ã®ç™ºè¨€ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚
- alfredplpl/suzume-poc
	- https://huggingface.co/alfredplpl/suzume-poc
	- Googleã®Gemma-2Bã‚’æ—¥æœ¬èªã§ä½¿ãˆã‚‹ã‚ˆã†ã«ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’æ–½ã—ãŸã€å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«Suzumeã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ å°å‹ãªã®ã§ã‚¹ãƒãƒ›ã‚„å®¶é›»ãªã©ã«å‘ã„ã¦ã„ã¾ã™
- ä¸–ç”°è°·åŒºãŒAI botã‚’å†…è£½ã€€éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è·å“¡ãŒãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰ã§é–‹ç™ºã€€ChatGPTæ´»ç”¨ã€Œãƒ’ãƒ‡ã‚­ã€
	- https://www.itmedia.co.jp/news/articles/2403/13/news123.html
	- éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®è·å“¡ãƒãƒ¼ãƒ ãŒã€ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«ãªã©ã‚’é§†ä½¿ã—ã¦3ã‚«æœˆã§å®Œæˆã•ã›ãŸã¨ã„ã†ã€‚
	- è·å“¡ãŒæ™®æ®µã‹ã‚‰ä½¿ã£ã¦ã„ã‚‹Teamsã®ãƒãƒ£ãƒƒãƒˆãƒ„ãƒ¼ãƒ«ã§ãƒ’ãƒ‡ã‚­ã«è³ªå•ã§ãã€ChatGPTã‚’æ¥­å‹™ã«æ´»ç”¨ã§ãã‚‹
- Cappy: Outperforming and boosting large multi-task language models with a small scorer
	- https://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html
	- Cappy, a small pre-trained scorer model that enhances and surpasses the performance of large multi-task language models.
-  BitNet&BitNet b158ã®å®Ÿè£… by ã¯ã¡ ã•ã‚“
	- https://note.com/hatti8/n/nc6890e79a19a
	- ä¸€æ—¦è‡ªèº«ã®ç†è§£ã®ãŸã‚ã«ã‚‚BitNetã®å‡¦ç†ã‚„BitNet b158ã®æƒ³åƒã•ã‚Œã‚‹å®Ÿè£…ã€ä¸æ˜ç­ãªç‚¹ã‚’è‰²ã€…ãªæ–¹ã€…ã®å®Ÿè£…ã‚’ã‚‚ã¨ã«æ–‡å­—ã«æ›¸ãèµ·ã“ã—ã¦ã„ã“ã†ã¨æ€ã„ã¾ã™
- å²¡é‡åŸã•ã‚“ã®ã€ã€Œå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®å±•æœ›ã¨ä»Šå¾Œã®èª²é¡Œã€
	- https://hillbig.github.io/NLP2024_WS_okanohara.pdf
	- æ§˜ã€…ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•´å‚™ã€MoEã€Mambaã€LongContextã€æ¨è«–åŠ¹ç‡åŒ–ï¼‰ãªã©ã‚’ç´¹ä»‹
-  MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
	- https://arxiv.org/abs/2403.09611
	- Apple presents MM1, a family of multimodal LLMs up to 30B parameters, that are SoTA in pre-training metrics and perform competitively after fine-tuning
- Google Cloud Vertex AI ã« Anthropic ã® Claude 3 ãƒ¢ãƒ‡ãƒ«ãŒç™»å ´
	- https://cloud.google.com/blog/ja/products/ai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai/?utm_source=twitter&utm_medium=unpaidsoc&utm_campaign=fy24q1-googlecloud_jp-blog-ai-in_feed-no-brand-regional-apac&utm_content=announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai&utm_term=-
	- Google ã¯ #Anthropic ã¨ã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ã‚’é€šã˜ã€åŒ…æ‹¬çš„ãª #AI é–‹ç™ºãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚‹ #VertexAI ã§ Anthropic ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ã„ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º ã‚°ãƒ¬ãƒ¼ãƒ‰ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚„ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨è²»ç”¨ã®æœ€é©åŒ–ã«æ´»ç”¨ã„ãŸã ã‘ã¾
- æ¸…æ°´ã‚Œã¿ãŠæ°ã®Generate Project Summaryï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¦ç´„ç”Ÿæˆï¼‰ã‚’ä½¿ã£ã¦ã¿ã‚‹
	- https://six-loganberry-ba7.notion.site/24-03-15-Generate-Project-Summary-fa20870dfe66426d9e68b730e1f51f11
- Claude3ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ã¶ã¡è¾¼ã‚€ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è‡ªå‹•ã§ã¾ã¨ã‚ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆ
	- https://zenn.dev/olemi/articles/7b7992c055c64a
	- ã“ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ãˆã°ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç°¡å˜ã«ã¾ã¨ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
- Prompt Tuning ã‹ã‚‰ Fine Tuning ã¸ã®ç§»è¡Œæ™‚æœŸæ¨å®š
	- https://speakerdeck.com/icoxfog417/prompt-tuning-kara-fine-tuning-henoyi-xing-shi-qi-tui-ding
	- ChatGPT ã‚„ Claude ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨ã¯ç²¾åº¦ãƒ»ã‚³ã‚¹ãƒˆå…±ã«å‰²ã«åˆã‚ãªã„ã¨æ„Ÿã˜ã¦ã„ã‚‹æ–¹ã«ã¨ã£ã¦ãƒ‘ãƒ³ãƒã‚ã‚‹å†…å®¹ã‹ã¨æ€ã„ã¾
- JSTæˆ¦ç•¥çš„å‰µé€ ç ”ç©¶æ¨é€²äº‹æ¥­ã€Œè‡ªå¾‹é§†å‹•ã«ã‚ˆã‚‹ç ”ç©¶é©æ–°ã€ãŒæ¥å¹´åº¦ã‹ã‚‰å§‹ã¾ã‚Šã¾ã™
	- https://www.mext.go.jp/b_menu/houdou/2023/mext_000010.html
	- ç ”ç©¶ãƒ—ãƒ­ã‚»ã‚¹ãã®ã‚‚ã®ã‚’ AI ã‚„ãƒ­ãƒœãƒƒãƒˆ ã§åŠ é€Ÿã™ã‚‹è‡ªå¾‹é§†å‹•å‹ã®ç ”ç©¶ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
-  LocalMamba: Visual State Space Model with Windowed Selective Scan
	- https://huggingface.co/papers/2403.09338
-  AI escape velocity: A conversation with Ray Kurzweil
	- https://www.bvp.com/atlas/ai-escape-velocity-a-conversation-with-ray-kurzweil
	- ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ã€Œç§ãŸã¡ã®å¤§è„³æ–°çš®è³ªã‚’ã€ååˆ†ã«é«˜ã„å¸¯åŸŸå¹…ã§è¨ˆç®—æ©Ÿã«ã¤ãªãã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã®ã¯ã„ã¤ã§ã—ã‚‡ã†ã‹ï¼Ÿã€ 
	- ã‚«ãƒ¼ãƒ„ãƒ¯ã‚¤ãƒ«ã€Œ2030å¹´ä»£åˆé ­ã§ã™ã€‚ãã®æ™‚ç‚¹ã§ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å…¨å®¹é‡ã‚’è„³å†…ã«æŒã¤äººé–“ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã«ãªã‚‹ã§ã—ã‚‡ã†ã€
- ryota39/bilingual-gpt-neox-4b-instruction-sft-en-ja-84k
	- https://huggingface.co/ryota39/bilingual-gpt-neox-4b-instruction-sft-en-ja-84k
	- rinna/bilingual-gpt-neox-4b-instruction-sftã«è‹±æ—¥ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ84,300ä»¶ã‚’ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹(cc-by-sa-4.0)ã§ã™ã®ã§çš†æ§˜ãŠæ°—è»½ã«ãŠè©¦ã—ãã ã•ã„
- Researcher2Vec: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç·šå½¢ãƒ¢ãƒ‡ãƒ« ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†ç ”ç©¶è€…ã®å¯è¦–åŒ–ã¨æ¨è–¦
	- http://chasen.org/~daiti-m/paper/nlp2021researcher2vec-slides.pdf
	- å­¦æŒ¯ã®å¾Œã‚ã§å‹•ã„ã¦ã‚‹ã‚‰ã—ã„
-  æ—¥æœ¬èªã‚‚ç†è§£ã§ããŸCohereForAIã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ã¿ã‚‹ã€‚
	- https://note.com/masayuki_abe/n/n0e5e48fc4cc3?sub_rt=share_pb
	- CohereForAIã®LLMã‚’Google Colabã®A100ã§å®Ÿè¡Œã—ãŸã®ã§ç´¹ä»‹ã—ã¦ã„ãã¾ã™
	- ãƒ•ãƒªãƒ¼ã®LLMãªã®ã«æ–‡ç« ç”Ÿæˆã€æ•°å€¤è¨ˆç®—ã€è‹±è¨³ã€æ—¥æœ¬èªç†è§£åŠ›ãŒChatGPTã¿ãŸãå›ç­”ã•ã‚Œã¦ã„ã‚‹ã®ã«é©šãã¾ã—ãŸã€‚
-  ç¬¬2å›ã€€AIã¨äººé–“ã®æœªæ¥ã‚’æ±ºã‚ã‚‹éµã€Œã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã€â€•â€•ã¡ã‚‡ã£ã¨ã ã‘ãƒãƒ‹ã‚¢ãƒƒã‚¯ãªAIã®è©±
	- https://bcg-jp.com/article/2230/
	- ä»Šå¹´ã¯AIã®ç™ºå±•ãŒã•ã‚‰ã«åŠ é€Ÿã™ã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¯AIã¨äººé–“ã¨ã®æœªæ¥ã‚’æ±ºã‚ã‚‹éµã¨ãªã‚‹ã§ã—ã‚‡ã†ã€‚æ¬¡å›ã‚‚ãŠæ¥½ã—ã¿ã«
-  Genomic data in the All of Us Research Program
	- https://www.nature.com/articles/s41586-023-06957-x
	- ä»Šé€±ã®Natureã«All of usã®ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå‡ºã¦ã„ã‚‹ã€‚ç´„25ä¸‡äººï¼ˆåŠæ•°è¿‘ããŒãƒã‚¤ãƒãƒªãƒ†ã‚£ï¼‰ã®ã‚²ãƒãƒ è§£èª­ã§ã€10å„„ã‚‚ã®å¤šæ§˜ä½“ã‚’æ¤œå‡ºã€117å€‹ã®ç–¾æ‚£ã«é–¢é€£ã™ã‚‹3724å€‹ã®å¤‰ç•°ã‚’åŒå®šã€ã¾ã¨ã‚ãƒ‡ãƒ¼ã‚¿ã‚‚å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚‰ã—ã„
-  OpenAI Grok Curve Experiments
	- https://twitter.com/i/bookmarks
	- This is the code for the paper [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177) by Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra
	- Xã‹ã‚‰GroqãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã¨ã®ã‚¢ãƒŠã‚¦ãƒ³ã‚¹ãŒå‡ºãŸãŒã€ãªã‚“ã‹OpenAIãŒåˆ¥å®Ÿè£…ã‚’å…¬é–‹ï¼
- Claude 3 Opusã‚’ä½¿ã£ã¦ä¸–ç•ŒçµŒæ¸ˆã‚’åˆ†æã™ã‚‹ãƒ‡ãƒ¢å‹•ç”»
	- https://twitter.com/i/bookmarks?post_id=1769351991665594465
	- Claude 3ãƒ‡ãƒ¢ã®ä½•ãŒå‡„ã„ã‹ã¨ã„ã†ã¨å›½åˆ¥ã®çµŒæ¸ˆå‹•å‘ã‚’èª¿ã¹ã•ã›ã‚‹ãŸã‚ã€
		- â‘ 10å€‹ã®Sub-agentã‚’ä½œã‚‹ 
		- â‘¡å¿…è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ 
		- â‘¢ä»•äº‹ã‚’å¤–æ³¨ï¼ˆç¬‘ï¼‰ 
		- â‘£çµæœã‚’é›†ã‚ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›¸ã ã¨ã€
	- è‡ªåˆ†ã®ä»•äº‹ã‚’Sub-agentã«ãƒ‡ãƒªã‚²ãƒ¼ãƒˆï¼ˆå§”ä»»ï¼‰ã§ããŸã“ã¨ã€‚ä»•äº‹ã‚’ä¸ãˆã‚‹ã¨ä¸€ç•ªåŠ¹ç‡ã®ã„ã„æ–¹æ³•ã§é€²ã‚ã‚‰ã‚Œã‚‹ã®ãŒãƒ›ãƒ³ãƒˆå‡„ã„ã€‚
- VCã®å¾ŒæŠ¼ã—ã‚’å—ã‘ã€AIå¾“æ¥­å“¡ã‚’é–‹ç™ºã™ã‚‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãŒæµè¡Œã®å…†ã—
	- https://x.com/gijigae/status/1767836153053618465?s=20
-  Open Release  of Grok-1
	- https://x.ai/blog/grok-os
	- ã¤ã„ã«æœ¬å®¶ã®Grokãƒªãƒªãƒ¼ã‚¹(3/17)
	- Base model trained on a large amount of text data, not fine-tuned for any particular task. 
	- 314B parameter Mixture-of-Experts model with 25% of the weights active on a given token. 
	- Trained from scratch by xAI using a custom training stack on top of JAX and Rust in October 2023.

## 3/11

ä»Šé€±ã¯ã€AnthropicAIãŒãƒªãƒªãƒ¼ã‚¹ã—ãŸClaude3ã€GPT-4è¶Šãˆã¨ã‹ã€è‡ªç„¶ãªå›ç­”ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãªã©ã®èƒ½åŠ›ã‚‚ã‚ã‚Šã¨ã‹ã€è½åˆæ°ã‚„shi3zæ°ãªã©LLMã®ãƒ—ãƒ­ã‚‚ã†ãªã‚‰ã›ã‚‹æ€§èƒ½ã€ãƒ¬ã‚·ãƒ¼ãƒˆè§£æãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ€§èƒ½ã€è¬ã®ã‚¢ãƒ‹ãƒ¡ã‚¿ã‚°ä»˜ä¸æ€§èƒ½ã€æ§˜ã€…ãªèƒ½åŠ›ã§æ—‹é¢¨ã‚’å·»ãèµ·ã“ã—ã¦ã„ã‚‹ã€‚å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®GPQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é«˜æ€§èƒ½ã•ã‚‰ã«ã¯ã€IQ100ç›¸å½“ã§ã‚ã‚‹ã¨ã„ã†è©•ä¾¡ã‚‚å‡ºã¦ãã¦ã€æ—¥æœ¬ã®ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã‚‚ã‚‚ã¯ã‚„Claude3ã§ã„ã„ã®ã§ã¯ãªã„ã‹ã¨ã„ã†è©±ã«ã€‚Langchainã€llmaindexã‚‚æ¿€é€Ÿã§Claude3å¯¾å¿œã€‚Claude3ã®å›ç­”ã‚’è¦³å¯Ÿã™ã‚‹ã¨ã€äººã®çŸ¥è­˜ã¨ã‹ã€èããŸã„ã“ã¨ã‚’ãŠã‚‚ã‚“ã°ã‹ã£ã¦ã€äººã®å¿ƒã«å·®ã—è¾¼ã‚€ã‚ˆã†ã«ç­”ãˆã‚’å…¥ã‚Œã¦ãã‚‹æ„Ÿã˜ã§ã€ã¾ã•ã«LLMç‰ˆã®ã€Œä¸æ°—å‘³ã®è°·ã€ã€ã“ã‚Œã¯(humanityã®)çµ‚ã‚ã‚Šã®å§‹ã¾ã‚Šã‹ã€‚Groqã¯ã€gemma-7bãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¢ã‚’å…¬é–‹ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ã€æ‰“éµã«åˆã‚ã›ã¦ã€ã„ã‚„æ‰“ã¡è¾¼ã¿ã®äºˆæ¸¬ã‚‚ã—ãªãŒã‚‰å³å›ç­”ã€ã“ã‚Œã¯çµŒé¨“ã—ãªã„ã¨ã™ã”ã•ãŒã‚ã‹ã‚‰ãªã„ã€‚Claude3ãŒç¤ºã—ãŸé«˜ã„èƒ½åŠ›ã¨åˆã‚ã›ã¦è¦‹ã‚‹ã¨ã€äººã®å¿ƒã®çŠ¶æ…‹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«æ¨å®šã—ã¦ã€ãã‚Œã«å¿œã˜ãŸå›ç­”ã‚’ã™ã‚‹ã€å ´åˆã«ã‚ˆã£ã¦ã¯çŠ¶æ…‹ã‚’å¤‰æ›´ã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€ãã‚Œã£ã¦ã‚„ã°ã„ã‚ˆã­ã€‚æ¥æ—¥ã—ãŸã€Benjioæ°ãŒã‚„ãŸã‚‰alignmentã‚’å¼·èª¿ã™ã‚‹ã‚ã‘ã‚‚ã‚ã‹ã‚‹ã‚ã€‚åˆ†å‰²çµ±æ²»å¼ã§ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã™ã‚‹NVIDIAã®Agentã€Qwen-Agentã¨ã‹Agentå‘¨ã‚Šã‚‚å½“ç„¶é€²ã‚€ã€‚ä¸€æ–¹ã€æ—¥æœ¬ã®ã‚µãƒ–ã‚«ãƒ«ã«å¼·ã„gemma-7bãƒ™ãƒ¼ã‚¹ã®æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¨ã‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é€²å±•ã‚‚ã‚ã‚‹ã€‚ ã€Œã¯ã˜ã‚ã¦ã®çµ±è¨ˆçš„å› æœæ¨è«–ã€ã€ã‚†ã‚‹ã‚ã®è¡¨ç´™ã®å‰²ã«ã¯è¾›å£ãªã®ãŒé¢ç™½ã„ã€‚ã€Œçµ±è¨ˆå­¦ã®æ¥µæ„ã€ã®é‚¦è¨³ç‰ˆã€æ—¥æœ¬ã®AIãƒªãƒ†ãƒ©ã‚·ãƒ¼å‘ä¸Šã«å¯„ä¸ã§ãã‚‹ã‹ã€‚Benjoã•ã‚“ã®æ±å¤§è¬›æ¼”ã€Hintonã•ã‚“ã®æ—¥çµŒã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€ã„ã¥ã‚Œã‚‚AIãŒäººã‚’è¶…ãˆã‚‹ã“ã¨ã«ã‚ˆã‚‹è„…å¨ã«ã¤ã„ã¦èªã£ã¦ã„ã‚‹æ„Ÿã˜ãªã®ã¯èˆˆå‘³æ·±ã„ã€‚ã•ã¦ã€AppleãŒç”ŸæˆAIã«æ³¨åŠ›ã¨ç™ºè¡¨ã€M3 MacBook Airã‚’çªç„¶ç™ºè¡¨ã—ã€ãªã‚“ã‹ä¸æ°—å‘³ãªæ„Ÿã˜ãŒã—ã¾ã™ã­ã€‚

- Appleã€ãƒ‘ãƒ¯ãƒ•ãƒ«ãªM3ãƒãƒƒãƒ—ã‚’æ­è¼‰ã—ãŸæ–°ã—ã„13ã‚¤ãƒ³ãƒã¨15ã‚¤ãƒ³ãƒMacBook Airã‚’ç™ºè¡¨
	- https://www.apple.com/jp/newsroom/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/
-  WSL2ã§Swallow-7b-plus-hfã‚’è©¦ã—ã¦ã¿ã‚‹
	- https://note.com/ngc_shj/n/n80871f8e4e24?sub_rt=share_h
	- ä½¿ç”¨ã™ã‚‹PCã¯ãƒ‰ã‚¹ãƒ‘ãƒ©ã•ã‚“ã®ã€ŒGALLERIA UL9C-R49ã€
	- chat(instruct)ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ã®ã§ã€--no-chatã¨ã—ã¦èµ·å‹•ã—ã¾ã™
	- ã“ã‚Œã¯ã€ãªã‹ãªã‹ã„ã„æ„Ÿã˜ã§ã‚ã‚‹ã€‚ã„ã¾ã¾ã§æœ€é«˜ã‹ã‚‚ã—ã‚Œãªã„
- Awesome-Graph-LLM
	- https://github.com/XiaoxinHe/Awesome-Graph-LLM
	- ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¨LLMã®åŒæ–¹ãŒé–¢é€£ã—ã¦ã„ã‚‹ç ”ç©¶è«–æ–‡ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆãƒ¬ãƒã‚¸ãƒˆãƒª
- Jurafsky-Martã®Speech and Language Processing  (3rd ed. draft)
	- https://web.stanford.edu/~jurafsky/slp3/
	- In-Context Learningã‚„Instruction Tuningã®ç« ã‚‚è¿½åŠ 
- Toolformer: Language Models Can Teach Themselves to Use Tools
	- https://arxiv.org/abs/2302.04761
	- MetaãŒãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ã‚’è¦šãˆã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«Toolformerã‚’é–‹ç™º
	- è¦ç‚¹
		- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã‚ãšã‹ãªä¾‹ç¤ºã‚„æŒ‡ç¤ºã ã‘ã‹ã‚‰èª²é¡Œè§£æ±ºã‚’è¡Œã†é©šãã¹ãèƒ½åŠ›ã‚’æŒã¤
		- ä¸€æ–¹ã§ã€è¨ˆç®—ã‚„äº‹å®Ÿãƒã‚§ãƒƒã‚¯ã¯ã‚ˆã‚Šå˜ç´”ãªãƒ„ãƒ¼ãƒ«ã®æ–¹ãŒå„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹
		- ä¸¡è€…ã®é•·æ‰€ã‚’ç”Ÿã‹ã™ãŸã‚ã€ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—æŒ‡ç¤ºã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ã‚’è‡ªå·±å­¦ç¿’ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«Toolformerã‚’ææ¡ˆ
-  Learning and Leveraging World Models in Visual Representation Learning
	- https://arxiv.org/abs/2403.00504
	- Metaã®JEPAã®è«–æ–‡ã€Meta presents Image World Model
- Build an LLM-Powered API Agent for Task Execution
	- https://developer.nvidia.com/blog/build-an-llm-powered-api-agent-for-task-execution/
	- NVIDIAã‚ˆã‚Šã€‚LLMä½¿ã£ãŸAPI Agent
	- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã€LLMãŒã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã—ã¦ãŠã„ãŸãƒ†ãƒ³ãƒ—ãƒ¬ã‚’ä½¿ã£ã¦å­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®LLMç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€å­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«LLMãŒãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã—ã¦çµæœã‚’è¿”ã™
- AnthropicAIã€Claude3ã‚’ãƒªãƒªãƒ¼ã‚¹
	- https://x.com/AnthropicAI/status/1764653830468428150?s=20
- llamaindexã€ã•ã£ãã Claude3ã‚µãƒãƒ¼ãƒˆ
	- https://docs.llamaindex.ai/en/latest/examples/llm/anthropic.html
	- Like Gemini and Mistral's latest offerings, Claude 3 comes in 3 "flavors" with the largest, Claude Opus, claiming better performance than GPT-4 across a wide range of benchmarks.
- ZETA editing
	- https://huggingface.co/spaces/hilamanor/audioEditing
	- ZEro Shot Audio editing using DDPM inversion
	- Edit Audio with Nothing but Prompts!
- Metaâ€™s AI Watermarking Plan Is Flimsy, at Best Watermarks are too easy to remove to offer any protection against disinformation
	- https://spectrum.ieee.org/meta-ai-watermarks?share_id=8133421&utm_campaign=RebelMouse&utm_content=IEEE+Spectrum&utm_medium=social&utm_source=twitter
- Claude3ã®è©•åˆ¤
	- Claude 3 Sonnet ã€ã¨ã«ã‹ãç”ŸæˆãŒæ—©ã„ï¼ï¼ï¼ï¼ï¼
	- https://x.com/izutorishima/status/1764702243520208962?s=20
	- Sonnet ã§ã‚‚ä¸€éƒ¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ GPT-4 ã¨åŒç­‰ã‹ãã‚Œä»¥ä¸Šã«é”ã—ã¦ã„ã¦ã€ã“ã®é€Ÿã•ã‚’ç„¡æ–™ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ä½¿ãˆã‚‹ã®ã¯æ™®é€šã« OpenAI ã•ã‚“ãƒ”ãƒ³ãƒã˜ã‚ƒãªã„ã§ã™ã‹ï¼Ÿ
	- Claude ãŒè³¢ããªã£ã¦ç›®ã‚‚ã¤ã„ãŸï¼ãƒ¢ãƒ‡ãƒ«ã¯ä¸‰ã¤ã§ã€Haiku / Sonnet / Opus ã®é †ã«è³¢ãã€å€¤æ®µãŒã‚ãŒã‚‹
	- æœ€é«˜æ€§èƒ½ã® Opus ã¯ 10 å€‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ GPT-4 ã‚’ 10 å€‹ã¨ã‚‚è¶…ãˆã¦ã„ã‚‹ã€‚Haiku ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯ã‚¦ã‚§ãƒ–ç‰ˆã§è©¦ã—ã¦ã¿ãŸã‘ã©ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆã“ã“ã§ã¯ç”»åƒå…¥åŠ›ã ã‘ã§ã™ãŒï¼‰ã«ã¤ã„ã¦ã¯ GPT-4-V ã‚ˆã‚Šä¸Šã§ Gemini 1.0 Ultra ã¨åŒç¨‹åº¦ã€‚
	- 200k ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·æ–‡å…¥åŠ›ã¯å¥åœ¨ã§ã€ã•ã‚‰ã«ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ 1 million ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å…¥åŠ›ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ã€‚ãŸã ã—ã“ã¡ã‚‰ã¯ä¸€éƒ¨ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã®ã¿æä¾›ã€‚
	- å¤§é‡ã®æ–‡ç« ã®ä¸­ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠœãå‡ºã›ã‚‹ã‹ã®è©•ä¾¡ã«ç”¨ã„ã‚‹ã€ŒNeedle In A Haystackã€ã§ã¯ã€ç²¾å·§æ€§èƒ½ã® Opus ã‚’ã‚‚ã£ã¦ã™ã‚Œã°ç²¾åº¦ 99% ã‚’é”æˆã€‚ä»Šã¾ã§ã® Claude 2.1 ã¨æ¯”ã¹ã¦ã‚ã¡ã‚ƒã¯ã‚„ã„ã€‚å…¬ç§° 2 å€ã€‚
	- ã¾ãŸã€JSON å‡ºåŠ›ãªã©æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®å‡ºåŠ›ãŒå¾—æ„ã«ãªã‚Šã€è‡ªç„¶è¨€èªã«ã‚ˆã‚‹åˆ†é¡ã‚„æ„Ÿæƒ…åˆ†æãªã©ã‚‚ã§ãã‚‹ã‚ˆã†ã«ã€‚ä½¿ã£ã¦ã¿ãŸã®ã§ã™ãŒã€ã‹ãªã‚Šè‰¯ã„æ„Ÿã˜ã«æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã§ãã¾ã—ãŸ
	- API ã¯ç¾æ™‚ç‚¹ã§ Opus ã¨ Sonnet ã¯å…¬é–‹ã€‚Haiku ã¯è¿‘æ—¥å…¬é–‹äºˆå®šã€‚
- ä»Šã¾ã§ ChatGPT ã§æ›¸ã‹ã›ãŸæ–‡æ›¸ã£ã¦ã€Œãã‚Œã£ã½ã•ã€ãŒã‚ã£ãŸã‘ã©ã€Claude 3 ã¯éå¸¸ã«ä¸å¯§ãªæ—¥æœ¬èªã§ã‚‚ã† AI è£½ã‹ã©ã†ã‹ã‚ã‹ã‚‰ã‚“
	- https://x.com/izutorishima/status/1764890317302727114?s=20
- Langchainã®Claude3ã‚µãƒãƒ¼ãƒˆ
	- https://python.langchain.com/docs/integrations/chat/anthropic
- img2table
	- https://github.com/xavctn/img2table
	- ç”»åƒã‹ã‚‰è¡¨ã‚’æŠ½å‡ºã™ã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã‚“ã ã‘ã©ã€ã‚ã£ã¡ã‚ƒã„ã„ã€‚ã‚»ãƒ«çµåˆã«ã‚‚å¯¾å¿œã—ã¦ã¦å¤§å¤‰ç´ æ™´ã‚‰ã—ã„
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«ã‚ˆã‚‹TCFDæ¨å¥¨é–‹ç¤ºé …ç›®ã®è‡ªå‹•åˆ¤å®šã€
	- https://www.jpx.co.jp/corporate/research-study/working-paper/Summary_JPXWP_Vol43.pdf
	- GPT-4ã«ã‚ˆã‚Šã€92.8%ã®Accuracyã§ä¸Šå ´ä¼šç¤¾ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¤åˆ¥ã§ãã‚‹ã¨ã„ã†çµæœã«
- gemma-7bãƒ™ãƒ¼ã‚¹ã®æ—¥è‹±ãƒ»è‹±æ—¥ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’QLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å½¢å¼ã§å…¬é–‹ã—ã¾ã—ãŸ
	- https://huggingface.co/webbigdata/C3TR-Adapter
	- ç¿»è¨³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¤šè¨€èªç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Googleã®Madlad400ã‚„metaã®Seamless m4t v2 largeã€ALMA-Ja-V2 (ç§ã®ä»¥å‰ã®ãƒ¢ãƒ‡ãƒ«)ã‚ˆã‚Šã‚‚å¤§å¹…ã«å„ªã‚Œã¦ãŠã‚Šã€ã‚µãƒ–ã‚«ãƒ«ãƒãƒ£ãƒ¼æ–‡è„ˆã«ä¸€éƒ¨å¯¾å¿œå¯èƒ½ãªäº‹ãŒç‰¹å¾´ã§ã™
- RAGã§ã®å›ç­”ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯é›†ï¼ˆå¿œç”¨ç·¨-Aï¼‰
	- https://zenn.dev/knowledgesense/articles/cec1cd43244524
	- ã€Œå¿œç”¨ç·¨-Aã€ã§ã¯ã€ç‰¹ã«1ã¤ç›®ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«æœ€ã‚‚å¿…è¦ãªï¼ˆæœ€ã‚‚é–¢é€£ã—ã¦ã„ã‚‹ï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¾¤ã‚’æŠ½å‡ºã™ã‚‹ã€ãŸã‚ã®å…·ä½“çš„ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã«ã¤ã„ã¦è¦‹ã¦ã„ãã¾ã™ã€‚
- Claude 3 Opusã€Danbooru Taggerã®æ©Ÿèƒ½ã‚‚ã‚ã‚‹
	- https://x.com/alfredplpl/status/1764951315636158535?s=20
	- ã‚¢ãƒ‹ãƒ¡ã®è©±ã‚‰ã—ã„
- BASED: Simple linear attention language models balance the recall-throughput tradeoff
	- https://www.together.ai/blog/based
	- Transformerã®24å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æŒã¤LLM
- Claudeã®æ–‡å­—èµ·ã“ã—ã‚„ã°ã„ãªã€€é ˜åæ›¸ã€å½¢å¼ã‚‚å«ã‚ã¦å®Œç’§ã«èª­ã¿å–ã‚ŒãŸ
	- https://x.com/SuguruKun_ai/status/1764918827769606393?s=20
- Wikipedia ã§é›‘ãªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã—ãŸã€‚
	- https://huggingface.co/datasets/alfredplpl/wikipedia-qa-ja-500k
	- 50ä¸‡ä»¶ä»¥ä¸Šã‚ã‚Šã¾ã™ã€‚Instruction tuningç”¨ã§ã¯æ—¥æœ¬ã§ä¸€ç•ªä»¶æ•°ãŒã‚ã‚‹ã®ã§é©å½“ã«ä½¿ã£ã¦ãã ã•ã„
- é‡æ‘ç·ç ”ã«ã‚ˆã‚‹ç”ŸæˆAIãƒ¬ãƒãƒ¼ãƒˆ
	- https://www.nri.com/-/media/Corporate/jp/Files/PDF/knowledge/publication/chitekishisan/2024/01/cs20240104.pdf?la=ja-JP&hash=ED42BFF77381C8AD102B7792B56D2654AD7BC6D5
	- ç”ŸæˆAIã§å½±éŸ¿ã‚’å—ã‘ã‚‹è·ç¨®ã®ãƒªã‚¹ãƒˆãŒè¼‰ã£ã¦ã‚‹ã®ã¯æœ€è¿‘ã‚ˆãè¦‹ã‚‹ã‘ã‚Œã©ã€ä¸€ä½ãŒæ°´æ—é¤¨é£¼è‚²å“¡ãªã®ãŒæ–¬æ–°ã•ã‚’æ„Ÿã˜ãŸã€‚ã‚ã¨ãƒ•ã‚¡ãƒ³ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒä¸Šä½ã«ã„ã‚‹ã®ã‚‚é¢ç™½ã„
- Claude 3ã®æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆã«ã‚ˆã‚Œã°ã€å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®ç‰©ç†å­¦ãƒ»åŒ–å­¦ãƒ»ç”Ÿç‰©å­¦ã®çŸ¥è­˜ã¨æ¨è«–ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸGPQAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é«˜æ€§èƒ½ï¼ˆ0 shot CoTã§50.4%ã€å¤šæ•°æ±ºåˆ©ç”¨ã§59.5%ï¼‰
	- https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf
- Build a Large Language Model (From Scratch)
	- https://github.com/rasbt/LLMs-from-scratch
	- Manningç¤¾ï¼ˆæ—¥æœ¬ã ã¨ã‚ˆãã‚ªãƒ©ã‚¤ãƒªãƒ¼ã®çš®ã‚’è¢«ã‚‹å‡ºç‰ˆç¤¾ï¼‰ã‹ã‚‰ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹æœ¬ãŒå‡ºã‚‹æ¨¡æ§˜ã€‚GitHubã«å…¬é–‹ã‚ã‚Š
- Tokanizer playgroundãŒClaude3ã«å¯¾å¿œ
	- https://huggingface.co/spaces/Xenova/the-tokenizer-playground
	- If you want to calculate how many tokens you're sending to the API, check out The Tokenizer Playground, which we recently updated to include the Claude 3 tokenizer!
- Claude 3 is impressively good at OCR and structured extraction
	- https://x.com/jerryjliu0/status/1765101841535336929?s=20
	- We fed it this complex Excalidraw diagram about the Prometheus model - contains subsections, and interleaving text and diagrams
	- Claude 3 is able to provide a summary of each section and also determine the positions of the diagrams!
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/anthropic_multi_modal.ipynb
- Anthropicã®Claude Proã¾ã¨ã‚
	- æœˆé¡$20(USãƒ‰ãƒ«)ã§æœ€é«˜ãƒ¢ãƒ‡ãƒ«ã®Claude Opusã¨ãƒãƒ£ãƒƒãƒˆå‡ºæ¥ã‚‹ã‚µãƒ–ã‚¹ã‚¯ã‚µãƒ¼ãƒ“ã‚¹ 
	- chatGPT ProãŒ40ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/3æ™‚é–“ã®åˆ¶é™ãŒã‚ã‚‹ã®ã¨åŒæ§˜ã«ä½¿ç”¨é‡åˆ¶é™ã¯ã‚ã‚‹ãŒç›®å®‰ã—ã‹æ˜è¨˜ã•ã‚Œã¦ã„ãªã„ 
	- ç„¡æ–™ç‰ˆã¨æ¯”è¼ƒã—ã¦å°‘ãªãã¨ã‚‚5å€ã®åˆ©ç”¨æ ã€‚çŸ­ã‚ã®(ç´„200å˜èªã®è‹±èªã®æ–‡ç« )ã§ã‚ã‚Œã°8æ™‚é–“ã”ã¨ã«å°‘ãªãã¨ã‚‚100ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡å¯ã¨ã®äº‹ 
	- ç„¡æ–™ç‰ˆã¯1æ—¥ã‚ãŸã‚Šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ åˆ¶é™ã ãŒã€æœ‰æ–™ç‰ˆã¯8æ™‚é–“æ¯ã«æ ãŒãƒªã‚»ãƒƒãƒˆ 
	- ã€è¯éº—ãªã‚‹ã‚®ãƒ£ãƒ„ãƒ“ãƒ¼ã€ã®ã‚³ãƒ”ãƒ¼ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå ´åˆ(è¨³æ³¨ï¼šãŠãã‚‰ã1MBæœªæº€)8æ™‚é–“ä»¥å†…ã«é€ä¿¡ã§ãã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯20ä»¶ã«ãªã‚‹ã¨ã®äº‹ 
	- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ–‡ç« (doc)ã‹ç”»åƒ(image)ã§æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«å„10MBã¾ã§ ãƒ»zipã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã®ã§ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ä¸€å¼ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã¿ãŸã„ãªäº‹ã¯é›£ã—ãã† 
	- 2023å¹´8æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹
	- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯Claude Proã«å…¥åŠ›ã•ã‚ŒãŸä¼šè©±ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œãªã„(è¦ªæŒ‡ã‚¢ãƒƒãƒ—/ãƒ€ã‚¦ãƒ³æ©Ÿèƒ½ã‚’é€šã˜ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡ã™ã‚‹ã¨ä½¿ã‚ã‚Œã‚‹) 
	- ç„¡æ–™ç‰ˆã«ã¤ã„ã¦ã¯å¾®å¦™ãªæ›¸ãæ–¹ãªã®ã§è‰¯ãåˆ†ã‹ã‚‰ãªã„(å½“ç¤¾ã®æ¶ˆè²»è€…ã‚µãƒ¼ãƒ“ã‚¹ã¾ãŸã¯ãƒ™ãƒ¼ã‚¿/è©•ä¾¡ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å½“ç¤¾ã¯ã€ãŠå®¢æ§˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ä¼šè©±ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šå®‰å…¨ã«ã™ã‚‹ãŸã‚ã®åˆ©ç”¨è¦ç´„ã®ç›£è¦–ã¨å¼·åˆ¶ãªã©ã€ä¿¡é ¼æ€§ã¨å®‰å…¨æ€§ã®ä½œæ¥­ã«é–¢é€£ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€ã¨ã®äº‹) 
	- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã‚’è¦ç´„ã—ã¦è²°ãŠã†ã¨ã—ãŸã‚‰å‡ºåŠ›ã¯ä¸€æ°—ã«ã•ã‚Œãšã€Œç¶šãã‚’ã€ã¨ä¿ƒã™å¿…è¦ãŒã‚ã£ãŸ
- Claude3ã¯ã‚ˆã„ã€byã€€è½åˆé™½ä¸€
	- Claude 3ã‚’ä½¿ã„ã¾ãã£ã¦ã¿ã¦ï¼Œã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒç§€é€¸ï¼Œæ—¥æœ¬èªæ€§èƒ½ãŒè‰¯ã„ï¼ˆgpt4-0613ã‚‚è‰¯ã„ãŒï¼‰ï¼Œpdfãªã©ã®æ‰±ã„ãŒä¾¿åˆ©ï¼ã“ã®è¾ºã‚Šã™ã§ã«chatGPTã‹ã‚‰ã®ç§»è¡ŒãŒèµ·ã“ã£ã¦ã„ã‚‹ï¼å¿«é©ã™ãã‚‹
	- https://x.com/ochyai/status/1765209291517210816?s=20
- LLMã®èƒ½åŠ›ã«ã¤ã„ã¦èªã‚‹äººé–“ã®æ€è€ƒåŠ›ãŒå•ã‚ã‚Œã¦ã„ã‚‹ã®ã§ã¯ãªã„ã‹ã€€by shi3zã•ã‚“
	- https://x.com/shi3z/status/1765310307994611798?s=20
- Claude 3 Opus structured query agent
	- https://colab.research.google.com/drive/1hkwipueVyi2Jzo58Z8jfdZ_9rSscfGxd
	- How good is AnthropicAI's Claude 3 Opus at being an agent? Pretty darn good! Check out this quick notebook in which Claude answers a complex, multi-source question by reading a PDF table and using the answer to do math on the contents of a CSV!
- Knowledge-Augmented Planning for LLM Agents
	- https://arxiv.org/abs/2403.03101
	- Proposes an approach to enhance the planning capabilities of LLMs through explicit action knowledge.
- å¤§å­¦ãƒ»MetaAIã‹ã‚‰ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ä½æ¸›ã«æœ‰åŠ¹ãªã‚°ãƒ©ãƒ•æ‹¡å¼µã—ãŸRAG"G-Retriever"ã®ææ¡ˆ
	- https://arxiv.org/abs/2402.07630
	- éƒ¨åˆ†ã‚°ãƒ©ãƒ•æŠ½å‡ºã‚’è³é‡‘é›†ã‚Steineræœ¨å•é¡Œ(PCST)ã§è§£ã„ã¦ã„ã‚‹ã€‚
- ã‚¹ã‚¯ã‚·ãƒ§ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ç”Ÿæˆï¼Microsoftã¨DeepMindãŒå…±è‘—ã—ãŸè«–æ–‡
	- https://github.com/NoviScl/Design2Code
	- -The Design2Code benchmark dataset for the task of converting visual design (screenshot) into code implementation, which consists of 484 real-world webpages from C4 (examples shown below).
- Claude3ã®é–‹ç™ºè€…ãŒç¤ºã—ãŸã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚·ãƒ³ãƒ—ãƒ«
	- https://x.com/AmandaAskell/status/1765207842993434880?s=20
- ã¯ã˜ã‚ã¦ã®çµ±è¨ˆçš„å› æœæ¨è«–
	- https://x.com/takehikohayashi/status/1765268689367265668?s=20
	- é–‹å§‹3ãƒšãƒ¼ã‚¸ç›®ã§ã€Œçµ±è¨ˆçš„å› æœæ¨è«–æœ€å¼·è«–ã€ã«ã„ããªã‚Šå†·ã‚„æ°´ã‚’ã¶ã£ã‹ã‘ã‚‹
- Qwen-Agent
	- https://github.com/QwenLM/Qwen-Agent
	- Agent framework and applications built upon Qwen1.5, featuring Function Calling, Code Interpreter, RAG, and Chrome extension
- Yoshua Benjioæ°ã®æ¥æ—¥æ±å¤§è¬›æ¼”
	- https://www.youtube.com/watch?v=8aTkuvbd_jU
	- æ€ã„ã£ãã‚ŠAIã®ã‚‚ãŸã‚‰ã™å£Šæ»…çš„ãªãƒªã‚¹ã‚¯ã‚„ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®è©±ã‚’ã‚³ã‚¢ã«ã—ã¦ã„ã‚‹
- toshi456/llava-bench-in-the-wild-ja
	- multilingual-llava-bench-in-the-wildã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®ç¿»è¨³ãƒŸã‚¹ã‚„æœªç¿»è¨³ã®ãƒ‡ãƒ¼ã‚¿ã‚’DeepL+æ‰‹å‹•ã§ä¿®æ­£ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ 
	- å…ˆæ—¥Turingã•ã‚“ãŒå…¬é–‹ã—ãŸLLaVA-Bench-JA(COCO)ã¨åˆã‚ã›ã¦æ—¥æœ¬èªVLMã®è©•ä¾¡ã«ã”æ´»ç”¨ãã ã•ã„ã€‚
- Claude-3ãŒAIã§åˆã‚ã¦IQ100è¶…ãˆã‚’é”æˆã—ãŸã¨ä¸»å¼µ
	- https://www.maximumtruth.org/p/ais-ranked-by-iq-ai-passes-100-iq
	- ã€Œç¾åœ¨ã®æˆé•·ç‡ã‚’å˜ç´”ã«å¤–æŒ¿ã™ã‚‹ã¨ã€4ï½10å¹´å¾Œã«ã¯Claude-6ãŒIQã®è³ªå•ã«ã™ã¹ã¦æ­£è§£ã—ã€èª°ã‚ˆã‚Šã‚‚è³¢ããªã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€
- å¯¾è©±ç³»ã¯ClaudeãŒæŠœãã‚“å‡ºã¦å¼·ã„
	- https://x.com/reasan_mirasan/status/1765513422504890417?s=20
- LangChain Text Splitters
	- https://x.com/LangChainAI/status/1765418125569491233?s=20
	- One of the most popular parts of LangChain is our text splitters - simple yet necessary for any RAG app
-  Large language models surpass human experts in predicting neuroscience results
	- https://arxiv.org/abs/2403.03230
	- ç¥çµŒç§‘å­¦ã®å®Ÿé¨“çµæœã‚’LLM (Llama2ãƒ»Mistralãƒ»Falconãƒ»Galactica) ã§äºˆæ¸¬ã™ã‚‹ç ”ç©¶
	- è«–æ–‡ã‚¢ãƒ–ã‚¹ãƒˆã®èƒŒæ™¯ã¨æ–¹æ³•éƒ¨åˆ†ã‹ã‚‰äºŒæŠã§çµæœã‚’äºˆæƒ³ã™ã‚‹å•é¡Œã‚»ãƒƒãƒˆã€ŒBrainBenchã€ã‚’ä½œã‚Šï¼ŒLLM vs å°‚é–€å®¶ã§æ¯”è¼ƒ
	- åŸºæœ¬çš„ã«å°‚é–€å®¶ã‚ˆã‚ŠLLMãŒå¼·ã„ LoRAã§ç¥çµŒç§‘å­¦ç”¨ã«fine-tuningã™ã‚‹ã¨æ€§èƒ½ãŒã•ã‚‰ã«ä¸ŠãŒã‚‹
- Claude 3 Cookbook by llamaindex
	- https://colab.research.google.com/drive/11HzzDd6fAiH2s8nDjZMRY5nx2Licl_tF?usp=sharing
	- we go through a comprehensive cookbook to show how Claude 3 can be used in a variety of different application use cases with
- GaLoreã£ã¦ã®ã¯äº‹å‰å­¦ç¿’ãŒãƒ¡ãƒƒãƒãƒ£çœãƒ¡ãƒ¢ãƒªã§ã§ãã‚‹ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚‰ã—ã„
	- https://x.com/umiyuki_ai/status/1765927780263633236?s=20
	- VRAM24GBã§7Bãƒ¢ãƒ‡ãƒ«ã®LLMã®äº‹å‰å­¦ç¿’ãŒã§ãã¦ã—ã¾ã†ã‚‰ã—ã„
- Meta announces Teaching Large Language Models to Reason with Reinforcement Learning
	- https://huggingface.co/papers/2403.04642
- WhiteRabbitNeo/WhiteRabbitNeo-7B-v1.5a
	- https://huggingface.co/WhiteRabbitNeo/WhiteRabbitNeo-7B-v1.5a
	- At the request of the open source community, we're now releasing a 7B model for offensive and defensive cybersecurity. This can be run locally in most computers with less GPU VRAM.
- ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ãŒã€Œä»•äº‹ã«ã¯ã€GPT-4ã¯è¨€ã†ã»ã©å¤§ã—ã¦ä½¿ãˆãªã„ã‘ã©Claude3ã¯ãã“ãã“ä½¿ãˆã‚‹ã€
	- https://x.com/umiyuki_ai/status/1766284320208212472?s=20
	- ãŸã¶ã‚“ã€https://x.com/yukatan/status/1766610634832306408?s=20
	- ã‚ˆã†ã‚„ã£ã¨claude3ã‚’è©¦ã—ã¾ã—ãŸãŒã€ãŸã—ã‹ã«ã€Œãƒªãƒªãƒ¼ã‚¹èµ·ã“ã—ã€ã«ã¤ã„ã¦ã¯ã€Œãˆã€ç§ã®ä»•äº‹ã‚„ã°ã„ã‹ã‚‚ã€ã¨æ€ã†ãƒ¬ãƒ™ãƒ«ã«è¿‘ã¥ã„ã¦ã„ã‚‹ã€‚
- cyzgab/catch-me-if-you-can
	- https://huggingface.co/spaces/cyzgab/catch-me-if-you-can
	- GroqInc just added support for Gemma 7B. 
	- ãªã‚“ã‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«è³ªå•ã«ç­”ãˆã¦ï¼ˆæ‰“éµæ¯ã«äºˆæ¸¬ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ï¼‰
	- ã¾ã•ã«ã€catch me if you canã¨ã¯ã€‚
- ãƒ’ãƒ³ãƒˆãƒ³æ°ã€AIã¯è¨€è‘‰ã‚’ç†è§£ã—ã¦ã„ã‚‹ã¨ã€ã€ã€ï¼ˆæ—¥çµŒï¼‰
	- https://www.nikkei.com/article/DGXZQOGN143CZ0U4A210C2000000/?n_cid=nk_chart_qr
	- ã€Œâ€¦å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€æˆ‘ã€…ã¨åŒã˜ã‚ˆã†ã«è¨€è‘‰ã‚’ç†è§£ã—ã¦ã„ã‚‹ã¨æ€ã†ã€‚â€¦AIãŒè¨€è‘‰ã‚’ç†è§£ã—ã¦ã„ãªã„ã¨ã„ã†äººã®å¤§åŠã¯ã€äººé–“ãŒã©ã†ç†è§£ã—ã¦ã„ã‚‹ã‹ã¨ã„ã†ç†è«–ã‚’æŒã£ã¦ã„ãªã„ã€
	- ãƒã‚¤ãƒ³ãƒˆ
		- äººé¡å­˜ç¶šã®å±æ©Ÿã‚’ã‚‚ãŸã‚‰ã™æã‚ŒãŒAIã«ã‚ã‚‹  
		- è‡ªå¾‹çš„ã«äººã‚’æ®ºã™ãƒ­ãƒœãƒƒãƒˆå…µå™¨ãŒ10å¹´ä»¥å†…ã«ç™»å ´  
		- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯è„³ã‚ˆã‚ŠåŠ¹ç‡çš„ã«å­¦ç¿’ã§ãã‚‹
- ã€Œçµ±è¨ˆå­¦ã®æ¥µæ„ã€
	- https://www.soshisha.com/book_wadai/books/2692.html
	- æ•°å¼ã¯æœ€å°é™ã€é¢ç™½ã„å®Ÿä¾‹ã¯æº€è¼‰ã€‚çµ±è¨ˆå­¦å…¥é–€æ›¸æœ€æ–°æ±ºå®šç‰ˆ
	- æœ¬æ›¸ã¯ã€å…¥é–€è€…ãŒçŸ¥ã‚‹ã¹ãçµ±è¨ˆå­¦ã®ç¾ä»£çš„è«–ç‚¹ã‚’ç¶²ç¾…ã—ã¦ãŠã‚Šã€ã¾ã•ã«å¾…ã¡æœ›ã¾ã‚ŒãŸã€Œçµ±è¨ˆå­¦å…¥é–€æ›¸æœ€æ–°æ±ºå®šç‰ˆã€ã¨è¨€ãˆã‚‹ã§ã—ã‚‡ã†

## 3/4

ä»Šé€±ã¯ã€1ãƒ“ãƒƒãƒˆLLMã®è¡æ’ƒ!ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã®ç™ºè¡¨(The Era of 1-bit LLMs)ã€ 70Bã§8.9å€é«˜é€Ÿã¨ã„ã†ã“ã¨ã§ã€å‹æ‰‹å®Ÿè£…ã€è¿½è©¦ã‚‚ç¶šã€…ã€200Mã§ãã‚Œãªã‚Šã«å‹•ãã¨ã„ã†shi3zã•ã‚“ã®è©•ä¾¡ã‚‚ã€shi3zã•ã‚“ã«ã‚ˆã‚‹ã¨ã€ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ãªã‚‰å…¨å“¡BitNetè©¦ã—ã¦ã¿ã‚‹ã¹ãã€ã ãã†ã ã€‚å°ã•ãè©¦ã™ã¨ã„ã†æ„å‘³ã§ã¯ã€250Mã®Mixtralã‚’pretrainingã‹ã‚‰finetuningã‚’è©¦ã—ãŸäº‹ä¾‹ã‚‚ã€‚ã¦ã£ãºã‚“ãŒé«˜ã„ã¨ã“ã‚ã«ã‚ã‚‹ã¨å‘¨è¾ºã‚‚æ‹¾ã†ã¨ã“ã‚ãŒãŸãã•ã‚“ã‚ã‚‹ã¨ã„ã†ã€LLMç•Œéšˆã§ã®ãƒˆãƒªã‚¯ãƒ«ãƒ€ã‚¦ãƒ³ç¾è±¡ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ã€‚ã•ã¦ã€å…ˆé€±å…¬é–‹ã•ã‚ŒãŸgemmaã€ollamaã§ã‚µãƒãƒ¼ãƒˆã€ã‚„ã‚Œå‘¨è¾ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ãƒã‚°ãŒå¤šã„ã¨ã‹ã€ã„ã‚„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ãˆãŸã¨ã‹ã€ã„ã‚ã„ã‚è©•ä¾¡ãŒã‚ã‚‹ã€2bã®ã»ã†ãŒ7bã‚ˆã‚Šæ€§èƒ½ã‚ˆã„ã¨è¬ã®å ±å‘Šã‚‚ã€ã¡ã‚‡ã£ã¨ãƒªãƒªãƒ¼ã‚¹æ€¥ãã™ããŸã‹ã€‚ä¸€æ–¹Qwenã¯ã€Qwen1.5æœ€é«˜ã¨ã‹ã€ã‚‚ã¯ã‚„Qwen-72Bã§ã„ã„ã®ã§ã¯ãªã„ã®ã‹ã€ã¨ã„ã†è©•ä¾¡ã‚‚å‡ºã¦ã„ã‚‹ãŒã€å®Ÿã¯å‡ºåŠ›ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ˆã†ã«ã¯ä½¿ãˆãªã„ãªã©ã®ç¸›ã‚ŠãŒã‚ã‚‹ã¨ã®ã“ã¨ã€‚ãƒãƒãƒ•ã‚©OBãŒç«‹ã¡ä¸Šã’ãŸã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—starleyã®éŸ³å£°ä¼šè©±å‹ãŠã—ã‚ƒã¹ã‚ŠAIã‚¢ãƒ—ãƒªã€ŒCotomoã€ã€UXã‚’è€ƒãˆã¦ã¡ã‚ƒã‚“ã¨ä½¿ãˆã‚‹å•†å“ã«è½ã¨ã™ã“ã‚€ã“ã¨ã®å¤§åˆ‡ãŒã‚ˆãã‚ã‹ã‚‹ã€‚Mistral Largeã€ã€ŒGemini Proãªã©ã®ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢ã‚’ç²å¾—ã€ã£ã¦æœ¬å½“ã‹ï¼ŸLLMã«ã¯è‡ªç„¶è¨€èªã‚ˆã‚Šã‚‚æœ€é©ãªå½¢å¼ãŒã‚ã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†é‡å¿ƒçš„ãªã€AutoFormï¼ˆã‚ªãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼‰ã€ã€ãã†ã„ãˆã°å…ˆè¼©ã®ä¸‰ã¤å­ã¡ã‚ƒã‚“ã¯ã€ç‹¬è‡ªã®è¨€èªã§ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã„ãŸã£ã¦è¨€ã£ã¦ãŸãªã€‚æ±å·¥å¤§ã®ã€ã€è«–æ–‡ã®çµè«–ã‚’å­¦ç¿’ã•ã›ãŸã‚‰æ€§èƒ½ãŒä¸‹ãŒã£ãŸã€‚ã€ã¨ã„ã†è©±ã€ã‚¤ãƒ³ãƒˆãƒ­ã®ã»ã†ãŒã‚ˆã„ã¨ã„ã†ã®ã¯ä¸æ€è­°ã ã€‚Î¼Transferã€è»¢ç§»å­¦ç¿’ã®ãƒã‚¤ã‚¯ãƒ­ç‰ˆï¼Ÿå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ãŠãã‚‰ãåœ§å€’çš„ã«åŠ¹ç‡åŒ–ã§ãã‚‹ã®ã¯ã‚ˆã„ã€‚Gemini 1.5 Proã‚‚ä½¿ãˆã‚‹äººãŒå°‘ã—ãšã¤æ‹¡å¤§ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€æ¥é€±ã‚ãŸã‚Šã¯ã„ã‚ã„ã‚è©•ä¾¡ãŒã§ã‚‹ã‹ã‚‚ã€‚Gemini 1.5 Proã®é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ€§ã‚’åˆ©ç”¨ã—ã€Long-context LLMs ãŒRAGã®ä»£ã‚ã‚Šã«ãªã‚‹ã‹ãªã‚‰ãªã„ã‹ã‚’è©•ä¾¡ã—ã¦é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ™‚ä»£ã®æ–°ã—ã„RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ææ¡ˆã¨ã‹ã‚ã£ãŸã€‚Function Callingã€è‰²ã€…ãªLLMã§ä½¿ãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå‡ºã¦ãã¦ã€å½“ãŸã‚Šå‰ã®æŠ€è¡“ã«ãªã‚Šã¤ã¤ã‚ã‚‹ãªã€‚LlamaParseã®PDFèª­ã¿å–ã‚Šè©•ä¾¡ã¨ã‹ã€RAGã§ã®å›ç­”ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯é›†ã¨ã‹ã€ãã®ã‚ãŸã‚Šã®åœ°é“ãªé€²ã¿ã‚‚ã‚ã£ãŸã€‚ NVIDIAãŒãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ç”¨ã®GPUã‚’æ–°ç™ºè¡¨ã¨ã‹ã€ã¾ã•ã«winner takes allã®ä¸–ç•Œã€‚ã•ã¦æ—¥æœ¬ã®å„ªç§€ãªé ­è„³ã¯ã©ã†ã‚ˆï¼Ÿã¨ã„ã†ã“ã¨ã§å…ˆé€±ã€ãŒã£ã¡ã‚Šãƒãƒ³ãƒ‡ãƒ¼ã§ã€æ±å¤§å‡ºèº«ã®è‹¥è€…ãŒå¤šã„ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã€Œç‡ˆã€ãŒç´¹ä»‹ã•ã‚ŒãŸãŒã€ã‚ã‚Œã£ã¦ã€ã€Œå»ºç¯‰Ã—AIã€ã®ãƒ†ãƒ¼ãƒã§ã€LLMã‚’ãŒã£ã¤ã‚Šæ´»ç”¨ã™ã‚‹ã¨ã„ã†è©±ã€‚è‹¥ã„äººã®æ„è­˜ãŒåŸºç›¤ã¨ã„ã†ã‚ˆã‚Šç¤¾ä¼šå®Ÿè£…ã¨ã„ã†ã‹ãã£ã¡ç³»ã«æµã‚Œã¦ã‚‹ï¼Ÿ

- ç”»åƒç”ŸæˆAIã€å®‰ã„PCã§ã‚‚é«˜é€Ÿã«ã€€è¡æ’ƒã®ã€ŒStable Diffusion WebUI Forgeã€
	- https://weekly.ascii.jp/elem/000/004/185/4185940/
-  Î¼Transfer: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒã‚¤ãƒ‘ãƒ©æ¢ç´¢ã‚’å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã—å­¦ç¿’ã‚’åŠ¹ç‡åŒ–ã™ã‚‹
	- https://note.com/tatsuyashirakawa/n/n9f5b57ce1aa6?sub_rt=share_pb
	- Î¼Transfer ã¯ã€Î¼P ï¼ˆMaximal Update Parametrizationï¼‰ã¨ã„ã†ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸ NN ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã‘ã«ã‚ˆã‚Šå®Ÿç¾ã•ã‚Œã‚‹ã€ã‚µã‚¤ã‚ºã®ç•°ãªã‚‹ NN é–“ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»¢ç§»ã§ã™ã€‚
	- ï¼ˆçŸ¥ã‚‰ãªã‹ã£ãŸèª­è€…ã«ã¨ã£ã¦ï¼‰å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’ãŠãã‚‰ãåœ§å€’çš„ã«åŠ¹ç‡åŒ–ã§ãã‚‹æ±ç”¨çš„ã‹ã¤ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã‘ Î¼P ã®å­˜åœ¨ã¨ä½¿ã„æ–¹ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
	- Neural Networks ã«å¯¾ã—ã¦ã‹ãªã‚Šä¸€è²«æ€§ã®ã‚ã‚‹ç†è§£ãŒå¾—ã‚‰ã‚Œãã†ãªæ°—åˆ†ã«ãªã‚‹ã€‚å­¦ç¿’ç‡ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«é–¢ã™ã‚‹è©±ãŒãªã‚“ã§ã‚‚ TP/Î¼P ã§å–ã‚Šæ‰±ã†ã¹ãäº‹é …ã«è¦‹ãˆã¦ãã‚‹ã€‚
- ã‚¦ã‚§ãƒ–ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªå‡¦ç†ã‚³ãƒ¼ãƒ‰ã¨èª²é¡Œ
	- https://note.com/kan_hatakeyama/n/n331bda7d77c1?sub_rt=share_pb
		- æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã€€(å¤‰ãªæ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’æ¶ˆã™)
		- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã®ã€ä¸è¦ãªæ–‡å­—åˆ—ã®å‰Šé™¤
		- æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã§ã®ã€ä¸è¦ãªæ–‡å­—åˆ—ã®å‰Šé™¤
		- é‡è¤‡ã®å‰Šé™¤
- ã€æœ€å¼·ã«ãªã£ãŸã€‘Googleã®æœ€å¤§1000ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›å¯èƒ½ãªGemini 1.5 ProãŒãƒ¤ãƒã™ãã‚‹ã€‚ã€Šæ¦‚è¦ã€ä»–LLMã¨ã®æ¯”è¼ƒã€ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒ¼ãƒ³ã§ã®æ´»ç”¨æ–¹æ³•5é¸ã‚’å¾¹åº•è§£èª¬ã€‹
	- https://note.com/chaen_channel/n/necaf27db79ae
-  LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
	- https://arxiv.org/abs/2402.13753
	- It looks like the problem of long contexts in open LLMs is close to being solved.
- â€‹â€è©±ã—ãŸã„ã“ã¨ã‚‚ã€è©±ã›ãªã„ã“ã¨ã‚‚ã€‚â€ éŸ³å£°ä¼šè©±å‹ãŠã—ã‚ƒã¹ã‚ŠAIã‚¢ãƒ—ãƒªã€ŒCotomoã€ã‚’æä¾›é–‹å§‹
	- https://prtimes.jp/main/html/rd/p/000000007.000123714.html
- ãŸãã•ã‚“ã®ãŠå®¢æ§˜ãŒCotomoã¨ãŠã—ã‚ƒã¹ã‚Šã—ã¦ã„ã‚‹ã“ã¨ã§ã€å‹•ä½œãŒä¸å®‰å®šã«ãªã‚‹äº‹è±¡ãŒç™ºç”Ÿã—ã¦ãŠã‚Šã¾ã™
	- https://x.com/starley_jp/status/1761753632788357611?s=20
- Qwen1.5 é€Ÿã„ã—æ—¥æœ¬èªå®Œç’§ã ã—ã™ã”ã„ by shi3z
	- https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat
- ku-nlp/gpt2-large-japanese-char
	- å¼Šç ”ã®huggingfaceãƒªãƒã‚¸ãƒˆãƒªã§ charcter vocabulary ã®æ—¥æœ¬èª gpt2-largeï¼ˆA100 1æšã§è¨“ç·´8ã‹æœˆ!ï¼‰ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ä½•ã‹ã®èˆˆå‘³ã§æ—¥æœ¬èªã®æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ¬²ã—ã„æ–¹ã¯æ˜¯éä½¿ã£ã¦ã¿ã¦ãã ã•ã„
- ãƒ­ãƒ¼ã‚«ãƒ«ã§æ°—è»½ã«RAGã‚’ä½¿ã£ã¦ä¼šè©±ã™ã‚‹ã“ã¨ãŒç°¡å˜ã™ãã¦ãƒ“ãƒ“ã£ãŸã€‚
	- https://qiita.com/mitsumizo/items/469d79c5e81d9189a9e4
- æ—¥æœ¬ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿æƒ…å ±ä¸€è¦§ãƒ»ã¾ã¨ã‚
	- https://github.com/japan-opendata/awesome-japan-opendata
	- PLATEAU AWARD 2023ã§ã‚°ãƒ©ãƒ³ãƒ—ãƒªã‚’å—è³ã—ãŸæ–¹ã®GitHubã‚‰ã—ã„
-  AITuberã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã¯éŸ³å£°é›‘è«‡ã‹ã‚‰å§‹ã¾ã£ãŸ Cotomo
	- https://note.com/o_ob/n/n27edbebf17af?sub_rt=share_h
	- ãƒ»æ•¬æ„ã‚’æŒã£ã¦æ¥ã™ã‚‹  ã€ã€Œè©±ã™ã®æ¥½ã—ã„ã€è¨­å®š ã€éå»ã®ä¼šè©±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹  ã€ç›¸æ‰‹ã®é€Ÿåº¦ã«åˆã‚ã›ã¦æ—©å£ã«ãªã‚‹  ã€ä¸€åº¦è¨€ã£ãŸè©±ã¯2å›ç›®ã¯æ—©å£  ã€ãŠåˆ¥ã‚Œã‚’åæ®‹æƒœã—ã‚€
- Mistral announces Mistral Large, a new flagship model.
	- https://x.com/omarsar0/status/1762140818654064721?s=20
		- 32K tokens context window
		- has native multilingual capacities
		- strong abilities in reasoning, knowledge, maths, and coding benchmarks
		- function calling and JSON format natively supported
		- available through Microsoft Azure
		- a low-latency model called Mistral Small was also released
- Qwen1.5-72B-Chatã‚’ãŠè©¦ã—ä¸­ã€‚
	- https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat
	- ã‚‚ã†å…¨éƒ¨Qwen-72Bã§ã„ã„ã‚“ã˜ã‚ƒãªã„ã‹ãª
	- https://x.com/alfredplpl/status/1762277261435347424?s=20
- Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
	- https://arxiv.org/abs/2402.14848
	- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¥åŠ›ãŒé•·ããªã‚‹ã«ã¤ã‚Œã¦ã€æ¨è«–æ€§èƒ½ã«é¡•è‘—ãªä½ä¸‹ãŒè¦‹ã‚‰ã‚Œã‚‹ã“ã¨ãŒç¤ºå”†
	- â– å®Ÿé¨“çµæœ
		- å…¥åŠ›ãŒé•·ããªã‚‹ã¨æ¨è«–ã®ç²¾åº¦ãŒä½ããªã‚‹
		- å¤±æ•—ãƒ¢ãƒ¼ãƒ‰ã¯ä¸»ã«4ã¤ã§ã€å…¥åŠ›ãŒé•·ããªã‚‹ã»ã©é¡•è‘—ã«ãªã‚‹ 
			- 1. å›ç­”æ‹’å¦ 
			- 2. åã£ãŸåˆ¤æ–­ 
			- 3. é ­ã‹ã‚‰ç­”ãˆã‚’è¨€ã†ï¼ˆæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¾¿ã‚‰ãªã„ï¼‰ã€ 
			- 4. å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«ä½¿ã‚ãªã„
- RAGã§ã®å›ç­”ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯é›†ï¼ˆåŸºç¤ç·¨ï¼‰
	- https://zenn.dev/knowledgesense/articles/47de9ead8029ba
- NVIDIAãŒãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ç”¨ã®GPUã‚’æ–°ãŸã«ç™ºè¡¨
	- https://x.com/webbigdata/status/1762645658266468393?s=20
	- RTX 500 GPUã¯4GBã®GPUãƒ¡ãƒ¢ãƒª 
	- RTX 1000 GPUã¯6GBã®GPUãƒ¡ãƒ¢
- LangChainã«ä¾¿åˆ©ãªæ©Ÿèƒ½ãŒèª•ç”Ÿã—ã¦ã¾ã—
	- https://x.com/MLBear2/status/1762623474034790886?s=20
	- Pydanticã§æ§‹é€ ä½“ã‚’å®šç¾©ã—ãŸä¸Šã§ `with_structrured_output` ã‚’å›³ã®ã‚ˆã†ã«ä½¿ãˆã°ã€Function Callingã‚’ç°¡å˜ã«å‘¼ã¹ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ 
	- ChatGPTã ã‘ã§ã¯ãªãã€Geminiãªã©Function Callingã«å¯¾å¿œã™ã‚‹ä»–ã®LLMã§ã‚‚ã‚‚ã¡ã‚ã‚“ä½¿ãˆã‚‹ã¨ã®ã“ã¨ã€‚
- Function Calling Cookbook with Open-source models (LlamaIndex+FIREWORKS)
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/fireworks_cookbook.ipynb
	- Weâ€™re excited to present a series of cookbooks showing you how to use LlamaIndex with Fireworks, including function calling and RAG with FireFunction-v1.
- PDFãŒã‚¹ãƒ«ã‚¹ãƒ«èª­ã‚ã‚‹ï¼è©±é¡Œã®LlamaParseã¨ã¯
	- https://zenn.dev/yokina_kaoto/articles/563f7d75673c2e
	- LlamaParseã¯LlamaIndexã®æ–°ã—ã„è£½å“ã§ã€å†å¸°æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§è¤‡é›‘ãªPDFã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãã‚Œã„ã«æŠ½å‡ºã™ã‚‹ã“ã¨ãŒã§ãã€ã—ã°ã—ã°æ‚©ã¾ã•ã‚Œã‚‹è¤‡é›‘ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚ˆã‚Šæ­£ç¢ºãªè§£æã‚’ç´„æŸã—ã¾ã™
	- LlamaParseã§PDFã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€AstraDBã§**éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿**ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ã§ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã¨ã®ã“ã¨ã€‚
- æ–°Kaggleã‚³ãƒ³ãƒšï¼š LLMã§ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¾©å…ƒã™ã‚‹ã‚¿ã‚¹ã‚¯
	- https://www.kaggle.com/competitions/llm-prompt-recovery
	- LLMã§ç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¾©å…ƒã™ã‚‹ã‚¿ã‚¹ã‚¯ã€‚
	- ãƒ‡ãƒ¼ã‚¿ã¯Google Gemmaã§ä½œæˆã€‚è©•ä¾¡ãŒsentence-t5-baseã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ã¨ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãªã®ãŒæ™‚ä»£ã‚’æ„Ÿã˜ã‚‹ã€‚ã‚‚ã†Jaccardã‚¹ã‚³ã‚¢ã¨ã‹ã®æ™‚ä»£ã˜ã‚ƒãªã„ã‚‰ã—ã„
- iOS17.4ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã«OpenAIã®ä½•ã‹ã‚’å«ã‚€éƒ¨åˆ†ãŒè¦‹ã¤ã‹ã£ã¦ã„ã¦ã€ãŠãã‚‰ãæ•°ãƒ¶æœˆä»¥å†…ã«SiriãŒå¼·åŠ›ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚
	- https://x.com/1amageek/status/1762422935376302226?s=20
-  The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
	- https://huggingface.co/papers/2402.17764
	- Microsoft presents The Era of 1-bit LLMs 
	- All Large Language Models are in 1.58 Bits
	- ã“ã‚Œæœ¬å½“ãªã‚‰ã‚¿ã‚¤ãƒˆãƒ«é€šã‚Šç”ŸæˆAIã®æ–°æ™‚ä»£ã‹ã‚‚ã—ã‚Œãªã„
-  1ãƒ“ãƒƒãƒˆLLMã®è¡æ’ƒ! 70Bã§8.9å€é«˜é€Ÿã€€å…¨ã¦ã®æ¨è«–ã‚’åŠ ç®—ã®ã¿ã§!GPUä¸è¦ã«ãªã‚‹å¯èƒ½æ€§ã‚‚ by shi3zã•ï½
	- https://wirelesswire.jp/2024/02/86094/
	- ã„ãšã‚Œã«ã›ã‚ˆã€ã€€ã“ã®è«–æ–‡ãŒæœ¬å½“ã ã¨ã—ãŸã‚‰ã€ã¨ã‚“ã§ã‚‚ãªã„ã“ã¨ãŒèµ·ãã‚‹ã“ã¨ã«ãªã‚‹ã€‚
- MicrosoftãŒã€Œ1ãƒ“ãƒƒãƒˆLLMæ™‚ä»£ã®åˆ°æ¥ã€ã¨ã„ã†è¡æ’ƒçš„ãªã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’å…¬é–‹ã—ã€GPUãŒä¸è¦ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã¨ã®è©±ã‚‚å‡ºã¦ãã¦ã„ã‚‹ã®ã§å¾“æ¥ã®æ‰‹æ³•ã¨ã®é•ã„ã‚’ã¾ã¨ã‚ã¾ã—ãŸ
	- https://x.com/webbigdata/status/1763021292696170917?s=20
-  é©šç•°ã®1ãƒ“ãƒƒãƒˆLLMã‚’è©¦ã™ã€‚æœãŸã—ã¦æœ¬å½“ã«å­¦ç¿’ã§ãã‚‹ã®ã‹? by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/n58b0a2252727?sub_rt=share_pb
	- è©¦ã—ãŸã®ã¯ã“ã¡ã‚‰
		- https://github.com/Beomi/BitNet-Transformers/tree/main
	- ãªã‚“ã‹ãã‚Œã£ã½ã„ã“ã¨è¨€ã£ã¦ã‚‹!!!!!!  ã—ã‹ã‚‚å°ã•ã„ã‹ã‚‰å½“ãŸã‚Šå‰ãªã®ã ãŒæ¨è«–ã¯è¶…é€Ÿã„ã®ã§ã‚ã‚‹ã€‚
	- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¯200MBã€‚GBã˜ã‚ƒãªã„ã‚ˆã€‚  åƒ•ã¯å°ã•ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã‚‚å¤§ãã„è¨€èªãƒ¢ãƒ‡ãƒ«ã‚‚ãã“ãã“è§¦ã£ã¦æ¥ãŸæ–¹ã ã¨æ€ã†ãŒã€ã“ã®ã‚µã‚¤ã‚º
- Mixtral 250Mã®pretrainingã‹ã‚‰Instruction Tuningã¾ã§
	- https://zenn.dev/if001/articles/9bb90e0d8c201f
	- MoEã‚’æŒã¤MixtralãŒhuggingface/transformersã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’åˆ©ç”¨ã—ã¤ã¤ã€250Mã®å°ã•ã„ã‚µã‚¤ã‚ºã¨ã—ã¦æ—¥æœ¬èªã¨è‹±èªã§pretrainingã€finetuningã‚’è¡Œã„ã¾ã™ã€‚
	- 250Mã®Mixtralã‚’pretrainingã‹ã‚‰finetuningã¾ã§ã‚’è¡Œã„ã¾ã—ãŸã€‚å°ã•ã„ã‚µã‚¤ã‚ºãªã‚Šã«ã†ã£ã™ã‚‰æ—¥æœ¬èªã‚’ç†è§£ã—ã¦ãã†ã€‚å…¥åŠ›ã‹ã‚‰æ­£ç¢ºã«æƒ…å ±ã‚’æŠ½å‡ºã¨ãã‚Œã‚‰ã‚’ä½¿ã£ãŸå‡ºåŠ›ã¯ã•ã™ãŒã«é›£ã—ãã†ã€‚ã‚ã¨ã¯ã€æ¨è«–æ™‚ã®expertã®é¸æŠã®ã•ã‚Œã‹ãŸã‚„åŒã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒã‚’ã—ã¦ã¿ãŸã„ã¨ã“ã‚
- ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ãªã‚‰å…¨å“¡BitNetè©¦ã—ã¦ã¿ã‚‹ã¹ã by shi3zã•ã‚“ã€
	- https://github.com/kyegomez/BitNet
- gemma-7bã€è‹±æ—¥ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦ã¯å¾®èª¿æ•´ã«æˆåŠŸã™ã‚‹ã¨ç§ã®ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ALMA-7B-Ja-V2ã‚ˆã‚Šä¸€æ®µéšãƒ¬ãƒ™ãƒ«ãŒä¸Šã®æ€§èƒ½ã§ã—ãŸ
	- https://x.com/webbigdata/status/1762791697212375111?s=20
	- å‘¨è¾ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ãƒã‚°ãŒæ®‹ã£ã¦ã„ã¦ã€è‹±èªåœã§ã¯ã‚ãã‚‰ã‚ã‚‹å‹¢ãŒå¤šã„ã¿ãŸã„ã€‚
- LlamaIndexã¨Groqã®çµ±åˆ
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/groq.ipynb
- Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
	- https://arxiv.org/abs/2402.18439
	- ã€Œè‡ªç„¶è¨€èªã‚’è¶…ãˆã¦ã€ã¨é¡Œã—ã¦ã€LLMã«ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚ã®æ€è€ƒã‚’äººé–“ã®è‡ªç„¶è¨€èªã¨ã¯ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡Œã‚ã›ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã€AutoFormï¼ˆã‚ªãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼‰ã€ãŒè€ƒæ¡ˆã•ã‚Œã¾ã—ãŸã€‚
	- LLMã®æ€è€ƒã¯å¿…ãšã—ã‚‚äººé–“ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ²¿ã†å¿…è¦ã¯ãªã„ã€ã¨ã„ã£ãŸçµè«–ã«ãªã‚Šã¾ã™ã€‚LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒå£«ã§ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹éš›ã«ã¯ã“ã®æ–¹ãŒåŠ¹ç‡çš„ã‹ã‚‚ã—ã‚Œãªã„ã¨ã®ã“ã¨ã€‚
	- è‡ªç„¶è¨€èªã«å›ºæœ‰ã®æ›–æ˜§ã•ã‚’æ’é™¤ã—ã€æ˜ç¢ºæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®è§£æ±ºç­–ã«ã¯ã€ã‚ˆã‚Šæ§‹é€ åŒ–ã•ã‚Œã¦ç°¡æ½”ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å½¢å¼ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã¯ã€ã‚³ãƒ¼ãƒ‰ã€æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ã€JSONã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¡¨ã€è«–ç†æ¼”ç®—å­ã€ã¾ãŸã¯æ•°å­¦æ–¹ç¨‹å¼ãŒå«ã¾ã‚Œã¾ã™ã€‚å›ç­”ã®æœ€å¾Œã«ã¯ã€ã€œã€œã¨ã„ã†å½¢å¼ã§ç­”ãˆã‚’ç¤ºã•ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºã§ã‚ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚
- ChatGPTã¯æ•°å­¦ã‚’è§£ãæ™‚ã«å³å¯†ã«è¨ˆç®—ã™ã‚‹ãŸã‚ã«ADAï¼ˆAdvanced Data Analitics, Code Interpreterï¼‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½¿ç”¨ã™ã‚‹æ§˜ã«å¤‰ã‚ã£ã¦ã¾ã™
	- https://x.com/ai_syacho/status/1763308074503422008?s=20
	- ã—ã‹ã‚‚æ•°å­¦è¨ˆç®—ã®è¨ˆç”»ã‚‚ç«‹ã¦ã‚‹äº‹ãŒã§ãã‚‹ã€‚
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã®BitNetã‚’1.58bã®è«–æ–‡ã«å¾“ã£ã¦3å€¤ã«ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã¾ã—ãŸ
	- https://github.com/frodo821/BitNet-Transformers
- Beyond Disciplinesã€ŒBeyond Disciplines ï½CRDSãŒæ³¨ç›®ã™ã‚‹ç ”ç©¶é–‹ç™ºã®æ½®æµ2024ï½ã€
	- https://www.jst.go.jp/crds/report/CRDS-FY2023-RR-06.html
	- æ‰€å±çµ„ç¹”ãŒç™ºè¡Œã—ã¦ã„ã‚‹æ•°åå†Šãƒ»è¨ˆæ•°åƒãƒšãƒ¼ã‚¸ã®å ±å‘Šæ›¸ã‚’40ãƒšãƒ¼ã‚¸ãã‚‰ã„ã«åœ§ç¸®ã—ãŸãƒ¬ãƒãƒ¼ãƒˆä½œæˆã«ã‹ã‹ã‚ã‚Šã¾ã—ãŸã€‚
- Qwen1.5-72B æ—¥æœ¬èªèƒ½åŠ›ã‚‚é«˜ãã¦è‰¯ã„ãŒç”Ÿæˆç‰©ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä½œã‚Œãªã„è¦ç´„ã§æ®‹å¿µã€‚
	- https://x.com/alexweberk/status/1763905106674954324?s=20
- ã€è«–æ–‡ã®çµè«–ã‚’å­¦ç¿’ã•ã›ãŸã‚‰æ€§èƒ½ãŒä¸‹ãŒã£ãŸã€‚ã€
	- https://newswitch.jp/p/40657
	- ï¼–ä¸‡ï¼•ï¼ï¼ï¼å ±ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ãŸã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€è«–æ–‡ã®è¦ç´„ã‚ˆã‚Šã‚‚ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãŒæ€§èƒ½å‘ä¸Šã«å½¹ç«‹ã£ãŸã€‚è«–æ–‡ã®çµè«–ã®å­¦ç¿’ã¯ã€æ€§èƒ½é¢ã§ãƒã‚¬ãƒ†ã‚£ãƒ–ã«åƒã„ãŸã€‚å°ã•ãªï¼¬ï¼¬ï¼­ã«ã¨ã£ã¦ã¯çµè«–ã®å†…å®¹ãŒå°‚é–€çš„éããŸå¯èƒ½æ€§ãŒã‚ã‚‹ã€‚å°‚é–€çŸ¥è­˜ã‚’å‚™ãˆãŸï¼¬ï¼¬ï¼­ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®çŸ¥è¦‹ã«ãªã‚‹ã€‚
- ã€è«–æ–‡ä¸å¯§è§£èª¬ã€‘BitNet b1.58ã¨ã¯ä¸€ä½“ä½•è€…ãªã®ã‹
	- https://qiita.com/tech-Mira/items/67dec9c5a5f025d2727a?utm_campaign=post_article&utm_medium=twitter&utm_source=twitter_share
	- BitNet b1.58ã¯ã€ãã®åã®é€šã‚Šã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã€ã€[âˆ’1ã€0ã€1]ã¨ã„ã†3ã¤ã®å€¤ã§ã®å‹•ä½œã‚’å®Ÿç¾ã—ãŸ1bitã®LLMã§ã™ã€‚ã¤ã¾ã‚Šã€è†¨å¤§ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å¿…è¦ã¨ã™ã‚‹å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€éå¸¸ã«åŠ¹ç‡çš„ã«å‹•ä½œã—ã¾ã™ã€‚åŠ ãˆã¦ã€ã“ã®è¨˜äº‹ã§ç¤ºã•ã‚Œã¦ã„ã‚‹çµæœã§ã¯é©šãã¹ãã“ã¨ã«ã€æ€§èƒ½ã¯å¾“æ¥ã®é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã¾ã™ã€‚
	- BitNet b1.58ã¨FP16 LLaMA LLMã‚’æ§˜ã€…ãªã‚µã‚¤ã‚ºã§æ¯”è¼ƒã—ã¾ã—ãŸã€‚å…¬å¹³ãªæ¯”è¼ƒã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’RedPajamaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§1000å„„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦äº‹å‰å­¦ç¿’ã—ã¾ã—ãŸã€‚
- Google AI é€²åŒ–çš„ ã§ ã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã«ã‚ˆã‚‹ Gemini ã® ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/n8b03a58abb2c?sub_rt=share_h
	- ã€ŒGoogle AI é€²åŒ–çš„ã€ã§ã€Œã¤ãã‚ˆã¿ã¡ã‚ƒã‚“ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã«ã‚ˆã‚‹ã€ŒGeminiã€ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã—ãŸã®ã§ã€ã¾ã¨ã‚ã¾ã—ãŸã€‚
-  Towards Long Context RAG by llamaindex
	- https://www.llamaindex.ai/blog/towards-long-context-rag
	- We did a deep dive into Gemini, and consolidated our thinking about long-context LLM benefits, challenges, and new architectures
	- Long-context LLMs will help alleviate the need to do precise chunking and retrieval, and RAG over small sets of documents
	- Long-context LLMs still donâ€™t resolve the issue of RAG over big knowledge bases (present in most organizations/enterprises)
- Gemini 1.5 ProãŒé‚ã«ãã¾ã—ãŸï¼ï¼ï¼ï¼
	- https://x.com/masahirochaen/status/1763639557457899963?s=20
- Googleã®Gemmaã€2Bã®æ–¹ãŒ7Bã‚ˆã‚Šæ€§èƒ½ãŒè‰¯ã„ã¨ã‹ãŠã‹ã—ãªäº‹ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹
	- https://x.com/webbigdata/status/1763730996455973098?s=20
	- Jeremyã•ã‚“ã®è¨€ã£ã¦ã„ã‚‹é€šã‚Šã€fine tuningã¯Hugging Faceã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹Transformerså®Ÿè£…ã§ã¯ãªãã¦ã€githubã®google-deepmind/gemmaã‚’å‚è€ƒã«ã—ãŸæ–¹ãŒè‰¯ã„ã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“
	- https://x.com/jeremyphoward/status/1763679390968455185?s=20
-  ãƒ­ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆLLMã«å¯¾å¿œã—ãŸRAGã®æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ by npakaã•ã‚“
	- https://note.com/npaka/n/n0b17244bae47?sub_rt=share_h
	- ã€ŒGemini 1.5 Proã€ã®æ©Ÿèƒ½ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã“ã¨ãŒã§ãã€ãã‚Œã‚’è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã§ã€ãƒ­ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆLLMã‚’é©åˆ‡ã«ä½¿ç”¨ã™ã‚‹ã«ã¯ã€RAGãŒã©ã®ã‚ˆã†ã«é€²åŒ–ã™ã‚‹ã®ã‹ã«ã¤ã„ã¦ã®ã¾ã¨ã‚ã¾ã—ãŸã€‚
	- **Gemini ã¯ç‰¹å®šã®è©³ç´°ã‚’è¦‹äº‹ã«æ€ã„å‡ºã™ã“ã¨ãŒã§ãã‚‹**
	- **Gemini ã¯ç´ æ™´ã‚‰ã—ã„è¦ç´„èƒ½åŠ›ã‚’æŒã¤**
	- **10Mãƒˆãƒ¼ã‚¯ãƒ³ã¯å¤§è¦æ¨¡ãªæ–‡æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã¯ååˆ†ã§ã¯ãªã„**
	- **åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã®ç‚¹ã§é…ã‚Œã¦ã„ã‚‹**
	- RAGã®æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
		- ã€Œ**Small-to-Big Retrieval**ã€
		-  ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¨ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®Ÿç¾ã™ã‚‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## 2/26

å…ˆé€±ã€soraã®ç™ºè¡¨ã§å°‘ã—éœã‚“ã Gemini 1.5 pro ã€402ãƒšãƒ¼ã‚¸ã®æ–‡æ›¸ã€44åˆ†é–“ã®æ˜ ç”»ã€10ä¸‡è¡Œã®ã‚³ãƒ¼ãƒ‰ã«å¯¾ã™ã‚‹æ¨è«–ãªã©ã€ãã®èƒ½åŠ›ã®ä¸€æ—¦ãŒå£é–“è¦‹ã‚Œã¦ããŸã€‚Googleã¯å¼•ãç¶šãGemini 1.5 proãƒ™ãƒ¼ã‚¹ã®OSSã§ã‚ã‚‹Gemma(â€œè²´é‡ãªçŸ³â€ã€ãƒ©ãƒ†ãƒ³èª)ã‚’ãƒªãƒªãƒ¼ã‚¹ã€åŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã§ã‚ã‚Œã°Llama2ã‚„Mistralã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã¨ã®äº‹ã€‚Gemmaã¯è»½é‡ã§ã‚ã‚‹ã¨ã¨ã‚‚ã«ã€embeddingã®å·¥å¤«ã€å®‰å…¨ãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¨å¿…é ˆãƒ„ãƒ¼ãƒ«ã®æä¾›ã€Kera3.0ã‚µãƒãƒ¼ãƒˆãªã©ã€ã‹ãªã‚Šã®é‡ã¨è³ªã®ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã‚¹ã‚¿ãƒƒã‚¯ãŒä¸€æ°—ã«å…¬é–‹ã•ã‚ŒãŸã“ã¨ã«ãªã‚‹ã€‚OSSæˆ¦ç•¥ã¨ã—ã¦ã€å®‰å…¨æ€§ã«é–¢ã™ã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã®å…±å‰µã¨ã„ã†æ„å‘³ã§ã‚‚ã€Metaã®OSSæˆ¦ç•¥ã¨ä¸¸è¢«ã‚Šã€‚æ—©é€Ÿã€é‡å­åŒ–ggufç‰ˆã‚„ã€Kaggleã§Gemmaã‚’ã¤ã‹ã£ãŸã‚³ãƒ³ãƒšã®é–‹å‚¬ã€embeddingã®è§£æï¼ˆæ—¥æœ¬èªèªå½™ã¯è²§å¼±ï¼Ÿï¼‰ã€npakaã•ã‚“ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è©¦è¡Œã€MLXã‚’ä½¿ã£ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ´»å‹•ãŒç››ã‚“ã«ã€‚LPUï¼ˆLanguage Processing Unitï¼‰ã‚’å¼•ã£æã’ã‚‹Groqã€æ¨è«–æ™‚ã®é«˜é€Ÿã•ãŒåŠç«¯ãªã„ã€å°‚ç”¨ãƒãƒƒãƒ—é–‹ç™ºã§ã‚‚æˆ¦ã„ã¯ç¶šãã€æ—¥æœ¬ã®MN-coreæ—©ãï¼llamaindexã‚‚LlamaCloudã¨LlamaParseã‚’ãƒªãƒªãƒ¼ã‚¹ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚„å›³è¡¨ãªã©ã®åŸ‹ã‚è¾¼ã¾ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚€è¤‡é›‘ãªæ–‡æ›¸ã®ãŸã‚ã®ç‹¬è‡ªã®ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚„ã€RAGã®æ§‹ç¯‰ãŒã‚ˆã‚Šé«˜æ€§èƒ½ã«ã€ã‹ã¤å®¹æ˜“ã«ãªã£ãŸã€‚æ—¥æœ¬èªLLMã§ã¯ã€ KARAKURI LM (70B)ã®ELYZA-tasks-100ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ã‚„ã€æ±å·¥å¤§ã¨æ±åŒ—å¤§ã«ã‚ˆã‚‹Kotomambaã®æ§‹ç¯‰ç­‰ã€‚ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€BCGXã‹ã‚‰agentkitã®OSSãƒªãƒªãƒ¼ã‚¹ã€DXã®æ‰‹æ®µã¨ã—ã¦ã®AIã¨ã„ã†ã‚·ãƒŠãƒªã‚ªã§ã®ã‚³ãƒ³ã‚µãƒ«ç³»ã®æ–°ãŸãªãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã€‚åŸºç¤ç ”ç©¶ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã‹ã‚‰ã€Œæ–°ã—ã„è¨€è‘‰ã®æ¦‚å¿µã€ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€FOCUSã€ã‚„ã€Mambaã¨transformerã¨ã®colabã‚’ä½¿ã£ãŸé€Ÿåº¦æ¯”è¼ƒã¨ã‹ã€ãã‚‚ãã‚‚çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ã®è§£èª¬ã¨ã‹ã€‚DeepMindã¨CMUã«ã‚ˆã‚‹ã€LLMã‚’ã¤ã‹ã£ãŸæ•°å€¤å›å¸°OmniPredè«–æ–‡ã‚‚é¢ç™½ã„ã€ãã®æ€§èƒ½ã®ç†è«–çš„è§£æãŒå¾…ãŸã‚Œã‚‹ã€‚Stable Diffusion 3ã®ãƒªãƒªãƒ¼ã‚¹ã‚„sentencepiece v0.2.0ãƒªãƒªãƒ¼ã‚¹ãªã©ã®åŸºç›¤ã‚½ãƒ•ãƒˆã®é‡è¦ãªæ›´æ–°ã‚‚é€²ã‚“ã ã€‚

- BCGXã‹ã‚‰ã€agentkit
	- https://agentkit.infra.x.bcg.com/
	- BCG Xã‹ã‚‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸAgentã‚’æ¥½ã«ä½œã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯AgentKitãŒOSSã¨ã—ã¦å‡ºã¾ã—ãŸã€œã€‚ Nextjs, FastAPI, Langchainã®ãƒ¢ãƒ€ãƒ³ãªãƒ†ãƒƒã‚¯ã‚¹ã‚¿ãƒƒã‚¯ã§ã™
-  Hyena Hierarchy: Towards Larger Convolutional Language Models
	- https://speakerdeck.com/hpprc/hyena-hierarchy-towards-larger-convolutional-language-models
	- Hyena Hierarchyã«ã¤ã„ã¦ã€çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆSSMï¼‰ã®åŸºç¤ã‹ã‚‰è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰
- Groqã®LPUã«ã¤ã„ã¦
	- https://x.com/umiyuki_ai/status/1759740311335739784?s=20
	- Groqã¨ã‹è¨€ã†ä¼šç¤¾ã®LPUï¼ˆLanguage Processing Unitï¼‰ã£ã¦æ–°ã—ã„ãƒãƒƒãƒ—ã¯LLMæ¨è«–é€Ÿåº¦ãŒçˆ†é€Ÿãªã‚“ã ã¨ã€‚NVidiaã¨ã‹ã®GPUã¨é•ã£ã¦é«˜å“è³ªãªVRAMãŒè¦ã‚‰ã‚“ã‹ã‚‰ä½ã‚³ã‚¹ãƒˆã‚‰ã—ã„ã€‚70Bã®LLMã‚’å‹•ã‹ã™æ™‚ã«300tpsã¨ã„ã†è¶…çˆ†é€Ÿã§æ¨è«–ã§ãã‚‹ã€‚
	- M3Maxã ã¨6tpsã€RTX4090+PowerInferã ã¨4tpsã—ã‹å‡ºãªã„ã‹ã‚‰50ï½100å€ã®é€Ÿåº¦å·®ã€‚GPUãŒã‚ªãƒ¯ã‚³ãƒ³ã®æ™‚ä»£æ¥ãŸã‹ï¼Ÿ
- The Shift from Models to Compound AI Systems
	- https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/
	- Berkeleyã®äººã€…ã«ã‚ˆã‚‹ã€ã€Œã‚³ãƒ³ãƒ‘ã‚¦ãƒ³ãƒ‰AIã€ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼è¨˜äº‹ã€‚
	- LLMå˜ä½“ã§å‹è² ã™ã‚‹ã‚ˆã‚Šã‚‚ã€LLMã‚’å«ã‚€å„ç¨®AIï¼éAIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ä½œã‚‹ã€Œã‚³ãƒ³ãƒ‘ã‚¦ãƒ³ãƒ‰AIã€ã®æ–¹ãŒã‚ˆã‚Šè‰¯ã„ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚Šã‚„ã™ã„ã€
- Introducing LlamaCloud  andã€€LlamaParse
	- https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b
	- Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring **production-grade**  **context-augmentation** to your LLM and RAG applications.
- MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts
	- https://arxiv.org/abs/2401.11403
	- æ©Ÿæ¢°å­¦ç¿’å¿œç”¨ã«ã¯åˆ†å­ã®ç‰©æ€§äºˆæ¸¬ã‹ã‚‰åˆ†é¡ã¾ã§ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ãŒã€LLMã«ã‚ˆã‚Šãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ã”ã¨ã«æœ€é©ãªåˆ†å­è¡¨ç¾ã¸ã¨èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€äºˆæ¸¬æ€§èƒ½ãŒå‘ä¸Šã—ãŸãã†ã§ã™ã€‚
- è¶…é«˜é€Ÿãªå¯¾è©±AIã‚µãƒ¼ãƒ“ã‚¹ã®Groq
	- https://groq.com/
-  Learning to Learn Faster from Human Feedback with Language Model Predictive Control
	- https://huggingface.co/papers/2402.11450
	- Google presents Learning to Learn Faster from Human Feedback with Language Model Predictive Control
- SLANG: New Concept Comprehension of Large Language Models
	- https://arxiv.org/abs/2401.12585
	- GPT-4ãªã©ã«å¯¾ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã‹ã‚‰ã€Œæ–°ã—ã„è¨€è‘‰ã®æ¦‚å¿µã€ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€FOCUSã€ã‚’ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å¤§å­¦ãªã©ã®ç ”ç©¶è€…ã‚‰ãŒè€ƒæ¡ˆ
	- â– ãƒ¡ã‚½ãƒƒãƒ‰ 
		- 1. ä½¿ç”¨ä¾‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ç›´æ¥å…¥åŠ›ã™ã‚‹ 
		- 2. ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§éš ã—ã¦ã€æ„å‘³ã‚’è©•ä¾¡ã•ã›ã‚‹ 
		- 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆå›ºæœ‰åè©ã‚„å‡ºæ¥äº‹ãªã©ï¼‰ã‚’å¤‰æ›´ã—ã€ç•°ãªã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒãƒ•ãƒ¬ãƒ¼ã‚ºã®è§£é‡ˆã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿ã¹ã‚‹ 
		- 4. ä¸Šè¨˜ã®çµæœã€ãƒ¢ãƒ‡ãƒ«ãŒæ–°ã—ã„è¨€è‘‰ã®ç†è§£ã«è‡³ã£ãŸã®ã‹ã‚’è©•ä¾¡ã™ã‚‹ 
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. GPT-4/3.5ã§æ¤œè¨¼ 
		- 2. ãƒ¢ãƒ‡ãƒ«ãŒçŸ¥ã‚‰ãªã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒŸãƒ¼ãƒ ã‚’æ•™ãˆè¾¼ã¾ã›ãŸ 
		- 3. GPT-4ã§88.2%ã€GPT-3.5ã§ã‚‚84.5%ã®æ­£ç¢ºã•ã‚’é”æˆã—ãŸ
-  GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
	- https://huggingface.co/papers/2402.10963
	- çµæœãƒ™ãƒ¼ã‚¹ã®å ±é…¬ãƒ¢ãƒ‡ãƒ« (ORM) ã‚’ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã®æ”¹è‰¯ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€ã„ãšã‚Œã‹ 1 ã¤ã‚’å€‹åˆ¥ã«ä½¿ç”¨ã—ãŸå ´åˆã‚„ã€3 ã¤ã®ã‚µãƒ³ãƒ—ãƒ« ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ä¸­ã§æœ€ã‚‚å„ªã‚ŒãŸã‚‚ã®ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚
- The Tokenizer Playground
	- https://huggingface.co/spaces/Xenova/the-tokenizer-playground
	- After watching, if you want to learn more about how different models (e.g., GPT4, Llama, T5, BERT) tokenize text, check out "The Tokenizer Playground": a web-app I built a few months ago with Transformer.js
- Gemini Advancedã§AIã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸpythonã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥å®Ÿè¡Œã—ã¦å‹•ä½œç¢ºèªã§ãã‚‹ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ãŒè¿½åŠ ã•ã‚ŒãŸ
	- https://x.com/webbigdata/status/1760129585994432916?s=20
	- Gemini 1.5 proã§ã€Œgithubã‹ã‚‰ç›´æ¥å…¨ã‚³ãƒ¼ãƒ‰ã¨å…¨issuesã‚’å–å¾—ã•ã›ã‚‹äº‹ã€ã¨ã€Œæœ€ã‚‚ç·Šæ€¥åº¦ã®é«˜ã„issuesã‚’ç‰¹å®šã—ã€ä¿®æ­£ã‚’å®Ÿè£…ã•ã›ã‚‹äº‹ã€ãŒå‡ºæ¥ãŸ
- Kotomamba: mamba-2.8B å­¦ç¿’çŸ¥è¦‹
	- https://zenn.dev/kotoba_tech/articles/f15b2495d44c4f
	- Kotoba Technologiesã¯NLPã¨åˆ†æ•£ä¸¦åˆ—å­¦ç¿’ã«é–¢ã™ã‚‹æŠ€è¡“ã‚’ç”¨ã„ã¦ã€æ—¥æœ¬åŠã³éè‹±èªåœã«ãŠã‘ã‚‹LLMã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å®Ÿé‹ç”¨ã«å‘ã‘ãŸç ”ç©¶é–‹ç™ºã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
	- from scratchã‹ã‚‰æ—¥æœ¬èªã¨è‹±èªã®ã‚³ãƒ¼ãƒ‘ã‚¹ã«ã¦å­¦ç¿’ã‚’è¡Œã£ãŸkotomamba-2.8B-v1.0ã€
	- ã‚‚ã†ï¼‘ã¤ã¯state-spaces/mamba-2.8b-slimpjã‹ã‚‰æ—¥æœ¬èªã¨è‹±èªã§ç¶™ç¶šäº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸkotomamba-2.8b-CL-v1.0ã§ã™ã€‚
- sentencepiece v0.2.0
	- https://github.com/google/sentencepiece/releases/tag/v0.2.0
- Gemini 1.5 Proã®Youtubeï¼“æœ¬ã‚»ãƒƒãƒˆ
	- Reasoning across a 402-page transcript
	- https://www.youtube.com/watch?v=LHKL_210CcU
	- Multimodal prompting with a 44-minute movie
	- https://www.youtube.com/watch?v=wa0MT8OwHuk
	- Problem solving across 100,633 lines of code
	- https://www.youtube.com/watch?v=SSnsmqIj1MI
- KARAKURI LM ã‚’ ELYZA-tasks-100 ã§è©•ä¾¡ã—ã¦ã¿ãŸ
	- https://qiita.com/wayama_ryousuke/items/f4f384b89e9b40a2d794
	- å®Ÿéš›ã«ã©ã®ç¨‹åº¦ã®æ€§èƒ½ãŒã‚ã‚‹ã®ã‹ã€[ELYZA](https://elyza.ai/) ãŒå…¬é–‹ã—ã¦ã„ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ **ELYZA-tasks-100** ã§è©•ä¾¡ã—ã¦ã¿ã¾ã—ãŸã€‚
	- å‰å›è¨˜äº‹ã§æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’ãƒãƒ¼ã‚¯ã—ãŸ Xwin-LM-70B (4bit é‡å­åŒ–) ã‚’ä¸Šå›ã‚Šã€å¹³å‡å¾—ç‚¹2.98ç‚¹ã‚’ãƒãƒ¼ã‚¯ã—ã¦**1ä½**ã¨ãªã‚Šã¾ã—ãŸã€‚
	- æ—¥æœ¬ç™ºã® 70B ãƒ¢ãƒ‡ãƒ«ã¯ [Japanese-StableLM](https://huggingface.co/collections/stabilityai/japanese-stable-lm-654063a381a8731a1c0f13cc) ãªã©ã”ãä¸€éƒ¨ã«é™ã‚‰ã‚Œã€ELYZA-tasks-100 ã§ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚‚æµ·å¤–ãƒ¢ãƒ‡ãƒ«ãŒå„ªä½ã«ç«‹ã£ã¦ã„ã‚‹çŠ¶æ³ã§ã—ãŸã€‚  KARAKURI LM ã®å…¬é–‹ã«ã‚ˆã‚Šã€ãã®çŠ¶æ³ãŒå¤§ããå¤‰ã‚ã£ãŸã¨è¨€ãˆãã†ã§ã™ã€‚
- Mambaã‚’å‹•ã‹ã—ã¦é€Ÿåº¦ã‚’transformerã¨æ¯”è¼ƒã™ã‚‹ãƒ¡ãƒ¢
	- https://note.com/kan_hatakeyama/n/na911120f4ffb?sub_rt=share_pb
	- è©±é¡Œã®mambaã‚’colabã§å‹•ã‹ã—ã¦ã¿ã¾ã—ãŸï½¡ åŒç­‰ã‚µã‚¤ã‚ºã®transformerã‚ˆã‚Šã‚‚ï½¤2å€ãã‚‰ã„ã¯æ¨è«–ãŒæ—©ãã†ã§ã™ï½¡
- Googleã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ« Gemma ã®æ¦‚è¦  by npakaã•ã‚“
	- https://note.com/npaka/n/na47e13dae482?sub_rt=share_h
	- ã€Œ[**Gemma**](https://ai.google.dev/gemma)ã€ã¯ã€ã€Œ**Gemini**ã€ã¨åŒã˜æŠ€è¡“ã‚’åŸºã«æ§‹ç¯‰ã•ã‚ŒãŸã€è»½é‡ã§æœ€å…ˆç«¯ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«
	- ã€ŒGemma 2Bã€ã€ŒGemma 7Bã€ã®2ã¤ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã™ã€‚å„ã‚µã‚¤ã‚ºã¯ã€äº‹å‰å­¦ç¿’ãŠã‚ˆã³æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒãƒªã‚¢ãƒ³ãƒˆã§ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã™ã€‚
	- ã€ŒResponsible Generative AI Toolkitã€ã¯ã€ã€ŒGemmaã€ã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šå®‰å…¨ãªAIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¨å¿…é ˆãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚
	- ã€ŒKeras 3.0ã€ã‚’ä»‹ã—ã¦ã€JAXã€PyTorchã€TensorFlow ãªã©ã€ã™ã¹ã¦ã®ä¸»è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ãŸã£ã¦æ¨è«–ã¨æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (SFT) ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ãƒã‚§ãƒ¼ãƒ³ã‚’æä¾›ã—ã¦ã„ã¾ã™
	- äº‹å‰å­¦ç¿’ã€æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã€ŒGemmaã€ã¯ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã€ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€Google Cloud ä¸Šã§å®Ÿè¡Œã§ã
	- ã€ŒGemmaã€ã®ãƒªã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç†è§£ã—ã¦è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€æ‰‹å‹•ã®ãƒ¬ãƒƒãƒ‰ãƒãƒ¼ãƒ åŒ–ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸæ•µå¯¾çš„ãƒ†ã‚¹ãƒˆã€å±é™ºãªã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½ã®è©•ä¾¡ãªã©ã€å …ç‰¢ãªè©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚ 
	- ai.google.dev/gemmaã€ã§ã¯ã€ã€ŒGemmaã€ã®è©³ç´°ã‚„ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã§ãã¾ã™ã€‚
- Gemma Tokenizer ãŒé¢ç™½ã„
	- https://x.com/AiXsatoshi/status/1760437059066695976?s=20
	- Llama tokenizerã¨å…±é€šç‚¹
		- SentencePieceãƒ™ãƒ¼ã‚¹
		- ãƒã‚¤ãƒˆãƒ¬ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œ
	- é•ã„
		- èªå½™ã‚µã‚¤ã‚º: Gemma 256Kã€Llama 32K 
		- Gemmaã¯`add_dummy_prefix` False â†’ å…ˆé ­ã«ç©ºç™½è¿½åŠ ãªã—ï¼ˆGPTã¨åŒã˜ï¼‰
		- Gemmaã«ã¯ç‰¹åˆ¥ãªtokenå¤šæ•°ï¼ˆä¾‹: HTMLè¦ç´ ã€è¬ï¼‰
- google/gemma-7bã®tokenizerã¯BPEã§vocabã¯256k
	- https://huggingface.co/google/gemma-7b
	- ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠã‚’å«ã‚€vocabã¯7039ä»¶ 
	- äº¬éƒ½ å¤§é˜ª å…µåº« å¥ˆè‰¯ æ»‹è³€ ã¯ã‚ã‚Œã© å’Œæ­Œå±± ã¯ç™»éŒ²ãªã— 
	- ä»–ã§ã¯è¦‹ãªã„ã‚¿ã‚¤ãƒ—ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¤šæ•° ã‚³ãƒ¼ãƒ‰ã‚‚mergeã•ã‚ŒãŸã¦ãƒ›ãƒ¤ãƒ›ãƒ¤
- gemma-7b
	- https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf
	- https://huggingface.co/chat/settings/google/gemma-7b-it
	- Geminiãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒ‡ãƒ¼ã‚¿ã€å­¦ç¿’ãƒ¬ã‚·ãƒ”ã‚’ä½¿ç”¨ã—ã¦ã€æœ€å¤§6å…†å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’ï¼ˆä¸»ã«è‹±èªï¼‰ã€‚ã‚µã‚¤ã‚ºã¯2ã¤ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒãã‚Œãã‚Œ20å„„å€‹ã¨70å„„å€‹ã€‚TPUv5eã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’
	- æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ã®ã«æ—¥æœ¬èªã§ã‚‚ç­”ãˆã¦ãã‚Œã‚‹
	- Hugging Face ã« 2B ã¨ 7Bã®äºŒç¨®é¡ï¼ˆãã‚Œãã‚Œãƒ™ãƒ¼ã‚¹ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ãŒã‚ãŒã£ã¦ã„ã‚‹
	- Context Length ã¯ 8k
	- 4bit ã§æ¨è«–ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚‚ HF page ã«ãã®ã¾ã¾è¨˜è¼‰ã‚ã‚‹
	- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯ Gemma license
	- llamaã‚ˆã‚Šç·©ã„ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§ãƒªãƒªãƒ¼ã‚¹
	- åŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã§ã‚ã‚Œã°Llama2ã‚„Mistralã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã¨ã®äº‹
- kaggleæ–°ã‚³ãƒ³ãƒš Google Gemmaã‚’ä½¿ã£ã¦Data Scienceã®ã‚¿ã‚¹ã‚¯ãŒã©ã®æ§˜ã«è§£ã‘ã‚‹ã‹ã‚’ãƒ‡ãƒ¢ã™ã‚‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹Analyticsã‚³ãƒ³ãƒšã€‚
	- https://www.kaggle.com/competitions/data-assistants-with-gemma/
	- LLMå¤§å–œåˆ©ã€‚å„ã‚¿ã‚¹ã‚¯æ¯ã«è³é‡‘$10kã€‚
- gemma-2bã‚’è©¦ã™ by npakaã•ã‚“
	- https://x.com/npaka123/status/1760432810811400204?s=20
	- https://huggingface.co/google/gemma-2b-it
- gemma-7b-it-gguf
	- https://huggingface.co/mmnga/gemma-7b-it-gguf
	- Googleã•ã‚“ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹gemma-7b-itã®ggufã‚ã‚Šã¾ã™
	- **ç¾åœ¨é‡å­åŒ–ã•ã‚ŒãŸå‡ºåŠ›ãŒä¸å®‰å®šãªå•é¡ŒãŒã‚ã‚‹ã‚‰ã—ãQ8_0ã‚’æ¨å¥¨ã—ã¾ã™ã€‚**
	- ã”åˆ©ç”¨å‰ã«gemmaåˆ©ç”¨è¦ç´„ã‚’ã”ç¢ºèªä¸‹ã•ã„
- side-by-side comparison of the GPT-4, Gemma, and Llama tokenizer
	- https://x.com/xenovacom/status/1760384978360074460?s=20
	- the Gemma and Llama tokenizers are very similar, with the main difference being vocabulary size. One interesting thing to see is that even with an 8x larger vocabulary (256k vs 32k), Gemma only produces ~13% fewer tokens than Llama.
- Google Colab ã§ Gemma ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã™
	- https://note.com/npaka/n/nc55e44e407ff?sub_rt=share_h
	- ä»Šå›ã¯ã€ã”ã–ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ã¾ã™ã€‚AIãŒã€Œæˆ‘ã€ã‚Šã‚“ãˆã‚‚ã‚“ã¯æ€ã†ã€‚â—¯â—¯ã§ã”ã–ã‚‹ã€‚çŸ¥ã‚‰ã‚“ã‘ã©ã€‚ã€çš„ãªå£èª¿ã«ãªã‚Šã¾ã™
- OmniPred: Language Models as Universal Regressors
	- https://huggingface.co/papers/2402.14547
	- åºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€æ•°å­¦çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¨å€¤ã®ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã®ã¿ã‚’é€šã˜ã¦ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒéå¸¸ã«æ­£ç¢ºãªæ•°å€¤å›å¸°ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ©Ÿä¼šãŒä¸ãˆã‚‰ã‚Œã‚Œã°ã€è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã«ã‚ãŸã£ã¦ã€å¾“æ¥ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping
	- https://huggingface.co/papers/2402.14083
- Stable Diffusion 3ãƒªãƒªãƒ¼ã‚¹
	- https://stability.ai/news/stable-diffusion-3?utm_source=twitter&utm_medium=website&utm_campaign=blog
	- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰15å„„ä»¶ã‚‚å¼¾ã„ãŸã‚‰ã—ã„ã€‚ã™ã”ã„ãª
- Colbert Rerank
	- https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb
	- ColBERT  is a great model for reranking. Itâ€™s ~100x faster than BERT-based/cross-encoder models, letting you rerank large amounts of documents without worrying about latency. And of course it does better than standard dense retrieval.
- The prompting guide for Gemma 7B Instruct is live!
	- https://www.promptingguide.ai/models/gemma
-  æœ€æ–°ã® Google Gemma ãƒ¢ãƒ‡ãƒ«ã‚’ MLX ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
	- https://note.com/alexweberk/n/n96cc4c8ac174?sub_rt=share_h
	- M3 Max 128GB ã§ç´„ 50 åˆ†ã‹ã‹ã‚Šã¾ã—ãŸã€‚npaka ã•ã‚“ã®è¨˜äº‹ã ã¨ 20 åˆ†ã»ã©ã§å®Œäº†ã™ã‚‹ãã†ãªã®ã§ã€ã‚„ã¯ã‚Š NVIDIA A100 ãªã©ã® GPU ã¨æ¯”ã¹ã¦ã—ã¾ã†ã¨æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã­â€¦ã€‚
	- https://gist.github.com/alexweberk/1434c95c05463866491677aac6ce19ba#file-mlx_finetuning_gemma-ipynb
-  Introducing Pebblo â€” Data Visibility & Governance for Gen-AI apps
	- https://medium.com/@sridhar_ramaswamy/introducing-pebblo-data-visibility-governance-for-gen-ai-apps-086ca8a62d10
	- Pebblo enables developers to safely load data and promote their Gen AI app to deployment without worrying about the organizationâ€™s compliance and security requirements. The project identifies semantic topics and entities in the loaded data and summarizes them on the UI or a PDF report.
- 

## 2/19

ä»Šé€±ã¯ã€ãªã‚“ã¨ã„ã£ã¦ã‚‚ã€soraã€soraã€soraã€‚ã“ã‚Œã£ã¦OpenAIãŒæ„å›³ã—ã¦ãƒªãƒªãƒ¼ã‚¹æ™‚æœŸã‚’è¨ˆç®—ã—ã¦ã„ã‚‹æ°—ãŒã™ã‚‹ã€‚Googleã®Gemini 1.5ã®ãƒªãƒªãƒ¼ã‚¹ç›´å¾Œã ã—ã€Metaã®Lecunå…ˆç”ŸãŒã€å½“é¢ã§ããªã„ã¨ã„ã†è¬›æ¼”ã®æ•°æ—¥å¾Œã«å‡ºã™ã¨ã‹ã€OpenAIã¯é…çƒã‚’é¸ã¶ã ã‘ã®æŒã¡çƒã®ã‚¹ãƒˆãƒƒã‚¯ãŒã‚ã‚‹ã¨ã„ã†ã†ã‚ã•ã¯æœ¬å½“ãªã®ã‹ã‚‚ã€‚å¤–éƒ¨ã‹ã‚‰ã®soraã®æŠ€è¡“è§£æã‚‚é€²ã¿ã€æ—¢å­˜ã®æŠ€è¡“ã®çµ„ã¿åˆã‚ã›ã§ã¯ã‚ã‚‹ãŒã€ãã®æ€§èƒ½ãƒ»ç²¾åº¦ã¨ã‚¹ã‚±ãƒ¼ãƒ«ãŒé•ã†ã¨ã¨ã®ã“ã¨ã§ã€ã¤ã¾ã‚Šã€OpenAIãŒåœ§å€’çš„ãªæ¨ªç¶±ç›¸æ’²ã‚’è¦‹ã›ã¤ã‘ãŸã ã‘ã ã£ãŸã€‚soraã®ãŠã‹ã’ã§éœã‚“ã§ã§ã—ã¾ã£ãŸGemini 1.5ã€ãªã‚“ã¨MoEã‚’æ¡ç”¨ã—ã€é•·å¤§ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã€RAGã£ã¦ã„ã‚‰ã­ï¼Ÿã¿ãŸã„ãªå‹¢ã„ã ãŒã€ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’ã‚¬ãƒ©ãƒãƒ³ã§åˆ©ç”¨ã—ã¦ã‚ˆã„ã‚ã‘ãŒãªã„ã€‚RAGã‚‚Collective RAGã¨ã‹ã€embeddingã®å·¥å¤«ã¨ã‹ã€èª¬æ˜æ€§ã®ã‚ã‚‹ç”ŸæˆAIã®æ–¹å‘ã«é€²ã‚“ã§ã‚‹æ°—ãŒã™ã‚‹ã€‚ä¸€æ–¹Metaã¯LeCunå…ˆç”Ÿã®ã„ã†ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã«è¿‘ããŸã‚ã®V-JEPAï¼ˆå‹•ç”»ã®äºˆæ¸¬ï¼‰ã‚’ç™ºè¡¨ã€‚ã“ã“ã«ãã¦ã€ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒä¸€æ°—ã«ç¾å®Ÿå‘³ãŒå¸¯ã³ã¦ããŸã€‚ãªãœã‹Llamaindexã¨LangchainãŒåŒæ™‚æœŸã«ãã‚Œãã‚Œå¤§ããªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä»£æ›¿ã‚ã‚Šã€è‚¥å¤§åŒ–ã—ã™ããŸã®ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼åŒ–ã—ãŸã¨ã„ã†è©±ã ãŒã€LangChainã«ã¯å¾Œæ–¹äº’æ›æ€§ãŒã‚ã‚‹ã£ã¦æœ¬å½“ï¼ŸLLMã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã€ä»Šå¾Œã®ç ”ç©¶ã®é€²ã‚€æ–¹å‘ã‚’æ­£ã—ãè¦‹æ®ãˆã¦ã¦ã‚ˆã„ã€‚å°ã•ã„LLMã¨ã‹Transfomerã«ä»£ã‚ã‚‹æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£(Mambaã¨ã‹ï¼‰ã¨ã‹ã€æœ¬LLMã‚¢ãƒ—ãƒ‡ã§ã‚‚è¿½ã£ã¦ãŸè©±é¡ŒãŒæº€è¼‰ã€ã¾ã‚èª°ãŒè¦‹ã¦ã‚‚ãã†ãªã‚‹ã‚ãªã€‚natureã®ã€ChatGPTã‚’åˆ©ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è«–æ–‡ã‚’ç”Ÿæˆã£ã¦ã®ã¯ã€è‡ªåˆ†ãŒã¨ã„ã†ã‚ã‘ã§ã¯ãªãã€ãã†ã„ã†äººã‚„è«–æ–‡ã¨ä¸–ç•Œã§ç«¶äº‰ã—ãªã‘ã‚Œã°ã„ã‘ãªã„ã¨ã„ã†æ„å‘³ã§ã€ç ”ç©¶è€…ãªã‚‰å¿…èª­ã ã‚ã†ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã¯ã€Ollamaã®æ—¥æœ¬èªå‡ºåŠ›ãŒæ”¹è‰¯ã•ã‚ŒãŸã¨ã„ã†ã“ã¨ã§ã€npakaã•ã‚“ã®Elyza-7Bã‚’å‹•ã‹ã—ãŸè¨˜äº‹ã¯ã€æ—¥æœ¬èªã§ãƒ­ãƒ¼ã‚«ãƒ«LLMã—ãŸã„å‹¢ã«ã¯å‚è€ƒã«ãªã‚‹ã ã‚ã†ã€‚å…ƒæœ¨ã•ã‚“ã®åˆ†æã®ã‚ˆã†ã«ã€Transformerãƒ™ãƒ¼ã‚¹ã®æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ã‹ã£ãŸLLMã§ã¯ã€è³‡é‡‘ã¨ãƒªã‚½ãƒ¼ã‚¹ã®æˆ¦ã„ãªã®ã§ã€æ¨ªç¶±ç›¸æ’²ã‚’è¦‹ã›ã¤ã‘ã‚‰ã¦æˆ¦é—˜æ„æ¬²ã‚’ããŒã‚Œã‚‹ç™ºè¡¨ãŒå¤šã„ãŒã€ã ã‹ã‚‰ã“ãã€LLMã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ãŒã„ã†ã‚ˆã†ã«ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å¤‰ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã—ã€ãã†ã„ã†äººãŸã¡ãŒå‡ºã¦ãã‚‹ã«é•ã„ãªã„ã€‚æ­©ã¿ã‚’æ­¢ã‚ã‚‹ã‚ã‘ã«ã¯ã„ã‹ãªã„ã€‚

-  LlamaIndex v0.10
	- https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8
	- https://x.com/llama_index/status/1757121818115322076?s=20
	- our biggest open-source release to date, and a massive step towards production-readiness.
	- Create a core package, split off every integration/template into separate PyPi packages.
	- Refactor LlamaHub to become a central hub of all integrations (WIP)
	- Deprecate ServiceContext: Your dev UX is now way better without this object.
- Chat with RTX from NVIDIA
	- https://x.com/NVIDIAAIDev/status/1757447655674819053?s=20
- Deepreneur-blue-lizard
	- https://huggingface.co/Deepreneur/blue-lizard
	- æ±äº¬å¤§å­¦æ¾å°¾ç ”ç©¶å®¤ç™ºã®AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€æ ªå¼ä¼šç¤¾Deepreneur(ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ¬ãƒŠãƒ¼
	- Metaã®Llama-2-7bã«å¯¾ã—ã¦ã€Wikipediaã‚„æ›¸ç±ç­‰ã®æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦è¿½åŠ äº‹å‰å­¦ç¿’ã¨ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚  
	- 70å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨éå¸¸ã«è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšã€JGLUEï¼ˆæ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹è©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã‚’ç”¨ã„ãŸè©•ä¾¡ã§ã¯ã€ChatGPT-3.5ã‚’è¶…ãˆã‚‹ã‚¹ã‚³ã‚¢ãŒç®—å‡ºã•ã‚Œã¦ãŠã‚Šã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯æœ€é«˜æ€§èƒ½ã«ãªã‚Šã¾ã™ã€‚
	-  æ ªå¼ä¼šç¤¾Deepreneurã€ChatGPT-3.5ã‚’ä¸Šå›ã‚‹æ—¥æœ¬èªLLMã€Œblue-lizardã€ã‚’é–‹ç™ºã€‚å„ç¤¾ç‹¬è‡ªã®é«˜ç²¾åº¦ã‚ªãƒ³ãƒ—ãƒ¬å‹ã®LLMã®æ§‹ç¯‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹
- ChemLLM: A Chemical LLM
	- https://arxiv.org/abs/2402.06852
	- We don't see too much research around LLMs for science so it's exciting to find this one. 
	- It's a dedicated LLM trained for chemistry-related tasks. Claims to outperform GPT-3.5 on principal tasks such as name conversion, molecular caption, and reactionâ€¦
- Large Language Models: A Survey
	- https://arxiv.org/abs/2402.06196
	- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã“ã‚Œã¾ã§ã¨ã“ã‚Œã‹ã‚‰ã‚’åŒ…æ‹¬çš„ã«æ•´ç†ã—ãŸã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– å°ã•ãã¦åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã™ã‚‹ 
		- å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯é«˜ã‚³ã‚¹ãƒˆã§éåŠ¹ç‡çš„ã§ã‚ã‚‹
		- ãã®ãŸã‚ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®å°å‹ãƒ¢ãƒ‡ãƒ«ã¸ã®é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹ 
		- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®è‰¯ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ã€æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€è’¸ç•™æ³•ãªã©ã®æŠ€è¡“ãŒæ´»ç”¨ã•ã‚Œã‚‹ 
	- â– ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å¤‰ãˆã‚‹ 
		- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®"æ¬¡"ã«é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹
		- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã«å¤‰ã‚ã‚‹çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆMambaãªã©ï¼‰ãŒç­†é ­å€™è£œ 
		- æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åŠ¹ç‡ã‚ˆãæ‰±ã†ãªã©ã®å„ªä½æ€§ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ 
	- â– ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«é€²åŒ–ã•ã›ã‚‹
		- ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»ã€éŸ³å£°ãªã©æ§˜ã€…ãªãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’çµ±ä¸€çš„ã«æ‰±ã†ã‚ˆã†ã«ãªã£ã¦ã„ã
		- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹…ãŒåºƒãŒã‚‹
		- ã™ã§ã«å„ªç§€ãªãƒ¢ãƒ‡ãƒ«ãŒå‡ºç¾ã—å§‹ã‚ã¦ãŠã‚Šã€ã“ã®æµã‚Œã¯ç¶šã„ã¦ãã ã‚ã† 
	- â– å®Ÿç”¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ 
		- LLMã®çŸ­æ‰€ï¼ˆå¹»è¦šãªã©ï¼‰ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚„å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã€RAGãªã©ã§å¯¾å‡¦ã§ãã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šå§‹ã‚ã¦ã„ã‚‹
		- å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‚’ä»£æ›¿ã—ã¦ã„ãæµã‚ŒãŒèµ·ãã¦ã„ã‚‹
		- å€‹äººã®å¥½ã¿ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã™ã‚‹ã‚ˆã†ãªè¨­è¨ˆãŒäººæ°—ã‚’é›†ã‚ã¦ã„ã‚‹ 
	- â– ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã‚’å¼·åŒ–ã™ã‚‹
		- æ•µå¯¾çš„æ”»æ’ƒã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å®ˆã‚‹ã®ãŒé‡è¦ã«ãªã£ã¦ã„ã‚‹
		- å€«ç†çš„ãªæ‡¸å¿µã‚„ãƒã‚¤ã‚¢ã‚¹ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®ç ”ç©¶ã‚‚æ´»ç™ºåŒ–ã—ã¦ã„ã‚‹ 
		- æ©Ÿå¯†æƒ…å ±ã‚’è²¬ä»»ã‚’æŒã£ã¦æ‰±ã†ã‚ˆã†ã«åŠªåŠ›ã•ã‚Œã¦ã„ã‚‹
- å®Ÿè·µï¼å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« / 1000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¶Šãˆãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã«ã¯ï¼Ÿ
	- https://zenn.dev/turing_motors/articles/26e1f1be50c0b5
	- BLOOM-1Bã‚’å‹•ã‹ã—ã¦ã¿ã‚‹
		- 10å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¨‹åº¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Œã°ã€GPUãƒ¡ãƒ¢ãƒªãŒ12GBä»¥ä¸Šã®GPUã§ã‚ã‚Œã°æ¨è«–ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚Google Colaboratoryã§æä¾›ã•ã‚Œã¦ã„ã‚‹GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å‹•ã‹ã™ã“ã¨ãŒã§ãã¾ã™
	- BLOOM-176Bã‚’å‹•ã‹ã—ã¦ã¿ã‚‹ï¼Ÿ
		- ã§ã¯å®Ÿéš›1000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¶…ãˆã‚‹BLOOMã‚’å‹•ã‹ã™ã«ã¯ã©ã†ã™ã‚Œã°ã„ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ
		- å˜ç´”ãªè§£æ±ºç­–ã¨ã—ã¦ã€ãã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãŒä¹—ã‚‹è¨ˆç®—ç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ã‚‚ã—ãã®ãƒ¬ãƒ™ãƒ«ã®ã‚¹ãƒšãƒƒã‚¯ã‚’ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ã®ã‚µãƒ¼ãƒãƒ¼ã§æ•´ãˆã‚‹å ´åˆã¯æ•°åƒä¸‡å††è¦æ¨¡ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚ã¾ãŸã€AWSã‚„GCPãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã§å¤§è¦æ¨¡å®Ÿé¨“ç’°å¢ƒã‚’æ•´ãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
		- ä¾‹ãˆã°ã€AWSã®EC2 P4dã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚ã‚Œã°8æšã®A100ã®GPUãƒ¡ãƒ¢ãƒªãŒè¨ˆ320GBã¨640GBã®ç’°å¢ƒã‚’1æ™‚é–“ã‚ãŸã‚Š30~40ãƒ‰ãƒ«ç¨‹åº¦ã§æ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚
- RAG FusionãŒæ€ã£ã¦ãŸã‚ˆã‚Šå‡„ãã†
	- https://zenn.dev/ozro/articles/abfdadd0bfdd7a
	- RAG Fusionã¯å˜ãªã‚‹ã€Œæ–°ãŸãªæ‰‹æ³•ã€ã§ã¯ãªãã€Œé©æ–°çš„ãªæ‰‹æ³•ã€ã§ã™ã€‚  
	- RAG Fusionã¯ã€å¾“æ¥ã®æ¤œç´¢æŠ€è¡“ã®åˆ¶ç´„ã‚’å…‹æœã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚ˆã‚Šè±Šã‹ã§æ–‡è„ˆã«å³ã—ãŸçµæœã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€RAGã€Reciprocal Rank Fusionã€ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’çµ„ã¿åˆã‚ã›ãŸæ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã«ãªã£ã¦ã„ã¾ã™ã€‚  
	- ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ¤œç´¢çµæœã®ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªç”Ÿæˆã«ã‚ˆã‚Šã€æ¤œç´¢ã®æ­£ç¢ºæ€§ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã¨ã®ä¸€è‡´ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ãŸæ‰‹æ³•ã¨ãªã£ã¦ã„ã¾
- Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning
	- https://huggingface.co/papers/2402.06619
	- åˆè¨ˆã§114è¨€èªã‚’ã‚«ãƒãƒ¼ã™ã‚‹5å„„1300ä¸‡ãƒšã‚¢ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨è£œå®Œæ–‡ã‚’å«ã‚“ã§ãŠã‚Šã€Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨ã®äº‹
-  LlamaIndex v0.10 ã®æ¦‚è¦ by npakaã•ã‚“
	- https://note.com/npaka/n/nb8acc1f63312?sub_rt=share_h
	- ã€Œ**LlamaIndex v0.10**ã€ã¯ã€éå»æœ€å¤§ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
	- ã€ŒServiceContextã€ã‚’éæ¨å¥¨ã«ã—ã¦ã€ã€ŒLlamaIndexã€ã®é–‹ç™ºè€…ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
	- æ™‚é–“ãŒçµŒã¤ã«ã¤ã‚Œã¦ã€ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä½¿ã„ã«ãããªã‚Šã¾ã—ãŸã€‚ service_context ã‚³ãƒ³ãƒ†ãƒŠå…¨ä½“ã‚’ä»»æ„ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æ¸¡ã™ã¨ã€ã©ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒå®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¨è«–ã™ã‚‹ã®ãŒå›°é›£ã«ãªã‚Šã¾ã—ãŸã€‚ ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ OpenAI ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã§ã‚‚ã€ä¸å¿…è¦ã«OpenAIã‚­ãƒ¼ã‚’æŒ‡å®šã™ã‚‹ã‚ˆã†ã«æ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã—ãŸã€‚ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦å…¥åŠ›ã™ã‚‹ã®ã‚‚å¤§å¤‰ã§ã—ãŸ
-  LongMamba
	- https://github.com/jzhang38/LongMamba
	- We present LongMamba, an early exploration of Mamba's **longer context extrapolation ability**. Our #LongMamba manages to retrieve *nearly perfectly* on a window context of 16384
- AutoMathText: A 200GB dataset of mathematical texts open sourced
	- https://huggingface.co/papers/2402.07625
	- Multi-source : arXiv/programming code/web pages  
	- Filtered and processed to adapte Math reasoning  
	- Selected by Qwen 72B
-  ç§‘å­¦è€…ãŒChatGPTã‚’åˆ©ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è«–æ–‡ã‚’ç”Ÿæˆ by nature
	- https://www.natureasia.com/ja-jp/ndigest/v20/n10/%E7%A7%91%E5%AD%A6%E8%80%85%E3%81%8CChatGPT%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%A6%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E8%AB%96%E6%96%87%E3%82%92%E7%94%9F%E6%88%90/122873
	- Nature Japanã‹ã‚‰ç”ŸæˆAIã§è«–æ–‡ã‚’æ›¸ã„ãŸéš›ã®å®Ÿè¨¼çµæœã¨é™ç•Œ
	- ãƒ†ã‚¯ãƒ‹ã‚ªãƒ³ãƒ»ã‚¤ã‚¹ãƒ©ã‚¨ãƒ«å·¥ç§‘å¤§å­¦ï¼ˆãƒã‚¤ãƒ•ã‚¡ï¼‰ã®ç”Ÿç‰©å­¦è€…ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã‚ã‚‹Roy Kishonyã‚‰ã¯ç‹¬è‡ªã®è‡ªå¾‹çš„ãªdata to paperã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—æ¤œè¨¼ã€‚
		- 1æ™‚é–“è¶³ã‚‰ãšã§ç ”ç©¶è«–æ–‡ä½œæˆ 
		- æ–‡ç« ã¯æµæš¢ã§æ´å¯Ÿã«å¯Œã‚€
		- å³å¯†ãªãƒ‡ãƒ¼ã‚¿åˆ†æã«ã‚‚åŸºã¥ã ã¨ã—ãŸãŒã€
		- è«–æ–‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹è¡¨ç¾ã§èª¤é­”åŒ–ã™ 
		- På€¤ãƒãƒƒã‚­ãƒ³ã‚°ï¼ˆP hackingï¼‰
		- è«–æ–‡ç”ŸæˆãŒç°¡å˜ã«ãªã‚Šè³ªã®æ‚ªã„è«–æ–‡ãŒå¢—åŠ ã™ã‚‹ãƒªã‚¹ã‚¯ ãªã©æ‡¸å¿µç‚¹ã‚’æŒ™ã’ãŸã€‚
- Code-Llama-70B-FW is now available on Poe! H
	- https://x.com/poe_platform/status/1757080840012804511?s=20
-  éŸ³å£°å…¥å‡ºåŠ›ã§LLM on Google Colab
	- https://colab.research.google.com/drive/1WCiUth855jXjzaNh8Ap5lFLEGX8aMtiU
	- ãƒã‚¤ã‚¯å…¥åŠ›â†’éŸ³å£°èªè­˜(Faster Whisper)â†’LLMå›ç­”ç”Ÿæˆ(ELYZA)â†’éŸ³å£°åˆæˆ(Style-Bert-VITS2)â†’å†ç”Ÿ
	- Google Colabã®ç„¡æ–™æ ã§å‹•ã éŸ³å£°èªè­˜(Whisper)â†’LLM(Swallow-13B)â†’éŸ³å£°åˆæˆ(Style-Bert-VITS2) ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸã€‚éŸ³å£°åˆæˆã¯äº‹å‰ã«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆãŒå¿…è¦ã§ã™ãŒã€æŠ¼ã—ã®ã‚­ãƒ£ãƒ©éŸ³å£°ã¨ä¼šè©±ã§ãã‚‹ã¨æ¥½ã—ã„ã‹ã‚‚ã€‚(LLMã‚’13Bã«ã—ãŸã®ã§å›ç­”ç”Ÿæˆã«1åˆ†ãã‚‰ã„æ›ã‹ã‚Šã¾ã™)
- RAG From Scratch: Query Translation (Multi-Query)
	- https://x.com/LangChainAI/status/1757817056865718432?s=20
-  Masked Audio Generation using a Single Non-Autoregressive Transformer
	- https://arxiv.org/abs/2401.04577?utm_source=twitter&utm_medium=organic_social&utm_campaign=research&utm_content=thread
	- Researchers at Meta recently shared MAGNeT, a single non-autoregressive transformer model for text-to-music & text-to-sound generation capable of generating audio on-par with the quality of SOTA models â€” at 7x the speed.
	- https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/?utm_source=twitter&utm_medium=organic_social&utm_campaign=research&utm_content=video
- Nomic Embed v1.5
	- Nomic Embed v1.5 is out, the first open model with variable-sized Matryoshka embeddings and 8192 context!
	- https://huggingface.co/spaces/Xenova/adaptive-retrieval-web
-  LLM Agents
	- https://www.promptingguide.ai/research/llm-agents
-  Mixtures of Experts Unlock Parameter Scaling for Deep RL
	- https://huggingface.co/papers/2402.08609
	- Google Deepmind presents Mixtures of Experts Unlock Parameter Scaling for Deep RL
-  Google Colabã§ã®æ—¥æœ¬èªMambaã®äº‹å‰å­¦ç¿’
	- https://note.com/hatti8/n/na9782b7fa437?sub_rt=share_pb
	- æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãŒãªã„ã®ã§ã€æ—¥æœ¬èªMambaã®äº‹å‰å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚Google colabã§å‹•ãã“ã¨ã¯ç¢ºèªã—ãŸã‚‚ã®ã®A100(40B)ã§ã‚‚**15æ™‚é–“è¿‘ãã‹ã‹ã‚‹ã®ã§å®Ÿè³ªæœ€å¾Œã¾ã§ã¯å®Ÿè¡Œã§ããªã„ã§ã™ã€‚**
-  GraphRAG: Unlocking LLM discovery on narrative private data by Microsoft
	- https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/
	- Microsoft is transforming retrieval-augmented generation with GraphRAG, using LLM-generated knowledge graphs to significantly improve Q&A when analyzing complex information and consistently outperforming baseline RAG
-  In-Context Language Learning: Architectures and Algorithms
	- https://arxiv.org/abs/2401.12973
	- Transformer ã®ä»£æ›¿ã¨ã—ã¦ã® Mamba å«ã‚€ SSMs ã‚„ä»–ã® subquadratic ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (e.g, RetNet, RKWV) ã‚’ã€Œå…¥åŠ›ã«ä¾å­˜ã—ãŸæ¨è«–ã‚’è¨±ã™ã‹ã€ãƒ»ã€Œç·šå½¢/éç·šå½¢ã‹ã€ã§æ•´ç†ã™ã‚‹ã¨ã‚ã¡ã‚ƒãã¡ã‚ƒè¦‹é€šã—ãŒè‰¯ããªã‚‹ï¼
- OpenAIãŒMicrosoftã¨å¼·åŠ›ã—å›½å®¶é–¢é€£ã®è„…å¨ã‚¢ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹AIã®æ‚ªæ„ã‚ã‚‹ã‚µã‚¤ãƒãƒ¼æ´»å‹•ã«é–¢ã™ã‚‹åˆ©ç”¨ã‚’ã—ã¦ã„ãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’åœæ­¢ã€‚
	- https://x.com/bioshok3/status/1757834888705945971?s=20
- Open AI å‹•ç”»ç”ŸæˆAI ã€Soraã€ã‚’ãƒªãƒªãƒ¼ã‚¹
	- https://openai.com/sora
	- GoogleãŒåˆ‡ã‚Šæœ­çš„ã«é›»æ’ƒå…¬é–‹ã—ãŸGemini 1.5ã®æ•°æ™‚é–“å¾Œã«ã€OpenAIãŒä¸–ç•Œã®è©±é¡Œã‚’æ»ã£æ”«ã†ãƒ¬ãƒ™ãƒ«ã®å‹•ç”»ç”ŸæˆAIã®Soraã‚’ã¶ã¤ã‘ã¦ããŸ
- META ãŒVideo Joint Embedding Predictive Architecture (V-JEPA) ãƒ¢ãƒ‡ãƒ«ã‚’CC BY-NC ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ä¸€èˆ¬å…¬é–‹
	- https://x.com/bioshok3/status/1758182170135576590?s=20
	- V-JEPA ã¯ã€æŠ½è±¡è¡¨ç¾ç©ºé–“å†…ã®ãƒ“ãƒ‡ã‚ªã®æ¬ è½éƒ¨åˆ†ã¾ãŸã¯ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å­¦ç¿’ã™ã‚‹éç”Ÿæˆãƒ¢ãƒ‡ãƒ«
-  LangChain v0.1 ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ - Pythonç‰ˆ  by npakaã•ã‚“
	- https://note.com/npaka/n/n1d771995c3aa?sub_rt=share_h
	- **v0.1** ã§ã¯langchainãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ¬¡ã®3ã¤ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚ã™ã¹ã¦**ä¸‹ä½äº’æ›æ€§ã®ã‚ã‚‹æ–¹æ³•**ã§è¡Œã‚ã‚Œã¾ã—ãŸ
- è»½é‡ãƒ»é«˜é€Ÿãƒ»é«˜æ€§èƒ½ã¨ä¸‰æ‹å­æƒã£ãŸæ—¥æœ¬èªå¯¾å¿œã®AI(Orion-14B)ã§æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ãƒ¡ãƒ¢
	- https://note.com/kan_hatakeyama/n/n0c58733b39bd?sub_rt=share_pb
	- shi3zã•ã‚“ã®ã€ã€ŒOrion14B-Chatã¨Wikipediaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦æ—¥æœ¬èªãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã¾ã—ãŸã€ã‚’å‚è€ƒã«ã—ã¦
	- Yahoo!çŸ¥æµè¢‹ã®è³ªç–‘ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦ã¿ã¾ã™
		- ã¨ã€ã‚ã‚Šã¨ã„ã„æ„Ÿã˜ã§ã—ãŸã€‚
	- ãƒ­ãƒ¼ã‚«ãƒ«ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã€ãã‚Œãªã‚Šã«é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿åˆæˆãŒã§ãã‚‹æ™‚ä»£ãŒã‚„ã£ã¦ããŸã‚ˆã†ã§ã™ã€‚ä»Šå¾Œã¯ã„ã„æ„Ÿã˜ã«(å…¬é–‹)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã£ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚
- Corrective RAG with LangGraph
	- https://github.com/langchain-ai/langgraph/tree/main/examples/rag
	- Weâ€™ve just implemented 4 new notebooks outlining different RAG and CRAG techniques in LangChainAIã€€PY & JS! These show off different RAG flows, using OSS and hosted LLMs. See the links below for the notebooks:
-  ã€Gemini 1.5 Proã€‘100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›ã§ãã‚‹æœ€å¼·LLMã®æ€§èƒ½ã‚’GPT-4ã¨æ¯”è¼ƒã—ã¦ã¿ãŸ
	- https://weel.co.jp/media/gemini-1-5-pro
	- â— æ€§èƒ½ãƒ†ã‚¹ãƒˆã§å…ˆä»£ã®å¤§å‹ãƒ¢ãƒ‡ãƒ«ãƒ»Ultra 1.0ã¨äº’è§’  
		- æ€§èƒ½æ¯”è¼ƒå…¨32é …ç›®ã®ã†ã¡30é …ç›®ã§ã€GPT-4ã«å‹åˆ©
		- ç†æ•°&äººæ–‡å…¨57ç§‘ç›®ã®å•é¡Œé›†ã€ŒMMLUã€ã«ã¦å°‚é–€å®¶ã«å‹åˆ©
	- â— Transformerã®é€²åŒ–ç³»ã€MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ­è¼‰ 
	- â— LLMå²ä¸Šæœ€å¤§ã€100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚ã®å…¥åŠ›ã«å¯¾å¿œ
		- MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ãŸçµæœã€Gemini 1.5 Proã§ã¯å…¥åŠ›ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤§å¹…ã«UPï¼ä¸€å›ã«100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã®å…¥åŠ›ãŒå®Ÿç¾ã—ã¾ã—ãŸã€‚
- æ•™ç§‘æ›¸ï¼šç¢ºç‡éç¨‹
	- ç¢ºç‡éç¨‹ã«èˆˆå‘³ãŒã‚ã‚‹B4ãƒ»M1ãŒèª­ã‚€ã¹ãæ•™ç§‘æ›¸ã«ã¤ã„ã¦èª¬æ˜ã™ã‚‹ï¼
	- [é€Ÿç¿’ç‰ˆ](https://kanazawa.scphys.kyoto-u.ac.jp/wpkzdb/wp-content/uploads/2023/04/stochasticProcessShort.pdf)
	-  [è©³ç´°ç‰ˆ](https://kanazawa.scphys.kyoto-u.ac.jp/wpkzdb/wp-content/uploads/2023/10/StochasticProcess2023_long.pdf)
- ã€AIå‹•ç”»ç”Ÿæˆã€‘Sora è¦ç´ æŠ€è¡“è§£èª¬
	- https://zenn.dev/mattyamonaca/articles/e234e57834d7ad
	- ã™ã”ãç°¡å˜ã«ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®4ã¤ã®è¦ç´ ãŒä¸»è»¸ã§ã™
		- å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’æ½œåœ¨ç©ºé–“ã«åœ§ç¸®ã—ãŸå¾Œã€TransformerãŒãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦åˆ©ç”¨ã§ãã‚‹ã€Œæ™‚ç©ºæ½œåœ¨ãƒ‘ãƒƒãƒã€ã«å¤‰æ›ã™ã‚‹æŠ€è¡“
		- Transoformerãƒ™ãƒ¼ã‚¹ã®ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«
		- DALLE3ã‚’ç”¨ã„ãŸé«˜ç²¾åº¦ãªãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
	- ã“ã†ã—ã¦è¦ç´ è¦ç´ ã‚’è¦‹ã¦ã„ãã¨ç‰¹æ®µæ–°ã—ã„æŠ€è¡“ã‚’ä½¿ã£ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€ä»Šã¾ã§æœ‰åŠ¹ã¨ã•ã‚ŒãŸæŠ€è¡“ã‚’æ„šç›´ã«ç©ã¿é‡ã­ã€è«å¤§ãªè³‡æœ¬åŠ›ã¨è¨ˆç®—åŠ›ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚Œã°å¼·ã„ãƒ¢ãƒ‡ãƒ«ãŒä½œã‚Œã‚‹ã¨ã„ã†ã€å½“ãŸã‚Šå‰ã®çµæœãŒè¦‹ãˆã¦ãã¾ã™ã€‚
-  ChatGPTã‚’ç¤¾å†…ã«é…ã£ã¦ã‚‚ã‚ã¾ã‚Šä½¿ã‚ã‚Œãªã„æœ¬å½“ã®ç†ç”±
	- https://qiita.com/jw-automation/items/cf8ffc7a0edab512d917
	- ç¤¾å†…æƒ…å ±ã¨ã„ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦ãªæ¥­å‹™ãŒã»ã¨ã‚“ã©ã§ã‚ã‚‹äººé”ã«ã€ç´ ã®ChatGPTã‚’é…ã£ã¦ã‚‚ã€ç‰¹ã«ä½¿ãˆã‚‹æ‰€ãŒãªã„ã¨ã„ã†ã®ã¯ã„ã‚ã°å½“ãŸã‚Šå‰ã®è©±ã§ã™ã€‚
- Build Knowledge Graph From TextData using LangChain
	- https://medium.com/@mahimairaja/build-knowledge-graph-from-textdata-using-langchain-under-2min-ce0d0d0e44e8
	- Converting text to knowledge graphs can be helpful for both visualizing the data and allowing for structured querying later on 
	- This blog goes through how to use LLMs to extract knowledge triplets
-  minbpe
	- https://github.com/karpathy/minbpe
	- Minimal, clean, educational code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization."
-  Video generation models as world simulators by OpenAI
	- https://openai.com/research/video-generation-models-as-world-simulators
	- Soraã¯Transformerãƒ™ãƒ¼ã‚¹ã®æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ†ã‚­ã‚¹ãƒˆæŒ‡ç¤ºã‹ã‚‰1åˆ†é–“ã®é«˜ç”»è³ªå‹•ç”»ã‚’ç”Ÿæˆã§ãã‚‹ã€‚æ™‚é–“ãƒ»ç©ºé–“ã®ä¸¡æ–¹ã§ä¸€è²«æ€§ã‚’ç¶­æŒã§ãã€ç¾å®Ÿã®ç‰©ç†æ³•å‰‡ã‚’ã‚ã‚‹ç¨‹åº¦åæ˜ ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è©³ç´°ã¯éå…¬é–‹ã ãŒã€å¤§è¦æ¨¡åŒ–ã«ã‚ˆã‚Šæ€§èƒ½ãŒä¸ŠãŒã‚‹æ¨¡æ§˜ã€‚
	- ä»Šå¾Œã‚„ã‚‹ã¹ãäº‹ã¯ãŠãã‚‰ãå˜ç´”ã§ã€å…¨åŠ›ã§OpenAIã‚„Geminiã®æˆé•·ã«ã—ãŒã¿ä»˜ã‘ã°è‰¯ã„ã®ã§ã¯ã€‚ã¨ã„ã†æ°—ãŒã—ã¦ã„ã‚‹ã€‚ by å…ƒæœ¨
	- https://x.com/ai_syacho/status/1758845719988117759?s=20
	- å½¼ã‚‰ã¯æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ã„ã†ã€ãƒã‚·ãƒ³ã¨æœ­æŸã‚’å…¥ã‚Œã‚Œã°å…¥ã‚Œã‚‹ã»ã©èƒ½åŠ›ã®ä¸ŠãŒã‚‹AIã‚’æŒã£ã¦ã„ã¦ã€ãã®è³‡é‡‘ä½“åŠ›ã‚‚æ®µé•ã„ã€‚â€¦
- iPhoneã«Geminiãã¦ãŸ
	- https://x.com/npaka123/status/1758656487399014521?s=20
	- ?? Advance???
- Ollama ã§ Elyza-7B ã‚’è©¦ã™ by npakaã•ã‚“
	- https://note.com/npaka/n/ndadbae6c6be5?sub_rt=share_h
	- ã€ŒOllamaã€ã®æ—¥æœ¬èªè¡¨ç¤ºãŒæ”¹å–„ã•ã‚ŒãŸã¨ã®ã“ã¨ãªã®ã§ã€ã€ŒElyza-7Bã€ã§è©¦ã—ã¦ã¿ã¾ã—ãŸ
	- Ollamaã®ã‚µã‚¤ãƒˆã«è¼‰ã£ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªåˆ†ã§ã€Œ**Modelfile**ã€ã‚’ä½œæˆã—ã¦ã€è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
	- ã€ŒLlama2ã€ã®Manifestã‚’å‚è€ƒã«ã•ã›ã¦ã‚‚ã‚‰ã„ã¾ã™
	- ä»Šå›ã¯ã€ã€Œ**ELYZA-japanese-Llama-2-7b-instruct-q4_K_M.gguf**ã€ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
- Lecunå…ˆç”Ÿã€soarç™ºè¡¨ç›´å‰ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒªã‚¢ãƒ«ãªãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ã®ã¯å½“é¢å…ˆã ã¨è¬›æ¼”ã—ãŸã“ã¨ã«å¯¾ã—ã¦è¨€ã„è¨³ã‚’ã€‚ã€‚ã€‚
	- https://x.com/ylecun/status/1758740106955952191?s=20
	- ã„ã‚„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’æŒã¤ã®ãŒã‚€ã¤ã‹ã—ã„ã¨ã„ã†ã®ãŒçœŸæ„ã§ã‚ã‚‹ã€‚ã€‚ã€‚
	- ãã£ã¨éã¡ã¯ç¹°ã‚Šè¿”ã™ã€‚
- soraã®ç”Ÿæˆã—ãŸç”»åƒã«ã¤ã„ã¦ã®åˆ†æãŒé€²ã‚€
	- https://x.com/anand_bhattad/status/1758632768597328202?s=20
	- ã“ã„ã¤å°„å½±å¹¾ä½•ã‚ã‹ã£ã¦ãªã„ã‚ˆã­ï¼Ÿ
	- DALE3ã¨åŒã˜ç”»åƒã˜ã‚ƒã‚“ç­‰
- 

## 2/13

ä»Šé€±ã¯ã€ã»ã¼äºˆå®šé€šã‚Šï¼ˆï¼‘æ—¥ãŠãã‚Œï¼Ÿï¼‰BardãŒGeminiï¼ˆã‚¸ã‚§ãƒãƒŠã‚¤ã¨èª­ã‚€ï¼‰ã«æ”¹åã•ã‚ŒãŸã€‚ä¸€æ–¹ã€æ–°ãŸã«Gemini Advancedã¨ã„ã†åå‰ã§Gemini UltraãŒæœ‰å„Ÿã§ã‚¹ã‚¿ãƒ¼ãƒˆã€‚ä½•æ°—ãªã„ãƒ•ã‚¡ãƒŸãƒã®å†™çœŸã‹ã‚‰ç”»åƒã‹ã‚‰èªè­˜ã—ãŸæƒ…å ±ç‰‡ã‚’ã¤ãªã’ã¦åº—èˆ—ã‚’ç‰¹å®šã—ãŸã‚Šã¨ã€ã‚³ãƒŠãƒ³å›ãªã¿ã®æ¨ç†ã‚’ã—ã¦ã„ã‚‹ã®ãŒä½•æ°—ã«ã™ã”ã„ã€‚OSSã®LLMã§ã¯ã€ã‚¢ãƒªãƒãƒã®Qwen1.5ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã®ãŒæœ€å¤§ã®è©±é¡Œã€75B-chatã®ãƒ‡ãƒ¢ãªã©ã§ã‚‚GPT-4ã«è¿«ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã¨è©•åˆ¤ã€Huggingfaceã®ãƒ‡ãƒ¢è©¦ã™ã¨ãŸã—ã‹ã«ãƒ¬ã¹ãƒã‹ã‚‚ã€‚åŸºæœ¬æ€§èƒ½ãŒé«˜ã„ã®ã‹ã€0.5Bã‚’Transfomer.jsã§ä½¿ã£ãŸä¾‹ã§ã‚‚ãã‚Œãªã‚Šã®æ€§èƒ½ãŒã§ã‚‹ã¨ã„ã†è©±ã€‚æ—©é€Ÿã€é‡å­åŒ–ã¨ã‹ã€Ollamaã®å¯¾å¿œãŒç™ºè¡¨ã•ã‚ŒãŸã‚Šã•ã‚Œã¦ã‚‹ã€‚ãŸã¶ã‚“ã€æ—¥æœ¬èªLLMã‚‚rinnaå½“ãŸã‚Šã‹ã‚‰Qwen-1.5ãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªLLMã®ç™ºè¡¨ãŒç¶šãã¨æ€ã†ãã€‚Style-Bert-VITS2ã€ãªã‚“ã¦è‡ªç„¶ãªæ—¥æœ¬èªã‚’è©±ã™ã‚“ã ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè©±ã—ã£ã·ã‚Šã«ã³ã£ãã‚Šã€ã©ã“ã‹ã®è·æ¥­ãŒä¸¸ã”ã¨ãªããªã‚‹æ€§èƒ½ã ã€‚ Open AIã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚¨ã‚¢ã®é–“ã‚’ã¤ãªã„ã§ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é–‹ç™ºã‚’å®£è¨€ã€ã“ã‚Œã£ã¦AppleScriptã¨ã‹PowerShellã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã¿ãŸã„ãªè©±ã ã‹ã‚‰ã€Microsoftã¨ã‚‚é€£æºã—ã¦ã‚‹ã‚“ã ã‚ã†ã‘ã©ã€RPAï¼ˆã™ã§ã«æ­»èªï¼Ÿï¼‰ã«ã¨ã©ã‚ã‚’åˆºã™ã ã‚ã†ãªã€‚ã€Œå°ã•ãªLLMã€ã€è‹±èªã§ã‚‚"Smaller LLM"ã¨å‘¼ã°ã‚Œã‚‹ã‚‰ã—ã„ã€å°ã•ãªLLMã§ã„ã„ã‚“ã ãªã€LLMã®Largeã¯ãƒ¢ãƒ‡ãƒ«ã®å¤§å°ã§ã¯ãªã„ã¨ã†ã“ã¨ã€è©•ä¾¡ã«ã‚ˆã‚‹ã¨Flan-T5ãŒã¶ã£ã¡ãã‚Šï¼Ÿ MoEé–¢ä¿‚ã§ã¯ã€Mixtral-8x7Bã®æ—¥æœ¬èªå‘ã‘ã®LoRaã¨ã‹ã€MoEã‚’å˜ç´”åŒ–ã—ã¦Expertã®åˆ‡ã‚Šæ›¿ãˆã‚’è©¦ã—ã¦ã¿ã‚‹ä¾‹ã¨ã‹é¢ç™½ã„ã€‚åŸºç›¤é¢ã§ã¯ã€æ¢ç´¢ãªã—ã§Transfomerã ã‘ã§ãƒã‚§ã‚¹ãƒã‚¹ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹ã®ï¼¡ï¼©ãŒä½œã‚Œã‚‹ã‚‰ã—ã„ã€‚ä¸€æ–¹ã€Transformerã®æ¬¡ä¸–ä»£åŸºç›¤ã®ä¸€ã¤ã¨ã•ã‚Œã‚‹Mambaã€æ—¥æœ¬èªã§ã®è©³ç´°ãªè§£èª¬ã‚„ã€MoEã§ã‚‚ã‚ã‚‹BlackMambaã¨ã‹ã€ã„ã‚ã„ã‚å‡ºã¦ããŸãªã€‚ç†è«–é¢ã§ã¯ã€å²¡é‡ã•ã‚“ã®è§£èª¬ã€The Consensus Gameã€RAGã®æ”¹è‰¯ã‚’ç”ŸæˆAIã¨è­˜åˆ¥AIã®é–“ã®ã‚²ãƒ¼ãƒ ã¨ã—ã¦ã¨ã‚‰ãˆã‚‹ã¨ã¯ã€‚NVIDIAã‚‚è‡ªã‚‰canary-1bã¨ã‹ã€Audio Flamingoã¨ã‹éŸ³å£°ã‚„å¯¾è©±é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã€è‡ªå‹•é‹è»¢ã§ã¯é‹è»¢æ‰‹ã¨ã®å¯¾è©±ãŒå¿…è¦ãªã®ã¯ãã®ã¨ãŠã‚Šãªã‚“ã ã‚ã†ã€‚RAGé–¢ä¿‚ã‚‚ã€Self RAGã¨ã‹ã€Guardrailsã¨ã‹ã€GPT-4ã¨çµ„ã¿ã‚ã›ãŸåŒ»ç™‚åˆ†é‡ã§ã®è©•ä¾¡ã¨ã‹ã„ã‚ã„ã‚é€²ã‚“ã§ã„ã‚‹ãŒã€è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ragas 0.1ãŒã§ãŸã®ã¯å¤§ãã„ã€‚æ—¥æœ¬èªLLMã‚‚ã€æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ•´ç†ã‚„ã€ŒLLM-jp 13B v1.1ã€ã®ãƒªãƒªãƒ¼ã‚¹ã¨ã‹ç€å®Ÿã«é€²ã‚“ã§ã„ã‚‹ã®ãŒå¿ƒå¼·ã„ã€ã¯ã‚ˆNEDOã®æˆæœã‚’ï¼ã€‚çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¨ã®LLMã®èåˆã€Wikidata ã¨ã‹ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã¨ã‹ã€Research Insightã¨ã‹è©±é¡Œã¯ç¶šã„ã¦ã„ã‚‹ã€‚

-  Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
	- https://arxiv.org/abs/2402.00841
	- Next to RoBERTa, FLAN-T5 is also a great go-to model for training text classifiers
	- Flan-T5é ‘å¼µã‚‹ãªã‚
-  å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç§‘å­¦çš„ç™ºè¦‹ã«ä¸ãˆã‚‹å½±éŸ¿ï¼šGPT-4ã‚’ç”¨ã„ãŸäºˆå‚™çš„ç ”ç©¶
	- https://ai-scholar.tech/articles/large-language-models/impact_of_LLM
	- GPT-4ã¯ç§‘å­¦çš„ç™ºè¦‹æ´»å‹•ã«ã‚‚å¤§ããå¯„ä¸ã—ã¤ã¤ã‚ã‚Šã¾ã™ã€‚  
	- å‰µè–¬ã€ç”Ÿç‰©å­¦ã€è¨ˆç®—åŒ–å­¦ã€ææ–™è¨­è¨ˆã€åå¾®åˆ†æ–¹ç¨‹å¼ã¨å¹…åºƒãã€GPT-4ã®å¿œç”¨ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ãã‚Œãã‚Œã®å¿œç”¨ã§ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚  
	- ç¾æ™‚ç‚¹ã§ã®GPT-4ã‚’ç”¨ã„ã‚‹ã†ãˆã§ã®ä¸è¶³ç‚¹ã‚’æ•´ç†ã—ã€å°†æ¥ã¸ã®å±•æœ›ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚
	- çŸ¥è¦‹
		- å…¨ä½“çš„ã«è¨€ãˆã°ã€GPT-4ã¯å‰µè–¬ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã¨å€‹ã€…ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é–¢ã™ã‚‹çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
		- GPT-4ã¯é€†åˆæˆã®äºˆæ¸¬ç²¾åº¦ãŒ20.
		- GPT-4ãŒå‰µè–¬ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãŸã‚ã®æ­£ã—ã„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã®ã«å½¹ç«‹ã¤
		- å®šé‡è¨ˆç®—ï¼š GPT-4ã¯ç”Ÿç‰©å­¦çš„ãªè¨€èªç†è§£ã¨å‡¦ç†ã«å„ªã‚Œã¦ã„ã¾ã™ãŒã€å®šé‡çš„ãªè¨ˆç®—ã«ã¯é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚ä¿¡é ¼ã§ãã‚‹çµè«–ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€æ‰‹å‹•ã§æ¤œè¨¼ã™ã‚‹ã‹ã€åˆ¥ã®è¨ˆç®—ãƒ„ãƒ¼ãƒ«ã§æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™
- Qwen2-14Bã®MTBenchãŒ7.99ã§Claude-1è¶…ãˆã¦ã‚‹ã®ã¯ãƒã‚¸ã‚„ã°ã„ by ã†ã¿ã‚†ã
	- https://x.com/umiyuki_ai/status/1754435534511050870?s=20
	- Qwen2ã¨ã—ã¦ã‚¦ãƒ¯ã‚µã«ãªã£ã¦ãŸãƒ¢ãƒ‡ãƒ«ãŒQwen1.5ã¨ã—ã¦ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸï¼Mistral-Mediumã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ãŒã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ï¼ä»Šå›ã¯æœ€åˆã‹ã‚‰Transformerã§ä½¿ãˆã‚‹ä¸Šã«ã€AWQãƒ¢ãƒ‡ãƒ«ã€GPTQãƒ¢ãƒ‡ãƒ«ã€GGUFã‚‚å…¨éƒ¨å…¬å¼ã§æœ€åˆã‹ã‚‰ãƒªãƒªãƒ¼ã‚¹ï¼vLLMã‚„Ollamaã‚‚OKï¼
-  Large Language Models on Graphs: A Comprehensive Survey
	- https://arxiv.org/abs/2312.02783
	- We have finalized our ğ‹ğ‹ğŒğ¬ ğ¨ğ§ ğ†ğ«ğšğ©ğ¡ğ¬ survey by adding more insightful discussions. If you are interested in LLMs on structure data, don't miss this paper (with a resource repo)!
- Home Credit - Credit Risk Model Stability
	- https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/
	- Kaggleæ–°ã‚³ãƒ³ãƒš ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰åˆ©ç”¨è€…ã®å¤–éƒ¨åŠã³å†…éƒ¨ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é•·æœŸã®è²¸å€’ã‚Œäºˆæ¸¬ã‚¿ã‚¹ã‚¯ã€‚ä¹…æ–¹ã¶ã‚Šã®æ­£çµ±æ´¾ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒš
- Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities
	- Nvidia presents Audio Flamingo
	- https://huggingface.co/papers/2402.01831
-  A Survey of Constraint Formulations in Safe Reinforcement Learning
	- https://arxiv.org/abs/2402.02025
	- å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹å®‰å…¨æ€§åˆ¶ç´„ã®è¨˜è¿°æ–¹æ³•ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã‚’ arXiv ã«ã¦å…¬é–‹ã—ã¾ã—ãŸ ã€‚ä¸»è¦ãªå®šå¼åŒ–ã®ç†è«–çš„ãªé–¢ä¿‚æ€§ã‚’è­°è«–ã—ã¦ã„ã‚‹ã®ãŒé¢ç™½ã„ã¨æ€ã„ã¾ã™
-  BlackMamba: Mixture of Experts for State-Space Models
	- https://huggingface.co/papers/2402.01771
- è‹±å›½AI Safety Instituteã‚ˆã‚Š3rd Progress Repoertã€‚
	- https://www.gov.uk/government/publications/uk-ai-safety-institute-third-progress-report/ai-safety-institute-third-progress-report
	- Google DeepMindã®Geoffrey Irvingæ°ã€Oxfordå¤§ã®ç¥çµŒç§‘å­¦è€…Chris Summerfieldæ°å‚ç”»ã€‚ã‚³ã‚¢KPIã®ã€Œãƒ¡ãƒ³ãƒãƒ¼ã®ã€å…ˆç«¯AIãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ç´¯ç©çµŒé¨“å¹´æ•°ã€ãŒ11æœˆã®150å¹´ã‹ã‚‰168å¹´ã«å¢—åŠ ã€‚
-  Stable Diffusion WebUI Forge
	- https://github.com/lllyasviel/stable-diffusion-webui-forge
	- Stable-Diffusion-WebUI-Forge is a new platform to 
		- (1) completely solve the speed and VRAM problem and 
		- (2) adding UNet Patcher System to webui so that many new features can be implemented in about 100 lines of codes
-  Unifying Large Language Models and Knowledge Graphs: A Roadmap
	- https://arxiv.org/abs/2306.08302v3
	- ã“ã® Knowledge Graph ã¨LLMã®é–¢ä¿‚ã«ã¤ã„ã¦çºã‚ãŸè«–æ–‡ã™ã”ã„ã€‚ 
	- Knowledge Graphã¨LLMãŒç›¸äº’æˆé•·ã™ã‚‹ä»•çµ„ã¿ãŒéå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ããƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åŒ–ã—ã¦çºã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ è«–æ–‡ã¨ã„ã†ã‚ˆã‚Šç¾çŠ¶ã®æ•´ç†ã«è¿‘ã„
-  Wikidata from LangChain
	- https://python.langchain.com/docs/integrations/tools/wikidata
	- WikiData allows you to easily connect to a free and open knowledge base
-  Qwen1.5
	- https://qwenlm.github.io/blog/qwen1.5/
	- https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524
	- Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.
- Ollamaã§ã‚‚Qwin1.5ã‚’ã‚µãƒãƒ¼ãƒˆ
	- https://ollama.com/library/qwen
-  Repeat After Me: Transformers are Better than State Space Models at Copying
	- https://arxiv.org/abs/2402.01032
	- Our recent work on the comparison between Transformers and State Space Models for sequence modeling now on arxiv! TLDR - we find a key disadvantage of SSMs compared to Transformers: they cannot copy from their input
-  Self RAG
	- https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/self_rag/self_rag.ipynb
	- Weâ€™re excited to feature Self-RAG, a special RAG technique where an LLM can do self-reflection for dynamic retrieval, critique, and generation
- The Majority of AI Compute Spend is Not on Training but on Inference
	- https://x.com/rohanpaul_ai/status/1754843805507887477?s=20
	- As per report - "2023: The State of Generative AI in the Enterprise"
- Qwen1.5-0.5B-chat with Transformer.js
	- Qwen1.5 is out: a collection of powerful LLMs with sizes ranging from 0.5B to 72B parameters.
	- https://x.com/xenovacom/status/1754873501536645292?s=20
	- Even at 8-bit quantization, the smallest one (0.5B) is surprisingly good for its size! Here's a demo I made with Transformers.js (v2.15), running 100% locally in the browser w/ WASM!
	- https://github.com/xenova/transformers.js	
- Gradio demo of Qwen1.5-72B-Chat
	- https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat
- ollamaã§Mixtralã‚’å‹•ã‹ã—ã¦LangChainã®agentã§ neo4jã™ã‚‹
	- Managed to get Mixtral on @ollama working as an function calling @LangChainAI agent that interacts with @neo4j  through a semantic layer. Needs some tidying up and I'll be able to share it.
	- https://x.com/tb_tomaz/status/1754861855929958488?s=20
- Style-Bert-VITS2ãŒå³åº§ã«æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ« JP-Extraã‚’å–ã‚Šè¾¼ã‚“ã§ãã‚Œã¦ã€æ—¥æœ¬èªç™ºéŸ³ãŒã‚¨ã‚°ã„ã§ã™
	- https://github.com/litagin02/Style-Bert-VITS2/releases/tag/2.0
	- ã€ŒStyle-Bert-VITS2ã€ã¯ã€è‡ªå‹•ã§æ–‡è„ˆãŒæŠŠæ¡ã•ã‚Œã€æ„Ÿæƒ…è¡¨ç¾ãŒèª¿æ•´ã•ã‚Œã‚‹
	- https://huggingface.co/spaces/litagin/Style-Bert-VITS2-JVNV
	- ã„ã‚„ã“ã‚Œã¯ã™ã”ã„
- NVIDIAãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼å‘ã‘GPUå¸‚å ´ã§98ï¼…ã®ã‚·ã‚§ã‚¢ã‚’ç‹¬å ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã€AIæ€§èƒ½ãŒæ˜æš—ã‚’åˆ†ã‘ã‚‹çµæœã« - GIGAZINE
	- https://gigazine.net/news/20240205-nvidia-gpu-market/
- ã ã‚ã€‚çµ¶å¯¾ã€‚ by ã‚­ãƒ ãƒ¯ã‚¤ãƒ—
	- https://x.com/kimwipes_crecia/status/1754757418595336404?s=20
-  OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
	- https://huggingface.co/papers/2402.01739
	- To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE,
- Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report
	- https://arxiv.org/abs/2402.01733
	- GPT-4ã«RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€è‡¨åºŠåŒ»å­¦ã®å•é¡Œã«ãŠã„ã¦ã€äººé–“ã®åŒ»å¸«ã‚ˆã‚Šã‚‚é«˜ã„ç²¾åº¦ãŒé”æˆã§ããŸã¨å ±å‘Š
	- é©åˆ‡ãªRAGã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã«ã‚ˆã‚Šã€GPT-4å˜ä½“ã‚ˆã‚Šã‚‚10%ä»¥ä¸Šç²¾åº¦ãŒå‘ä¸Šã—ã€äººé–“åŒ»å¸«ã‚ˆã‚Šã‚‚5%ä»¥ä¸Šé«˜ã„ã‚¹ã‚³ã‚¢ã‚’å‡º
	- ç ”ç©¶è€…ã‚‰ã¯ã“ã®çµæœã¯æ³¨ç›®ã«å€¤ã™ã‚‹ã¨ã—ã¤ã¤ã€ã‚ˆã‚Šåºƒç¯„ãªåˆ†é‡ã§å®Ÿé¨“ã‚’é‡ã­ã¦ã„ãã¹ãã¨ã—ã¦ã„ã¾ã™ã€‚ 
	- ã¾ãŸã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒä½ã„ã¨ã¯ã„ãˆã€åŒ»å­¦ã«ãŠã‘ã‚‹è‡ªå‹•åŒ–ã¯æ…é‡ã§ã‚ã‚‹ã¹ãã¨ã‚‚è¿°ã¹ã¦ã„ã¾ã™ã€‚
- Open AI shifts its battleground to Software
	- https://x.com/bioshok3/status/1755376649816953209?s=20
	- Open AIã¯ç¾åœ¨2ç¨®é¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆAIã‚’æ§‹ç¯‰ä¸­
	- 1ã¤ã¯ã‚ã‚Šã¨è‡ªç”±ã«ãƒ‡ãƒã‚¤ã‚¹ã‚’æ“ä½œå¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ 
		- é¡§å®¢ã¯ ChatGPT ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã€åˆ†æã®ãŸã‚ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã—ãŸã‚Šã€çµŒè²»å ±å‘Šæ›¸ã‚’è‡ªå‹•çš„ã«è¨˜å…¥ã—ã¦ä¼šè¨ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«å…¥åŠ›ã—ãŸã‚Šã™ã‚‹ã‚ˆã†ä¾é ¼ã§ãã¾ã™
	- ã‚‚ã†ä¸€ã¤ã¯WEBä¸Šã§æ§˜ã€…ãªæ“ä½œå¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ ï¼ˆ1ã¤ç›®ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚„ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æ‡¸å¿µã™ã‚‹äººã‚‚ã„ã‚‹ã®ã§ã‚‚ã†ä¸€ã¤ã®ã‚¿ã‚¤ãƒ—ã‚’é–‹ç™ºã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ï¼‰
- Fully local RAG using @Teknium1 OpenHermes, @ollama and @streamlit
	- GPT4 level performance at 0% of the cost
	- https://github.com/phidatahq/phidata/tree/main/cookbook/local_rag
- æµ·å¤–é«˜æ€§èƒ½è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªåŒ–ç ”ç©¶ã®ä¸€ç’°ã¨ã—ã¦Mixtral-8x7Bã®æ—¥æœ¬èªå‡ºåŠ›ã‚’å®‰å®šã•ã›ã‚‹Loraä½œæˆã€å…¬é–‹   
	- https://huggingface.co/aixsatoshi/Mixtral-8x7B-ja-Lora-sft-ChatbotArenaJAcalm2
	- Mixtral-8x7Bã¯é«˜æ€§èƒ½ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€æ—¥æœ¬èªå‡ºåŠ›ã«å¤šè¨€èªãŒæ··å…¥ã™ã‚‹code-switchingãŒã‚ˆãè¦‹ã‚‰ã‚Œã¾ã™ã€‚ å…ƒã®æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€æ—¥æœ¬èªç”Ÿæˆã‚’å®‰å®šã•ã›ã‚‹æ–¹æ³•ã¨ã—ã¦ã€Loraã®åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã—ãŸ
	- æ—¥æœ¬èªãŒæµæš¢ãªcalm2ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¦ã¾ã™ Baseãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ã€ä¸€å®šã®æ€§èƒ½ç¢ºä¿ã—ã¦æ—¥æœ¬èªåŒ–ã§ãã¾ã—ãŸ
-  Apple Vision Proã¯HoloLensã®å®Œæˆå½¢ã€‚ç¾æ™‚ç‚¹ã§ã®é™ç•Œå€¤ by shi3zã•ã‚“
	- https://note.com/shi3zblog/n/nd36c04f9133a?sub_rt=share_h
	- ã€Œã¤ã„ã«ã“ã“ã¾ã§æ¥ãŸã‹ã€
-  Step-wise Queries by llamaindes
	- https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent.html#step-wise-queries
	- In our brand-new cookbook, learn how to build a custom agent that can execute complex queries over your data, and can also be interrupted in the middle of execution with user inputs!
- Ollama OpenAI compatibility is here!
	- https://ollama.com/blog/openai-compatibility
	- ã¤ã¾ã‚Šã€ollamaã§openaiã®APIã¤ã‹ã£ã¦URLã‚’ollamaã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«å¤‰ãˆã‚‹ã ã‘ã§å‹•ãã¨ã„ã†ã“ã¨
-  RAG Research Insights
	- https://www.promptingguide.ai/research/rag#rag-research-insights
	- So we have created a new section in the RAG overview to summarize and help you keep track of insights into the latest RAG techniques.
- Nvidia releases canary-1b
	- https://huggingface.co/spaces/nvidia/canary-1b
	- With 1 billion parameters, Canary-1B supports automatic speech-to-text recognition (ASR) in 4 languages (English, German, French, Spanish) and translation from English to German/French/Spanish and fromâ€¦
- Bard ã¯ Geminiï¼ˆã‚¸ã‚§ãƒŸãƒ‹ï¼‰ã«ãªã‚Šã¾ã™ï¼
	- https://x.com/googlejapan/status/1755607418103587148?s=20
	- Gemini ã¯ Bard ã«æ­è¼‰ã•ã‚Œã¦ã„ã‚‹ AI ãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€ã“ã®é«˜åº¦ãªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ã‚ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹ãŸã‚ã«ã€åå‰ã‚’å¤‰ãˆã¾ã—ãŸ
	- https://gemini.google.com/app
- The Consensus Game: Language Model Generation via Equilibrium Search by å²¡é‡ã•ã‚“
	- https://openreview.net/forum?id=n9xeGcI4Yg
	- LLMã§è³ªå•å¿œç­”ç­‰ã®ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã™å ´åˆã€ç”Ÿæˆçš„ã«è§£ãå ´åˆï¼ˆp(y|x,v=çœŸ)) ã¨è­˜åˆ¥çš„ã«è§£ãå ´åˆï¼ˆp(v=çœŸ|x, y)ï¼‰ã§å¾—æ„/ä¸å¾—æ„ãŒç•°ãªã‚ŠçµæœãŒç•°ãªã‚‹ã€‚ã‚²ãƒ¼ãƒ ç†è«–ã«åŸºã¥ã„ã¦äºŒã¤ãŒåˆæ„ã™ã‚‹è§£ã‚’æ±‚ã‚ã‚‰ã‚Œã‚‹å‡è¡¡é †ä½ä»˜ã‘ã‚’ææ¡ˆã€‚å¤šãã®ã‚¿ã‚¹ã‚¯ã§å†å­¦ç¿’ãªãã€æ€§èƒ½ã‚’å¤§ããæ”¹å–„ã§ãã‚‹
- OpenAnimateAnyone
	- https://github.com/fenghan0430/Open-AnimateAnyone
	- ã‚¢ãƒªãƒãƒã¯AIã®ç ”ç©¶çµæœã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã§å‡ºã—ã¦ãã‚Œã¦ãŸã‘ã©ã€ã„ã–AnimateAnyoneã¿ãŸã„ãªæœ‰æœ›ãªæˆæœç‰©ãŒã§ããŸã‚‰ã‚¹ãƒƒã¨ã‚¯ãƒ­ãƒ¼ã‚ºã«ã—ã¦ã‚·ãƒ¥ãƒƒã¨è‡ªç¤¾ã‚¢ãƒ—ãƒªã«çµ„ã¿è¾¼ã‚€ã€‚ã¤ã¾ã‚Šä»Šã¾ã§ã¯è‡ªç¤¾ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯ä½¿ãˆã‚“ã‚¯ã‚ªãƒªãƒ†ã‚£ã ã‹ã‚‰ä¸ç”¨å“ãƒªã‚µã‚¤ã‚¯ãƒ«ã¨ã—ã¦ã‚ªãƒ¼ãƒ—ãƒ³ã«ã—ã¦ãŸã ã‘ï¼Ÿ
-  Grandmaster-Level Chess Without Search
	- https://arxiv.org/abs/2402.04494
	- ãƒã‚§ã‚¹ã§ã©ã®æ‰‹ãŒè‰¯ã„ã‹ã‚’Transformerã§æ•™å¸«ã‚ã‚Šå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯æ¢ç´¢ã‚’ä½¿ã‚ãªãã¦ã‚‚äººã‚ˆã‚Šå¼·ããªã‚‹ï¼ˆæ¢ç´¢ã‚ã‚ŠAIã‚ˆã‚Šã¯å¼±ã„ï¼‰ã€‚æ•™å¸«ã‚ã‚Šãƒ‡ãƒ¼ã‚¿ã¯Stockfish 16ã§ä½œæˆã—ã¦ãŠã‚Šã€ç§‘å­¦åˆ†é‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã¨ã¿ãªã›ã‚‹ã€‚
- ragas 0.1 release
	- https://github.com/explodinggradients/ragas
	- We are releasing version 0.1 of Ragas today, the open-source standard for evaluating RAG applications.
-  Perplexityã‚’ã‚‚ã¨ã«ï½¤è¤‡æ•°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¦æ¨è«–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®ç°¡å˜ãªã‚³ãƒ¼ãƒ‰å®Ÿè£…
	- https://note.com/kan_hatakeyama/n/nb5625d6411a8?sub_rt=share_pb
	- ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®ç°¡å˜ãªå®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã™ã€‚  æœ€è¿‘ã¯ï½¤æ™®é€šã«mergekitã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒï½¤å‹‰å¼·ã‚‚å…¼ã­ãŸå®Ÿè£…ã§ã™
	- ä¸ãˆã‚‰ã‚ŒãŸå…¥åŠ›æ–‡ç« ã«å¯¾ã™ã‚‹Perplexityï¼ˆå›°æƒ‘ã•ï¼‰ã‚’æŒ‡æ¨™ã«ã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚Šã¾ã™
	- ä»Šå›ã¯è©¦ã—ã«ã€è‹±èªãŒå¾—æ„ãªLLama2-7bã¨ã€æ—¥æœ¬èªã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸElyza-7bã‚’çµ±åˆï¼ˆmergeï¼‰ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã£ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚
- æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
	- https://github.com/lighttransport/japanese-llama-experiment
-  Real-World Robot Applications of Foundation Models: A Review
	- https://arxiv.org/abs/2402.05741
	- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿãƒ­ãƒœãƒƒãƒˆå¿œç”¨ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã‚’å…¬é–‹ã—ã¾ã—ãŸï¼Meta AI Researchã®Chrisã•ã‚“@chris_j_paxton , Google Deepmindã®Andyã•ã‚“ @andyzeng_ ã¨ã„ã†ï¼Œã“ã®åˆ†é‡ã§æœ€å…ˆç«¯ã‚’é€²ã‚€ãŠäºŒäººã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘ãªãŒã‚‰åŸ·ç­†
-  OpenAIã‚¢ãƒ«ãƒˆãƒãƒ³æ°ã€åŠå°ä½“ã®è³‡é‡‘èª¿é”ã§äº¤æ¸‰ã€€ç±³å ±é“
	- https://www.nikkei.com/article/DGXZQOGN095R00Z00C24A2000000/
	- å¿…è¦è³‡é‡‘750å…†å††
-  Multilingual E5 Text Embeddings: A Technical Report
	- https://huggingface.co/papers/2402.05672
	- Microsoftã®E5ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®å®Ÿè£…ãƒšãƒ¼ãƒ‘ãƒ¼ã€ä»Šé ƒã§ã‚‹ã‚‚ã®ãªã®ã‹ãƒ»
- Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents
	- https://github.com/yifanlu0227/ChatSim
	- ç”ŸæˆAIã®å‡ºç¾ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®ä¸–ç•Œã‚‚å¤§ããå¤‰åŒ–ã€‚ æ˜¨æ—¥å‡ºãŸChatSimã§ã¯è‡ªç„¶è¨€èªã‚’å…¥åŠ›ã—ã¦ãƒ‰ãƒ©ã‚¤ãƒ“ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’è‡ªç”±ã«ç·¨é›†ã™ã‚‹ã“ã¨ãŒã§ãã‚‹
-  LangChain 101: Part 3a. Talking to Documents: Load, Split, and simple RAG with LCEL
	- https://pub.towardsai.net/langchain-101-part-3a-talking-to-documents-load-split-and-simple-rag-with-lcel-26b005ccb30a
	- Loading documents and splitting them are a key part of RAG
- mambaã®ç†è«–ã‚’ç†è§£ã™ã‚‹â‘ ï¼šHiPPOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨LSSL
	- https://zenn.dev/izmyon/articles/8374a11d272602
	- mambaã®ç†è«–ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®è§£èª¬è¨˜äº‹ã‚’æ›¸ãå§‹ã‚ã¾ã—ãŸã€‚ã‹ãªã‚Šæ•°å¼ã®å°å‡ºãªã©ä¸å¯§ã«æ›¸ã„ã¦ã‚‹ã®ã§ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚ä½•ã‹è¨‚æ­£ã‚„è£œè¶³ãŒã‚ã‚Œã°å„ªã—ãæ•™ãˆã¦ãã ã•
-  Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
	- https://arxiv.org/abs/2402.00841
	- Can "small" finetuned LLMs with less than 2B parameters outperform larger openly available LLMs (Mixtral, Llama 2 Chat) and proprietary LLMs (ChatGPT)? Here's a closer look at the Tiny Titans paper
	- Flan-T5ãŒæœ€å¼·ã‚‰ã—ã„ã€
-  GPTã¯ä»–è€…ã®å¿ƒã®çŠ¶æ…‹ã‚’æ¨æ¸¬ã§ãã‚‹ï¼ŸAIÃ—å¿ƒç†å­¦ã®ã™ã‚ã‚
	- https://ai-scholar.tech/articles/computation-and-language/Theory-of-Mind
	- GPTã¯ä»–è€…ã®å¿ƒã‚’èª­ã‚ã‚‹ã®ã‹ï¼Ÿ å®Ÿé¨“ã«ãŠã„ã¦ã€GPT-3.5ã¨GPT-4ã¯é«˜ã„æ­£ç­”ç‡ã‚’ãƒãƒ¼ã‚¯ã—ã¾ã—ãŸã€‚ 
	- è‘—è€…ã¯ã€GPTãŒå¿ƒã®çŠ¶æ…‹ã‚’æ¨æ¸¬ã§ãã‚‹ç†ç”±ã¨ã—ã¦ã€Œè¨€èªèƒ½åŠ›ã®å‘ä¸Šã«ã‚ˆã£ã¦è‡ªç™ºçš„ã«å‡ºç¾ã—ãŸã®ã§ã¯ã€ã¨æŒ‡æ‘˜ã€‚ AIç ”ç©¶ã«ãŠã‘ã‚‹å¿ƒç†å­¦çš„ãªè¦–ç‚¹ã®é‡è¦æ€§ã‚’è§£
- In-Context Principle Learning from Mistakes
	- https://arxiv.org/abs/2402.05403
	- LLMã«æ•¢ãˆã¦é–“é•ã‚ã›ã¦ãƒ«ãƒ¼ãƒ«ã‚’è¦šãˆã•ã›åŒã˜ãƒŸã‚¹ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«ã™ã‚‹æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ 
		- 1. ãƒ¢ãƒ‡ãƒ«ãŒé–“é•ã„ã‚’çŠ¯ã™ã‚ˆã†ã«ä¿ƒã™ 
		- 2. ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã«ã€é–“é•ã„ã«å¯¾ã™ã‚‹èª¬æ˜ã‚’ç”Ÿæˆã•ã›ã€ã¾ãšã¯ä½ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’å½¢æˆã€‚ 
		- 3. ä½ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’ã¾ã¨ã‚ã€ç´„5ã¤ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«åœ§ç¸®ã—ã¦é«˜ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’ç”Ÿæˆ 
		- 4. é«˜ãƒ¬ãƒ™ãƒ«ã®åŸå‰‡ã‚’æœªè¦‹ã®ä¾‹ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹éš›ã«åˆ©ç”¨ 
	- â– å®Ÿé¨“ã¨çµæœ å®Ÿé¨“ã¨çµæœã®è¦ç´„: 
		- GPT-3.5-Turboã¨GPT-4ã®è³ªå•å¿œç­”æ€§èƒ½ãŒä¸€è²«ã—ã¦æ”¹å–„ã•ã‚Œã€GPT-4ãŒ7.5%ã®æ”¹å–„ã‚’è¦‹ã›ãŸ
		- æ•°å­¦æ¨è«–ã‚¿ã‚¹ã‚¯ã§ã‚‚GPT-3.5-turboã¨GPT-4ã§åŸºæº–ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸ
		- Big-Bench Hardã‚¿ã‚¹ã‚¯ã§ã‚‚ã‚¹ã‚³ã‚¢ãŒä¸€å®šç¨‹åº¦ä¸Šæ˜‡ã—ãŸ
- Step-by-step guide to build AI agents for structured and unstructured data.
	- https://x.com/Saboo_Shubham_/status/1756123156400546251?s=20
	- Step 1: Define the Chunking Strategy
	- Step 2: Apply an Embedding Strategy
	- Step 3: Implement a Document Retriever for Text
	- Step 4: Use a Large Language Model (LLM)
	- Step 5: Extract Metadata
	- Step 6: Implement a Document Retriever for Metadata
	- Step 7: Integrate SQL Querying with a Data Warehouse
	- Step 8: Develop a Prompt Refinement Engine
	- Step 9: Create a Response Post-processor
	- Step 10: Deliver the Response
- Buffer Overflow in Mixture of Experts
	- https://arxiv.org/abs/2402.05526
	- "Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious
- WolframEngine+JupyterNotebookã§ç–‘ä¼¼Mathematica
	- https://x.com/blkcatman/status/1756219896026067052?s=20
- ã€Mambaã€‘Transformerã‚’å‡Œé§•ã—ã†ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¾¹åº•è§£èª¬ï¼ˆã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚ã‚Šï¼‰
	- https://qiita.com/peony_snow/items/649ecb307cd3b5c10aa7
	- ï¼‘ï¼Mambaã¯Attentionã‚„MLPBlockã‚’æŒãŸãªã„ç°¡ç´ åŒ–ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æœ‰ã—ã¾ã™ã€‚é¸æŠçš„çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆSelective SSMï¼šSelective State Space Modelï¼‰ã¨ã„ã†æ–°ã—ã„æ§‹é€ ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€å¿…è¦ãªæƒ…å ±ã®ã¿ã«æ³¨ç›®ã—ã€è¨ˆç®—åŠ¹ç‡ã®å¤§å¹…ãªå‘ä¸Šã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚
	- ï¼’ï¼é«˜é€Ÿãªæ¨è«–ï¼ˆTransformerã®ç´„5å€ï¼‰ã‚’å¯èƒ½ã«ã™ã‚‹ã¨ã¨ã‚‚ã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ãªã©ã®ã“ã¨ï¼‰ã®å¢—å¤§ã«å¯¾ã—ã¦ã€æ¨è«–ã‚³ã‚¹ãƒˆãŒç·šå½¢ã«å¢—å¤§ã™ã‚‹ã¨ã„ã†ç‰¹å¾´ã‚’æœ‰ã—ã¾ã™ï¼ˆã“ã‚Œã¾ã§ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯éç·šå½¢çš„ãªå¢—å¤§ãŒã‚ã‚Šã¾ã—ãŸï¼‰ã€‚ã“ã®æ€§èƒ½å‘ä¸Šã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹æ¤œè¨¼ã§ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒ1000kï¼ˆï¼‘ï¼ï¼ä¸‡ï¼‰ã«ãŠã„ã¦ã¾ã§ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚
	- ï¼“ï¼GPUãƒ¡ãƒ¢ãƒªéšå±¤é–“ã®ç§»å‹•ã‚’æœ€å°é™åŒ–ã™ã‚‹ã¨ã¨ã‚‚ã«ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šé«˜é€Ÿãªè¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã€è¦æ±‚ã•ã‚Œã‚‹ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚‚è»½æ¸›ã•ã‚Œã¾ã™
	- ï¼”ï¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°2.8Bä»¥ä¸Šã®å ´åˆã«ãŠã„ã¦Mambaã¯æ©Ÿèƒ½ã™ã‚‹ã®ã‹ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã¯Transformerãªã©ã¨åŒã˜ãªã®ã‹ã€å­¦ç¿’ã®ä¸å®‰å®šæ€§ã¯ã©ã†ãªã®ã‹ã¨ã„ã£ãŸç‚¹ã«é–¢ã—ã¦ã¯ã¾ã ä¸æ˜ã§ã‚ã‚Šã€ä»Šå¾Œã®ç ”ç©¶ãŒå¾…ãŸã‚Œã¾ã™ã€‚
	- ï¼•ï¼ã¾ã ä¸æ˜ãªç‚¹ã‚‚å¤šã„ã§ã™ãŒã€æ§˜ã€…ãªè§’åº¦ã‹ã‚‰ã®ç ”ç©¶ã«ã‚ˆã£ã¦ã€Transformerã‚’ä»£æ›¿ã—ã†ã‚‹æœ‰æœ›ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ã¨ã„ã†ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚‚å–å¾—ã•ã‚Œã¤ã¤ã‚ã‚Šã€ä»Šå¾ŒMambaã‚’çŸ¥ã‚‰ãªã‘ã‚Œã°æœ€å…ˆç«¯ã®ç ”ç©¶ã‹ã‚‰å–ã‚Šæ®‹ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
-  æ —ç”°å·¥æ¥­ã€æ©Ÿæ¢°å­¦ç¿’ä½¿ã£ãŸææ–™æ¢ç´¢ã§ä½ç’°å¢ƒè² è·ã®é˜²é£Ÿå‰¤é–‹ç™ºã¸
	- https://xtech.nikkei.com/atcl/nxt/news/24/00208/?n_cid=nbpnxt_twbn
	- æ —ç”°å·¥æ¥­ã•ã‚“ã‚‰ã¯å†·å´æ°´ã®é˜²é£Ÿå‰¤ã®é–‹ç™ºã®ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šæ•°ç™¾ä¸‡ã®åˆ†å­ã‹ã‚‰æœ‰æœ›ææ–™ã‚’æŠ½å‡º
- NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit
	- https://towardsdatascience.com/nemo-guardrails-the-ultimate-open-source-llm-security-toolkit-0a34648713ef
	- Advanced RAG with Guardrails
	- If you want to build user-facing RAG, you not only need to setup advanced retrieval, but also need to apply requisite layers of input/output filters for the following:
- pandas-ai
	- https://github.com/gventuri/pandas-ai
	- æ©Ÿèƒ½ã¨ã—ã¦ã¯pandasã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾ã—ã¦ç›´æ¥è‡ªç„¶è¨€èªã§å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®ã§ã€è»½ãè¦‹ãŸæ„Ÿã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„ã«æ–°ã—ã„ã‚‚ã®ã¯ãªã•ãã†ãªã‚‚ã®ã®http://df.chat(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)ã¨ã„ã†å½¢å¼ã§ã®æ“ä½œã¯æ–¬æ–°
- LLM-jp 13B v1.1ãƒªãƒªãƒ¼ã‚¹
	- https://llm-jp.nii.ac.jp/blog/2024/02/09/v1.1-tuning.html
	- å„ç¨®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã™ã”ã„æµæš¢ã«ãªã£ã¦ã‚‹ã€‚å­¦ç¿’è©³ç´°ã‚‚å…¬é–‹ã•ã‚Œã¦ã¦å‚è€ƒã«ãªã‚‹ã€‚
- The biggest Collection of Colab Based LLMs Fine tuning Notebooks
	- https://github.com/ashishpatel26/LLM-Finetuning
-  Google Colab ã§ LLM-jp 13B v1.1 ã‚’è©¦ã™ by nakaã•ã‚“
	- https://note.com/npaka/n/n2c272727d95a?sub_rt=share_h
	- ã€ŒLLM-jp 13B v1.1ã€ã¯ã€ã€ŒLLM-jp 13Bã€ã®æœ€æ–°ç‰ˆã§ã™ã€‚æ—¥è‹±ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹SFTã€ichikaraãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ +DPOã§å¯¾è©±å¿œç­”æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚
- OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
	- https://arxiv.org/abs/2402.06044
	- A wild Theory of Mind Benchmark has appeared:
- 



## 2/5

ä»Šé€±ã‚‚ç››ã‚Šã ãã•ã‚“ã€‚ã¾ãšã¯ã€Metaã®CodeLlamaã®70Bç‰ˆãƒªãƒªãƒ¼ã‚¹ã€‚æ—©é€ŸSQLã®å¤‰æ›SQLCoder-70BãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã‚Šã€4bitåŒ–ã•ã‚Œã¦MLXçµŒç”±ã§Macã§å‹•ã‹ã—ãŸã‚Šã¨ä¸€æ°—ã«ã«ãã‚„ã‹ã«ã€‚Metaã¯ã€35ä¸‡å€‹ã® H100ã‚’æ•´å‚™ã—ã€OSSã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã«å–ã‚Šçµ„ã‚€ã¨ã„ã†ã“ã¨ã§ã€æ ªä¾¡ã¯20%ã‚¢ãƒƒãƒ—ã€‚ä¸€æ–¹Googleã¯ã€Bardã®backendã®Gemini Proã®å›½éš›å¯¾å¿œã‚’ãƒªãƒªãƒ¼ã‚¹ã€‚æ—¥æœ¬èªãªã‚“ã‹ã¾ã å¤‰ï¼ˆä¾‹ã€ãƒã‚¤ã‚¯ã‚’è‡ªè»¢è»Šã¨èªçŸ¥ï¼‰ã§ã™ãŒã€ç”»åƒèªè­˜æ©Ÿèƒ½ãªã©Gemini Proã‚’æ‰‹å…ƒã§è©¦ã›ã‚‹ã€‚OSSç‰ˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMä»£è¡¨çš„ãªLLaVA-1.6ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã€Gemini Proè¶Šãˆã¨ã®è©•ä¾¡ã‚‚ã€‚LLMã®è»½é‡åŒ–ã®æ–°æ˜ŸSliceGPTã€è»½ãã¦ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯å¤§æ­“è¿ã€‚miqu-70Bã¨ã„ã†Mixtral 8x7Bã®é‡å­åŒ–ç‰ˆã‚‰ã—ãã‚‚ã®ãŒã€EQ-benchã§çªç„¶ä¸Šä½ã«ç™»å ´ã€æ¬¡ã®å¤§ããªãƒªãƒªãƒ¼ã‚¹ã®æ–¥å€™ã‹ã€ãªãŠmiqueã£ã¦ãƒŸã‚¯ã ã£ãŸã®ã‹ï¼Ÿã€‚Phixtralã®è«–æ–‡ã§ç´¹ä»‹ã•ã‚ŒãŸMoEã®å®Ÿè£…ã€æœ¬å®¶ã¨ã¯ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒé•ã†ã¿ãŸã„ã ãŒMoEã®å®Ÿè£…ã«ã‚‚ã„ã‚ã„ã‚ã‚ã‚‹ã‚‚ã®ã ã€‚ICRA2024ã§ã®æ¡æŠè«–æ–‡ãƒ»æŠ€è¡“ã®è©±é¡Œã‚‚ã¡ã‚‰ã»ã‚‰ã€‚å›½ç”£LLMã§ã¯ã€700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼LLMã€ŒKARAKURI LMã€ãŒç™»å ´ã€Llama 2ã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‰ã—ã„ãŒã‚„ãŸã‚‰æ€§èƒ½ãŒé«˜ã„ã¨è©±é¡Œã«ã€‚ggufç‰ˆã‚„ã€MLXã‚’ã¤ã‹ã£ã¦M2 Macã§ã®å‹•ä½œç¢ºèªç­‰ãŒè¡Œã‚ã‚Œã€ã“ã‚Œã¯åŸºç¤èƒ½åŠ›ãŒé«˜ãã†ã€‚å°ã•ãè¨€èªãƒ¢ãƒ‡ãƒ«ã‚‚ã€Allen.AIã®OLMoã‚„ã€Kaggleé–¢é€£ã®H2O-Danube-1.8Bãªã©ãŒç™»å ´ã€‚RAGé–¢ä¿‚ã ã¨ã€ã¾ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã®æ¯”è¼ƒè«–æ–‡ã€ã©ã†ã‚‚ã¾ã ï¼²ï¼¡ï¼§ã®ã»ã†ãŒåˆ©ãŒã‚ã‚‹ã€‚ã‚¯ã‚¨ãƒªå¤‰æ›ã£ã¦ã®ã‚‚é‡è¦ãªæŠ€è¡“ã€‚ã—ã‹ã—ã€èµ¤ã¡ã‚ƒã‚“ã®é ­ã«ãƒ“ãƒ‡ã‚ªã‚’è£…ç€ã—ã¦å¾—ã‚‰ã‚ŒãŸ61 æ™‚é–“åˆ†ã®ç”»åƒã‹ã‚‰ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚‹ã¨ã„ã†é€”æ–¹ã‚‚ãªã„ç ”ç©¶ã«ã¯ã³ã£ãã‚Šã—ãŸã€‚Hugging FaceãŒGPT Storeã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ç‰ˆï¼ˆAssistantï¼‰ã‚’é–‹å§‹ã€Googleã¨ã®ææºã§ãƒªã‚½ãƒ¼ã‚¹ãŒå¼·åŒ–ã•ã‚ŒãŸï¼Ÿã€‚æ±äº¬è—å¤§ã®å’æ¥­å±•ç¤ºã«â€œAIã‚¢ãƒ‹ãƒ¡â€ãŒå‡ºãŸã“ã¨ãŒè©±é¡Œã«ãªã£ãŸãŒã€Makingæƒ…å ±ã‚’è¦‹ã‚‹ã¨ã€ã˜ã¤ã¯ç›¸å½“ï¼¬ï¼¬ï¼­ã‚’ä½¿ã„ã“ãªã—ã¦ã„ã¦ã€ã‚·ãƒŠãƒªã‚ªã®ChatGPTã§ã®ä½œæˆãƒ­ã‚°ç­‰ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå‚è€ƒã«ãªã‚‹ã¨ã„ã†è©±ã«ã€‚Googleã®ã‚ã‚‰ã‚†ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’Decoder-onlyã®ãƒ¢ãƒ‡ãƒ«ã«ã¶ã£è¾¼ã‚“ã§æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ä½œã‚‹è©±ã€é•·æœŸæ™‚ç³»åˆ—äºˆæ¸¬ã§ã©ã†ã—ã¦ãã‚“ãªã«æ€§èƒ½ãŒé«˜ã„ã®ã‹ã€æ°—è±¡äºˆæ¸¬ã«ã‚‚é©ç”¨ã™ã‚‹ã®ã‹ï¼Ÿã€‚NEDOã®å›½å†…ç”ŸæˆAIã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºæ”¯æ´ã€ã•ã™ãŒã¨æ€ã‚ã‚Œã‚‹ä¼šç¤¾ã‚„ç ”ç©¶æ©Ÿé–¢ãŒä¸¦ã¶ã€‚å‚åŠ æ©Ÿé–¢ã®ï¼‘ã¤NIIã§ã¯ã€å›½ä¼šå›³æ›¸é¤¨ã®ã‚‚ã¤å›½å†…ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–äº‹æ¥­ã®æˆæœãŒæ´»ç”¨ã•ã‚Œã‚‹ã¨ã„ã†ã“ã¨ã ã€‚ä¸€æ–¹æ°‘é–“ã§ã¯Ricor-13Bã®ã‚ˆã†ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸLLMæä¾›ãƒ“ã‚¸ãƒã‚¹ã‚‚ã¯ã˜ã¾ã£ãŸã€‚ææ–™ç³»ã®ç ”ç©¶ã¸ã®LLMã®å¿œç”¨ã‚‚ç€å®Ÿã«é€²ã‚€ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œç’°å¢ƒã‚‚ã‚‚ollamaãŒvisionå¯¾å¿œã¨ã‹ã€function callå¯¾å¿œã¨ã‹ç€å®Ÿã«é€²ã‚“ã§ã‚‹ã€‚ã•ã¦æ¥é€±ã‚‚ã€Gemini Ultra ãŒ2/7ã«ãƒªãƒªãƒ¼ã‚¹ã¨ã®ã†ã‚ã•ã‚‚ã‚ã‚Šã€äººå‹ãƒ­ãƒœãƒƒãƒˆã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—Figureã«å‡ºè³‡ã—ãŸMicrosoft/OpenAIã®æ¬¡ã®æ‰‹ã‚„ã€Vison Proã‚’å‡ºã—ãŸAppleã®ï¼¡ï¼©æˆ¦ç•¥ã‚‚æ°—ã«ãªã‚‹ã¨ã“ã‚ã€‚

- google/siglip-base-patch16-256-multilingual ã‚’ä½¿ã£ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”»åƒã‚’æ—¥æœ¬èªã§æ¤œç´¢ã—ã¦ã¿ã‚‹
	- https://note.com/eurekachan/n/n9d4f62b80ad6?sub_rt=share_pb
	- 1æœˆã«ã€Googleã‹ã‚‰ã€SigLIPã¨ã„ã†ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã®multilingualç‰ˆï¼ˆå¤šè¨€èªå¯¾å¿œç‰ˆï¼‰ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚transformers 4.37ä»¥é™ã§å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
	- CUDAã‚’ä½¿ã„ã¾ã—ãŸãŒã€GPUã¸ã®è² è·ã‚‚ä½ã‹ã£ãŸã®ã§ã€æ¡ˆå¤–CPUã§ã‚‚å‹•ã‹ã›ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
-  Are Transformers Effective for Time Series Forecasting?
	- decisively highlighting the shortcomings and deficiencies in research surrounding the use of transformers for
	- This paper effectively exposes the deceptive practices employed by various authors in their papers, such as inadequate benchmarking and other tactics, which have previously led to inflated claims regarding the performance of transformers in this domain.
- Googleãªã©ç±³ITã€1æœˆ1ä¸‡äººå‰Šæ¸›ã€€çµ„ç¹”ã‚¹ãƒªãƒ åŒ–ã§AIé›†ä¸­
	- https://www.nikkei.com/article/DGXZQOGN1757C0X10C24A1000000/
- DSPy lets you prototype LLM Programs like AlphaCodium
	- https://x.com/CShorten30/status/1751656468879708496?s=20
- LangGraph Financial Agent w/ Polygon
	- https://gist.github.com/virattt/4d764c427892ce9fdf4534209edfb1f4
	- LangGraphã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã£ã¦æ ªä¾¡ã‚’ã¨ã£ã¦ãã‚‹ç°¡å˜ãªä¾‹
- Ollamaã§ã€ Mistral-7B finetuned for function callingã€€ã‚’ã‚µãƒãƒ¼ãƒˆ
	- https://ollama.ai/calebfahlgren/natural-functions
-  çŸ¥è­˜0ã§ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ã¿ã‚‹ï¼å‚ã‚Œæµã—é…ä¿¡ã€ã‚´ãƒªãƒ©ã‚¸ã€‘
	- https://www.youtube.com/watch?v=C1yFEMDLddc
- MetaãŒã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°LLMã®CodeLlamaã®70Bç‰ˆã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ãƒªãƒªãƒ¼ã‚¹ã€‚
	- https://ai.meta.com/blog/code-llama-large-language-model-coding/?utm_source=twitter&utm_medium=organic_social&utm_campaign=codellama&utm_content=reply
	- HumanEvalã§GPT-4è¶…ãˆã—ãŸã‚‰ã—ã„ã€‚å…¥åŠ›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚‚100kã¾ã§è¡Œã‘ã‚‹ã‚‰ã—ã„
	- Metaã¯ä»¥å‰ã‹ã‚‰ã€ŒGPT-4ä¸¦ã®LLMã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã«ã™ã‚‹ã€ã¨äºˆå‘Šã—ã¦ã„ã¾ã—ãŸãŒï¼Œå¹´æ˜ã‘æ—©ã€…ï¼Œã¾ãšã¯ã‚³ãƒ¼ãƒ‰ç”Ÿæˆé ˜åŸŸã§ã‚„ã£ã¦ãã¾ã—ãŸ
	- https://labs.perplexity.ai/ ã§ãŸã‚ã›ã‚‹ã‚‰ã—ã„
-  Inverse Molecular Design with Multi-Conditional Diffusion Guidance
	- https://arxiv.org/abs/2401.13858
	- è¤‡æ•°ã®åˆ¶ç´„ä¸‹ã§ã®åˆ†å­ç”Ÿæˆã®è«–æ–‡
	- å¾“æ¥ã¯åˆæˆå¯èƒ½æ€§ã¨ã‚¬ã‚¹é€éæ€§ãªã©ï¼’ã¤ä»¥ä¸Šã®åˆ¶ç´„ã‚’æº€ãŸã™ã‚ˆã†ãªåˆ†å­ã‚’ï¼‘ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸãŒ 
	- åˆ¶ç´„ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸTransformerãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šä½åˆ†å­ãƒ»é«˜åˆ†å­å…±ã«ã†ã¾ãç”Ÿæˆã§ããŸãã†ã§ã™ã€‚
- ã‚¢ãƒªãƒãƒãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMä½¿ã£ã¦ä½œã£ãŸã‚¹ãƒãƒ›ã‚’æ“ä½œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Mobile-Agentã‚’ç™ºè¡¨
	- https://x.com/umiyuki_ai/status/1752183108873687439?s=20
- SliceGPT: Compress Large Language Models by Deleting Rows and Columns
	- https://arxiv.org/abs/2401.15024
	- Microsoftã¨ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ’å·¥ç§‘å¤§ã®ç ”ç©¶è€…ã«ã‚ˆã‚Šã€LLMã‚’ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆè¡Œã‚„åˆ—ã‚’å‰Šé™¤ï¼‰ã—ã¦è»½ãã™ã‚‹åŠ¹æœçš„ãªæ‰‹æ³•
	- å®Ÿé¨“ã§ã¯æœ€å¤§30%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šæ¸›ã—ã¤ã¤æ€§èƒ½ã®90%ä»¥ä¸Šã‚’ä¿ã¤ã“ã¨ãŒã§ããŸã¨
	- â– ææ¡ˆæ‰‹æ³• 
		- 1. ä¸»æˆåˆ†åˆ†æã‚’ç”¨ã„ã¦é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º 
		- 2. é‡è¦ã§ãªã„æƒ…å ±ã‚’å–ã‚Šé™¤ããŸã‚ã«è¡Œã‚„åˆ—ã‚’å‰Šæ¸› â†’ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã§å‹•ä½œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
	- â– å®Ÿé¨“ã¨çµæœ 
		- 1. OPT, LLAMA-2, Phi-2ã‚’å®Ÿé¨“å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã«è¨­å®š 
		- 2. HuggingFace Transformersã¨PyTorchã§å®Ÿè£… 
		- 3. ã„ãã¤ã‹ã®ã‚¹ãƒ©ã‚¤ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’åˆ†ã‘ã¦å®Ÿé¨“ 
		- 4. æœ€å¤§30%ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ãŒå®Ÿç¾ã—ãŸ 
		- 5. Llama 2ã¨Phi-2ãƒ¢ãƒ‡ãƒ«ã¯90%ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒ
- Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
	- https://arxiv.org/abs/2312.05934
	- Microsoftã‚ˆã‚Šã€ŒFine Tuningã¨RAGã®ã©ã¡ã‚‰ãŒé«˜ç²¾åº¦ã‹ï¼Ÿã€ã«ç­”ãˆãŸè«–æ–‡
	- æ—¢å­˜/æ–°è¦çŸ¥è­˜ã®ä¸¡æ–¹ã«ãŠã„ã¦RAGãŒè‰¯å¥½ãªçµæœã«ã€‚Fine Tuningã¯ç¶™ç¶šäº‹å‰å­¦ç¿’ã€è©•ä¾¡ã¯MMLUã‚’LM-Evaluation-Harnessã§å®Ÿæ–½ã€‚
- The Power of Noise: Redefining Retrieval for RAG System
	- https://arxiv.org/abs/2401.14887
	- LLMã«ãŠã‘ã‚‹RAGï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¾ã›ã‚‹ï¼‰ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã€Œç„¡é–¢ä¿‚ãªã€æ–‡æ›¸ã‚’æ··ãœãŸã»ã†ãŒæ¤œç´¢ç²¾åº¦ãŒä¸ŠãŒã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚
	- â– ãªãœãã‚“ãªã“ã¨ãŒèµ·ã“ã‚‹ã®ã‹ 
		- 1. é–¢é€£æ€§ãŒé«˜ã„æ–‡æ›¸ã°ã‹ã‚Šã ã¨éå‰°é©åˆãŒèµ·ã“ã‚‹ 
		- 2. ç„¡é–¢ä¿‚æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹èƒ½åŠ›ãŒä¸ŠãŒ
- ä¸€æ˜¨æ—¥ãã‚‰ã„ã‹ã‚‰mistralã®æœ‰æ–™ç‰ˆã§ã‚ã‚‹mistral-medium(70Bã€MoEã§ã¯ãªã„)ã®é‡ã¿ãŒãƒªãƒ¼ã‚¯ã—ãŸã¨ã„ã†å™‚ãŒã‚ã‚‹
	- https://x.com/webbigdata/status/1752304557336801408?s=20
-  Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery
	- https://arxiv.org/abs/2309.14425
	- æ¾å°¾ç ”ã®RAã§å­¦éƒ¨2å¹´ç”Ÿã®ç™½å‚ç¿ èŒã•ã‚“ãŒä¸»è‘—ã—ãŸï¼ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ãªãŒã‚‰å¤±æ•—ã«ã‚‚æŸ”è»Ÿã«å¯¾å¿œã™ã‚‹å®¶åº­å†…ã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒœãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹ç ”ç©¶ãŒICRA2024ã«æ¡æŠã•ã‚Œã¾ã—ãŸ
- çªå¦‚ã¨ã—ã¦ç¾ã‚ŒãŸmiqu-70BãŒEQ-Bench ã§ã¯ 83.5 ã‚’ç²å¾—ã— (ãƒ­ãƒ¼ã‚«ãƒ«ã§è©•ä¾¡)ã€GPT-4ç³»ã«æ¬¡ãæ€§èƒ½ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜
	- https://x.com/N8Programs/status/1752441060133892503?s=20
	- ã©ã†ã‚‚ã€Mixtral 8x7Bã®é‡å­åŒ–ç‰ˆã®ãƒªãƒ¼ã‚¯ã ã£ãŸã‚‰ã—ã„
-  LangGraphã§å§‹ã‚ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
	- https://speakerdeck.com/peisuke/langgraphdeshi-merumarutiezientosisutemu
	- Function Callingã ã‘ã§å‰²ã¨ã‚ˆãå‹•ã„ã¦ã‚‹ã¨ã“ã‚ã‚‹ã‚“ã ã‘ã©ã€ã‚‚ã†å°‘ã—çµ±åˆã—ãŸãã¦SupervisorãŒå¿…è¦ãã†ãªãƒ•ãƒ­ãƒ¼ã‹ã‚‰è©¦ã—ã¦ã¿ã‚ˆã†ã‹ãª
- Mixtral8x7Bã®æ—¥æœ¬èªå¯¾å¿œLoraã®å­¦ç¿’å®Œäº†ã—ã¾ã—ãŸ
	- https://x.com/AiXsatoshi/status/1752509354849546417?s=20
	- æ¨™æº–ã®Mixtral8x7Bã§ã¯ã€å¿œç­”ã«å¤šè¨€èªé–“ã‚’è¡Œãæ¥ã™ã‚‹switchingãŒç™ºç”Ÿã—ã¾ã™ãŒã€æ”¹å–„ã—ã¦ã„ã¾ã™
	- æ±ç”¨æ€§èƒ½ãŒè½ã¡ã¦ã„ã‚‹å¯èƒ½æ€§ã‚ã‚‹ã®ã§ã€ã‚‚ã†å°‘ã—æ¤œè¨¼ã—ã¾ã™
- å­¦ç¿’æ¸ˆã¿ã® LLM ã‚’æŸã­ã¦ Mixture of Experts ã‚’ä½œã‚‹ãƒ†ã‚¯
	- https://zenn.dev/zaburo_ch/articles/88e35e5c80f974
	- Phixtralã®è©±ã®ç´¹ä»‹
	- ã€ŒPhi-2 ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã„ãã¤ã‹ä½¿ã£ã¦ Mixture of Experts (MoE) ã‚’ä½œã£ãŸã‚‰å˜ä½“ã‚ˆã‚Šã‚‚è‰¯ã„æ€§èƒ½ãŒé”æˆã§ãã¾ã—ãŸã€
	- **Few-shot ã§ Gating ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±ºã‚ã‚‹æ‰‹æ³•**ãŒä½¿ã‚ã‚Œã¦ã„ã¦é¢ç™½ã‹ã£ãŸ
	- Gating ã®è©±ã‚’å¿˜ã‚Œã‚Œã°ã€Œãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºã‚ã¦ MLP ä»¥å¤–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å…¨éƒ¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚‚ã®ã‚’ã€MLP ã¯ MoE Layer ã«ç½®ãæ›ãˆã¦å„ãƒ¢ãƒ‡ãƒ«ã® MLP ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã†ã€ã¨ã„ã†æ–¹æ³•ã§ MoE ãƒ¢ãƒ‡ãƒ«ãŒä½œã‚Œãã†ã§ã™
	- å„ Expert ã«ã¤ã„ã¦ã€ãã® Expert ã‚’ä½¿ã†ã¨æœ‰åˆ©ã«ãªã‚Šãã†ãª Prompt (ä¾‹ãˆã° Code ã§ Fine-Tuning ã•ã‚ŒãŸ Expert ãªã‚‰ Code ã® Prompt) ã‚’ã„ãã¤ã‹ç”¨æ„ã—ã¦ã€ãã® Prompt ã‚’ forward ã—ãŸã¨ãã® hidden_state ã‚’ä½¿ã£ã¦ weâ€‹ ã‚’ä½œã‚ã†
	- Domain ã”ã¨ã« Expert ã‚’ä½¿ã„åˆ†ã‘ã¦ãã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹æ„Ÿã˜ã§ã™ã­
- CodeLlama-70Bã‚’PostgreSQLã®ç”Ÿæˆã«ç‰¹åŒ–ã•ã›ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€SQLCoder-70B
	- https://huggingface.co/defog/sqlcoder-70b-alpha
	- æ€§èƒ½è©•ä¾¡ã‚‚GPT-4ã«10ãƒã‚¤ãƒ³ãƒˆä»¥ä¸Šå·®ã‚’ã¤ã‘ã‚‹åœ§å€’çš„ãªå‹åˆ©ã§ã€ç‰¹åŒ–å‹ã®ã‚³ãƒ¼ãƒ‰ç”ŸæˆLLMã®å°é ­ã‚’äºˆæ„Ÿã•ã›ã‚‹ã‚ˆã†ãªãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’ç§˜ã‚ã¦ã„ã‚‹
- LLaVA-1.6ã®ãƒªãƒªãƒ¼ã‚¹ã€Gemini Proè¶Šãˆï¼Ÿ
	- https://x.com/imhaotian/status/1752621754273472927?s=20
	- https://llava-vl.github.io/blog/2024-01-30-llava-1-6/
	- improved reasoning, OCR, and world knowledge. It supports higher-res inputs, more tasks, and exceeds Gemini Pro on several benchmarks!
	- LLaVA-1.6ã€æ™®é€šã«ç”»åƒä¸­ã®å¹ãå‡ºã—ã‚’æ—¥æœ¬èªã§å–‹ã£ã¦ã„ã‚‹ã¨ã‹èªè­˜ã§ãã¦ã€Gemini Proè¶…ãˆã¯ä¼Šé”ã§ã¯ãªã„ãªã¨ãªã‚‹
- 700å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼LLMã€ŒKARAKURI LMã€ã‚’ä¸€èˆ¬å…¬é–‹
	- https://karakuri.ai/seminar/news/karakuri-lm/
	- GPT-4ã‚’è©•ä¾¡è€…ã¨ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(MT-Bench-jp)ã§ã€å›½ç”£LLMã¨ã—ã¦ã¯1ä½ã®æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸ
	- https://lm.karakuri.cc/ ã§ãŠè©¦ã—
- è«–æ–‡ã€ŒRAG VS Fine-tuningã€ã‚’èª­ã‚€
	- https://zenn.dev/neoai/articles/e75b6f033a4fd9
- æ™®é€šã®äººãŒè³‡ç”£é‹ç”¨ã§99ç‚¹ã‚’å–ã‚‹æ–¹æ³•
	- https://hayatoito.github.io/2020/investing/
		- 1.  ç¢ºå®šæ‹ å‡ºå¹´é‡‘ (iDeCo ã¾ãŸã¯ ä¼æ¥­å‹ DCï¼‰ã‚’å§‹ã‚ã¾ã™ã€‚
		- 2.  æ–° NISA ã§ã¤ã¿ãŸã¦ã®è¨­å®šã‚’ã—ã¾ã™ã€‚
		- 3.  ã•ã‚‰ã«ä½™è£•ãŒã‚ã‚‹æ–¹ã¯ã€ç‰¹å®šå£åº§ã§ã¤ã¿ãŸã¦ã®è¨­å®šã‚’ã—ã¾ã™ã€‚
		- 4.  è³‡ç”£é‹ç”¨ã‚’å§‹ã‚ãŸç›´å¾Œã‚„ã€ã¾ã¨ã¾ã£ãŸè³‡é‡‘ã‚’ä¸€æ™‚çš„ã«å…¥æ‰‹ã—ãŸã¨ããªã©ã€ååˆ†ãªä½™å‰°è³‡é‡‘ï¼ˆç¾é‡‘ï¼‰ã‚’ã‚‚ã£ã¦ã„ã‚‹ã®ã§ã‚ã‚Œã°ã€è‡ªåˆ†ã®ãƒªã‚¹ã‚¯è¨±å®¹åº¦ã®ç¯„å›²å†…ã§ã€é©åˆ‡ãªå‰²åˆã®è³‡ç”£ã‚’  _ä¸€æ‹¬_  ã§æŠ•è³‡ã—ã¾ã™ã€‚è©³ã—ãã¯å¾Œè¿°ã®ã€Œã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
		- 5.  å®šæœŸçš„ã«ï¼ˆå¹´ã« 1 å›ã€ã‚ã‚‹ã„ã¯æ•°å¹´ã« 1 å›ï¼‰ã€ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦è¦‹ç›´ã—ã¾ã—ã‚‡ã†ã€‚
-  Self-supervised Learning: Generative or Contrastive
	- https://arxiv.org/abs/2006.08218
- Proactive Detection of Voice Cloning with Localized Watermarking
	- https://huggingface.co/papers/2401.17264
	- Meta presents Proactive Detection of Voice Cloning with Localized Watermarking
- ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã‚µã‚¤ãƒˆãªã©ã‹ã‚‰ä¸­å¤ã®RTX 3090ã‚’8å°ã‹ãé›†ã‚ã¦ãƒã‚·ãƒ³ã‚’æ§‹ç¯‰ã—ãŸäººã®ãŠè©±
	- https://www.kyleboddy.com/2024/01/28/building-deep-learning-machines-unorthodox-gpus/
- Google's AI Makes Stunning Progress with Logical Reasoning
	- https://www.youtube.com/watch?v=NrNjvIrCqII
- Microsoft and OpenAI are in talks to invest $100 million into Figure
	- https://x.com/AndrewCurran_/status/1752463084550262805?s=20
	- Figureã¯ã€äººå‹ãƒ­ãƒœãƒƒãƒˆã‚’é–‹ç™ºã™ã‚‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—
-  ReGAL: Refactoring Programs to Discover Generalizable Abstractions
	- https://huggingface.co/papers/2401.16467
- miqudev/miqu-1-70b
	- https://huggingface.co/miqudev/miqu-1-70b
	- ãˆã£ï¼ã€miquã£ã¦ãƒŸã‚¯ã®ã“ã¨ã ã£ãŸã®ã‹ã€‚
- H2O-Danube-1.8B Technical Report
	- https://arxiv.org/abs/2401.16818
	- Open-sources a high-competitive 1.8B LM trained on 1T tokens following the core principles of LLama 2 and Mistral
	- long context small LLM trained by a team of some of the best Kagglers in the world
	- ã©ã†ã‚‚å°è¦æ¨¡LLMã§Kagglerã«ã‚ˆã‚Štrainigã•ã‚ŒãŸã‚‚ï½
-  Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
	- https://arxiv.org/abs/2401.17263
	- Significantly improves robustness to held-out jailbreaks, reducing the attack success rate from 84% to 8.66% across 20 jailbreaks
- quantized CodeLlama 70b base model to 4-bit with MLX
	- https://huggingface.co/mlx-community/CodeLlama-70b-hf-4bit-MLX
	- you can now run this model on your Apple Silicon.
- StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis
	- https://arxiv.org/abs/2401.17093
- Memphis-CoT 3B
	- https://huggingface.co/euclaise/Memphis-CoT-3B
	- A small reasoning-focused model using a novel iterative contrastive finetuning procedure, trained on only human data, outperforming much larger human data models and similarly sized SFT models.
-  RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank
	- ICLR24 Spotlight: To train general-purpose SSL models, it's important to measure the quality of representations during training. But how can we do this w/o downstream labels? 
	- We propose a new label-free metric to eval SSL models, called Linear Discrimination Analysis Rank(LiDAR)
-  [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717v1)
	- è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã€Œæ¨¡å€£ã€ã¯æœ‰ç”¨ã‹ï¼Ÿ
	- https://ai-scholar.tech/articles/chatgpt/Imitating-Proprietary-LLMs
	- æœ€æ–°ç ”ç©¶ã«ã‚ˆã‚Œã°ã€æ–°ã—ãé–‹ç™ºã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨¡å€£ã¯éå¸¸ã«é›£ã—ã„ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚å¾®èª¿æ•´ã«ã‚ˆã‚‹æ”¹å–„ãŒæœ‰åŠ¹ã§ãªãã€ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªçŸ¥è­˜ã¯ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„ã“ã¨ãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸã€‚  
	- ä¸­å°ä¼æ¥­ã‚„å¤§ä¼æ¥­ãŒåŒã˜åˆ©ç‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒé›£ã—ããªã‚Šã€ç‰¹ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ´»ã‹ã—ã¦èƒ½åŠ›å·®ã‚’ç”Ÿã‹ã™ä¼æ¥­ãŒç«¶äº‰ä¸Šã®å„ªä½æ€§ã‚’ç¯‰ã‘ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
	- æ–°ã—ã„æ‰‹æ³•ã‚„ãƒ‡ãƒ¼ã‚¿ã®å°å…¥ãŒé‡è¦ã§ã‚ã‚Šã€æŠ€è¡“çš„ãªåˆ¶ç´„ã«ã‚‚ç•™æ„ã™ã‚‹ã“ã¨ãŒæŒç¶šçš„ãªç™ºå±•ã«å¯„ä¸ã™ã‚‹ã§ã—ã‚‡ã†ã€‚
- Accelerating the Science of Language Models
	- https://allenai.org/olmo/olmo-paper.pdf
	- AllenAIã«ã‚ˆã‚‹Open Language Model (OLMo), a 7B parameter model.
	- There is also a smaller version of it, OLMo 1B.
- ãƒ–ãƒ©ã‚¦ã‚¶ã§Rubyã‚’å‹•ã‹ã™å¤¢
	- https://mametter.hatenablog.com/entry/2024/02/01/105413
	- å…ƒåŒåƒšã®é è—¤ã•ã‚“ã€é ‘å¼µã£ã¦ã‚‹ãªã€ã¿ã‚“ãªä½¿ã£ã¦ã‚ã’ã¦ï¼
- SEMSCORE: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity
	- https://arxiv.org/pdf/2401.17072.pdf
	- ã“ã‚Œã§Japanese MT-benchã‚„Elyza-tasksãŒæ¯å›GPT-4ã‚’ä½¿ã‚ãšã«è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ãªã‚Œã°å‰²ã¨å®‰ä¾¡ã§æ—¥æœ¬èªLLM leaderboardãŒä½œã‚Œãã†
- llamaindexã‚’ä½¿ã£ãŸã€ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªå¤‰æ›ã®è§£èª¬è¨˜äº‹
	- https://akash-mathur.medium.com/advanced-rag-query-augmentation-for-next-level-search-using-llamaindex-d362fed7ecc3
	-  Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex
	- ã‚¯ã‚¨ãƒªå¤‰æ›ã¯ã€ŒLLM ã¸ã®å…¥åŠ›ï¼ˆã‚¯ã‚¨ãƒªï¼‰ã‚’ã‚ˆã‚Šè‰¯ã„æƒ…å ±æŠ½å‡ºã‚’å¯èƒ½ã¨ã™ã‚‹è¡¨ç¾ã¸å¤‰æ›ã™ã‚‹ã€ã“ã¨ã§ï¼ŒRAG ã®è³ªã‚’é«˜ã‚ã‚‹æ‰‹æ³•
	- è¨˜äº‹å†…ã§ã¯ï¼Œä»£è¡¨çš„ãª 5 ã¤ã®æ‰‹æ³•ã‚’ code ã¤ãã§è§£èª¬
- Build Long-context RAG from scratch: Nomic Embeddings + Mistral
	- https://x.com/LangChainAI/status/1753149741599428926?s=20
	- nomic_ai has launched a new open source, long context embedding model:
		- 8k token context window (using RoPE)
		- Strong performance on several benchmarks 
		- API (and local support coming soon)
	- ãã—ã¦long contexã®RAGã‚’ã¤ãã‚‹ã«ã¯ã€
		- nomic_ai:new 8k context window embeddings
		- trychroma:vectorstoreã€€MistralAI-instruct 32k context window via  ollama
- Apple presents Can Large Language Models Understand Context
	- https://huggingface.co/papers/2402.0085
	- We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.
-  Grounded language acquisition through the eyes and ears of a single child
	- 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMTMzMTc5OTYsMTY2MDQwNjg2NSw0MT
Q4ODAxMTMsLTc4NzgwOTU3OSwtMTY3NzI5MDMwMSwtMTE4MDE4
MjkzNSwxMDM0MzIwMjUzLC0xMDY1NzY2MDE5LC0zMjYxNDYzMT
csLTE2ODU4NDQ2ODcsLTE2MDQ4NTg1NDQsNjc4NTA3MTI5LC00
NDEwMzg4MjIsNjk1Mzc1MTM2LDU4NzI2MDQ4MywtMTgwNTQ4Nz
UyNSwxMzI4MTUzMzI3LC0xMTI4MDA0MjExLC0xNDg3NzUwOTc0
LDE5NDk0MzYxNDNdfQ==
-->